>>> 'Pkg.add("Optim")' log
INFO: Installing Calculus v0.1.15
INFO: Installing ForwardDiff v0.2.5
INFO: Installing NaNMath v0.2.1
INFO: Installing Optim v0.6.1
INFO: Installing PositiveFactorizations v0.0.2
INFO: Package database updated
INFO: METADATA is out-of-date â€” you may not have the latest version of Optim
INFO: Use `Pkg.update()` to get the latest versions of your packages

>>> 'Pkg.test("Optim")' log
Julia Version 0.5.0
Commit 3c9d753 (2016-09-19 18:14 UTC)
Platform Info:
  System: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-100-generic #147-Ubuntu SMP Tue Oct 18 16:48:51 UTC 2016 x86_64 x86_64
Memory: 2.939289093017578 GB (1209.00390625 MB free)
Uptime: 8939.0 sec
Load Avg:  1.0048828125  1.0146484375  1.0419921875
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3499 MHz     516248 s         64 s      53998 s     218539 s          3 s
#2  3499 MHz     116509 s         98 s      23176 s     739994 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.5
2 required packages:
 - JSON                          0.8.0
 - Optim                         0.6.1
5 additional packages:
 - Calculus                      0.1.15
 - Compat                        0.9.3
 - ForwardDiff                   0.2.5
 - NaNMath                       0.2.1
 - PositiveFactorizations        0.0.2
INFO: Testing Optim
Running tests:
 * types.jl
 * bfgs.jl
 * gradient_descent.jl
 * momentum_gradient_descent.jl
 * grid_search.jl
 * l_bfgs.jl
 * levenberg_marquardt.jl
WARNING: Problem solving for delta_x: predicted residual increase.
18.95337979630715 (predicted_residual) >
3.6729087897253905 (residual) + 3.552713678800501e-15 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.83860477635018 (predicted_residual) >
2.9812894464055533 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
2.5347148551516008 (predicted_residual) >
2.1895906529240503 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
2.4662071702097417 (predicted_residual) >
2.0497617658979004 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
2.402223943602616 (predicted_residual) >
1.9102621894841052 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
1.9548243717543228 (predicted_residual) >
1.8272637677633554 (residual) + 2.220446049250313e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.289097004965162 (predicted_residual) >
1.7676814916153436 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
0.5282850316144354 (predicted_residual) >
0.43569220875204495 (residual) + 1.1102230246251565e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
0.38479244475307006 (predicted_residual) >
0.3191560404065722 (residual) + 5.551115123125783e-17 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
0.0867292689898465 (predicted_residual) >
0.07825577278289107 (residual) + 1.3877787807814457e-17 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
0.05060004314422784 (predicted_residual) >
0.04864792336713843 (residual) + 6.938893903907228e-18 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.3671457655620114 (predicted_residual) >
3.3582265009021715 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.368047096608802 (predicted_residual) >
3.36714576556202 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.368137102022189 (predicted_residual) >
3.368047096608803 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.3681461013210745 (predicted_residual) >
3.368137102022198 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.368147001174647 (predicted_residual) >
3.3681461013210723 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.6257200160581533 (predicted_residual) >
3.621898322475188 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.63485635714771 (predicted_residual) >
3.6257200160594407 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.635770923793392 (predicted_residual) >
3.634856357147663 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.6358622472182742 (predicted_residual) >
3.6357709237933946 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.635871378113383 (predicted_residual) >
3.6358622472182813 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.635872291436749 (predicted_residual) >
3.6358713781133725 (residual) + 4.440892098500626e-16 (eps)
     0     4.093196e+03              NaN
 * lambda: 10.0

     1     2.798224e+03     5.199044e+02
 * g(x): 519.9043679445367
 * lambda: 1.0
 * dx: [-1.37537,-2.26849,-0.516919]

     2     4.992133e+02     2.193666e+02
 * g(x): 219.36656868648777
 * lambda: 0.1
 * dx: [-3.33453,-5.79199,-2.0025]

     3     9.266003e+00     2.104323e+01
 * g(x): 21.043228273795627
 * lambda: 0.010000000000000002
 * dx: [-0.157349,1.53033,-2.12792]

     4     9.815963e-01     3.012647e-01
 * g(x): 0.3012647255075523
 * lambda: 0.0010000000000000002
 * dx: [-0.0853385,1.42208,-0.344097]

     5     9.379170e-01     7.194056e-03
 * g(x): 0.007194056052137798
 * lambda: 0.00010000000000000003
 * dx: [0.0577301,0.095152,-0.0171907]

     6     9.379163e-01     7.827369e-07
 * g(x): 7.827368690149772e-7
 * lambda: 1.0000000000000004e-5
 * dx: [0.000258738,-0.000143761,-8.19913e-5]

     7     9.379163e-01     7.416512e-12
 * g(x): 7.41651184910097e-12
 * lambda: 1.0000000000000004e-6
 * dx: [9.76997e-8,-9.59622e-8,-7.41904e-9]

 * newton.jl
 * newton_trust_region.jl
 * cg.jl
 * nelder_mead.jl
WARNING: NelderMead(a::Real,g::Real,b::Real) is deprecated, use NelderMead(initial_simplex=AffineSimplexer(),parameters=FixedParameters(a,g,b,0.5)) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in Optim.NelderMead{Ts<:Optim.Simplexer,Tp<:Optim.NMParameters}(::Float64, ::Float64, ::Float64) at ./deprecated.jl:50
 in #NelderMead#69(::Array{Any,1}, ::Type{T}) at /home/vagrant/.julia/v0.5/Optim/src/nelder_mead.jl:54
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{Optim.NelderMead}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:488
 in macro expansion; at /home/vagrant/.julia/v0.5/Optim/test/runtests.jl:40 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:488
 in process_options(::Base.JLOptions) at ./client.jl:262
 in _start() at ./client.jl:318
while loading /home/vagrant/.julia/v0.5/Optim/test/nelder_mead.jl, in expression starting on line 19
WARNING: NelderMead(a::Real,g::Real,b::Real) is deprecated, use NelderMead(initial_simplex=AffineSimplexer(),parameters=FixedParameters(a,g,b,0.5)) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in Optim.NelderMead{Ts<:Optim.Simplexer,Tp<:Optim.NMParameters}(::Float64, ::Float64, ::Float64) at ./deprecated.jl:50
 in #NelderMead#69(::Array{Any,1}, ::Type{T}) at /home/vagrant/.julia/v0.5/Optim/src/nelder_mead.jl:54
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{Optim.NelderMead}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:488
 in macro expansion; at /home/vagrant/.julia/v0.5/Optim/test/runtests.jl:40 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:488
 in process_options(::Base.JLOptions) at ./client.jl:262
 in _start() at ./client.jl:318
while loading /home/vagrant/.julia/v0.5/Optim/test/nelder_mead.jl, in expression starting on line 20
WARNING: NelderMead(a::Real,g::Real,b::Real) is deprecated, use NelderMead(initial_simplex=AffineSimplexer(),parameters=FixedParameters(a,g,b,0.5)) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in Optim.NelderMead{Ts<:Optim.Simplexer,Tp<:Optim.NMParameters}(::Float64, ::Float64, ::Float64) at ./deprecated.jl:50
 in #NelderMead#69(::Array{Any,1}, ::Type{T}) at /home/vagrant/.julia/v0.5/Optim/src/nelder_mead.jl:54
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{Optim.NelderMead}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:488
 in macro expansion; at /home/vagrant/.julia/v0.5/Optim/test/runtests.jl:40 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:488
 in process_options(::Base.JLOptions) at ./client.jl:262
 in _start() at ./client.jl:318
while loading /home/vagrant/.julia/v0.5/Optim/test/nelder_mead.jl, in expression starting on line 21
 * optimize.jl
 * simulated_annealing.jl
 * particle_swarm.jl
 * interpolating_line_search.jl
 * api.jl
Iter     Function value   Gradient norm 
     0     1.000000e+00     2.000000e+00
 * g(x): [-2.0,0.0]
 * ~inv(H): [1.0 0.0; 0.0 1.0]
 * x: [0.0,0.0]
     1     8.415971e-01     1.578600e+00
 * g(x): [-1.5786,-1.46329]
 * ~inv(H): [12.2608 3.47244; 3.47244 1.0]
 * x: [0.0855362,0.0]
     2     6.928766e-01     1.076526e+01
 * g(x): [6.59004,-10.7653]
 * ~inv(H): [0.0975543 0.0556186; 0.0556186 0.0403018]
 * x: [0.365059,0.0794421]
     3     4.336347e-01     1.082559e+00
 * g(x): [-1.08256,-0.342414]
 * ~inv(H): [0.086429 0.0613833; 0.0613833 0.0486031]
 * x: [0.341713,0.115056]
     4     3.268654e-01     1.203708e+00
 * g(x): [-0.0983968,-1.20371]
 * ~inv(H): [0.278428 0.213952; 0.213952 0.168913]
 * x: [0.431455,0.180135]
     5     2.989460e-01     1.820631e+00
 * g(x): [0.599899,-1.82063]
 * ~inv(H): [0.177273 0.152973; 0.152973 0.135601]
 * x: [0.460872,0.2033]
     6     2.380606e-01     3.824597e+00
 * g(x): [3.31788,-3.8246]
 * ~inv(H): [0.117499 0.114329; 0.114329 0.114488]
 * x: [0.551122,0.284612]
     7     1.377832e-01     4.305388e+00
 * g(x): [4.30539,-3.65706]
 * ~inv(H): [0.105591 0.128794; 0.128794 0.168365]
 * x: [0.676971,0.440004]
     8     9.932907e-02     1.240226e+00
 * g(x): [-1.24023,0.445925]
 * ~inv(H): [0.120479 0.164949; 0.164949 0.23082]
 * x: [0.685625,0.472311]
     9     6.543213e-02     6.765640e-01
 * g(x): [0.502941,-0.676564]
 * ~inv(H): [0.366262 0.514598; 0.514598 0.726543]
 * x: [0.74645,0.553804]
    10     5.511958e-02     1.419213e+00
 * g(x): [1.41921,-1.21117]
 * ~inv(H): [0.211473 0.312469; 0.312469 0.464599]
 * x: [0.773169,0.591735]
Iter     Function value   Gradient norm 
     0     1.000000e+00     2.000000e+00
 * g(x): [-2.0,0.0]
 * ~inv(H): [1.0 0.0; 0.0 1.0]
 * x: [0.0,0.0]
     1     8.415971e-01     1.578600e+00
 * g(x): [-1.5786,-1.46329]
 * ~inv(H): [12.2608 3.47244; 3.47244 1.0]
 * x: [0.0855362,0.0]
     2     6.928766e-01     1.076526e+01
 * g(x): [6.59004,-10.7653]
 * ~inv(H): [0.0975543 0.0556186; 0.0556186 0.0403018]
 * x: [0.365059,0.0794421]
     3     4.336347e-01     1.082559e+00
 * g(x): [-1.08256,-0.342414]
 * ~inv(H): [0.086429 0.0613833; 0.0613833 0.0486031]
 * x: [0.341713,0.115056]
     4     3.268654e-01     1.203708e+00
 * g(x): [-0.0983968,-1.20371]
 * ~inv(H): [0.278428 0.213952; 0.213952 0.168913]
 * x: [0.431455,0.180135]
     5     2.989460e-01     1.820631e+00
 * g(x): [0.599899,-1.82063]
 * ~inv(H): [0.177273 0.152973; 0.152973 0.135601]
 * x: [0.460872,0.2033]
     6     2.380606e-01     3.824597e+00
 * g(x): [3.31788,-3.8246]
 * ~inv(H): [0.117499 0.114329; 0.114329 0.114488]
 * x: [0.551122,0.284612]
     7     1.377832e-01     4.305388e+00
 * g(x): [4.30539,-3.65706]
 * ~inv(H): [0.105591 0.128794; 0.128794 0.168365]
 * x: [0.676971,0.440004]
     8     9.932907e-02     1.240226e+00
 * g(x): [-1.24023,0.445925]
 * ~inv(H): [0.120479 0.164949; 0.164949 0.23082]
 * x: [0.685625,0.472311]
     9     6.543213e-02     6.765640e-01
 * g(x): [0.502941,-0.676564]
 * ~inv(H): [0.366262 0.514598; 0.514598 0.726543]
 * x: [0.74645,0.553804]
    10     5.511958e-02     1.419213e+00
 * g(x): [1.41921,-1.21117]
 * ~inv(H): [0.211473 0.312469; 0.312469 0.464599]
 * x: [0.773169,0.591735]
    11     4.013082e-02     3.376464e+00
 * g(x): [3.37646,-2.22554]
 * ~inv(H): [0.16596 0.260826; 0.260826 0.41284]
 * x: [0.833422,0.683464]
    12     6.980795e-03     5.135785e-01
 * g(x): [-0.513578,0.189509]
 * ~inv(H): [0.372794 0.635081; 0.635081 1.08853]
 * x: [0.916988,0.841814]
    13     4.822164e-03     1.639696e+00
 * g(x): [1.6397,-0.919736]
 * ~inv(H): [0.21278 0.385121; 0.385121 0.700513]
 * x: [0.947968,0.894044]
    14     1.768810e-03     4.107685e-01
 * g(x): [-0.410769,0.17126]
 * ~inv(H): [0.303376 0.580128; 0.580128 1.11429]
 * x: [0.958824,0.920199]
    15     4.218795e-04     1.186910e-01
 * g(x): [0.118691,-0.0811141]
 * ~inv(H): [0.490374 0.945393; 0.945393 1.82672]
 * x: [0.979865,0.959729]
    16     9.522312e-05     3.374010e-01
 * g(x): [0.337401,-0.1739]
 * ~inv(H): [0.436789 0.860303; 0.860303 1.69847]
 * x: [0.995571,0.990291]
    17     1.824025e-06     2.888770e-02
 * g(x): [-0.0288877,0.0132835]
 * ~inv(H): [0.494558 0.985152; 0.985152 1.96745]
 * x: [0.998824,0.997716]
    18     3.847551e-09     1.733584e-03
 * g(x): [0.00173358,-0.00090904]
 * ~inv(H): [0.495163 0.988462; 0.988462 1.978]
 * x: [0.999958,0.999911]
    19     6.629175e-13     2.322282e-05
 * g(x): [2.32228e-5,-1.10116e-5]
 * ~inv(H): [0.503249 1.00615; 1.00615 2.01662]
 * x: [1.0,1.0]
    20     1.745741e-19     1.175798e-08
 * g(x): [-1.1758e-8,6.16125e-9]
 * ~inv(H): [0.499855 0.999694; 0.999694 2.00436]
 * x: [1.0,1.0]
    21     5.374115e-30     9.015011e-14
 * g(x): [9.01501e-14,-4.44089e-14]
 * ~inv(H): [0.499998 0.999996; 0.999996 2.00499]
 * x: [1.0,1.0]
Iter     Function value    âˆš(Î£(yáµ¢-yÌ„)Â²)/n 
------   --------------    --------------
     0     9.506641e-01     4.576214e-02
 * step_type: initial
 * centroid: [0.0125,0.0]
     1     9.506641e-01     2.023096e-02
 * step_type: outside contraction
 * centroid: [0.0125,0.0]
     2     9.506641e-01     2.172172e-02
 * step_type: expansion
 * centroid: [0.021875,-0.00625]
     3     9.262175e-01     5.243757e-02
 * step_type: expansion
 * centroid: [0.0453125,-0.009375]
     4     8.292372e-01     4.259749e-02
 * step_type: reflection
 * centroid: [0.0820313,-0.0109375]
     5     8.292372e-01     4.265507e-02
 * step_type: reflection
 * centroid: [0.11875,-0.0125]
     6     8.138907e-01     3.109209e-02
 * step_type: reflection
 * centroid: [0.135156,-0.0046875]
     7     7.569606e-01     3.215435e-02
 * step_type: reflection
 * centroid: [0.151562,0.003125]
     8     7.382898e-01     2.418419e-02
 * step_type: reflection
 * centroid: [0.167969,0.0109375]
     9     6.989376e-01     2.426367e-02
 * step_type: reflection
 * centroid: [0.184375,0.01875]
    10     6.800415e-01     2.124416e-02
 * step_type: reflection
 * centroid: [0.200781,0.0265625]
    11     6.475000e-01     1.809652e-02
 * step_type: reflection
 * centroid: [0.217187,0.034375]
    12     6.377042e-01     2.151280e-02
 * step_type: reflection
 * centroid: [0.233594,0.0421875]
    13     5.977620e-01     1.648361e-02
 * step_type: reflection
 * centroid: [0.25,0.05]
    14     5.977620e-01     2.780850e-02
 * step_type: reflection
 * centroid: [0.266406,0.0578125]
    15     5.476196e-01     2.429423e-02
 * step_type: inside contraction
 * centroid: [0.246094,0.0671875]
    16     5.449374e-01     1.754792e-02
 * step_type: reflection
 * centroid: [0.268555,0.0683594]
    17     5.091263e-01     1.465262e-02
 * step_type: inside contraction
 * centroid: [0.291016,0.0695313]
    18     5.091263e-01     2.928711e-02
 * step_type: expansion
 * centroid: [0.29209,0.0748047]
    19     4.564435e-01     2.393325e-02
 * step_type: reflection
 * centroid: [0.317236,0.0891602]
    20     4.564435e-01     4.392438e-02
 * step_type: expansion
 * centroid: [0.342383,0.103516]
    21     3.653678e-01     3.913535e-02
 * step_type: reflection
 * centroid: [0.369678,0.128418]
    22     3.653678e-01     3.661549e-02
 * step_type: reflection
 * centroid: [0.396973,0.15332]
    23     2.993956e-01     2.794297e-02
 * step_type: inside contraction
 * centroid: [0.4396,0.180762]
    24     2.993956e-01     3.615204e-02
 * step_type: expansion
 * centroid: [0.438757,0.185718]
    25     2.597777e-01     2.976324e-02
 * step_type: reflection
 * centroid: [0.479279,0.225549]
    26     2.265853e-01     4.845727e-02
 * step_type: expansion
 * centroid: [0.5198,0.265381]
    27     1.444892e-01     3.581898e-02
 * step_type: inside contraction
 * centroid: [0.586774,0.335071]
    28     1.444892e-01     2.978437e-02
 * step_type: reflection
 * centroid: [0.582414,0.337534]
    29     1.444892e-01     2.452313e-02
 * step_type: reflection
 * centroid: [0.621251,0.387277]
    30     9.866233e-02     1.906929e-02
 * step_type: inside contraction
 * centroid: [0.664447,0.434558]
    31     9.866233e-02     2.637123e-02
 * step_type: expansion
 * centroid: [0.671976,0.448225]
    32     6.483338e-02     1.579108e-02
 * step_type: inside contraction
 * centroid: [0.733996,0.529675]
    33     6.483338e-02     1.844031e-02
 * step_type: expansion
 * centroid: [0.726021,0.52298]
    34     5.484730e-02     6.612510e-03
 * step_type: reflection
 * centroid: [0.768102,0.587691]
    35     4.879623e-02     1.884123e-02
 * step_type: expansion
 * centroid: [0.810183,0.652403]
    36     1.219851e-02     1.494519e-02
 * step_type: inside contraction
 * centroid: [0.875001,0.755792]
    37     1.219851e-02     7.778578e-03
 * step_type: outside contraction
 * centroid: [0.862189,0.742589]
    38     1.219851e-02     6.490233e-03
 * step_type: reflection
 * centroid: [0.88499,0.784381]
    39     5.976421e-03     2.702671e-03
 * step_type: inside contraction
 * centroid: [0.923931,0.848906]
    40     5.976421e-03     3.461744e-03
 * step_type: expansion
 * centroid: [0.921561,0.847988]
    41     2.624191e-03     1.520029e-03
 * step_type: inside contraction
 * centroid: [0.954576,0.910217]
    42     2.624191e-03     1.337027e-03
 * step_type: inside contraction
 * centroid: [0.943952,0.893741]
    43     2.624191e-03     1.226069e-03
 * step_type: reflection
 * centroid: [0.953881,0.910977]
    44     3.582590e-04     9.655910e-04
 * step_type: reflection
 * centroid: [0.97235,0.946971]
    45     3.582590e-04     3.537509e-04
 * step_type: outside contraction
 * centroid: [0.990819,0.982965]
    46     4.826749e-05     1.270919e-04
 * step_type: inside contraction
 * centroid: [0.993649,0.986894]
    47     4.826749e-05     5.661519e-05
 * step_type: inside contraction
 * centroid: [1.00076,1.00194]
    48     4.826749e-05     1.499616e-05
 * step_type: inside contraction
 * centroid: [0.998269,0.996328]
    49     3.589907e-05     1.447031e-05
 * step_type: inside contraction
 * centroid: [1.0012,1.00245]
    50     1.331661e-05     1.424732e-05
 * step_type: inside contraction
 * centroid: [0.996874,0.994041]
    51     1.565213e-06     4.917747e-06
 * step_type: outside contraction
 * centroid: [0.998653,0.997287]
    52     1.565213e-06     3.456266e-06
 * step_type: inside contraction
 * centroid: [1.00012,1.00004]
    53     1.565213e-06     9.104041e-07
 * step_type: reflection
 * centroid: [0.999592,0.999106]
    54     1.565213e-06     6.870496e-07
 * step_type: inside contraction
 * centroid: [1.00042,1.00088]
    55     4.966342e-07     5.777837e-07
 * step_type: inside contraction
 * centroid: [1.00014,1.00022]
    56     2.272707e-07     1.307209e-07
 * step_type: inside contraction
 * centroid: [0.999674,0.999361]
    57     2.120249e-07     7.108936e-08
 * step_type: inside contraction
 * centroid: [1.00016,1.00033]
    58     6.942358e-08     8.182068e-08
 * step_type: inside contraction
 * centroid: [1.00003,1.00004]
    59     1.876334e-08     2.129210e-08
 * step_type: inside contraction
 * centroid: [0.999885,0.999773]
    60     1.876334e-08     9.270314e-09
 * step_type: inside contraction
 * centroid: [1.00006,1.00012]
Iter     Function value   Gradient norm 
     0    -1.033256e-01              NaN
 * x_minimum: -0.8541019662496847
 * x_lower: -2.0
 * x_upper: 1.0
     1    -1.033256e-01              NaN
 * x_minimum: -0.8541019662496847
 * x_lower: -2.0
 * x_upper: -0.14589803375031563
     2    -1.033256e-01              NaN
 * x_minimum: -0.8541019662496847
 * x_lower: -1.291796067500631
 * x_upper: -0.14589803375031563
     3    -1.033256e-01              NaN
 * x_minimum: -0.8541019662496847
 * x_lower: -1.291796067500631
 * x_upper: -0.5835921350012621
     4    -1.033256e-01              NaN
 * x_minimum: -0.8541019662496847
 * x_lower: -1.0212862362522084
 * x_upper: -0.5835921350012621
     5    -1.249988e-01              NaN
 * x_minimum: -0.7507764050037857
 * x_lower: -0.8541019662496847
 * x_upper: -0.5835921350012621
     6    -1.249988e-01              NaN
 * x_minimum: -0.7507764050037857
 * x_lower: -0.8541019662496847
 * x_upper: -0.6869176962471611
     7    -1.249988e-01              NaN
 * x_minimum: -0.7507764050037857
 * x_lower: -0.7902432574930602
 * x_upper: -0.6869176962471611
     8    -1.249988e-01              NaN
 * x_minimum: -0.7507764050037857
 * x_lower: -0.7902432574930602
 * x_upper: -0.7263845487364357
     9    -1.249988e-01              NaN
 * x_minimum: -0.7507764050037857
 * x_lower: -0.7658514012257102
 * x_upper: -0.7263845487364357
    10    -1.249988e-01              NaN
 * x_minimum: -0.7507764050037857
 * x_lower: -0.7658514012257102
 * x_upper: -0.7414595449583602
    11    -1.249988e-01              NaN
 * x_minimum: -0.7507764050037857
 * x_lower: -0.7565345411802846
 * x_upper: -0.7414595449583602
    12    -1.249988e-01              NaN
 * x_minimum: -0.7507764050037857
 * x_lower: -0.7565345411802846
 * x_upper: -0.7472176811348591
    13    -1.249988e-01              NaN
 * x_minimum: -0.7507764050037857
 * x_lower: -0.752975817311358
 * x_upper: -0.7472176811348591
    14    -1.249993e-01              NaN
 * x_minimum: -0.7494170934424312
 * x_lower: -0.7507764050037857
 * x_upper: -0.7472176811348591
    15    -1.249993e-01              NaN
 * x_minimum: -0.7494170934424312
 * x_lower: -0.7507764050037857
 * x_upper: -0.7485769926962135
    16    -1.250000e-01              NaN
 * x_minimum: -0.749936304257568
 * x_lower: -0.7507764050037857
 * x_upper: -0.7494170934424312
    17    -1.250000e-01              NaN
 * x_minimum: -0.749936304257568
 * x_lower: -0.750257194188649
 * x_upper: -0.7494170934424312
    18    -1.250000e-01              NaN
 * x_minimum: -0.749936304257568
 * x_lower: -0.750257194188649
 * x_upper: -0.7497379833735123
    19    -1.250000e-01              NaN
 * x_minimum: -0.7500588733045933
 * x_lower: -0.750257194188649
 * x_upper: -0.749936304257568
    20    -1.250000e-01              NaN
 * x_minimum: -0.7500588733045933
 * x_lower: -0.7501346251416237
 * x_upper: -0.749936304257568
    21    -1.250000e-01              NaN
 * x_minimum: -0.7500120560945983
 * x_lower: -0.7500588733045933
 * x_upper: -0.749936304257568
    22    -1.250000e-01              NaN
 * x_minimum: -0.7500120560945983
 * x_lower: -0.7500588733045933
 * x_upper: -0.749983121467563
    23    -1.250000e-01              NaN
 * x_minimum: -0.7500120560945983
 * x_lower: -0.750029938677558
 * x_upper: -0.749983121467563
    24    -1.250000e-01              NaN
 * x_minimum: -0.7500010040505226
 * x_lower: -0.7500120560945983
 * x_upper: -0.749983121467563
    25    -1.250000e-01              NaN
 * x_minimum: -0.7500010040505226
 * x_lower: -0.7500120560945983
 * x_upper: -0.7499941735116387
    26    -1.250000e-01              NaN
 * x_minimum: -0.7500010040505226
 * x_lower: -0.7500052255557144
 * x_upper: -0.7499941735116387
    27    -1.250000e-01              NaN
 * x_minimum: -0.7500010040505226
 * x_lower: -0.7500052255557144
 * x_upper: -0.7499983950168304
    28    -1.250000e-01              NaN
 * x_minimum: -0.7500010040505226
 * x_lower: -0.7500026165220222
 * x_upper: -0.7499983950168304
    29    -1.250000e-01              NaN
 * x_minimum: -0.75000000748833
 * x_lower: -0.7500010040505226
 * x_upper: -0.7499983950168304
    30    -1.250000e-01              NaN
 * x_minimum: -0.75000000748833
 * x_lower: -0.7500010040505226
 * x_upper: -0.7499993915790231
    31    -1.250000e-01              NaN
 * x_minimum: -0.75000000748833
 * x_lower: -0.7500003881412156
 * x_upper: -0.7499993915790231
    32    -1.250000e-01              NaN
 * x_minimum: -0.75000000748833
 * x_lower: -0.7500003881412156
 * x_upper: -0.7499997722319087
    33    -1.250000e-01              NaN
 * x_minimum: -0.75000000748833
 * x_lower: -0.7500001528847944
 * x_upper: -0.7499997722319087
    34    -1.250000e-01              NaN
 * x_minimum: -0.75000000748833
 * x_lower: -0.7500001528847944
 * x_upper: -0.7499999176283731
    35    -1.250000e-01              NaN
 * x_minimum: -0.75000000748833
 * x_lower: -0.7500000630248376
 * x_upper: -0.7499999176283731
    36    -1.250000e-01              NaN
 * x_minimum: -0.75000000748833
 * x_lower: -0.7500000630248376
 * x_upper: -0.7499999731648807
    37    -1.250000e-01              NaN
 * x_minimum: -0.75000000748833
 * x_lower: -0.7500000287013883
 * x_upper: -0.7499999731648807
    38    -1.250000e-01              NaN
 * x_minimum: -0.749999994377939
 * x_lower: -0.75000000748833
 * x_upper: -0.7499999731648807
 * golden_section.jl
 * brent.jl
 * type_stability.jl
 * array.jl
 * constrained.jl
WARNING: could not attach metadata for @simd loop.
 * callbacks.jl
 * precon.jl
Test a basic preconditioning example
N = 10
Optim.GradientDescent{T} WITHOUT preconditioning : g_calls = 890, f_calls = 890
Optim.GradientDescent{T} WITH preconditioning : g_calls = 23, f_calls = 23
Optim.ConjugateGradient{T} WITHOUT preconditioning : g_calls = 49, f_calls = 73
Optim.ConjugateGradient{T} WITH preconditioning : g_calls = 11, f_calls = 16
Optim.LBFGS{T} WITHOUT preconditioning : g_calls = 33, f_calls = 33
Optim.LBFGS{T} WITH preconditioning : g_calls = 25, f_calls = 25
N = 50
Optim.GradientDescent{T} WITHOUT preconditioning : g_calls = 3501, f_calls = 3501
    (gradient descent is not expected to converge)
Optim.GradientDescent{T} WITH preconditioning : g_calls = 12, f_calls = 12
Optim.ConjugateGradient{T} WITHOUT preconditioning : g_calls = 185, f_calls = 276
Optim.ConjugateGradient{T} WITH preconditioning : g_calls = 7, f_calls = 10
Optim.LBFGS{T} WITHOUT preconditioning : g_calls = 386, f_calls = 386
Optim.LBFGS{T} WITH preconditioning : g_calls = 15, f_calls = 15
N = 250
Optim.GradientDescent{T} WITHOUT preconditioning : g_calls = 3502, f_calls = 3502
    (gradient descent is not expected to converge)
Optim.GradientDescent{T} WITH preconditioning : g_calls = 9, f_calls = 9
Optim.ConjugateGradient{T} WITHOUT preconditioning : g_calls = 1371, f_calls = 1993
Optim.ConjugateGradient{T} WITH preconditioning : g_calls = 5, f_calls = 7
Optim.LBFGS{T} WITHOUT preconditioning : g_calls = 1283, f_calls = 1283
Optim.LBFGS{T} WITH preconditioning : g_calls = 12, f_calls = 12
 * initial_convergence.jl
INFO: Optim tests passed

>>> End of log
