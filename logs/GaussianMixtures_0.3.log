>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from git://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing ArrayViews v0.6.2
INFO: Installing BinDeps v0.3.12
INFO: Installing Blosc v0.1.2
INFO: Installing Clustering v0.3.3
INFO: Installing Distances v0.2.0
INFO: Installing Distributions v0.7.3
INFO: Installing GaussianMixtures v0.0.7
INFO: Installing HDF5 v0.4.17
INFO: Installing NumericExtensions v0.6.2
INFO: Installing NumericFuns v0.2.3
INFO: Installing PDMats v0.3.3
INFO: Installing SHA v0.0.4
INFO: Installing StatsBase v0.6.15
INFO: Installing URIParser v0.0.5
INFO: Building Blosc
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.3.10
Commit c8ceeef* (2015-06-24 13:54 UTC)
Platform Info:
  System: Linux (x86_64-unknown-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E5-2650 0 @ 2.00GHz
  WORD_SIZE: 64
  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas
  LIBM: libopenlibm
  LLVM: libLLVM-3.3
INFO: Testing GaussianMixtures
Warning: could not import Base.add! into NumericExtensions
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       2.164666e+03
      1       1.468246e+03      -6.964203e+02 |        7
      2       1.153551e+03      -3.146950e+02 |        5
      3       1.025834e+03      -1.277165e+02 |        6
      4       9.309416e+02      -9.489279e+01 |        4
      5       9.079239e+02      -2.301772e+01 |        2
      6       9.009716e+02      -6.952240e+00 |        0
      7       9.009716e+02       0.000000e+00 |        0
K-means converged with 7 iterations (objv = 900.9716141341928)
 0.000	Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
 1.626	K-means with 272 data points using 7 iterations
      	11.3 data points per parameter
 2.491	EM with 272 data points 0 iterations avll -2.077129
      	5.8 data points per parameter
 2.855	GMM converted to Variational GMM
 4.593	iteration 1, lowerbound -3.768982
 4.597	iteration 2, lowerbound -3.606153
 4.598	iteration 3, lowerbound -3.437050
 4.598	iteration 4, lowerbound -3.260846
 4.599	iteration 5, lowerbound -3.099093
 4.643	dropping number of Gaussions to 7
 4.643	iteration 6, lowerbound -2.960146
 4.644	iteration 7, lowerbound -2.862791
 4.644	dropping number of Gaussions to 6
 4.644	iteration 8, lowerbound -2.816068
 4.644	dropping number of Gaussions to 4
 4.644	iteration 9, lowerbound -2.776650
 4.645	iteration 10, lowerbound -2.757350
 4.645	dropping number of Gaussions to 3
 4.645	iteration 11, lowerbound -2.738505
 4.645	iteration 12, lowerbound -2.715972
 4.645	iteration 13, lowerbound -2.691354
 4.646	iteration 14, lowerbound -2.660663
 4.646	iteration 15, lowerbound -2.624275
 4.646	iteration 16, lowerbound -2.583441
 4.646	iteration 17, lowerbound -2.540250
 4.647	iteration 18, lowerbound -2.497245
 4.647	iteration 19, lowerbound -2.456625
 4.647	iteration 20, lowerbound -2.419444
 4.647	iteration 21, lowerbound -2.385512
 4.647	iteration 22, lowerbound -2.354527
 4.648	iteration 23, lowerbound -2.328193
 4.648	iteration 24, lowerbound -2.311245
 4.648	iteration 25, lowerbound -2.307851
 4.648	dropping number of Gaussions to 2
 4.648	iteration 26, lowerbound -2.302917
 4.648	iteration 27, lowerbound -2.299259
 4.648	iteration 28, lowerbound -2.299256
 4.648	iteration 29, lowerbound -2.299254
 4.649	iteration 30, lowerbound -2.299254
 4.649	iteration 31, lowerbound -2.299253
 4.649	iteration 32, lowerbound -2.299253
 4.649	iteration 33, lowerbound -2.299253
 4.649	iteration 34, lowerbound -2.299253
 4.649	iteration 35, lowerbound -2.299253
 4.650	iteration 36, lowerbound -2.299253
 4.650	iteration 37, lowerbound -2.299253
 4.650	iteration 38, lowerbound -2.299253
 4.662	39 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
nothing
α = [178.04509266753166,95.95490733246837]
β = [178.04509266753166,95.95490733246837]
m = [4.250300729688867 79.28686689169722
 2.0002292540667628 53.851987153147284]
ν = [180.04509266753166,97.95490733246837]
W = [
[0.18404155498101338 -0.007644049089368011
 0.0 0.008581705100142585],

[0.37587636735980867 -0.00895312390053899
 0.0 0.012748664796167687]]
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 99999.99999999993
avll from stats: -0.9975974682077455
ERROR: MemoryError()
 in - at array.jl:723
 in llpg at /home/vagrant/.julia/v0.3/GaussianMixtures/src/train.jl:262
 in anonymous at no file:15
 in include at ./boot.jl:245
 in include_from_node1 at ./loading.jl:128
 in include at ./boot.jl:245
 in include_from_node1 at loading.jl:128
 in process_options at ./client.jl:285
 in _start at ./client.jl:354
while loading /home/vagrant/.julia/v0.3/GaussianMixtures/test/test.jl, in expression starting on line 4
while loading /home/vagrant/.julia/v0.3/GaussianMixtures/test/runtests.jl, in expression starting on line 5

==========================[ ERROR: GaussianMixtures ]===========================

failed process: Process(`/home/vagrant/julia/bin/julia /home/vagrant/.julia/v0.3/GaussianMixtures/test/runtests.jl`, ProcessExited(1)) [1]

================================================================================
INFO: No packages to install, update or remove
ERROR: GaussianMixtures had test errors
 in error at error.jl:21
 in test at pkg/entry.jl:718
 in anonymous at pkg/dir.jl:28
 in cd at ./file.jl:20
 in cd at pkg/dir.jl:28
 in test at pkg.jl:67
 in process_options at ./client.jl:213
 in _start at ./client.jl:354


>>> End of log
