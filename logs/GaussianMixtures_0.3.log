>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from git://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing ArrayViews v0.6.4
INFO: Installing Blosc v0.1.5
INFO: Installing Clustering v0.5.0
INFO: Installing Distances v0.2.2
INFO: Installing Distributions v0.8.10
INFO: Installing Docile v0.5.23
INFO: Installing FileIO v0.0.6
INFO: Installing GaussianMixtures v0.0.12
INFO: Installing HDF5 v0.6.2
INFO: Installing JLD v0.5.9
INFO: Installing PDMats v0.4.2
INFO: Installing ScikitLearnBase v0.1.0
INFO: Installing StatsBase v0.7.4
INFO: Installing StatsFuns v0.2.2
INFO: Building Blosc
INFO: Building HDF5
INFO: Package database updated
INFO: METADATA is out-of-date — you may not have the latest version of GaussianMixtures
INFO: Use `Pkg.update()` to get the latest versions of your packages

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.3.12
Commit 80aa779 (2015-10-26 12:41 UTC)
Platform Info:
  System: Linux (x86_64-unknown-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas
  LIBM: libopenlibm
  LLVM: libLLVM-3.3
INFO: Testing GaussianMixtures
INFO: Testing Data
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.610836e+03
      1       9.584991e+02      -6.523373e+02 |        8
      2       8.810812e+02      -7.741790e+01 |        0
      3       8.810812e+02       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 881.0812341532646)
INFO: K-means with 272 data points using 3 iterations
11.3 data points per parameter
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
INFO: EM with 272 data points 0 iterations avll -2.072762
5.8 data points per parameter
INFO: iteration 1, lowerbound -3.875929
INFO: iteration 2, lowerbound -3.805404
INFO: iteration 3, lowerbound -3.737852
INFO: iteration 4, lowerbound -3.653200
INFO: iteration 5, lowerbound -3.549742
INFO: iteration 6, lowerbound -3.431357
INFO: dropping number of Gaussions to 7
INFO: iteration 7, lowerbound -3.308534
INFO: iteration 8, lowerbound -3.202951
INFO: dropping number of Gaussions to 6
INFO: iteration 9, lowerbound -3.133422
INFO: iteration 10, lowerbound -3.094350
INFO: dropping number of Gaussions to 4
INFO: iteration 11, lowerbound -3.058337
INFO: iteration 12, lowerbound -3.010184
INFO: iteration 13, lowerbound -2.960494
INFO: iteration 14, lowerbound -2.908489
INFO: iteration 15, lowerbound -2.861730
INFO: iteration 16, lowerbound -2.826124
INFO: iteration 17, lowerbound -2.804997
INFO: iteration 18, lowerbound -2.797295
INFO: dropping number of Gaussions to 3
INFO: iteration 19, lowerbound -2.782370
INFO: iteration 20, lowerbound -2.764332
INFO: iteration 21, lowerbound -2.744176
INFO: iteration 22, lowerbound -2.716893
INFO: iteration 23, lowerbound -2.681720
INFO: iteration 24, lowerbound -2.639079
INFO: iteration 25, lowerbound -2.590924
INFO: iteration 26, lowerbound -2.540556
INFO: iteration 27, lowerbound -2.491731
INFO: iteration 28, lowerbound -2.447316
INFO: iteration 29, lowerbound -2.408311
INFO: iteration 30, lowerbound -2.374069
INFO: iteration 31, lowerbound -2.344076
INFO: iteration 32, lowerbound -2.320536
INFO: iteration 33, lowerbound -2.308350
INFO: dropping number of Gaussions to 2
INFO: iteration 34, lowerbound -2.303103
INFO: iteration 35, lowerbound -2.299263
INFO: iteration 36, lowerbound -2.299258
INFO: iteration 37, lowerbound -2.299255
INFO: iteration 38, lowerbound -2.299254
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: 47 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
[Fri 01 Jul 2016 07:55:55 AM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Fri 01 Jul 2016 07:55:56 AM UTC: K-means with 272 data points using 3 iterations
11.3 data points per parameter
,Fri 01 Jul 2016 07:55:58 AM UTC: EM with 272 data points 0 iterations avll -2.072762
5.8 data points per parameter
,Fri 01 Jul 2016 07:55:58 AM UTC: GMM converted to Variational GMM
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 1, lowerbound -3.875929
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 2, lowerbound -3.805404
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 3, lowerbound -3.737852
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 4, lowerbound -3.653200
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 5, lowerbound -3.549742
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 6, lowerbound -3.431357
,Fri 01 Jul 2016 07:56:01 AM UTC: dropping number of Gaussions to 7
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 7, lowerbound -3.308534
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 8, lowerbound -3.202951
,Fri 01 Jul 2016 07:56:01 AM UTC: dropping number of Gaussions to 6
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 9, lowerbound -3.133422
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 10, lowerbound -3.094350
,Fri 01 Jul 2016 07:56:01 AM UTC: dropping number of Gaussions to 4
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 11, lowerbound -3.058337
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 12, lowerbound -3.010184
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 13, lowerbound -2.960494
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 14, lowerbound -2.908489
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 15, lowerbound -2.861730
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 16, lowerbound -2.826124
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 17, lowerbound -2.804997
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 18, lowerbound -2.797295
,Fri 01 Jul 2016 07:56:01 AM UTC: dropping number of Gaussions to 3
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 19, lowerbound -2.782370
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 20, lowerbound -2.764332
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 21, lowerbound -2.744176
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 22, lowerbound -2.716893
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 23, lowerbound -2.681720
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 24, lowerbound -2.639079
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 25, lowerbound -2.590924
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 26, lowerbound -2.540556
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 27, lowerbound -2.491731
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 28, lowerbound -2.447316
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 29, lowerbound -2.408311
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 30, lowerbound -2.374069
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 31, lowerbound -2.344076
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 32, lowerbound -2.320536
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 33, lowerbound -2.308350
,Fri 01 Jul 2016 07:56:01 AM UTC: dropping number of Gaussions to 2
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 34, lowerbound -2.303103
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 35, lowerbound -2.299263
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 36, lowerbound -2.299258
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 37, lowerbound -2.299255
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 38, lowerbound -2.299254
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 39, lowerbound -2.299253
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 40, lowerbound -2.299253
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 41, lowerbound -2.299253
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 42, lowerbound -2.299253
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 43, lowerbound -2.299253
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 44, lowerbound -2.299253
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 45, lowerbound -2.299253
,Fri 01 Jul 2016 07:56:01 AM UTC: iteration 46, lowerbound -2.299253
,Fri 01 Jul 2016 07:56:01 AM UTC: 47 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.04509296130956,95.9549070386905]
β = [178.04509296130956,95.9549070386905]
m = [4.250300727306102 79.28686685665511
 2.000229251599125 53.85198714029612]
ν = [180.04509296130956,97.9549070386905]
W = [
[0.18404155465241945 -0.007644049120668242
 0.0 0.008581705056100158],

[0.3758763714618625 -0.008953123949239233
 0.0 0.012748664808649043]]
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -1.0003506481150317
avll from llpg:  -1.000350648115028
ERROR: MemoryError()
 in A_mul_Bt at linalg/matmul.jl:141
 in llpg at /home/vagrant/.julia/v0.3/GaussianMixtures/src/train.jl:294
 in avll at /home/vagrant/.julia/v0.3/GaussianMixtures/src/train.jl:336
 in anonymous at no file:15
 in include at ./boot.jl:245
 in include_from_node1 at ./loading.jl:128
 in include at ./boot.jl:245
 in include_from_node1 at loading.jl:128
 in process_options at ./client.jl:285
 in _start at ./client.jl:354
while loading /home/vagrant/.julia/v0.3/GaussianMixtures/test/train.jl, in expression starting on line 2
while loading /home/vagrant/.julia/v0.3/GaussianMixtures/test/runtests.jl, in expression starting on line 7
==========================[ ERROR: GaussianMixtures ]===========================

failed process: Process(`/home/vagrant/julia/bin/julia /home/vagrant/.julia/v0.3/GaussianMixtures/test/runtests.jl`, ProcessExited(1)) [1]

================================================================================
INFO: No packages to install, update or remove
ERROR: GaussianMixtures had test errors
 in error at error.jl:21
 in test at pkg/entry.jl:718
 in anonymous at pkg/dir.jl:28
 in cd at ./file.jl:20
 in cd at pkg/dir.jl:28
 in test at pkg.jl:67
 in process_options at ./client.jl:213
 in _start at ./client.jl:354

>>> End of log
