>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from git://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing ArrayViews v0.6.3
INFO: Installing Blosc v0.1.4
INFO: Installing Clustering v0.4.0
INFO: Installing Distances v0.2.0
INFO: Installing Distributions v0.8.4
INFO: Installing GaussianMixtures v0.0.8
INFO: Installing HDF5 v0.5.5
INFO: Installing JLD v0.5.4
INFO: Installing NumericExtensions v0.6.2
INFO: Installing NumericFuns v0.2.3
INFO: Installing PDMats v0.3.5
INFO: Installing StatsBase v0.7.1
INFO: Installing StatsFuns v0.1.2
INFO: Building Blosc
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.3.11
Commit 483dbf5* (2015-07-27 06:18 UTC)
Platform Info:
  System: Linux (x86_64-unknown-linux-gnu)
  CPU: Intel(R) Core(TM) i5-2500K CPU @ 3.30GHz
  WORD_SIZE: 64
  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Sandybridge)
  LAPACK: libopenblas
  LIBM: libopenlibm
  LLVM: libLLVM-3.3
INFO: Testing GaussianMixtures
Warning: could not import Base.add! into NumericExtensions
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.849287e+03
      1       1.054578e+03      -7.947093e+02 |        6
      2       9.710500e+02      -8.352818e+01 |        2
      3       9.658484e+02      -5.201614e+00 |        0
      4       9.658484e+02       0.000000e+00 |        0
K-means converged with 4 iterations (objv = 965.8483755891102)
 0.000	Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
 1.225	K-means with 272 data points using 4 iterations
      	11.3 data points per parameter
 1.914	EM with 272 data points 0 iterations avll -2.082748
      	5.8 data points per parameter
 2.180	GMM converted to Variational GMM
 3.351	iteration 1, lowerbound -3.864654
 3.354	iteration 2, lowerbound -3.745301
 3.355	iteration 3, lowerbound -3.626885
 3.355	iteration 4, lowerbound -3.502529
 3.355	iteration 5, lowerbound -3.383250
 3.390	dropping number of Gaussions to 7
 3.390	iteration 6, lowerbound -3.276329
 3.390	iteration 7, lowerbound -3.192208
 3.422	dropping number of Gaussions to 6
 3.422	iteration 8, lowerbound -3.143802
 3.422	dropping number of Gaussions to 5
 3.422	iteration 9, lowerbound -3.110824
 3.422	dropping number of Gaussions to 4
 3.423	iteration 10, lowerbound -3.083739
 3.423	iteration 11, lowerbound -3.050426
 3.423	iteration 12, lowerbound -3.009519
 3.423	iteration 13, lowerbound -2.959684
 3.423	iteration 14, lowerbound -2.907466
 3.423	iteration 15, lowerbound -2.860422
 3.424	iteration 16, lowerbound -2.824389
 3.424	iteration 17, lowerbound -2.802549
 3.424	iteration 18, lowerbound -2.793636
 3.424	dropping number of Gaussions to 3
 3.424	iteration 19, lowerbound -2.776659
 3.424	iteration 20, lowerbound -2.755878
 3.424	iteration 21, lowerbound -2.732207
 3.425	iteration 22, lowerbound -2.700849
 3.425	iteration 23, lowerbound -2.661561
 3.425	iteration 24, lowerbound -2.615535
 3.425	iteration 25, lowerbound -2.565516
 3.425	iteration 26, lowerbound -2.515242
 3.425	iteration 27, lowerbound -2.468230
 3.426	iteration 28, lowerbound -2.426468
 3.426	iteration 29, lowerbound -2.389984
 3.426	iteration 30, lowerbound -2.357869
 3.426	iteration 31, lowerbound -2.330719
 3.426	iteration 32, lowerbound -2.312482
 3.426	iteration 33, lowerbound -2.307564
 3.426	dropping number of Gaussions to 2
 3.426	iteration 34, lowerbound -2.302921
 3.426	iteration 35, lowerbound -2.299260
 3.427	iteration 36, lowerbound -2.299256
 3.427	iteration 37, lowerbound -2.299254
 3.427	iteration 38, lowerbound -2.299254
 3.427	iteration 39, lowerbound -2.299253
 3.427	iteration 40, lowerbound -2.299253
 3.427	iteration 41, lowerbound -2.299253
 3.427	iteration 42, lowerbound -2.299253
 3.427	iteration 43, lowerbound -2.299253
 3.427	iteration 44, lowerbound -2.299253
 3.428	iteration 45, lowerbound -2.299253
 3.428	iteration 46, lowerbound -2.299253
 3.437	47 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
nothing
α = [178.04509270703747,95.95490729296256]
β = [178.04509270703747,95.95490729296256]
m = [4.250300729368444 79.28686688698492
 2.000229253734928 53.85198715141913]
ν = [180.04509270703747,97.95490729296256]
W = [
[0.18404155493682656 -0.007644049093576794
 0.0 0.008581705094219948],

[0.3758763679114329 -0.008953123907087794
 0.0 0.012748664797846162]]
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 99999.99999999991
avll from stats: -1.0069905498966027
avll from llpg:  -1.0069905498966019
ERROR: MemoryError()
 in A_mul_Bt at linalg/matmul.jl:141
 in llpg at /home/vagrant/.julia/v0.3/GaussianMixtures/src/train.jl:259
 in avll at /home/vagrant/.julia/v0.3/GaussianMixtures/src/train.jl:301
 in anonymous at no file:17
 in include at ./boot.jl:245
 in include_from_node1 at ./loading.jl:128
 in include at ./boot.jl:245
 in include_from_node1 at loading.jl:128
 in process_options at ./client.jl:285
 in _start at ./client.jl:354
while loading /home/vagrant/.julia/v0.3/GaussianMixtures/test/test.jl, in expression starting on line 4
while loading /home/vagrant/.julia/v0.3/GaussianMixtures/test/runtests.jl, in expression starting on line 5
==========================[ ERROR: GaussianMixtures ]===========================

failed process: Process(`/home/vagrant/julia/bin/julia /home/vagrant/.julia/v0.3/GaussianMixtures/test/runtests.jl`, ProcessExited(1)) [1]

================================================================================
INFO: No packages to install, update or remove
ERROR: GaussianMixtures had test errors
 in error at error.jl:21
 in test at pkg/entry.jl:718
 in anonymous at pkg/dir.jl:28
 in cd at ./file.jl:20
 in cd at pkg/dir.jl:28
 in test at pkg.jl:67
 in process_options at ./client.jl:213
 in _start at ./client.jl:354

>>> End of log
