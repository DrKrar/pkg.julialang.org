>>> 'Pkg.add("Mocha")' log
INFO: Cloning cache of JLD from git://github.com/JuliaLang/JLD.jl.git
INFO: Cloning cache of Logging from git://github.com/kmsquire/Logging.jl.git
INFO: Cloning cache of Mocha from git://github.com/pluskid/Mocha.jl.git
INFO: Installing BinDeps v0.3.18
INFO: Installing Blosc v0.1.4
INFO: Installing Dates v0.3.2
INFO: Installing HDF5 v0.5.6
INFO: Installing HttpCommon v0.1.2
INFO: Installing JLD v0.5.5
INFO: Installing Logging v0.1.1
INFO: Installing Mocha v0.0.9
INFO: Installing SHA v0.1.2
INFO: Installing URIParser v0.0.7
INFO: Building Blosc
INFO: Building HDF5
INFO: Building Mocha
Running `g++ -fPIC -Wall -O3 -shared -fopenmp -o libmochaext.so im2col.cpp pooling.cpp`
INFO: Package database updated

>>> 'Pkg.test("Mocha")' log
Julia Version 0.3.11
Commit 483dbf5* (2015-07-27 06:18 UTC)
Platform Info:
  System: Linux (x86_64-unknown-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas
  LIBM: libopenlibm
  LLVM: libLLVM-3.3
INFO: Testing Mocha
Configuring Mocha...
 * CUDA       disabled by default
 * Native Ext disabled by default
Mocha configured, continue loading module...
-- Testing network topology with duplicated blobs
06-Oct 19:10:06:INFO:root:Constructing net net on CPUBackend...
06-Oct 19:10:06:INFO:root:Topological sorting 1 layers...
06-Oct 19:10:06:INFO:root:Constructing net net on CPUBackend...
06-Oct 19:10:06:INFO:root:Topological sorting 2 layers...
-- Testing network topology with missing blobs
06-Oct 19:10:06:INFO:root:Constructing net net on CPUBackend...
06-Oct 19:10:06:INFO:root:Topological sorting 1 layers...
-- Testing network topology with circular dependency
06-Oct 19:10:07:INFO:root:Constructing net net on CPUBackend...
06-Oct 19:10:07:INFO:root:Topological sorting 2 layers...
-- Testing network topology with multiple back-propagate path
    > Good blob sharing
06-Oct 19:10:07:INFO:root:Constructing net net on CPUBackend...
06-Oct 19:10:07:INFO:root:Topological sorting 5 layers...
06-Oct 19:10:07:INFO:root:Setup layers...
06-Oct 19:10:08:INFO:root:Network constructed!
06-Oct 19:10:08:DEBUG:root:Destroying network net
    > Bad blob sharing
06-Oct 19:10:08:INFO:root:Constructing net net on CPUBackend...
06-Oct 19:10:08:INFO:root:Topological sorting 6 layers...
06-Oct 19:10:08:INFO:root:Setup layers...
06-Oct 19:10:08:INFO:root:Network constructed!
-- Testing network topology with dangling blob
    > Good case
06-Oct 19:10:08:INFO:root:Constructing net net on CPUBackend...
06-Oct 19:10:08:INFO:root:Topological sorting 4 layers...
06-Oct 19:10:08:INFO:root:Setup layers...
06-Oct 19:10:09:INFO:root:Network constructed!
06-Oct 19:10:09:DEBUG:root:Destroying network net
    > Bad case
06-Oct 19:10:09:INFO:root:Constructing net net on CPUBackend...
06-Oct 19:10:09:INFO:root:Topological sorting 4 layers...
06-Oct 19:10:09:INFO:root:Setup layers...
06-Oct 19:10:09:INFO:root:Network constructed!
06-Oct 19:10:09:DEBUG:root:Destroying network net
    > Good case 2
06-Oct 19:10:09:INFO:root:Constructing net net on CPUBackend...
06-Oct 19:10:09:INFO:root:Topological sorting 5 layers...
06-Oct 19:10:09:INFO:root:Setup layers...
06-Oct 19:10:09:INFO:root:Network constructed!
06-Oct 19:10:09:DEBUG:root:Destroying network net
    > Bad case 2
06-Oct 19:10:09:INFO:root:Constructing net net on CPUBackend...
06-Oct 19:10:09:INFO:root:Topological sorting 6 layers...
06-Oct 19:10:09:INFO:root:Setup layers...
06-Oct 19:10:09:INFO:root:Network constructed!
06-Oct 19:10:09:DEBUG:root:Destroying network net
-- Testing gradients on simple network (example for gradient checking code)
06-Oct 19:10:10:INFO:root:Constructing net TEST on CPUBackend...
06-Oct 19:10:10:INFO:root:Topological sorting 4 layers...
06-Oct 19:10:10:INFO:root:Setup layers...
06-Oct 19:10:10:INFO:root:Network constructed!
Warning: could not attach metadata for @simd loop.
Warning: could not attach metadata for @simd loop.
-- Testing simple reference counting...
-- Testing glob Utilities
-- Testing RawBLAS{Float32} Utilities
-- Testing RawBLAS{Float64} Utilities
-- Testing blob reshape on CPUBackend{Float32}...
-- Testing blob reshape on CPUBackend{Float64}...
-- Testing ReLU neuron on CPUBackend{Float32}...
    > Forward
    > Backward
-- Testing ReLU neuron on CPUBackend{Float64}...
    > Forward
    > Backward
-- Testing Sigmoid neuron on CPUBackend{Float32}...
    > Forward
    > Backward
-- Testing Sigmoid neuron on CPUBackend{Float64}...
    > Forward
    > Backward
-- Testing Tanh neuron on CPUBackend{Float32}...
    > Forward
    > Backward
-- Testing Tanh neuron on CPUBackend{Float64}...
    > Forward
    > Backward
-- Testing L2 regularizer on CPUBackend{Float32}...
-- Testing L2 regularizer on CPUBackend{Float64}...
-- Testing L1 regularizer on CPUBackend{Float32}...
-- Testing L1 regularizer on CPUBackend{Float64}...
-- Testing L2 constraint on CPUBackend{Float32}...
-- Testing L2 constraint on CPUBackend{Float64}...
-- Testing DataTransformers on CPUBackend{Float32}...
    > SubMean
    > Scale
-- Testing DataTransformers on CPUBackend{Float64}...
    > SubMean
    > Scale
-- Testing Convolution(frozen=true) on CPUBackend{Float64} filter=(3,4)...
    > Setup
    > Forward
    > Backward
-- Testing Convolution(frozen=false) on CPUBackend{Float64} filter=(3,4)...
    > Setup
    > Forward
    > Backward
-- Testing Convolution(frozen=true) on CPUBackend{Float64} filter=(1,1)...
    > Setup
    > Forward
    > Backward
-- Testing Convolution(frozen=false) on CPUBackend{Float64} filter=(1,1)...
    > Setup
    > Forward
    > Backward
-- Testing Convolution(frozen=true) on CPUBackend{Float32} filter=(3,4)...
    > Setup
    > Forward
    > Backward
-- Testing Convolution(frozen=false) on CPUBackend{Float32} filter=(3,4)...
    > Setup
    > Forward
    > Backward
-- Testing Convolution(frozen=true) on CPUBackend{Float32} filter=(1,1)...
    > Setup
    > Forward
    > Backward
-- Testing Convolution(frozen=false) on CPUBackend{Float32} filter=(1,1)...
    > Setup
    > Forward
    > Backward
-- Testing TiedInnerProductLayer on CPUBackend{Float32}...
    > Setup
06-Oct 19:10:20:INFO:root:Constructing net test-tied-ip on CPUBackend...
06-Oct 19:10:20:INFO:root:Topological sorting 3 layers...
06-Oct 19:10:20:INFO:root:Setup layers...
06-Oct 19:10:20:INFO:root:Network constructed!
06-Oct 19:10:20:DEBUG:root:Init network test-tied-ip
06-Oct 19:10:20:DEBUG:root:Init parameter weight for layer ip1
06-Oct 19:10:20:DEBUG:root:Init parameter bias for layer ip1
06-Oct 19:10:20:DEBUG:root:Init parameter bias for layer ip2
    > Forward
    > Backward
06-Oct 19:10:21:DEBUG:root:Destroying network test-tied-ip
-- Testing TiedInnerProductLayer on CPUBackend{Float64}...
    > Setup
06-Oct 19:10:21:INFO:root:Constructing net test-tied-ip on CPUBackend...
06-Oct 19:10:21:INFO:root:Topological sorting 3 layers...
06-Oct 19:10:21:INFO:root:Setup layers...
06-Oct 19:10:21:INFO:root:Network constructed!
06-Oct 19:10:21:DEBUG:root:Init network test-tied-ip
06-Oct 19:10:21:DEBUG:root:Init parameter weight for layer ip1
06-Oct 19:10:21:DEBUG:root:Init parameter bias for layer ip1
06-Oct 19:10:21:DEBUG:root:Init parameter bias for layer ip2
    > Forward
    > Backward
06-Oct 19:10:21:DEBUG:root:Destroying network test-tied-ip
-- Testing SquareLossLayer on CPUBackend{Float32}...
    > (10,7,8,10)
-- Testing SquareLossLayer on CPUBackend{Float64}...
    > (10,8)
-- Testing SplitLayer on CPUBackend{Float32}...
    > Setup
    > Forward
    > Backward
-- Testing SplitLayer on CPUBackend{Float64}...
    > Setup
    > Forward
    > Backward
-- Testing SoftmaxLossLayer on CPUBackend{Float64} ...
    > (7,10) (operate on dimension 1)
    > Forward
    > Backward
-- Testing SoftmaxLossLayer on CPUBackend{Float64} ...
    > (8,10,7,7) (operate on dimension 2)
    > Forward
    > Backward
-- Testing SoftmaxLossLayer on CPUBackend{Float64} ...
    > (7,10,11,8,8) (operate on dimension 2)
    > Forward
    > Backward
-- Testing SoftmaxLossLayer on CPUBackend{Float64} (with weights)...
    > (9,6) (operate on dimension 1)
    > Forward
    > Backward
-- Testing SoftmaxLossLayer on CPUBackend{Float64} (with weights)...
    > (10,9,8,7) (operate on dimension 3)
    > Forward
    > Backward
-- Testing SoftmaxLossLayer on CPUBackend{Float64} (with weights)...
    > (8,10,10,6,9) (operate on dimension 4)
    > Forward
    > Backward
-- Testing SoftmaxLossLayer on CPUBackend{Float32} ...
    > (8,6) (operate on dimension 1)
    > Forward
    > Backward
-- Testing SoftmaxLossLayer on CPUBackend{Float32} ...
    > (9,6,11,10) (operate on dimension 1)
    > Forward
    > Backward
-- Testing SoftmaxLossLayer on CPUBackend{Float32} ...
    > (11,6,11,6,11) (operate on dimension 4)
    > Forward
    > Backward
-- Testing SoftmaxLossLayer on CPUBackend{Float32} (with weights)...
    > (8,9) (operate on dimension 1)
    > Forward
    > Backward
-- Testing SoftmaxLossLayer on CPUBackend{Float32} (with weights)...
    > (9,11,11,11) (operate on dimension 3)
    > Forward
    > Backward
-- Testing SoftmaxLossLayer on CPUBackend{Float32} (with weights)...
    > (7,10,7,9,9) (operate on dimension 4)
    > Forward
    > Backward
-- Testing SoftmaxLayer on CPUBackend{Float64}...
    > 2-dimensional input, normalize along dimension 1
    > Forward
    > Backward
-- Testing SoftmaxLayer on CPUBackend{Float64}...
    > 4-dimensional input, normalize along dimension 2
    > Forward
    > Backward
-- Testing SoftmaxLayer on CPUBackend{Float64}...
    > 5-dimensional input, normalize along dimension 1
    > Forward
    > Backward
-- Testing SoftmaxLayer on CPUBackend{Float32}...
    > 2-dimensional input, normalize along dimension 1
    > Forward
    > Backward
-- Testing SoftmaxLayer on CPUBackend{Float32}...
    > 4-dimensional input, normalize along dimension 2
    > Forward
    > Backward
-- Testing SoftmaxLayer on CPUBackend{Float32}...
    > 5-dimensional input, normalize along dimension 1
    > Forward
    > Backward
-- Testing SoftlabelSoftmaxLossLayer on CPUBackend{Float64}...
    > (6,11) (operate on dimension 1)
    > Forward
    > Backward
-- Testing SoftlabelSoftmaxLossLayer on CPUBackend{Float64}...
    > (9,9,8,11) (operate on dimension 1)
    > Forward
    > Backward
-- Testing SoftlabelSoftmaxLossLayer on CPUBackend{Float64}...
    > (6,10,11,11,10) (operate on dimension 4)
    > Forward
    > Backward
-- Testing SoftlabelSoftmaxLossLayer on CPUBackend{Float32}...
    > (6,10) (operate on dimension 1)
    > Forward
    > Backward
-- Testing SoftlabelSoftmaxLossLayer on CPUBackend{Float32}...
    > (9,11,7,9) (operate on dimension 3)
    > Forward
    > Backward
-- Testing SoftlabelSoftmaxLossLayer on CPUBackend{Float32}...
    > (8,11,11,7,9) (operate on dimension 2)
    > Forward
    > Backward
-- Testing convolution layer with shared param on CPUBackend{Float64}...
06-Oct 19:10:28:INFO:root:Constructing net test-shared-params on CPUBackend...
06-Oct 19:10:28:INFO:root:Topological sorting 5 layers...
06-Oct 19:10:28:INFO:root:Setup layers...
06-Oct 19:10:28:DEBUG:root:ConvolutionLayer(conv2): sharing filters and bias
06-Oct 19:10:28:INFO:root:Network constructed!
06-Oct 19:10:28:DEBUG:root:Init network test-shared-params
06-Oct 19:10:28:DEBUG:root:Init parameter filter for layer conv1
06-Oct 19:10:28:DEBUG:root:Init parameter bias for layer conv1
06-Oct 19:10:29:DEBUG:root:Destroying network test-shared-params
-- Testing inner-product layer with shared param on CPUBackend{Float64}...
06-Oct 19:10:29:INFO:root:Constructing net test-shared-params on CPUBackend...
06-Oct 19:10:29:INFO:root:Topological sorting 5 layers...
06-Oct 19:10:29:INFO:root:Setup layers...
06-Oct 19:10:29:DEBUG:root:InnerProductLayer(ip2): sharing weights and bias
06-Oct 19:10:29:INFO:root:Network constructed!
06-Oct 19:10:29:DEBUG:root:Init network test-shared-params
06-Oct 19:10:29:DEBUG:root:Init parameter weight for layer ip1
06-Oct 19:10:29:DEBUG:root:Init parameter bias for layer ip1
06-Oct 19:10:29:DEBUG:root:Destroying network test-shared-params
-- Testing convolution layer with shared param on CPUBackend{Float32}...
06-Oct 19:10:29:INFO:root:Constructing net test-shared-params on CPUBackend...
06-Oct 19:10:29:INFO:root:Topological sorting 5 layers...
06-Oct 19:10:29:INFO:root:Setup layers...
06-Oct 19:10:29:DEBUG:root:ConvolutionLayer(conv2): sharing filters and bias
06-Oct 19:10:29:INFO:root:Network constructed!
06-Oct 19:10:29:DEBUG:root:Init network test-shared-params
06-Oct 19:10:29:DEBUG:root:Init parameter filter for layer conv1
06-Oct 19:10:29:DEBUG:root:Init parameter bias for layer conv1
06-Oct 19:10:29:DEBUG:root:Destroying network test-shared-params
-- Testing inner-product layer with shared param on CPUBackend{Float32}...
06-Oct 19:10:29:INFO:root:Constructing net test-shared-params on CPUBackend...
06-Oct 19:10:29:INFO:root:Topological sorting 5 layers...
06-Oct 19:10:29:INFO:root:Setup layers...
06-Oct 19:10:29:DEBUG:root:InnerProductLayer(ip2): sharing weights and bias
06-Oct 19:10:29:INFO:root:Network constructed!
06-Oct 19:10:29:DEBUG:root:Init network test-shared-params
06-Oct 19:10:29:DEBUG:root:Init parameter weight for layer ip1
06-Oct 19:10:29:DEBUG:root:Init parameter bias for layer ip1
06-Oct 19:10:29:DEBUG:root:Destroying network test-shared-params
-- Testing ReshapeLayer on CPUBackend{Float64}...
    > Setup
    > Forward
    > Backward
-- Testing ReshapeLayer on CPUBackend{Float32}...
    > Setup
    > Forward
    > Backward
-- Testing RandomMask on CPUBackend{Float64}
    > 3 input blobs with tensor dims [3,5,1]
    > Setup
    > Forward
    > Backward
-- Testing RandomMask on CPUBackend{Float32}
    > 3 input blobs with tensor dims [2,4,6]
    > Setup
    > Forward
    > Backward
-- Testing PowerLayer on CPUBackend{Float32}...
    > scale=0.57, shift=0.84, power=2, tensor_dim=2
    > scale=0, shift=0.45, power=6, tensor_dim=6
    > scale=0.12, shift=0.9, power=2, tensor_dim=3
    > scale=0.77, shift=0, power=3, tensor_dim=2
    > scale=0.68, shift=0.96, power=4, tensor_dim=4
    > scale=0.89, shift=0.32, power=0, tensor_dim=2
    > scale=0.25, shift=0.33, power=1, tensor_dim=1
    > scale=0.03, shift=0.77, power=-1, tensor_dim=5
-- Testing PowerLayer on CPUBackend{Float64}...
    > scale=0.07, shift=0.39, power=2, tensor_dim=3
    > scale=0, shift=0.86, power=2, tensor_dim=6
    > scale=0.28, shift=0.76, power=2, tensor_dim=3
    > scale=0.59, shift=0, power=3, tensor_dim=5
    > scale=0.67, shift=0.31, power=4, tensor_dim=3
    > scale=0.74, shift=0.33, power=0, tensor_dim=1
    > scale=0.77, shift=0.14, power=1, tensor_dim=4
    > scale=0.46, shift=0.74, power=-1, tensor_dim=4
-- Testing Pooling(Max)  on CPUBackend{Float64}...
    > Setup
    > Forward
    > Backward
-- Testing Pooling(Mean)  on CPUBackend{Float64}...
    > Setup
    > Forward
    > Backward
-- Testing Pooling(Max) with padding on CPUBackend{Float64}...
    > Setup
    > Forward
    > Backward
-- Testing Pooling(Mean) with padding on CPUBackend{Float64}...
    > Setup
    > Forward
    > Backward
-- Testing Pooling(Max)  on CPUBackend{Float32}...
    > Setup
    > Forward
    > Backward
-- Testing Pooling(Mean)  on CPUBackend{Float32}...
    > Setup
    > Forward
    > Backward
-- Testing Pooling(Max) with padding on CPUBackend{Float32}...
    > Setup
    > Forward
    > Backward
-- Testing Pooling(Mean) with padding on CPUBackend{Float32}...
    > Setup
    > Forward
    > Backward
-- Testing MultinomialLogisticLossLayer{equal,local} on CPUBackend{Float64}...
    > [7,11] (operate on dimension 1)
-- Testing MultinomialLogisticLossLayer{equal,local} on CPUBackend{Float64}...
    > [8,8,10,7] (operate on dimension 3)
-- Testing MultinomialLogisticLossLayer{equal,local} on CPUBackend{Float64}...
    > [10,7,11,11,11] (operate on dimension 4)
-- Testing MultinomialLogisticLossLayer{local,local} on CPUBackend{Float64}...
    > [8,10] (operate on dimension 1)
-- Testing MultinomialLogisticLossLayer{local,local} on CPUBackend{Float64}...
    > [10,10,7,8] (operate on dimension 3)
-- Testing MultinomialLogisticLossLayer{local,local} on CPUBackend{Float64}...
    > [11,8,8,11,8] (operate on dimension 4)
-- Testing MultinomialLogisticLossLayer{global,global} on CPUBackend{Float64}...
    > [11,10] (operate on dimension 1)
-- Testing MultinomialLogisticLossLayer{global,global} on CPUBackend{Float64}...
    > [7,8,6,11] (operate on dimension 3)
-- Testing MultinomialLogisticLossLayer{global,global} on CPUBackend{Float64}...
    > [9,6,9,8,11] (operate on dimension 4)
-- Testing MultinomialLogisticLossLayer{global,local} on CPUBackend{Float64}...
    > [7,7] (operate on dimension 1)
-- Testing MultinomialLogisticLossLayer{global,local} on CPUBackend{Float64}...
    > [11,6,11,7] (operate on dimension 3)
-- Testing MultinomialLogisticLossLayer{global,local} on CPUBackend{Float64}...
    > [10,6,10,6,7] (operate on dimension 4)
-- Testing MultinomialLogisticLossLayer{no,no} on CPUBackend{Float64}...
    > [9,6] (operate on dimension 1)
-- Testing MultinomialLogisticLossLayer{no,no} on CPUBackend{Float64}...
    > [10,7,9,8] (operate on dimension 2)
-- Testing MultinomialLogisticLossLayer{no,no} on CPUBackend{Float64}...
    > [7,7,9,11,6] (operate on dimension 1)
-- Testing MultinomialLogisticLossLayer{equal,local} on CPUBackend{Float32}...
    > [10,7] (operate on dimension 1)
-- Testing MultinomialLogisticLossLayer{equal,local} on CPUBackend{Float32}...
    > [9,10,11,7] (operate on dimension 3)
-- Testing MultinomialLogisticLossLayer{equal,local} on CPUBackend{Float32}...
    > [11,10,8,6,7] (operate on dimension 4)
-- Testing MultinomialLogisticLossLayer{local,local} on CPUBackend{Float32}...
    > [7,11] (operate on dimension 1)
-- Testing MultinomialLogisticLossLayer{local,local} on CPUBackend{Float32}...
    > [8,10,11,6] (operate on dimension 3)
-- Testing MultinomialLogisticLossLayer{local,local} on CPUBackend{Float32}...
    > [11,7,6,7,9] (operate on dimension 4)
-- Testing MultinomialLogisticLossLayer{global,global} on CPUBackend{Float32}...
    > [9,9] (operate on dimension 1)
-- Testing MultinomialLogisticLossLayer{global,global} on CPUBackend{Float32}...
    > [9,9,10,6] (operate on dimension 3)
-- Testing MultinomialLogisticLossLayer{global,global} on CPUBackend{Float32}...
    > [8,8,7,8,9] (operate on dimension 4)
-- Testing MultinomialLogisticLossLayer{global,local} on CPUBackend{Float32}...
    > [6,11] (operate on dimension 1)
-- Testing MultinomialLogisticLossLayer{global,local} on CPUBackend{Float32}...
    > [6,7,6,11] (operate on dimension 3)
-- Testing MultinomialLogisticLossLayer{global,local} on CPUBackend{Float32}...
    > [6,7,8,7,7] (operate on dimension 4)
-- Testing MultinomialLogisticLossLayer{no,no} on CPUBackend{Float32}...
    > [9,8] (operate on dimension 1)
-- Testing MultinomialLogisticLossLayer{no,no} on CPUBackend{Float32}...
    > [8,11,10,9] (operate on dimension 1)
-- Testing MultinomialLogisticLossLayer{no,no} on CPUBackend{Float32}...
    > [6,8,8,10,7] (operate on dimension 4)
-- Testing Memory Output Layer on CPUBackend{Float32}...
    > (5,3)
-- Testing Memory Output Layer on CPUBackend{Float64}...
    > (8,6,2)
-- Testing Memory Data Layer on CPUBackend{Float32}...
    > (1,3,1,6)
-- Testing Memory Data Layer on CPUBackend{Float64}...
    > (2,1)
-- Testing LRN(AcrossChannel) on CPUBackend{Float32}...
    > Setup with dims (7,6,8,7)
    > Forward
    > Backward
-- Testing LRN(WithinChannel) on CPUBackend{Float32}...
    > Setup with dims (6,10,11,11)
    > Forward
    > Backward
-- Testing LRN(AcrossChannel) on CPUBackend{Float64}...
    > Setup with dims (8,9,9,6)
    > Forward
    > Backward
-- Testing LRN(WithinChannel) on CPUBackend{Float64}...
    > Setup with dims (6,11,6,7)
    > Forward
    > Backward
-- Testing InplaceLayer on CPUBackend{Float64}...
    > Setup
06-Oct 19:10:56:INFO:root:Constructing net test-inplace on CPUBackend...
06-Oct 19:10:56:INFO:root:Topological sorting 5 layers...
06-Oct 19:10:56:INFO:root:Setup layers...
06-Oct 19:10:56:INFO:root:Network constructed!
06-Oct 19:10:56:DEBUG:root:Init network test-inplace
06-Oct 19:10:56:DEBUG:root:Init parameter weight for layer ip1
06-Oct 19:10:56:DEBUG:root:Init parameter bias for layer ip1
06-Oct 19:10:56:DEBUG:root:Init parameter weight for layer ip2
06-Oct 19:10:56:DEBUG:root:Init parameter bias for layer ip2
    > Forward
    > Backward
06-Oct 19:10:56:DEBUG:root:Destroying network test-inplace
-- Testing InplaceLayer on CPUBackend{Float32}...
    > Setup
06-Oct 19:10:56:INFO:root:Constructing net test-inplace on CPUBackend...
06-Oct 19:10:56:INFO:root:Topological sorting 5 layers...
06-Oct 19:10:56:INFO:root:Setup layers...
06-Oct 19:10:56:INFO:root:Network constructed!
06-Oct 19:10:56:DEBUG:root:Init network test-inplace
06-Oct 19:10:56:DEBUG:root:Init parameter weight for layer ip1
06-Oct 19:10:56:DEBUG:root:Init parameter bias for layer ip1
06-Oct 19:10:56:DEBUG:root:Init parameter weight for layer ip2
06-Oct 19:10:56:DEBUG:root:Init parameter bias for layer ip2
    > Forward
    > Backward
06-Oct 19:10:56:DEBUG:root:Destroying network test-inplace
-- Testing InnerProductLayer on CPUBackend{Float32}...
    > Setup
    > Forward
    > Backward
-- Testing InnerProductLayer on CPUBackend{Float64}...
    > Setup
    > Forward
    > Backward
-- Testing Index2OnehotLayer on CPUBackend{Float32}...
    > 2-dimensional input, expanding along dimension 1
-- Testing Index2OnehotLayer on CPUBackend{Float32}...
    > 4-dimensional input, expanding along dimension 2
-- Testing Index2OnehotLayer on CPUBackend{Float32}...
    > 5-dimensional input, expanding along dimension 4
-- Testing Index2OnehotLayer on CPUBackend{Float64}...
    > 2-dimensional input, expanding along dimension 1
-- Testing Index2OnehotLayer on CPUBackend{Float64}...
    > 4-dimensional input, expanding along dimension 3
-- Testing Index2OnehotLayer on CPUBackend{Float64}...
    > 5-dimensional input, expanding along dimension 4
-- Testing IdentityLayer on CPUBackend{Float32}...
    > Setup
    > Forward
-- Testing IdentityLayer on CPUBackend{Float64}...
    > Setup
    > Forward
-- Testing HDF5 Output Layer on CPUBackend{Float32}...
    > (7,4)
06-Oct 19:10:58:WARNING:root:HDF5OutputLayer: output file '/tmp/Mocha-26134-tlyFP1gwP3K0k0uYhjqJsm2qzs41HCm5.hdf5' already exists, overwriting
-- Testing HDF5 Output Layer on CPUBackend{Float64}...
    > (8,8)
06-Oct 19:10:59:WARNING:root:HDF5OutputLayer: output file '/tmp/Mocha-26134-IgQ2yN3BWFZXff57F4CWGUedeGiObqDB.hdf5' already exists, overwriting
-- Testing  HDF5 Data Layer on CPUBackend{Float32}...
    > (1,1,7,3,2)
-- Testing (Async) HDF5 Data Layer on CPUBackend{Float32}...
    > (4,2,6,5,5)
06-Oct 19:11:00:INFO:root:AsyncHDF5DataLayer: Stopping IO task...
06-Oct 19:11:00:INFO:root:AsyncHDF5DataLayer: IO Task reaching the end...
-- Testing  HDF5 Data Layer on CPUBackend{Float64}...
    > (2,8,5,1)
-- Testing (Async) HDF5 Data Layer on CPUBackend{Float64}...
    > (1,8,4,1,1)
06-Oct 19:11:01:INFO:root:AsyncHDF5DataLayer: Stopping IO task...
06-Oct 19:11:01:INFO:root:AsyncHDF5DataLayer: IO Task reaching the end...
-- Testing  HDF5 Data Layer (shuffle,n=6,b=4) on CPUBackend{Float32}...
-- Testing (Async) HDF5 Data Layer (shuffle,n=6,b=4) on CPUBackend{Float32}...
06-Oct 19:11:01:INFO:root:AsyncHDF5DataLayer: Stopping IO task...
06-Oct 19:11:01:INFO:root:AsyncHDF5DataLayer: IO Task reaching the end...
-- Testing  HDF5 Data Layer (shuffle,n=4,b=6) on CPUBackend{Float32}...
-- Testing (Async) HDF5 Data Layer (shuffle,n=4,b=6) on CPUBackend{Float32}...
06-Oct 19:11:01:INFO:root:AsyncHDF5DataLayer: Stopping IO task...
06-Oct 19:11:01:INFO:root:AsyncHDF5DataLayer: IO Task reaching the end...
-- Testing  HDF5 Data Layer (shuffle,n=6,b=4) on CPUBackend{Float64}...
-- Testing (Async) HDF5 Data Layer (shuffle,n=6,b=4) on CPUBackend{Float64}...
06-Oct 19:11:01:INFO:root:AsyncHDF5DataLayer: Stopping IO task...
06-Oct 19:11:01:INFO:root:AsyncHDF5DataLayer: IO Task reaching the end...
-- Testing  HDF5 Data Layer (shuffle,n=4,b=6) on CPUBackend{Float64}...
-- Testing (Async) HDF5 Data Layer (shuffle,n=4,b=6) on CPUBackend{Float64}...
06-Oct 19:11:01:INFO:root:AsyncHDF5DataLayer: Stopping IO task...
06-Oct 19:11:01:INFO:root:AsyncHDF5DataLayer: IO Task reaching the end...
-- Testing ElementWiseLayer{Add()} on CPUBackend{Float32}...
    > (7,3)
-- Testing ElementWiseLayer{Subtract()} on CPUBackend{Float32}...
    > (4,5)
-- Testing ElementWiseLayer{Multiply()} on CPUBackend{Float32}...
    > (4,3,7,1,8,3)
-- Testing ElementWiseLayer{Divide()} on CPUBackend{Float32}...
    > (3,4,2,3,2)
-- Testing ElementWiseLayer{Add()} on CPUBackend{Float64}...
    > (6,5,7,4,1,5)
-- Testing ElementWiseLayer{Subtract()} on CPUBackend{Float64}...
    > (2,6,6,5,7,5)
-- Testing ElementWiseLayer{Multiply()} on CPUBackend{Float64}...
    > (8,)
-- Testing ElementWiseLayer{Divide()} on CPUBackend{Float64}...
    > (7,)
-- Testing Dropout on CPUBackend{Float64}...
    > (7,8,7)
    > Setup
    > Forward
    > Backward
-- Testing Dropout on CPUBackend{Float32}...
    > (4,4,2)
    > Setup
    > Forward
    > Backward
-- Testing CropLayer on CPUBackend{Float64} ...
    > Setup
    > Forward
-- Testing CropLayer on CPUBackend{Float64} with mirror...
    > Setup
    > Forward
-- Testing CropLayer{rnd} on CPUBackend{Float64} ...
    > Setup
    > Forward
-- Testing CropLayer{rnd} on CPUBackend{Float64} with mirror...
    > Setup
    > Forward
-- Testing CropLayer on CPUBackend{Float32} ...
    > Setup
    > Forward
-- Testing CropLayer on CPUBackend{Float32} with mirror...
    > Setup
    > Forward
-- Testing CropLayer{rnd} on CPUBackend{Float32} ...
    > Setup
    > Forward
-- Testing CropLayer{rnd} on CPUBackend{Float32} with mirror...
    > Setup
    > Forward
-- Testing ConcatLayer(dim=1) on CPUBackend{Float64}...
    > 2-dimensional tensor
    > Forward
    > Backward
-- Testing ConcatLayer(dim=2) on CPUBackend{Float64}...
    > 4-dimensional tensor
    > Forward
    > Backward
-- Testing ConcatLayer(dim=3) on CPUBackend{Float64}...
    > 4-dimensional tensor
    > Forward
    > Backward
-- Testing ConcatLayer(dim=4) on CPUBackend{Float64}...
    > 6-dimensional tensor
    > Forward
    > Backward
-- Testing ConcatLayer(dim=5) on CPUBackend{Float64}...
    > 5-dimensional tensor
    > Forward
    > Backward
-- Testing ConcatLayer(dim=1) on CPUBackend{Float32}...
    > 4-dimensional tensor
    > Forward
    > Backward
-- Testing ConcatLayer(dim=2) on CPUBackend{Float32}...
    > 6-dimensional tensor
    > Forward
    > Backward
-- Testing ConcatLayer(dim=3) on CPUBackend{Float32}...
    > 3-dimensional tensor
    > Forward
    > Backward
-- Testing ConcatLayer(dim=4) on CPUBackend{Float32}...
    > 4-dimensional tensor
    > Forward
    > Backward
-- Testing ConcatLayer(dim=5) on CPUBackend{Float32}...
    > 7-dimensional tensor
    > Forward
    > Backward
-- Testing ChannelPooling(Max) on CPUBackend{Float32}...
    > Setup (pool along dimension 1 for 2-D tensors)
    > Forward
    > Backward
-- Testing ChannelPooling(Max) on CPUBackend{Float32}...
    > Setup (pool along dimension 1 for 3-D tensors)
    > Forward
    > Backward
-- Testing ChannelPooling(Max) on CPUBackend{Float32}...
    > Setup (pool along dimension 2 for 4-D tensors)
    > Forward
    > Backward
-- Testing ChannelPooling(Max) on CPUBackend{Float32}...
    > Setup (pool along dimension 3 for 5-D tensors)
    > Forward
    > Backward
-- Testing ChannelPooling(Max) on CPUBackend{Float32}...
    > Setup (pool along dimension 5 for 6-D tensors)
    > Forward
    > Backward
-- Testing ChannelPooling(Mean) on CPUBackend{Float32}...
    > Setup (pool along dimension 1 for 2-D tensors)
    > Forward
    > Backward
-- Testing ChannelPooling(Mean) on CPUBackend{Float32}...
    > Setup (pool along dimension 1 for 3-D tensors)
    > Forward
    > Backward
-- Testing ChannelPooling(Mean) on CPUBackend{Float32}...
    > Setup (pool along dimension 1 for 4-D tensors)
    > Forward
    > Backward
-- Testing ChannelPooling(Mean) on CPUBackend{Float32}...
    > Setup (pool along dimension 2 for 5-D tensors)
    > Forward
    > Backward
-- Testing ChannelPooling(Mean) on CPUBackend{Float32}...
    > Setup (pool along dimension 3 for 6-D tensors)
    > Forward
    > Backward
-- Testing ChannelPooling(Max) on CPUBackend{Float64}...
    > Setup (pool along dimension 1 for 2-D tensors)
    > Forward
    > Backward
-- Testing ChannelPooling(Max) on CPUBackend{Float64}...
    > Setup (pool along dimension 1 for 3-D tensors)
    > Forward
    > Backward
-- Testing ChannelPooling(Max) on CPUBackend{Float64}...
    > Setup (pool along dimension 1 for 4-D tensors)
    > Forward
    > Backward
-- Testing ChannelPooling(Max) on CPUBackend{Float64}...
    > Setup (pool along dimension 3 for 5-D tensors)
    > Forward
    > Backward
-- Testing ChannelPooling(Max) on CPUBackend{Float64}...
    > Setup (pool along dimension 2 for 6-D tensors)
    > Forward
    > Backward
-- Testing ChannelPooling(Mean) on CPUBackend{Float64}...
    > Setup (pool along dimension 1 for 2-D tensors)
    > Forward
    > Backward
-- Testing ChannelPooling(Mean) on CPUBackend{Float64}...
    > Setup (pool along dimension 1 for 3-D tensors)
    > Forward
    > Backward
-- Testing ChannelPooling(Mean) on CPUBackend{Float64}...
    > Setup (pool along dimension 2 for 4-D tensors)
    > Forward
    > Backward
-- Testing ChannelPooling(Mean) on CPUBackend{Float64}...
    > Setup (pool along dimension 1 for 5-D tensors)
    > Forward
    > Backward
-- Testing ChannelPooling(Mean) on CPUBackend{Float64}...
    > Setup (pool along dimension 2 for 6-D tensors)
    > Forward
    > Backward
-- Testing ArgmaxLayer on CPUBackend{Float64}...
    > 2-dimensional tensor
    > Setup
    > Forward
-- Testing ArgmaxLayer on CPUBackend{Float64}...
    > 4-dimensional tensor
    > Setup
    > Forward
-- Testing ArgmaxLayer on CPUBackend{Float64}...
    > 5-dimensional tensor
    > Setup
    > Forward
-- Testing ArgmaxLayer on CPUBackend{Float32}...
    > 2-dimensional tensor
    > Setup
    > Forward
-- Testing ArgmaxLayer on CPUBackend{Float32}...
    > 4-dimensional tensor
    > Setup
    > Forward
-- Testing ArgmaxLayer on CPUBackend{Float32}...
    > 5-dimensional tensor
    > Setup
    > Forward
-- Testing AccuracyLayer on CPUBackend{Float32}...
    > (11,11) (operate on dimension 1)
    > Forward
    > Forward Again
    > Forward Again and Again
-- Testing AccuracyLayer on CPUBackend{Float32}...
    > (8,9,9,6) (operate on dimension 1)
    > Forward
    > Forward Again
    > Forward Again and Again
-- Testing AccuracyLayer on CPUBackend{Float32}...
    > (11,7,9,7,8) (operate on dimension 1)
    > Forward
    > Forward Again
    > Forward Again and Again
-- Testing AccuracyLayer on CPUBackend{Float64}...
    > (7,6) (operate on dimension 1)
    > Forward
    > Forward Again
    > Forward Again and Again
-- Testing AccuracyLayer on CPUBackend{Float64}...
    > (11,8,11,8) (operate on dimension 1)
    > Forward
    > Forward Again
    > Forward Again and Again
-- Testing AccuracyLayer on CPUBackend{Float64}...
    > (7,10,11,11,9) (operate on dimension 4)
    > Forward
    > Forward Again
    > Forward Again and Again
INFO: Mocha tests passed
INFO: No packages to install, update or remove

>>> End of log
