>>> 'Pkg.add("ScikitLearn")' log
INFO: Cloning cache of ScikitLearn from https://github.com/cstjean/ScikitLearn.jl.git
INFO: Installing BinDeps v0.4.7
INFO: Installing Conda v0.5.1
INFO: Installing DataStructures v0.5.3
INFO: Installing Iterators v0.3.0
INFO: Installing MacroTools v0.3.4
INFO: Installing Parameters v0.6.0
INFO: Installing PyCall v1.10.0
INFO: Installing Requires v0.3.0
INFO: Installing SHA v0.3.1
INFO: Installing ScikitLearn v0.2.4
INFO: Installing ScikitLearnBase v0.2.2
INFO: Installing SpecialFunctions v0.1.1
INFO: Installing StatsBase v0.13.1
INFO: Installing URIParser v0.1.8
INFO: Building Conda
INFO: Building PyCall
INFO: Using the Python distribution in the Conda package by default.
To use a different Python version, set ENV["PYTHON"]="pythoncommand" and re-run Pkg.build("PyCall").
Fetching package metadata .........
Solving package specifications: .

# All requested packages already installed.
# packages in environment at /home/vagrant/.julia/v0.5/Conda/deps/usr:
#
numpy                     1.12.0                   py27_0  
INFO: PyCall is using /home/vagrant/.julia/v0.5/Conda/deps/usr/bin/python (Python 2.7.13) at /home/vagrant/.julia/v0.5/Conda/deps/usr/bin/python, libpython = /home/vagrant/.julia/v0.5/Conda/deps/usr/lib/libpython2.7
INFO: /home/vagrant/.julia/v0.5/PyCall/deps/deps.jl has not changed
INFO: /home/vagrant/.julia/v0.5/PyCall/deps/PYTHON has not changed
INFO: Package database updated
INFO: METADATA is out-of-date â€” you may not have the latest version of ScikitLearn
INFO: Use `Pkg.update()` to get the latest versions of your packages

>>> 'Pkg.test("ScikitLearn")' log
Julia Version 0.5.0
Commit 3c9d753 (2016-09-19 18:14 UTC)
Platform Info:
  System: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-110-generic #157-Ubuntu SMP Mon Feb 20 11:54:05 UTC 2017 x86_64 x86_64
Memory: 2.9392738342285156 GB (1137.796875 MB free)
Uptime: 26272.0 sec
Load Avg:  0.912109375  1.03515625  1.10205078125
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3499 MHz    1583700 s       2334 s     153525 s     548992 s         98 s
#2  3499 MHz     472862 s      12518 s      78542 s    1973845 s          3 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.5
2 required packages:
 - JSON                          0.8.3
 - ScikitLearn                   0.2.4
14 additional packages:
 - BinDeps                       0.4.7
 - Compat                        0.19.0
 - Conda                         0.5.1
 - DataStructures                0.5.3
 - Iterators                     0.3.0
 - MacroTools                    0.3.4
 - Parameters                    0.6.0
 - PyCall                        1.10.0
 - Requires                      0.3.0
 - SHA                           0.3.1
 - ScikitLearnBase               0.2.2
 - SpecialFunctions              0.1.1
 - StatsBase                     0.13.1
 - URIParser                     0.1.8
INFO: Computing test dependencies for ScikitLearn...
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Cloning cache of GaussianProcesses from https://github.com/STOR-i/GaussianProcesses.jl.git
INFO: Cloning cache of LowRankModels from https://github.com/madeleineudell/LowRankModels.jl.git
INFO: Installing ArrayViews v0.6.4
INFO: Installing Blosc v0.2.0
INFO: Installing Calculus v0.2.0
INFO: Installing Clustering v0.7.0
INFO: Installing ColorTypes v0.3.4
INFO: Installing Colors v0.7.3
INFO: Installing Combinatorics v0.3.2
INFO: Installing DataArrays v0.3.12
INFO: Installing DataFrames v0.8.5
INFO: Installing DecisionTree v0.5.1
INFO: Installing DiffBase v0.0.4
INFO: Installing Distances v0.4.1
INFO: Installing Distributions v0.12.1
INFO: Installing FileIO v0.3.0
INFO: Installing FixedPointNumbers v0.3.4
INFO: Installing ForwardDiff v0.3.4
INFO: Installing GZip v0.2.20
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing GaussianProcesses v0.4.0
INFO: Installing HDF5 v0.7.3
INFO: Installing Hiccup v0.1.1
INFO: Installing JLD v0.6.9
INFO: Installing Juno v0.2.6
INFO: Installing LaTeXStrings v0.2.0
INFO: Installing LegacyStrings v0.2.0
INFO: Installing LineSearches v0.1.5
INFO: Installing LowRankModels v0.2.0
INFO: Installing Media v0.2.5
INFO: Installing NBInclude v1.1.0
INFO: Installing NMF v0.2.5
INFO: Installing NaNMath v0.2.2
INFO: Installing NearestNeighbors v0.2.0
INFO: Installing Optim v0.7.7
INFO: Installing PDMats v0.5.6
INFO: Installing PolynomialFactors v0.0.3
INFO: Installing Polynomials v0.1.2
INFO: Installing PositiveFactorizations v0.0.4
INFO: Installing Primes v0.1.2
INFO: Installing PyPlot v2.3.1
INFO: Installing QuadGK v0.1.1
INFO: Installing RData v0.0.4
INFO: Installing RDatasets v0.2.0
INFO: Installing Reexport v0.0.3
INFO: Installing Rmath v0.1.6
INFO: Installing Roots v0.3.0
INFO: Installing SortingAlgorithms v0.1.0
INFO: Installing StaticArrays v0.3.0
INFO: Installing StatsFuns v0.4.0
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Building Conda
INFO: Building PyCall
INFO: Using the Python distribution in the Conda package by default.
To use a different Python version, set ENV["PYTHON"]="pythoncommand" and re-run Pkg.build("PyCall").
Fetching package metadata .........
Solving package specifications: .

# All requested packages already installed.
# packages in environment at /home/vagrant/.julia/v0.5/Conda/deps/usr:
#
numpy                     1.12.0                   py27_0  
INFO: PyCall is using /home/vagrant/.julia/v0.5/Conda/deps/usr/bin/python (Python 2.7.13) at /home/vagrant/.julia/v0.5/Conda/deps/usr/bin/python, libpython = /home/vagrant/.julia/v0.5/Conda/deps/usr/lib/libpython2.7
INFO: /home/vagrant/.julia/v0.5/PyCall/deps/deps.jl has not changed
INFO: /home/vagrant/.julia/v0.5/PyCall/deps/PYTHON has not changed
INFO: Testing ScikitLearn
INFO: Installing sklearn via the Conda scikit-learn package...
Fetching package metadata .........
Solving package specifications: .

Package plan for installation in environment /home/vagrant/.julia/v0.5/Conda/deps/usr:

The following NEW packages will be INSTALLED:

    scikit-learn: 0.18.1-np112py27_1

scikit-learn-0   0% |                              | ETA:  --:--:--   0.00  B/sscikit-learn-0   1% |                               | ETA:  0:00:19 581.29 kB/sscikit-learn-0   2% |                               | ETA:  0:00:26 426.44 kB/sscikit-learn-0   3% |                               | ETA:  0:00:17 635.33 kB/sscikit-learn-0   4% |#                              | ETA:  0:00:13 841.02 kB/sscikit-learn-0   5% |#                              | ETA:  0:00:10   1.03 MB/sscikit-learn-0   6% |#                              | ETA:  0:00:08   1.22 MB/sscikit-learn-0   7% |##                             | ETA:  0:00:07   1.41 MB/sscikit-learn-0   8% |##                             | ETA:  0:00:06   1.59 MB/sscikit-learn-0   9% |##                             | ETA:  0:00:05   1.78 MB/sscikit-learn-0  10% |###                            | ETA:  0:00:05   1.97 MB/sscikit-learn-0  11% |###                            | ETA:  0:00:04   2.14 MB/sscikit-learn-0  12% |###                            | ETA:  0:00:04   2.31 MB/sscikit-learn-0  13% |####                           | ETA:  0:00:04   2.43 MB/sscikit-learn-0  14% |####                           | ETA:  0:00:03   2.59 MB/sscikit-learn-0  15% |####                           | ETA:  0:00:03   2.75 MB/sscikit-learn-0  16% |####                           | ETA:  0:00:03   2.91 MB/sscikit-learn-0  17% |#####                          | ETA:  0:00:03   3.07 MB/sscikit-learn-0  18% |#####                          | ETA:  0:00:02   3.24 MB/sscikit-learn-0  19% |#####                          | ETA:  0:00:02   3.39 MB/sscikit-learn-0  20% |######                         | ETA:  0:00:02   3.55 MB/sscikit-learn-0  21% |######                         | ETA:  0:00:02   3.71 MB/sscikit-learn-0  22% |######                         | ETA:  0:00:02   3.87 MB/sscikit-learn-0  23% |#######                        | ETA:  0:00:02   4.02 MB/sscikit-learn-0  24% |#######                        | ETA:  0:00:02   4.17 MB/sscikit-learn-0  25% |#######                        | ETA:  0:00:01   4.32 MB/sscikit-learn-0  26% |########                       | ETA:  0:00:01   4.47 MB/sscikit-learn-0  27% |########                       | ETA:  0:00:01   4.61 MB/sscikit-learn-0  28% |########                       | ETA:  0:00:01   4.75 MB/sscikit-learn-0  29% |########                       | ETA:  0:00:01   4.90 MB/sscikit-learn-0  30% |#########                      | ETA:  0:00:01   5.03 MB/sscikit-learn-0  31% |#########                      | ETA:  0:00:01   5.17 MB/sscikit-learn-0  32% |#########                      | ETA:  0:00:01   5.30 MB/sscikit-learn-0  33% |##########                     | ETA:  0:00:01   5.42 MB/sscikit-learn-0  34% |##########                     | ETA:  0:00:01   5.56 MB/sscikit-learn-0  35% |##########                     | ETA:  0:00:01   5.69 MB/sscikit-learn-0  36% |###########                    | ETA:  0:00:01   5.81 MB/sscikit-learn-0  37% |###########                    | ETA:  0:00:01   5.94 MB/sscikit-learn-0  38% |###########                    | ETA:  0:00:01   6.06 MB/sscikit-learn-0  39% |############                   | ETA:  0:00:01   6.19 MB/sscikit-learn-0  40% |############                   | ETA:  0:00:01   6.32 MB/sscikit-learn-0  41% |############                   | ETA:  0:00:01   6.44 MB/sscikit-learn-0  42% |#############                  | ETA:  0:00:01   6.57 MB/sscikit-learn-0  43% |#############                  | ETA:  0:00:00   6.69 MB/sscikit-learn-0  44% |#############                  | ETA:  0:00:00   6.81 MB/sscikit-learn-0  45% |#############                  | ETA:  0:00:00   6.93 MB/sscikit-learn-0  46% |##############                 | ETA:  0:00:00   7.05 MB/sscikit-learn-0  47% |##############                 | ETA:  0:00:00   7.17 MB/sscikit-learn-0  48% |##############                 | ETA:  0:00:00   7.28 MB/sscikit-learn-0  49% |###############                | ETA:  0:00:00   7.40 MB/sscikit-learn-0  50% |###############                | ETA:  0:00:00   7.52 MB/sscikit-learn-0  51% |###############                | ETA:  0:00:00   7.63 MB/sscikit-learn-0  52% |################               | ETA:  0:00:00   7.74 MB/sscikit-learn-0  53% |################               | ETA:  0:00:00   7.85 MB/sscikit-learn-0  54% |################               | ETA:  0:00:00   7.95 MB/sscikit-learn-0  55% |#################              | ETA:  0:00:00   8.06 MB/sscikit-learn-0  56% |#################              | ETA:  0:00:00   8.17 MB/sscikit-learn-0  57% |#################              | ETA:  0:00:00   8.27 MB/sscikit-learn-0  58% |#################              | ETA:  0:00:00   8.38 MB/sscikit-learn-0  59% |##################             | ETA:  0:00:00   8.48 MB/sscikit-learn-0  60% |##################             | ETA:  0:00:00   8.58 MB/sscikit-learn-0  61% |##################             | ETA:  0:00:00   8.69 MB/sscikit-learn-0  62% |###################            | ETA:  0:00:00   8.78 MB/sscikit-learn-0  63% |###################            | ETA:  0:00:00   8.88 MB/sscikit-learn-0  64% |###################            | ETA:  0:00:00   8.97 MB/sscikit-learn-0  65% |####################           | ETA:  0:00:00   9.07 MB/sscikit-learn-0  66% |####################           | ETA:  0:00:00   9.17 MB/sscikit-learn-0  67% |####################           | ETA:  0:00:00   9.27 MB/sscikit-learn-0  68% |#####################          | ETA:  0:00:00   9.37 MB/sscikit-learn-0  69% |#####################          | ETA:  0:00:00   9.47 MB/sscikit-learn-0  70% |#####################          | ETA:  0:00:00   9.56 MB/sscikit-learn-0  71% |######################         | ETA:  0:00:00   9.66 MB/sscikit-learn-0  72% |######################         | ETA:  0:00:00   9.75 MB/sscikit-learn-0  73% |######################         | ETA:  0:00:00   9.84 MB/sscikit-learn-0  74% |######################         | ETA:  0:00:00   9.93 MB/sscikit-learn-0  75% |#######################        | ETA:  0:00:00  10.02 MB/sscikit-learn-0  76% |#######################        | ETA:  0:00:00  10.11 MB/sscikit-learn-0  77% |#######################        | ETA:  0:00:00  10.20 MB/sscikit-learn-0  78% |########################       | ETA:  0:00:00  10.29 MB/sscikit-learn-0  79% |########################       | ETA:  0:00:00  10.38 MB/sscikit-learn-0  80% |########################       | ETA:  0:00:00  10.46 MB/sscikit-learn-0  81% |#########################      | ETA:  0:00:00  10.55 MB/sscikit-learn-0  82% |#########################      | ETA:  0:00:00  10.64 MB/sscikit-learn-0  83% |#########################      | ETA:  0:00:00  10.73 MB/sscikit-learn-0  84% |##########################     | ETA:  0:00:00  10.81 MB/sscikit-learn-0  85% |##########################     | ETA:  0:00:00  10.90 MB/sscikit-learn-0  86% |##########################     | ETA:  0:00:00  11.00 MB/sscikit-learn-0  87% |##########################     | ETA:  0:00:00  11.10 MB/sscikit-learn-0  88% |###########################    | ETA:  0:00:00  11.17 MB/sscikit-learn-0  89% |###########################    | ETA:  0:00:00  11.26 MB/sscikit-learn-0  90% |###########################    | ETA:  0:00:00  11.35 MB/sscikit-learn-0  91% |############################   | ETA:  0:00:00  11.45 MB/sscikit-learn-0  92% |############################   | ETA:  0:00:00  11.55 MB/sscikit-learn-0  93% |############################   | ETA:  0:00:00  11.66 MB/sscikit-learn-0  94% |#############################  | ETA:  0:00:00  11.76 MB/sscikit-learn-0  95% |#############################  | ETA:  0:00:00  11.86 MB/sscikit-learn-0  96% |#############################  | ETA:  0:00:00  11.96 MB/sscikit-learn-0  97% |############################## | ETA:  0:00:00  12.06 MB/sscikit-learn-0  98% |############################## | ETA:  0:00:00  12.15 MB/sscikit-learn-0  99% |############################## | ETA:  0:00:00  12.25 MB/sscikit-learn-0 100% |###############################| Time: 0:00:00  12.35 MB/s
WARNING: Method definition require(Symbol) in module Base at loading.jl:345 overwritten in module Main at /home/vagrant/.julia/v0.5/Requires/src/require.jl:12.
WARNING: Method definition fit_predict!(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition #sample(Array{Any, 1}, ScikitLearnBase.#sample, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition fit!(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition get_params(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition score_samples(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition clone(PyCall.PyObject) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/Skcore.jl:48 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/Skcore.jl:48.
WARNING: Method definition #fit!(Array{Any, 1}, ScikitLearnBase.#fit!, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition #fit_transform!(Array{Any, 1}, ScikitLearnBase.#fit_transform!, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition #fit_predict!(Array{Any, 1}, ScikitLearnBase.#fit_predict!, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition #set_params!(Array{Any, 1}, ScikitLearnBase.#set_params!, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition predict_proba(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition predict_proba(Any, Any) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/grid_search.jl:358 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/grid_search.jl:358.
WARNING: Method definition transform(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition transform(Any, Any) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/grid_search.jl:358 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/grid_search.jl:358.
WARNING: Method definition set_params!(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition score(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition is_pairwise(PyCall.PyObject) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/Skcore.jl:52 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/Skcore.jl:52.
WARNING: Method definition is_pairwise(Any) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/Skcore.jl:51 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/Skcore.jl:51.
WARNING: Method definition #predict(Array{Any, 1}, ScikitLearnBase.#predict, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition decision_function(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition decision_function(Any, Any) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/grid_search.jl:358 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/grid_search.jl:358.
WARNING: Method definition #get_feature_names(Array{Any, 1}, ScikitLearnBase.#get_feature_names, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition fit_transform!(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition is_classifier(PyCall.PyObject) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/Skcore.jl:49 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/Skcore.jl:49.
WARNING: Method definition inverse_transform(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition inverse_transform(Any, Any) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/grid_search.jl:358 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/grid_search.jl:358.
WARNING: Method definition predict_log_proba(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition predict_log_proba(Any, Any) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/grid_search.jl:358 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/grid_search.jl:358.
WARNING: Method definition #inverse_transform(Array{Any, 1}, ScikitLearnBase.#inverse_transform, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition #partial_fit!(Array{Any, 1}, ScikitLearnBase.#partial_fit!, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition #score(Array{Any, 1}, ScikitLearnBase.#score, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition get_components(PyCall.PyObject) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/Skcore.jl:56 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/Skcore.jl:56.
WARNING: Method definition #score_samples(Array{Any, 1}, ScikitLearnBase.#score_samples, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition #decision_function(Array{Any, 1}, ScikitLearnBase.#decision_function, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition get_classes(PyCall.PyObject) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/Skcore.jl:55 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/Skcore.jl:55.
WARNING: Method definition partial_fit!(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition predict(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition predict(Any, Any) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/grid_search.jl:358 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/grid_search.jl:358.
WARNING: Method definition #transform(Array{Any, 1}, ScikitLearnBase.#transform, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition get_feature_names(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition #predict_proba(Array{Any, 1}, ScikitLearnBase.#predict_proba, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition #get_params(Array{Any, 1}, ScikitLearnBase.#get_params, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition #predict_log_proba(Array{Any, 1}, ScikitLearnBase.#predict_log_proba, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition sample(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.5/ScikitLearn/src/Skcore.jl:95.
WARNING: declare_hyperparameters(...) is deprecated. Use @declare_hyperparameters(...) instead.
/home/vagrant/.julia/v0.5/Conda/deps/usr/lib/python2.7/site-packages/sklearn/utils/deprecation.py:52: DeprecationWarning: Class RandomizedPCA is deprecated; RandomizedPCA was deprecated in 0.18 and will be removed in 0.20. Use PCA(svd_solver='randomized') instead. The new implementation DOES NOT store whiten ``components_``. Apply transform to get them.
  warnings.warn(msg, category=DeprecationWarning)
/home/vagrant/.julia/v0.5/Conda/deps/usr/lib/python2.7/site-packages/sklearn/svm/base.py:552: ChangedBehaviorWarning: The decision_function_shape default value will change from 'ovo' to 'ovr' in 0.19. This will change the shape of the decision function returned by SVC.
  "SVC.", ChangedBehaviorWarning)
/home/vagrant/.julia/v0.5/Conda/deps/usr/lib/python2.7/site-packages/sklearn/utils/deprecation.py:52: DeprecationWarning: Class RandomizedPCA is deprecated; RandomizedPCA was deprecated in 0.18 and will be removed in 0.20. Use PCA(svd_solver='randomized') instead. The new implementation DOES NOT store whiten ``components_``. Apply transform to get them.
  warnings.warn(msg, category=DeprecationWarning)
/home/vagrant/.julia/v0.5/Conda/deps/usr/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
WARNING: declare_hyperparameters(...) is deprecated. Use @declare_hyperparameters(...) instead.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: ignoring conflicting import of CrossValidation.cross_val_score into Main
Testing ../examples/Classifier_Comparison.ipynb
WARNING: No working GUI backend found for matplotlib
Testing ../examples/Classifier_Comparison_Julia.ipynb
WARNING: replacing module Testing
Testing ../examples/Clustering_Comparison.ipynb
WARNING: replacing module Testing
/home/vagrant/.julia/v0.5/Conda/deps/usr/lib/python2.7/site-packages/sklearn/manifold/spectral_embedding_.py:229: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.
  warnings.warn("Graph is not fully connected, spectral embedding"
/home/vagrant/.julia/v0.5/Conda/deps/usr/lib/python2.7/site-packages/sklearn/cluster/hierarchical.py:193: UserWarning: the number of connected components of the connectivity matrix is 2 > 1. Completing it to avoid stopping the tree early.
  connectivity, n_components = _fix_connectivity(X, connectivity)
/home/vagrant/.julia/v0.5/Conda/deps/usr/lib/python2.7/site-packages/sklearn/cluster/hierarchical.py:418: UserWarning: the number of connected components of the connectivity matrix is 2 > 1. Completing it to avoid stopping the tree early.
  connectivity, n_components = _fix_connectivity(X, connectivity)
/home/vagrant/.julia/v0.5/Conda/deps/usr/lib/python2.7/site-packages/sklearn/cluster/hierarchical.py:193: UserWarning: the number of connected components of the connectivity matrix is 3 > 1. Completing it to avoid stopping the tree early.
  connectivity, n_components = _fix_connectivity(X, connectivity)
/home/vagrant/.julia/v0.5/Conda/deps/usr/lib/python2.7/site-packages/sklearn/cluster/hierarchical.py:418: UserWarning: the number of connected components of the connectivity matrix is 3 > 1. Completing it to avoid stopping the tree early.
  connectivity, n_components = _fix_connectivity(X, connectivity)
Testing ../examples/Cross_Validated_Predictions.ipynb
WARNING: replacing module Testing
Testing ../examples/Decision_Tree_Regression.ipynb
WARNING: replacing module Testing
Testing ../examples/Decision_Tree_Regression_Julia.ipynb
WARNING: replacing module Testing
Testing ../examples/Density_Estimation.ipynb
WARNING: replacing module Testing
/home/vagrant/.julia/v0.5/Conda/deps/usr/lib/python2.7/site-packages/sklearn/utils/deprecation.py:52: DeprecationWarning: Class GMM is deprecated; The class GMM is deprecated in 0.18 and will be  removed in 0.20. Use class GaussianMixture instead.
  warnings.warn(msg, category=DeprecationWarning)
/home/vagrant/.julia/v0.5/Conda/deps/usr/lib/python2.7/site-packages/sklearn/utils/deprecation.py:70: DeprecationWarning: Function distribute_covar_matrix_to_match_covariance_type is deprecated; The functon distribute_covar_matrix_to_match_covariance_typeis deprecated in 0.18 and will be removed in 0.20.
  warnings.warn(msg, category=DeprecationWarning)
/home/vagrant/.julia/v0.5/Conda/deps/usr/lib/python2.7/site-packages/sklearn/utils/deprecation.py:70: DeprecationWarning: Function log_multivariate_normal_density is deprecated; The function log_multivariate_normal_density is deprecated in 0.18 and will be removed in 0.20.
  warnings.warn(msg, category=DeprecationWarning)
/home/vagrant/.julia/v0.5/Conda/deps/usr/lib/python2.7/site-packages/sklearn/utils/deprecation.py:70: DeprecationWarning: Function log_multivariate_normal_density is deprecated; The function log_multivariate_normal_density is deprecated in 0.18 and will be removed in 0.20.
  warnings.warn(msg, category=DeprecationWarning)
/home/vagrant/.julia/v0.5/Conda/deps/usr/lib/python2.7/site-packages/sklearn/utils/deprecation.py:70: DeprecationWarning: Function log_multivariate_normal_density is deprecated; The function log_multivariate_normal_density is deprecated in 0.18 and will be removed in 0.20.
  warnings.warn(msg, category=DeprecationWarning)
/home/vagrant/.julia/v0.5/Conda/deps/usr/lib/python2.7/site-packages/sklearn/utils/deprecation.py:70: DeprecationWarning: Function log_multivariate_normal_density is deprecated; The function log_multivariate_normal_density is deprecated in 0.18 and will be removed in 0.20.
  warnings.warn(msg, category=DeprecationWarning)
/home/vagrant/.julia/v0.5/Conda/deps/usr/lib/python2.7/site-packages/sklearn/utils/deprecation.py:70: DeprecationWarning: Function log_multivariate_normal_density is deprecated; The function log_multivariate_normal_density is deprecated in 0.18 and will be removed in 0.20.
  warnings.warn(msg, category=DeprecationWarning)
/home/vagrant/.julia/v0.5/Conda/deps/usr/lib/python2.7/site-packages/sklearn/utils/deprecation.py:70: DeprecationWarning: Function log_multivariate_normal_density is deprecated; The function log_multivariate_normal_density is deprecated in 0.18 and will be removed in 0.20.
  warnings.warn(msg, category=DeprecationWarning)
Testing ../examples/Density_Estimation_Julia.ipynb
WARNING: replacing module Testing
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 600 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       7.850874e+03
      1       5.165666e+03      -2.685208e+03 |        0
      2       5.165666e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 5165.6661634557)
INFO: K-means with 600 data points using 2 iterations
100.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.026541
INFO: iteration 2, average log likelihood -2.026538
INFO: iteration 3, average log likelihood -2.026538
INFO: iteration 4, average log likelihood -2.026538
INFO: iteration 5, average log likelihood -2.026538
INFO: iteration 6, average log likelihood -2.026538
INFO: iteration 7, average log likelihood -2.026538
INFO: iteration 8, average log likelihood -2.026538
INFO: iteration 9, average log likelihood -2.026538
INFO: iteration 10, average log likelihood -2.026538
INFO: EM with 600 data points 10 iterations avll -2.026538
54.5 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.821221e+04
      1       4.717918e+03      -1.349429e+04 |        2
      2       4.713803e+03      -4.115211e+00 |        0
      3       4.713803e+03       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 4713.802531784694)
INFO: K-means with 400 data points using 3 iterations
66.7 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.060321
INFO: iteration 2, average log likelihood -2.060312
INFO: iteration 3, average log likelihood -2.060312
INFO: iteration 4, average log likelihood -2.060312
INFO: iteration 5, average log likelihood -2.060312
INFO: iteration 6, average log likelihood -2.060312
INFO: iteration 7, average log likelihood -2.060312
INFO: iteration 8, average log likelihood -2.060312
INFO: iteration 9, average log likelihood -2.060312
INFO: iteration 10, average log likelihood -2.060312
INFO: EM with 400 data points 10 iterations avll -2.060312
36.4 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       4.482854e+03
      1       3.436488e+03      -1.046366e+03 |        0
      2       3.436488e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 3436.4881168493393)
INFO: K-means with 400 data points using 2 iterations
66.7 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.039125
INFO: iteration 2, average log likelihood -2.039119
INFO: iteration 3, average log likelihood -2.039119
INFO: iteration 4, average log likelihood -2.039119
INFO: iteration 5, average log likelihood -2.039119
INFO: iteration 6, average log likelihood -2.039119
INFO: iteration 7, average log likelihood -2.039119
INFO: iteration 8, average log likelihood -2.039119
INFO: iteration 9, average log likelihood -2.039119
INFO: iteration 10, average log likelihood -2.039119
INFO: EM with 400 data points 10 iterations avll -2.039119
36.4 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       4.513648e+03
      1       2.162762e+03      -2.350886e+03 |        0
      2       2.162762e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 2162.76192501717)
INFO: K-means with 400 data points using 2 iterations
66.7 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -1.845672
INFO: iteration 2, average log likelihood -1.845664
INFO: iteration 3, average log likelihood -1.845664
INFO: iteration 4, average log likelihood -1.845664
INFO: iteration 5, average log likelihood -1.845664
INFO: iteration 6, average log likelihood -1.845664
INFO: iteration 7, average log likelihood -1.845664
INFO: iteration 8, average log likelihood -1.845664
INFO: iteration 9, average log likelihood -1.845664
INFO: iteration 10, average log likelihood -1.845664
INFO: EM with 400 data points 10 iterations avll -1.845664
36.4 data points per parameter
INFO: Initializing GMM, 3 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       5.399472e+03
      1       2.419982e+03      -2.979490e+03 |        2
      2       2.174379e+03      -2.456035e+02 |        2
      3       2.052976e+03      -1.214027e+02 |        2
      4       2.009106e+03      -4.386963e+01 |        2
      5       1.964242e+03      -4.486403e+01 |        2
      6       1.929268e+03      -3.497478e+01 |        2
      7       1.903249e+03      -2.601857e+01 |        2
      8       1.890915e+03      -1.233384e+01 |        2
      9       1.881258e+03      -9.657317e+00 |        2
     10       1.879398e+03      -1.859491e+00 |        2
     11       1.879267e+03      -1.317339e-01 |        0
     12       1.879267e+03       0.000000e+00 |        0
K-means converged with 12 iterations (objv = 1879.2667044382113)
INFO: K-means with 400 data points using 12 iterations
44.4 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 3 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.073097
INFO: iteration 2, average log likelihood -2.063712
INFO: iteration 3, average log likelihood -2.061034
INFO: iteration 4, average log likelihood -2.059899
INFO: iteration 5, average log likelihood -2.059317
INFO: iteration 6, average log likelihood -2.058981
INFO: iteration 7, average log likelihood -2.058771
INFO: iteration 8, average log likelihood -2.058631
INFO: iteration 9, average log likelihood -2.058532
INFO: iteration 10, average log likelihood -2.058460
INFO: EM with 400 data points 10 iterations avll -2.058460
23.5 data points per parameter
INFO: Initializing GMM, 3 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       4.499241e+03
      1       2.035751e+03      -2.463490e+03 |        2
      2       1.844393e+03      -1.913580e+02 |        2
      3       1.729037e+03      -1.153562e+02 |        2
      4       1.664768e+03      -6.426910e+01 |        2
      5       1.623391e+03      -4.137703e+01 |        2
      6       1.595023e+03      -2.836753e+01 |        2
      7       1.579721e+03      -1.530260e+01 |        2
      8       1.567036e+03      -1.268527e+01 |        2
      9       1.566232e+03      -8.038358e-01 |        2
     10       1.566041e+03      -1.905610e-01 |        0
     11       1.566041e+03       0.000000e+00 |        0
K-means converged with 11 iterations (objv = 1566.0411445832556)
INFO: K-means with 400 data points using 11 iterations
44.4 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 3 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.048481
INFO: iteration 2, average log likelihood -2.042271
INFO: iteration 3, average log likelihood -2.040435
INFO: iteration 4, average log likelihood -2.039638
INFO: iteration 5, average log likelihood -2.039221
INFO: iteration 6, average log likelihood -2.038977
INFO: iteration 7, average log likelihood -2.038823
INFO: iteration 8, average log likelihood -2.038720
INFO: iteration 9, average log likelihood -2.038648
INFO: iteration 10, average log likelihood -2.038596
INFO: EM with 400 data points 10 iterations avll -2.038596
23.5 data points per parameter
INFO: Initializing GMM, 3 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.779983e+03
      1       1.396880e+03      -3.831030e+02 |        2
      2       1.296204e+03      -1.006756e+02 |        2
      3       1.251941e+03      -4.426325e+01 |        2
      4       1.240744e+03      -1.119661e+01 |        0
      5       1.240744e+03       0.000000e+00 |        0
K-means converged with 5 iterations (objv = 1240.7441157403418)
INFO: K-means with 400 data points using 5 iterations
44.4 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 3 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -1.846966
INFO: iteration 2, average log likelihood -1.844312
INFO: iteration 3, average log likelihood -1.843706
INFO: iteration 4, average log likelihood -1.843485
INFO: iteration 5, average log likelihood -1.843385
INFO: iteration 6, average log likelihood -1.843333
INFO: iteration 7, average log likelihood -1.843302
INFO: iteration 8, average log likelihood -1.843284
INFO: iteration 9, average log likelihood -1.843271
INFO: iteration 10, average log likelihood -1.843262
INFO: EM with 400 data points 10 iterations avll -1.843262
23.5 data points per parameter
INFO: Initializing GMM, 4 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       2.027605e+03
      1       1.219705e+03      -8.079005e+02 |        3
      2       1.110925e+03      -1.087801e+02 |        3
      3       1.085668e+03      -2.525635e+01 |        2
      4       1.081649e+03      -4.019731e+00 |        2
      5       1.078715e+03      -2.933943e+00 |        2
      6       1.077851e+03      -8.634101e-01 |        0
      7       1.077851e+03       0.000000e+00 |        0
K-means converged with 7 iterations (objv = 1077.8512850137074)
INFO: K-means with 400 data points using 7 iterations
33.3 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 4 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.066971
INFO: iteration 2, average log likelihood -2.059522
INFO: iteration 3, average log likelihood -2.057279
INFO: iteration 4, average log likelihood -2.056240
INFO: iteration 5, average log likelihood -2.055647
INFO: iteration 6, average log likelihood -2.055261
INFO: iteration 7, average log likelihood -2.054985
INFO: iteration 8, average log likelihood -2.054774
INFO: iteration 9, average log likelihood -2.054604
INFO: iteration 10, average log likelihood -2.054461
INFO: EM with 400 data points 10 iterations avll -2.054461
17.4 data points per parameter
INFO: Initializing GMM, 4 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       2.254447e+03
      1       1.599607e+03      -6.548396e+02 |        4
      2       1.526053e+03      -7.355386e+01 |        4
      3       1.482710e+03      -4.334311e+01 |        4
      4       1.453337e+03      -2.937267e+01 |        4
      5       1.437610e+03      -1.572739e+01 |        4
      6       1.424843e+03      -1.276744e+01 |        4
      7       1.423950e+03      -8.926239e-01 |        4
      8       1.423742e+03      -2.076569e-01 |        0
      9       1.423742e+03       0.000000e+00 |        0
K-means converged with 9 iterations (objv = 1423.7423072752404)
INFO: K-means with 400 data points using 9 iterations
33.3 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 4 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.057174
INFO: iteration 2, average log likelihood -2.043711
INFO: iteration 3, average log likelihood -2.039123
INFO: iteration 4, average log likelihood -2.036080
INFO: iteration 5, average log likelihood -2.033800
INFO: iteration 6, average log likelihood -2.032461
INFO: iteration 7, average log likelihood -2.031634
INFO: iteration 8, average log likelihood -2.031089
INFO: iteration 9, average log likelihood -2.030719
INFO: iteration 10, average log likelihood -2.030466
INFO: EM with 400 data points 10 iterations avll -2.030466
17.4 data points per parameter
INFO: Initializing GMM, 4 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.221258e+03
      1       9.960676e+02      -2.251900e+02 |        3
      2       9.319000e+02      -6.416760e+01 |        3
      3       9.224220e+02      -9.478055e+00 |        2
      4       9.203370e+02      -2.085000e+00 |        3
      5       9.193201e+02      -1.016937e+00 |        0
      6       9.193201e+02       0.000000e+00 |        0
K-means converged with 6 iterations (objv = 919.3200542988874)
INFO: K-means with 400 data points using 6 iterations
33.3 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 4 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -1.841782
INFO: iteration 2, average log likelihood -1.840068
INFO: iteration 3, average log likelihood -1.839757
INFO: iteration 4, average log likelihood -1.839661
INFO: iteration 5, average log likelihood -1.839621
INFO: iteration 6, average log likelihood -1.839599
INFO: iteration 7, average log likelihood -1.839584
INFO: iteration 8, average log likelihood -1.839572
INFO: iteration 9, average log likelihood -1.839560
INFO: iteration 10, average log likelihood -1.839549
INFO: EM with 400 data points 10 iterations avll -1.839549
17.4 data points per parameter
INFO: Initializing GMM, 5 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       2.064129e+03
      1       1.473136e+03      -5.909922e+02 |        5
      2       1.381896e+03      -9.123998e+01 |        5
      3       1.336459e+03      -4.543716e+01 |        5
      4       1.290968e+03      -4.549115e+01 |        5
      5       1.249547e+03      -4.142154e+01 |        5
      6       1.193908e+03      -5.563888e+01 |        5
      7       1.147284e+03      -4.662385e+01 |        5
      8       1.121778e+03      -2.550640e+01 |        5
      9       1.100424e+03      -2.135330e+01 |        5
     10       1.082504e+03      -1.792050e+01 |        5
     11       1.064938e+03      -1.756617e+01 |        5
     12       1.039741e+03      -2.519679e+01 |        5
     13       1.020528e+03      -1.921285e+01 |        5
     14       1.013611e+03      -6.916698e+00 |        5
     15       1.009732e+03      -3.878957e+00 |        4
     16       1.008709e+03      -1.023144e+00 |        2
     17       1.008679e+03      -3.027137e-02 |        0
     18       1.008679e+03       0.000000e+00 |        0
K-means converged with 18 iterations (objv = 1008.6788360253636)
INFO: K-means with 400 data points using 18 iterations
26.7 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 5 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.062952
INFO: iteration 2, average log likelihood -2.054566
INFO: iteration 3, average log likelihood -2.052150
INFO: iteration 4, average log likelihood -2.051053
INFO: iteration 5, average log likelihood -2.050428
INFO: iteration 6, average log likelihood -2.050018
INFO: iteration 7, average log likelihood -2.049721
INFO: iteration 8, average log likelihood -2.049490
INFO: iteration 9, average log likelihood -2.049301
INFO: iteration 10, average log likelihood -2.049139
INFO: EM with 400 data points 10 iterations avll -2.049139
13.8 data points per parameter
INFO: Initializing GMM, 5 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.330008e+03
      1       1.070374e+03      -2.596344e+02 |        5
      2       1.033233e+03      -3.714067e+01 |        5
      3       1.011645e+03      -2.158854e+01 |        3
      4       1.001525e+03      -1.012008e+01 |        3
      5       9.865255e+02      -1.499931e+01 |        3
      6       9.777706e+02      -8.754873e+00 |        3
      7       9.693675e+02      -8.403075e+00 |        3
      8       9.622345e+02      -7.133011e+00 |        3
      9       9.572220e+02      -5.012565e+00 |        3
     10       9.525161e+02      -4.705911e+00 |        3
     11       9.428834e+02      -9.632658e+00 |        3
     12       9.279863e+02      -1.489714e+01 |        3
     13       9.196181e+02      -8.368133e+00 |        3
     14       9.162512e+02      -3.366898e+00 |        3
     15       9.124784e+02      -3.772787e+00 |        2
     16       9.123312e+02      -1.472307e-01 |        0
     17       9.123312e+02       0.000000e+00 |        0
K-means converged with 17 iterations (objv = 912.3312165848175)
INFO: K-means with 400 data points using 17 iterations
26.7 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 5 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.052925
INFO: iteration 2, average log likelihood -2.038996
INFO: iteration 3, average log likelihood -2.033803
INFO: iteration 4, average log likelihood -2.031361
INFO: iteration 5, average log likelihood -2.029937
INFO: iteration 6, average log likelihood -2.028999
INFO: iteration 7, average log likelihood -2.028338
INFO: iteration 8, average log likelihood -2.027855
INFO: iteration 9, average log likelihood -2.027490
INFO: iteration 10, average log likelihood -2.027209
INFO: EM with 400 data points 10 iterations avll -2.027209
13.8 data points per parameter
INFO: Initializing GMM, 5 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.361257e+03
      1       9.063186e+02      -4.549381e+02 |        5
      2       9.002513e+02      -6.067355e+00 |        3
      3       8.976158e+02      -2.635467e+00 |        3
      4       8.954073e+02      -2.208544e+00 |        3
      5       8.931700e+02      -2.237268e+00 |        3
      6       8.914833e+02      -1.686760e+00 |        2
      7       8.909720e+02      -5.112851e-01 |        2
      8       8.906234e+02      -3.485786e-01 |        2
      9       8.906046e+02      -1.874453e-02 |        2
     10       8.905777e+02      -2.694399e-02 |        0
     11       8.905777e+02       0.000000e+00 |        0
K-means converged with 11 iterations (objv = 890.5777033708408)
INFO: K-means with 400 data points using 11 iterations
26.7 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 5 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -1.866597
INFO: iteration 2, average log likelihood -1.847483
INFO: iteration 3, average log likelihood -1.842273
INFO: iteration 4, average log likelihood -1.840068
INFO: iteration 5, average log likelihood -1.838909
INFO: iteration 6, average log likelihood -1.838214
INFO: iteration 7, average log likelihood -1.837756
INFO: iteration 8, average log likelihood -1.837433
INFO: iteration 9, average log likelihood -1.837194
INFO: iteration 10, average log likelihood -1.837009
INFO: EM with 400 data points 10 iterations avll -1.837009
13.8 data points per parameter
INFO: Initializing GMM, 6 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.330284e+03
      1       6.986024e+02      -6.316814e+02 |        5
      2       6.401278e+02      -5.847456e+01 |        5
      3       6.295455e+02      -1.058237e+01 |        5
      4       6.240665e+02      -5.479028e+00 |        4
      5       6.216327e+02      -2.433801e+00 |        3
      6       6.202920e+02      -1.340627e+00 |        3
      7       6.201182e+02      -1.738221e-01 |        0
      8       6.201182e+02       0.000000e+00 |        0
K-means converged with 8 iterations (objv = 620.1182014824072)
INFO: K-means with 400 data points using 8 iterations
22.2 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 6 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.054284
INFO: iteration 2, average log likelihood -2.046531
INFO: iteration 3, average log likelihood -2.044151
INFO: iteration 4, average log likelihood -2.043007
INFO: iteration 5, average log likelihood -2.042314
INFO: iteration 6, average log likelihood -2.041830
INFO: iteration 7, average log likelihood -2.041459
INFO: iteration 8, average log likelihood -2.041155
INFO: iteration 9, average log likelihood -2.040897
INFO: iteration 10, average log likelihood -2.040672
INFO: EM with 400 data points 10 iterations avll -2.040672
11.4 data points per parameter
INFO: Initializing GMM, 6 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.600975e+03
      1       9.340277e+02      -6.669477e+02 |        6
      2       8.699684e+02      -6.405929e+01 |        6
      3       8.385330e+02      -3.143543e+01 |        6
      4       8.229759e+02      -1.555709e+01 |        5
      5       8.197077e+02      -3.268214e+00 |        4
      6       8.182413e+02      -1.466322e+00 |        3
      7       8.165274e+02      -1.713939e+00 |        2
      8       8.158777e+02      -6.497450e-01 |        3
      9       8.157692e+02      -1.085035e-01 |        0
     10       8.157692e+02       0.000000e+00 |        0
K-means converged with 10 iterations (objv = 815.7691602213533)
INFO: K-means with 400 data points using 10 iterations
22.2 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 6 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.055365
INFO: iteration 2, average log likelihood -2.038774
INFO: iteration 3, average log likelihood -2.033620
INFO: iteration 4, average log likelihood -2.031226
INFO: iteration 5, average log likelihood -2.029873
INFO: iteration 6, average log likelihood -2.029008
INFO: iteration 7, average log likelihood -2.028405
INFO: iteration 8, average log likelihood -2.027957
INFO: iteration 9, average log likelihood -2.027608
INFO: iteration 10, average log likelihood -2.027324
INFO: EM with 400 data points 10 iterations avll -2.027324
11.4 data points per parameter
INFO: Initializing GMM, 6 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.451421e+03
      1       8.306607e+02      -6.207602e+02 |        6
      2       7.118493e+02      -1.188114e+02 |        6
      3       6.162297e+02      -9.561958e+01 |        6
      4       5.842937e+02      -3.193598e+01 |        5
      5       5.733342e+02      -1.095956e+01 |        5
      6       5.722676e+02      -1.066614e+00 |        5
      7       5.719144e+02      -3.531945e-01 |        0
      8       5.719144e+02       0.000000e+00 |        0
K-means converged with 8 iterations (objv = 571.9143585738548)
INFO: K-means with 400 data points using 8 iterations
22.2 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 6 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -1.856488
INFO: iteration 2, average log likelihood -1.840784
INFO: iteration 3, average log likelihood -1.836360
INFO: iteration 4, average log likelihood -1.834406
INFO: iteration 5, average log likelihood -1.833328
INFO: iteration 6, average log likelihood -1.832648
INFO: iteration 7, average log likelihood -1.832178
INFO: iteration 8, average log likelihood -1.831835
INFO: iteration 9, average log likelihood -1.831574
INFO: iteration 10, average log likelihood -1.831368
INFO: EM with 400 data points 10 iterations avll -1.831368
11.4 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 600 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       2.411528e+05
      1       4.081095e+04      -2.003419e+05 |        2
      2       5.181026e+03      -3.562992e+04 |        2
      3       5.165666e+03      -1.536031e+01 |        0
      4       5.165666e+03       0.000000e+00 |        0
K-means converged with 4 iterations (objv = 5165.6661634557)
INFO: K-means with 600 data points using 4 iterations
100.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.026541
INFO: iteration 2, average log likelihood -2.026538
INFO: iteration 3, average log likelihood -2.026538
INFO: iteration 4, average log likelihood -2.026538
INFO: iteration 5, average log likelihood -2.026538
INFO: iteration 6, average log likelihood -2.026538
INFO: iteration 7, average log likelihood -2.026538
INFO: iteration 8, average log likelihood -2.026538
INFO: iteration 9, average log likelihood -2.026538
INFO: iteration 10, average log likelihood -2.026538
INFO: EM with 600 data points 10 iterations avll -2.026538
54.5 data points per parameter
Testing ../examples/Feature_Stacker.ipynb
WARNING: replacing module Testing
Fitting 3 folds for each of 18 candidates, totalling 54 fits
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=1
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=1, score=0.96078  -  0.1s
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=1
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=1, score=0.90196  -  0.0s
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=1
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=1, score=0.97917  -  0.0s
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=1
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=1, score=0.94118  -  0.0s
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=1
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=1, score=0.92157  -  0.0s
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=1
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=1, score=0.97917  -  0.0s
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=1
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=1, score=0.96078  -  0.0s
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=1
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=1, score=0.92157  -  0.0s
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=1
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=1, score=0.97917  -  0.0s
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=2
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=2, score=0.96078  -  0.0s
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=2
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=2, score=0.92157  -  0.0s
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=2
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=2, score=0.97917  -  0.0s
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=2
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=2, score=0.96078  -  0.0s
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=2
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=2, score=0.92157  -  0.0s
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=2
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=2, score=1.00000  -  0.0s
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=2
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=2, score=0.98039  -  0.0s
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=2
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=2, score=0.90196  -  0.0s
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=2
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=2, score=1.00000  -  0.0s
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=1
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=1, score=0.96078  -  0.0s
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=1
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=1, score=0.90196  -  0.0s
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=1
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=1, score=0.97917  -  0.0s
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=1
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=1, score=0.98039  -  0.0s
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=1
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=1, score=0.94118  -  0.0s
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=1
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=1, score=0.97917  -  0.0s
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=1
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=1, score=0.98039  -  0.0s
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=1
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=1, score=0.94118  -  0.0s
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=1
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=1, score=0.97917  -  0.0s
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=2
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=2, score=0.98039  -  0.0s
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=2
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=2, score=0.94118  -  0.0s
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=2
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=2, score=0.97917  -  0.0s
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=2
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=2, score=1.00000  -  0.0s
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=2
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=2, score=0.96078  -  0.0s
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=2
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=2, score=0.97917  -  0.0s
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=2
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=2, score=0.98039  -  0.0s
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=2
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=2, score=0.92157  -  0.0s
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=2
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=2, score=1.00000  -  0.0s
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=1
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=1, score=0.98039  -  0.0s
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=1
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=1, score=0.94118  -  0.0s
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=1
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=1, score=0.97917  -  0.0s
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=1
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=1, score=1.00000  -  0.0s
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=1
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=1, score=0.94118  -  0.0s
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=1
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=1, score=0.97917  -  0.0s
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=1
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=1, score=1.00000  -  0.0s
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=1
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=1, score=0.92157  -  0.0s
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=1
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=1, score=1.00000  -  0.0s
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=2
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=2, score=0.98039  -  0.0s
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=2
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=2, score=0.94118  -  0.0s
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=2
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=2, score=0.97917  -  0.0s
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=2
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=2, score=1.00000  -  0.0s
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=2
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=2, score=0.96078  -  0.0s
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=2
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=2, score=0.97917  -  0.0s
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=2
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=2, score=1.00000  -  0.0s
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=2
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=2, score=0.92157  -  0.0s
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=2
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=2, score=1.00000  -  0.0s
ScikitLearn.Skcore.Pipeline(Tuple{Any,Any}[("features",ScikitLearn.Skcore.FeatureUnion(Tuple{Any,Any}[("pca",PyObject PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)),("univ_select",PyObject SelectKBest(k=2, score_func=<function f_classif at 0x7f5e9c2ee9b0>))],1,nothing)),("svm",PyObject SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False))],Any[ScikitLearn.Skcore.FeatureUnion(Tuple{Any,Any}[("pca",PyObject PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)),("univ_select",PyObject SelectKBest(k=2, score_func=<function f_classif at 0x7f5e9c2ee9b0>))],1,nothing),PyObject SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)])Testing ../examples/Gaussian_Processes_Julia.ipynb
WARNING: replacing module Testing
(get_params(best_gp))[:logNoise] = 2.5999999999999996
(get_params(best_gp))[:k_lÏƒ] = 4.8
Testing ../examples/Outlier_Detection.ipynb
WARNING: replacing module Testing
Testing ../examples/Pipeline_PCA_Logistic.ipynb
WARNING: replacing module Testing
Testing ../examples/Plot_Kmeans_Digits.ipynb
WARNING: replacing module Testing
n_digits: 10, 	 n_samples 1797, 	 n_features 64
____________________________________________________________________________
init    time  inertia    homo   compl  v-meas     ARI AMI  silhouette
k-means++   0.24s    69434   0.601   0.650   0.625   0.467   0.597    0.124
   random   0.21s    69441   0.602   0.650   0.625   0.464   0.598    0.137
PCA-based   0.04s    70804   0.670   0.697   0.683   0.561   0.667    0.133
____________________________________________________________________________Testing ../examples/Plot_Kmeans_Digits_Julia.ipynb
WARNING: replacing module Testing
n_digits: 10, 	 n_samples 1797, 	 n_features 64
____________________________________________________________________________
init    time  inertia    homo   compl  v-meas     ARI AMI  silhouette
k-means++   25.64s    NaN   0.189   0.229   0.207   0.108   0.181    -0.061
________________________________________________________________________Fitting GLRM
Iteration 10: objective value = 215.5748965962755
Fitting GLRM
Iteration 10: objective value = 1508.522178988117
Iteration 20: objective value = 1470.7605920428118
  2.962115 seconds (30.32 M allocations: 860.945 MB, 13.29% gc time)
Testing ../examples/Randomized_Search.ipynb
WARNING: replacing module Testing
RandomizedSearchCV took 9.30 seconds for 20 candidates, parameter settings.
Model with rank:1
Mean validation score: 0.927 (std: 0.012)
Parameters: Dict{Symbol,Any}(Pair{Symbol,Any}(:max_features,6),Pair{Symbol,Any}(:bootstrap,false),Pair{Symbol,Any}(:min_samples_split,7),Pair{Symbol,Any}(:max_depth,nothing),Pair{Symbol,Any}(:criterion,"gini"),Pair{Symbol,Any}(:min_samples_leaf,4))

Model with rank:2
Mean validation score: 0.922 (std: 0.007)
Parameters: Dict{Symbol,Any}(Pair{Symbol,Any}(:max_features,4),Pair{Symbol,Any}(:bootstrap,false),Pair{Symbol,Any}(:min_samples_split,8),Pair{Symbol,Any}(:max_depth,nothing),Pair{Symbol,Any}(:criterion,"entropy"),Pair{Symbol,Any}(:min_samples_leaf,6))

Model with rank:3
Mean validation score: 0.916 (std: 0.023)
Parameters: Dict{Symbol,Any}(Pair{Symbol,Any}(:max_features,10),Pair{Symbol,Any}(:bootstrap,false),Pair{Symbol,Any}(:min_samples_split,8),Pair{Symbol,Any}(:max_depth,nothing),Pair{Symbol,Any}(:criterion,"gini"),Pair{Symbol,Any}(:min_samples_leaf,8))

GridSearchCV took 96.25 seconds for 216 candidate parameter settings.
Model with rank:1
Mean validation score: 0.933 (std: 0.009)
Parameters: Dict{Symbol,Any}(Pair{Symbol,Any}(:max_features,10),Pair{Symbol,Any}(:bootstrap,false),Pair{Symbol,Any}(:min_samples_split,3),Pair{Symbol,Any}(:max_depth,nothing),Pair{Symbol,Any}(:min_samples_leaf,1),Pair{Symbol,Any}(:criterion,"entropy"))

Model with rank:2
Mean validation score: 0.932 (std: 0.014)
Parameters: Dict{Symbol,Any}(Pair{Symbol,Any}(:max_features,10),Pair{Symbol,Any}(:bootstrap,false),Pair{Symbol,Any}(:min_samples_split,2),Pair{Symbol,Any}(:max_depth,nothing),Pair{Symbol,Any}(:min_samples_leaf,3),Pair{Symbol,Any}(:criterion,"entropy"))

Model with rank:3
Mean validation score: 0.932 (std: 0.014)
Parameters: Dict{Symbol,Any}(Pair{Symbol,Any}(:max_features,10),Pair{Symbol,Any}(:bootstrap,false),Pair{Symbol,Any}(:min_samples_split,3),Pair{Symbol,Any}(:max_depth,nothing),Pair{Symbol,Any}(:min_samples_leaf,3),Pair{Symbol,Any}(:criterion,"entropy"))

(predict(random_search,X))[1:10] = [0,1,2,3,4,5,6,7,8,9]
(predict_proba(random_search,X))[1:10] = [1.0,0.0,0.02,0.0,0.144167,0.0,0.0125,0.0,0.0,0.0]
score(random_search,X,y) = 0.9994435169727324
Testing ../examples/RBM.ipynb
WARNING: replacing module Testing
Logistic regression using RBM features:
             precision    recall  f1-score   support

          0       0.99      0.98      0.99       174
          1       0.94      0.95      0.94       184
          2       0.95      0.98      0.96       166
          3       0.96      0.87      0.91       194
          4       0.96      0.95      0.96       186
          5       0.90      0.92      0.91       181
          6       0.99      0.98      0.98       207
          7       0.94      0.98      0.96       154
          8       0.89      0.90      0.89       182
          9       0.90      0.92      0.91       169

avg / total       0.94      0.94      0.94      1797

Logistic regression using raw pixel features:
             precision    recall  f1-score   support

          0       0.86      0.94      0.90       174
          1       0.55      0.53      0.54       184
          2       0.79      0.86      0.82       166
          3       0.78      0.73      0.75       194
          4       0.86      0.83      0.85       186
          5       0.79      0.77      0.78       181
          6       0.89      0.89      0.89       207
          7       0.85      0.93      0.89       154
          8       0.65      0.60      0.63       182
          9       0.71      0.72      0.72       169

avg / total       0.77      0.78      0.78      1797

Testing ../examples/Simple_1D_Kernel_Density.ipynb
WARNING: replacing module Testing
WARNING: the no-op `transpose` fallback is deprecated, and no more specific `transpose` method for Ptr{PyCall.PyObject_struct} exists. Consider `permutedims(x, [2, 1])` or writing a specific `transpose(x::Ptr{PyCall.PyObject_struct})` method if appropriate.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in transpose(::Ptr{PyCall.PyObject_struct}) at ./deprecated.jl:770
 in transpose_f!(::Base.#transpose, ::Array{Ptr{PyCall.PyObject_struct},2}, ::Array{Ptr{PyCall.PyObject_struct},2}) at ./arraymath.jl:369
 in transpose(::Array{Ptr{PyCall.PyObject_struct},2}) at ./arraymath.jl:407
 in copy(::PyCall.PyArray{Ptr{PyCall.PyObject_struct},2}) at /home/vagrant/.julia/v0.5/PyCall/src/numpy.jl:340
 in convert(::Type{Array{Ptr{PyCall.PyObject_struct},N}}, ::PyCall.PyObject) at /home/vagrant/.julia/v0.5/PyCall/src/numpy.jl:456
 in convert(::Type{Array{PyCall.PyObject,N}}, ::PyCall.PyObject) at /home/vagrant/.julia/v0.5/PyCall/src/numpy.jl:487
 in (::PyCall.##12#13{DataType,PyCall.PyObject})(::Int64) at /home/vagrant/.julia/v0.5/PyCall/src/conversions.jl:181
 in ntuple(::PyCall.##12#13{DataType,PyCall.PyObject}, ::Int64) at ./tuple.jl:65
 in convert(::Type{Tuple{PyPlot.Figure,Array{PyCall.PyObject,N}}}, ::PyCall.PyObject) at /home/vagrant/.julia/v0.5/PyCall/src/conversions.jl:181
 in convert(::Type{PyCall.PyAny}, ::PyCall.PyObject) at /home/vagrant/.julia/v0.5/PyCall/src/conversions.jl:798
 in #pycall#70(::Array{Any,1}, ::Function, ::PyCall.PyObject, ::Type{PyCall.PyAny}, ::Int64, ::Vararg{Int64,N}) at /home/vagrant/.julia/v0.5/PyCall/src/PyCall.jl:572
 in (::PyCall.#kw##pycall)(::Array{Any,1}, ::PyCall.#pycall, ::PyCall.PyObject, ::Type{PyCall.PyAny}, ::Int64, ::Vararg{Int64,N}) at ./<missing>:0
 in #call#71(::Array{Any,1}, ::PyCall.PyObject, ::Int64, ::Vararg{Int64,N}) at /home/vagrant/.julia/v0.5/PyCall/src/PyCall.jl:575
 in (::PyCall.#kw#PyObject)(::Array{Any,1}, ::PyCall.PyObject, ::Int64, ::Vararg{Int64,N}) at ./<missing>:0
 in include_string(::String, ::String) at ./loading.jl:441
 in my_include_string(::String, ::String, ::String) at /home/vagrant/.julia/v0.5/NBInclude/src/NBInclude.jl:31
 in #nbinclude#1(::Bool, ::UnitRange{Int64}, ::Regex, ::Function, ::String) at /home/vagrant/.julia/v0.5/NBInclude/src/NBInclude.jl:109
 in nbinclude(::String) at /home/vagrant/.julia/v0.5/NBInclude/src/NBInclude.jl:73
 in eval(::Module, ::Any) at ./boot.jl:234
 in macro expansion; at /home/vagrant/.julia/v0.5/ScikitLearn/test/run_examples.jl:6 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:488 (repeats 2 times)
 in process_options(::Base.JLOptions) at ./client.jl:262
 in _start() at ./client.jl:318
while loading /home/vagrant/.julia/v0.5/ScikitLearn/test/../examples/Simple_1D_Kernel_Density.ipynb:In[2], in expression starting on line 10
Testing ../examples/Text_Feature_Extraction.ipynb
WARNING: replacing module Testing
Loading 20 newsgroups dataset for categories:String["alt.atheism","talk.religion.misc"]No handlers could be found for logger "sklearn.datasets.twenty_newsgroups"
857 documents
2 categories

Performing grid search...
pipeline:String["vect","tfidf","clf"]
parameters:
Dict{String,Any}(Pair{String,Any}("vect__max_df",(0.5,0.75,1.0)),Pair{String,Any}("clf__penalty",("l2","elasticnet")),Pair{String,Any}("clf__alpha",(1.0e-5,1.0e-6)),Pair{String,Any}("vect__ngram_range",((1,1),(1,2))))
Fitting 3 folds for each of 24 candidates, totalling 72 fits
done in 17.016s
Best score: 0.840
Best parameters set:
	vect__max_df: 0.5
	clf__penalty: l2
	clf__alpha: 1.0e-5
	vect__ngram_range: (1,2)
Testing ../examples/Two_Class_Adaboost.ipynb
WARNING: replacing module Testing
Testing ../examples/Underfitting_vs_Overfitting.ipynb
WARNING: replacing module Testing
[BernoulliRBM] Iteration 1, pseudo-likelihood = -25.54, time = 0.43s
[BernoulliRBM] Iteration 2, pseudo-likelihood = -23.94, time = 0.81s
[BernoulliRBM] Iteration 3, pseudo-likelihood = -22.98, time = 0.54s
[BernoulliRBM] Iteration 4, pseudo-likelihood = -21.98, time = 0.41s
[BernoulliRBM] Iteration 5, pseudo-likelihood = -21.48, time = 0.42s
[BernoulliRBM] Iteration 6, pseudo-likelihood = -21.06, time = 0.45s
[BernoulliRBM] Iteration 7, pseudo-likelihood = -20.93, time = 0.45s
[BernoulliRBM] Iteration 8, pseudo-likelihood = -20.49, time = 0.45s
[BernoulliRBM] Iteration 9, pseudo-likelihood = -20.27, time = 0.81s
[BernoulliRBM] Iteration 10, pseudo-likelihood = -20.18, time = 0.74s
[BernoulliRBM] Iteration 11, pseudo-likelihood = -20.02, time = 0.81s
[BernoulliRBM] Iteration 12, pseudo-likelihood = -19.73, time = 0.57s
[BernoulliRBM] Iteration 13, pseudo-likelihood = -19.86, time = 0.88s
[BernoulliRBM] Iteration 14, pseudo-likelihood = -19.64, time = 0.83s
[BernoulliRBM] Iteration 15, pseudo-likelihood = -19.61, time = 0.82s
[BernoulliRBM] Iteration 16, pseudo-likelihood = -19.40, time = 0.58s
[BernoulliRBM] Iteration 17, pseudo-likelihood = -19.19, time = 0.45s
[BernoulliRBM] Iteration 18, pseudo-likelihood = -19.16, time = 0.84s
[BernoulliRBM] Iteration 19, pseudo-likelihood = -19.13, time = 0.54s
[BernoulliRBM] Iteration 20, pseudo-likelihood = -19.08, time = 0.41s
INFO: ScikitLearn tests passed
INFO: Removing ArrayViews v0.6.4
INFO: Removing Blosc v0.2.0
INFO: Removing Calculus v0.2.0
INFO: Removing Clustering v0.7.0
INFO: Removing ColorTypes v0.3.4
INFO: Removing Colors v0.7.3
INFO: Removing Combinatorics v0.3.2
INFO: Removing DataArrays v0.3.12
INFO: Removing DataFrames v0.8.5
INFO: Removing DecisionTree v0.5.1
INFO: Removing DiffBase v0.0.4
INFO: Removing Distances v0.4.1
INFO: Removing Distributions v0.12.1
INFO: Removing FileIO v0.3.0
INFO: Removing FixedPointNumbers v0.3.4
INFO: Removing ForwardDiff v0.3.4
INFO: Removing GZip v0.2.20
INFO: Removing GaussianMixtures v0.1.0
INFO: Removing GaussianProcesses v0.4.0
INFO: Removing HDF5 v0.7.3
INFO: Removing Hiccup v0.1.1
INFO: Removing JLD v0.6.9
INFO: Removing Juno v0.2.6
INFO: Removing LaTeXStrings v0.2.0
INFO: Removing LegacyStrings v0.2.0
INFO: Removing LineSearches v0.1.5
INFO: Removing LowRankModels v0.2.0
INFO: Removing Media v0.2.5
INFO: Removing NBInclude v1.1.0
INFO: Removing NMF v0.2.5
INFO: Removing NaNMath v0.2.2
INFO: Removing NearestNeighbors v0.2.0
INFO: Removing Optim v0.7.7
INFO: Removing PDMats v0.5.6
INFO: Removing PolynomialFactors v0.0.3
INFO: Removing Polynomials v0.1.2
INFO: Removing PositiveFactorizations v0.0.4
INFO: Removing Primes v0.1.2
INFO: Removing PyPlot v2.3.1
INFO: Removing QuadGK v0.1.1
INFO: Removing RData v0.0.4
INFO: Removing RDatasets v0.2.0
INFO: Removing Reexport v0.0.3
INFO: Removing Rmath v0.1.6
INFO: Removing Roots v0.3.0
INFO: Removing SortingAlgorithms v0.1.0
INFO: Removing StaticArrays v0.3.0
INFO: Removing StatsFuns v0.4.0

>>> End of log
