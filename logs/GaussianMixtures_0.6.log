>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.6.6
INFO: Installing JLD v0.6.4
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.0
INFO: Installing Rmath v0.1.3
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.1.1
INFO: Installing StaticArrays v0.0.8
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.787
Commit c71f205 (2016-09-26 16:28 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-96-generic #143-Ubuntu SMP Mon Aug 29 20:15:20 UTC 2016 x86_64 x86_64
Memory: 2.9392929077148438 GB (690.12109375 MB free)
Uptime: 25003.0 sec
Load Avg:  1.01708984375  1.01708984375  1.04541015625
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3498 MHz    1528001 s       2128 s     185640 s     529952 s         63 s
#2  3498 MHz     641750 s       7658 s     101304 s    1644449 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.2
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.6.6
 - JLD                           0.6.4
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.0
 - Rmath                         0.1.3
 - SHA                           0.2.1
 - ScikitLearnBase               0.1.1
 - StaticArrays                  0.0.8
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:345
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:378
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:346
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1739
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-2.769463217911615e6,[2436.01,97564.0],
[3771.27 -2746.91 -431.161; -3664.26 3203.23 -251.509],

Array{Float64,2}[
[6909.48 -3294.02 -672.982; -3294.02 4712.63 1783.56; -672.982 1783.56 3308.83],

[92413.1 3371.2 588.655; 3371.2 95901.0 -1080.96; 588.655 -1080.96 97267.6]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.260697e+03
      1       8.332744e+02      -4.274228e+02 |        5
      2       7.937434e+02      -3.953103e+01 |        2
      3       7.909377e+02      -2.805673e+00 |        0
      4       7.909377e+02       0.000000e+00 |        0
K-means converged with 4 iterations (objv = 790.9376803155674)
INFO: K-means with 272 data points using 4 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.049088
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.755067
INFO: iteration 2, lowerbound -3.613334
INFO: iteration 3, lowerbound -3.453806
INFO: iteration 4, lowerbound -3.269444
INFO: iteration 5, lowerbound -3.082005
INFO: iteration 6, lowerbound -2.927729
INFO: dropping number of Gaussions to 7
INFO: iteration 7, lowerbound -2.834276
INFO: dropping number of Gaussions to 6
INFO: iteration 8, lowerbound -2.796803
INFO: dropping number of Gaussions to 4
INFO: iteration 9, lowerbound -2.774529
INFO: dropping number of Gaussions to 3
INFO: iteration 10, lowerbound -2.752697
INFO: iteration 11, lowerbound -2.735077
INFO: iteration 12, lowerbound -2.716174
INFO: iteration 13, lowerbound -2.691522
INFO: iteration 14, lowerbound -2.660774
INFO: iteration 15, lowerbound -2.624314
INFO: iteration 16, lowerbound -2.583402
INFO: iteration 17, lowerbound -2.540141
INFO: iteration 18, lowerbound -2.497084
INFO: iteration 19, lowerbound -2.456436
INFO: iteration 20, lowerbound -2.419245
INFO: iteration 21, lowerbound -2.385316
INFO: iteration 22, lowerbound -2.354346
INFO: iteration 23, lowerbound -2.328053
INFO: iteration 24, lowerbound -2.311180
INFO: iteration 25, lowerbound -2.307871
INFO: dropping number of Gaussions to 2
INFO: iteration 26, lowerbound -2.302916
INFO: iteration 27, lowerbound -2.299259
INFO: iteration 28, lowerbound -2.299256
INFO: iteration 29, lowerbound -2.299254
INFO: iteration 30, lowerbound -2.299254
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Sun 09 Oct 2016 11:26:29 AM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Sun 09 Oct 2016 11:26:31 AM UTC: K-means with 272 data points using 4 iterations
11.3 data points per parameter
,Sun 09 Oct 2016 11:26:32 AM UTC: EM with 272 data points 0 iterations avll -2.049088
5.8 data points per parameter
,Sun 09 Oct 2016 11:26:33 AM UTC: GMM converted to Variational GMM
,Sun 09 Oct 2016 11:26:36 AM UTC: iteration 1, lowerbound -3.755067
,Sun 09 Oct 2016 11:26:36 AM UTC: iteration 2, lowerbound -3.613334
,Sun 09 Oct 2016 11:26:36 AM UTC: iteration 3, lowerbound -3.453806
,Sun 09 Oct 2016 11:26:36 AM UTC: iteration 4, lowerbound -3.269444
,Sun 09 Oct 2016 11:26:36 AM UTC: iteration 5, lowerbound -3.082005
,Sun 09 Oct 2016 11:26:36 AM UTC: iteration 6, lowerbound -2.927729
,Sun 09 Oct 2016 11:26:36 AM UTC: dropping number of Gaussions to 7
,Sun 09 Oct 2016 11:26:36 AM UTC: iteration 7, lowerbound -2.834276
,Sun 09 Oct 2016 11:26:36 AM UTC: dropping number of Gaussions to 6
,Sun 09 Oct 2016 11:26:36 AM UTC: iteration 8, lowerbound -2.796803
,Sun 09 Oct 2016 11:26:37 AM UTC: dropping number of Gaussions to 4
,Sun 09 Oct 2016 11:26:37 AM UTC: iteration 9, lowerbound -2.774529
,Sun 09 Oct 2016 11:26:37 AM UTC: dropping number of Gaussions to 3
,Sun 09 Oct 2016 11:26:37 AM UTC: iteration 10, lowerbound -2.752697
,Sun 09 Oct 2016 11:26:37 AM UTC: iteration 11, lowerbound -2.735077
,Sun 09 Oct 2016 11:26:37 AM UTC: iteration 12, lowerbound -2.716174
,Sun 09 Oct 2016 11:26:37 AM UTC: iteration 13, lowerbound -2.691522
,Sun 09 Oct 2016 11:26:37 AM UTC: iteration 14, lowerbound -2.660774
,Sun 09 Oct 2016 11:26:37 AM UTC: iteration 15, lowerbound -2.624314
,Sun 09 Oct 2016 11:26:37 AM UTC: iteration 16, lowerbound -2.583402
,Sun 09 Oct 2016 11:26:37 AM UTC: iteration 17, lowerbound -2.540141
,Sun 09 Oct 2016 11:26:37 AM UTC: iteration 18, lowerbound -2.497084
,Sun 09 Oct 2016 11:26:37 AM UTC: iteration 19, lowerbound -2.456436
,Sun 09 Oct 2016 11:26:37 AM UTC: iteration 20, lowerbound -2.419245
,Sun 09 Oct 2016 11:26:38 AM UTC: iteration 21, lowerbound -2.385316
,Sun 09 Oct 2016 11:26:38 AM UTC: iteration 22, lowerbound -2.354346
,Sun 09 Oct 2016 11:26:38 AM UTC: iteration 23, lowerbound -2.328053
,Sun 09 Oct 2016 11:26:38 AM UTC: iteration 24, lowerbound -2.311180
,Sun 09 Oct 2016 11:26:38 AM UTC: iteration 25, lowerbound -2.307871
,Sun 09 Oct 2016 11:26:38 AM UTC: dropping number of Gaussions to 2
,Sun 09 Oct 2016 11:26:38 AM UTC: iteration 26, lowerbound -2.302916
,Sun 09 Oct 2016 11:26:38 AM UTC: iteration 27, lowerbound -2.299259
,Sun 09 Oct 2016 11:26:38 AM UTC: iteration 28, lowerbound -2.299256
,Sun 09 Oct 2016 11:26:38 AM UTC: iteration 29, lowerbound -2.299254
,Sun 09 Oct 2016 11:26:38 AM UTC: iteration 30, lowerbound -2.299254
,Sun 09 Oct 2016 11:26:38 AM UTC: iteration 31, lowerbound -2.299253
,Sun 09 Oct 2016 11:26:38 AM UTC: iteration 32, lowerbound -2.299253
,Sun 09 Oct 2016 11:26:38 AM UTC: iteration 33, lowerbound -2.299253
,Sun 09 Oct 2016 11:26:38 AM UTC: iteration 34, lowerbound -2.299253
,Sun 09 Oct 2016 11:26:39 AM UTC: iteration 35, lowerbound -2.299253
,Sun 09 Oct 2016 11:26:39 AM UTC: iteration 36, lowerbound -2.299253
,Sun 09 Oct 2016 11:26:39 AM UTC: iteration 37, lowerbound -2.299253
,Sun 09 Oct 2016 11:26:39 AM UTC: iteration 38, lowerbound -2.299253
,Sun 09 Oct 2016 11:26:39 AM UTC: iteration 39, lowerbound -2.299253
,Sun 09 Oct 2016 11:26:39 AM UTC: iteration 40, lowerbound -2.299253
,Sun 09 Oct 2016 11:26:39 AM UTC: iteration 41, lowerbound -2.299253
,Sun 09 Oct 2016 11:26:39 AM UTC: iteration 42, lowerbound -2.299253
,Sun 09 Oct 2016 11:26:39 AM UTC: iteration 43, lowerbound -2.299253
,Sun 09 Oct 2016 11:26:39 AM UTC: iteration 44, lowerbound -2.299253
,Sun 09 Oct 2016 11:26:39 AM UTC: iteration 45, lowerbound -2.299253
,Sun 09 Oct 2016 11:26:39 AM UTC: iteration 46, lowerbound -2.299253
,Sun 09 Oct 2016 11:26:39 AM UTC: iteration 47, lowerbound -2.299253
,Sun 09 Oct 2016 11:26:39 AM UTC: iteration 48, lowerbound -2.299253
,Sun 09 Oct 2016 11:26:40 AM UTC: iteration 49, lowerbound -2.299253
,Sun 09 Oct 2016 11:26:40 AM UTC: iteration 50, lowerbound -2.299253
,Sun 09 Oct 2016 11:26:40 AM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.9549,178.045]
β = [95.9549,178.045]
m = [2.00023 53.852; 4.2503 79.2869]
ν = [97.9549,180.045]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.375876 -0.00895312; 0.0 0.0127487],

[0.184042 -0.00764405; 0.0 0.00858171]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 99999.99999999996
avll from stats: -0.9920625235806323
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9920625235806299
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9920625235806297
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -0.9754124321708904
avll from llpg:  -0.9754124321708904
avll direct:     -0.9754124321708904
sum posterior: 100000.0
32×26 Array{Float64,2}:
 -0.132599     0.140706    -0.287116    -0.0523399    -0.0302935   -0.0157729  -0.138709     0.142294   -0.0724397    0.0622192    0.0649192   -0.0322107   -0.0665672    0.000355867  -0.0436703  -0.0309968   -0.0992977    0.0786596   -0.111799    -0.115709    -0.103433     0.061816     0.0327916    -0.0538324    0.00354839   -0.218108  
 -0.00126103   0.063236    -0.0484623    0.0954692     0.0402217   -0.0166278   0.00100432   0.0415681  -0.00824257   0.0291075    0.0732934   -0.140895    -0.13358     -0.069107      0.026867   -0.101258     0.0488854    0.0811914    0.0171691   -0.0769531    0.0369495    0.217713     0.0453887     0.104313    -0.0713288    -0.041088  
 -0.0652619    0.00476447   0.00309848  -0.0872485    -0.151968     0.0339503  -0.00294032  -0.151904    0.0679281   -0.0400597   -0.0779877   -0.0371463    0.104334    -0.139963     -0.0588277   0.021859    -0.126789     0.250017     0.124966     0.0064409   -0.0111114   -0.128748    -0.0989815    -0.0704276    0.00760258   -0.0369751 
 -0.0278984   -0.00582866  -0.112517     0.0640693     0.0615674    0.0293865  -0.054365     0.0609215  -0.124549     0.028046     0.0911757   -0.109696    -0.0629129    0.113872     -0.0625486  -0.0158371   -0.142336    -0.0980844   -0.0615989    0.0895345   -0.115402    -0.166802     0.0491791     0.0535252    0.0372546    -0.0119849 
  0.0979103   -0.0081898    0.187733    -0.214737     -0.108845     0.10087     0.061088    -0.0522761  -0.148229     0.00965383  -0.177057     0.0640143    0.0686185   -0.00309271   -0.0599421   0.244723    -0.09956     -0.10418      0.0533896    0.0960018   -0.121225    -0.0779837    0.0309949    -0.010637    -0.0195097     0.0888618 
  0.130134    -0.0348046    0.0670975    0.0297999    -0.0323139   -0.0118736   0.016744     0.0923763  -0.0550287    0.213861    -0.117667     0.123941    -0.00517793   0.0237456    -0.0151913   0.0578811    0.0954653   -0.022834    -0.00832912  -0.0716151    0.010981    -0.142615    -0.0270018     0.0036879   -0.0484516    -0.116574  
 -0.0618335    0.0792355   -0.0629522   -0.00615505    0.0189644    0.062126   -0.0329436    0.197709   -0.0601       0.0699811   -0.0873418   -0.0610181   -0.163553    -0.0368845    -0.0170246   0.162081    -0.134673     0.0789124   -0.0323051    0.034126    -0.0342111   -0.026156    -0.011695      0.0750162   -0.0219678     0.0285797 
 -0.0159951    0.0558606    0.0397702    0.0798152     0.0017897    0.0290402   0.0762232   -0.133955    0.101988    -0.0549534    0.0929837   -0.0673724   -0.137919     0.0554989     0.0131979   0.0291732    0.00580942   0.176881    -0.00176109   0.0817379   -0.0558945   -0.0983428    0.0450262    -0.0614185    0.0231418     0.0357793 
 -0.139714     0.0612814    0.0767177    0.249013      0.0963187   -0.074087   -0.206868    -0.169113   -0.0911982   -0.270425     0.0367016   -0.0212067   -0.0602726    0.0310127     0.0507644  -0.0716523   -0.113569    -0.0261839   -0.0227803    0.145341    -0.167687    -0.00755461  -0.10552       0.141788    -0.0855516    -0.0579588 
  0.0752512    0.00504762   0.0067548   -0.205146      0.148983    -0.151385   -0.0465633   -0.0971213  -0.0250179   -0.148686    -0.10908      0.0118704   -0.132059    -0.0497191     0.137827   -0.0150509   -0.0135931    0.0303166   -0.0300177    0.0519289    0.0899093   -0.202161     0.125955     -0.00511316   0.0342249    -0.0613222 
  0.039658     0.0224812   -0.0486462    0.00507733    0.220034     0.148959   -0.172432     0.0324426  -0.0176895    0.10375      0.0169898    0.0555158   -0.0318001    0.0942099    -0.0423497  -0.0134731   -0.10024      0.0167599   -0.214943     0.0281532    0.0626826    0.165902    -0.0710092     0.024926    -0.235244      0.0961737 
 -0.104075     0.0852412   -0.061949     0.0417545     0.135525     0.0719763   0.1211      -0.11541     0.0955549   -0.0291264   -0.126214    -0.0984442    0.0094404   -0.0236448    -0.13433     0.114364     0.114195     0.00658501   0.0646687   -0.00655094   0.0522519    0.0215071    0.0376611    -0.115938    -0.0557692    -0.0458292 
  0.0456165    0.195924     0.255141     0.000203384   0.121827    -0.10263    -0.0306534   -0.104489   -0.176257     0.077731    -0.013094     0.0482708   -0.0644949   -0.0373465     0.0488506  -0.0717453    0.123844    -0.0109288    0.0297209    0.0328697   -0.152376     0.153002     0.176175      0.0089593    0.172133     -0.12012   
  0.0181788    0.0691702   -0.183428    -0.023169      0.128862     0.208977   -0.0726033   -0.18517     0.0513465    0.101496    -0.0171038    0.0718555    0.0173722   -0.00964647   -0.0689751   0.133565    -0.121677    -0.0883668   -0.069106     0.0700816    0.165845     0.0622556   -0.000512566  -0.0625288    0.0699551     0.106037  
 -0.129509     0.0342863   -0.0681873    0.0792534    -0.263642    -0.159069   -0.0696853   -0.0523095  -0.0770028    0.0140055    0.0103217    0.131024     0.0875421   -0.0241226    -0.202054   -0.0338779    0.0857681    0.0436224   -0.0265708   -0.0619688   -0.0271486    0.164774    -0.0373536     0.0524649   -0.228694     -0.0942899 
 -0.0428583   -0.0430476   -0.122939     0.0548667    -0.0211568    0.0630082   0.148964     0.0828471  -0.115071     0.00497947   0.0418934    0.0699741   -0.0793002    0.090371      0.0796677   0.0189829    0.203724     0.0821089   -0.144966    -0.0478339   -0.0184767   -0.00392981   0.0731879    -0.00539666   0.095406     -0.00964167
 -0.166226    -0.074485    -0.21159     -0.11586       0.0719554   -0.206263   -0.0349018    0.020514    0.0221325   -0.0345843    0.0499593   -0.180206    -0.0962149    0.183106     -0.0156503   0.0186644    0.0881164   -0.10019     -0.186979     0.0405743    0.0161108   -0.123594     0.103565     -0.133891     0.0907035     0.0767633 
  0.0454222   -0.0117734   -0.0337204   -0.0862584     0.116519    -0.113343   -0.100117     0.0876118  -0.0892931   -0.068037     0.20632     -0.0178212    0.0932291   -0.0066733     0.164705    0.0396439    0.0297508   -0.0701725   -0.0234032   -0.138433    -0.0168605   -0.0773102   -0.0565633     0.0640448    0.0151531     0.0619532 
 -0.145206     0.0351581    0.0213786    0.0330267    -0.0947933   -0.105209   -0.100708     0.149236   -0.0355507    0.138817     0.0367839    0.0868771    0.0329545   -0.105232     -0.157939    0.0743542    0.00190298  -0.0807679    0.145854     0.0279313    0.201779    -0.0397006    0.108934      0.0773646    0.0337372    -0.270916  
 -0.0315275    0.210373    -0.00769335   0.0526715    -0.0172453    0.134638   -0.114974    -0.0588242  -0.105299    -0.189135     0.168252     0.00992442  -0.126984    -0.0603974     0.0668747   0.0152948    0.223857    -0.133383    -0.0688858   -0.106902    -0.234537    -0.040565    -0.168042     -0.0836938   -0.0561573    -0.0749752 
 -0.0298989   -0.00834262   0.0567102   -0.202123      0.0229132    0.0303524  -0.0297863    0.186268   -0.0347265    0.109752     0.0609081   -0.0724224   -0.239043    -0.0791578    -0.195659    0.0887438    0.0225646   -0.0032803    0.121021    -0.0583251    0.0192624   -0.133331    -0.0773655    -0.0309662    0.161936      0.0778885 
  0.0132946   -0.0582249    0.0619953   -0.0393732     0.157054     0.0492033   0.0562099    0.0585781  -0.00217354   0.00523217  -0.00593423   0.00738922   0.026462     0.0413757    -0.0847287   0.0676393    0.041002    -0.186391     0.00444525  -0.146081     0.0134424   -0.0744035    0.0346405    -0.148258    -0.0890941    -0.0765905 
  0.0159967   -0.0959703    0.0346371   -0.225802      0.0589826    0.0134189  -0.0853352   -0.0442957  -0.123482    -0.0922295    0.00530868   0.0484485    0.207881     0.04397      -0.0285205  -0.0269288   -0.00888743   0.017791    -0.239619    -0.0823568   -0.101553    -0.113986    -0.149626      0.0166545    0.135517     -0.0616501 
  0.131287     0.00315236   0.0391757    0.0803048    -0.0754513    0.0297971   0.207366     0.0310067   0.0346067   -0.16407     -0.071722     0.0319152    0.0372935    0.00900765   -0.0206339   0.00227151   0.015859     0.162885    -0.14513      0.0948279   -0.178727    -0.0594653   -0.0418932    -0.00344274  -0.0352202     0.061282  
 -0.0912631   -0.00654847   0.042031     0.0404085    -0.0661025    0.0984908  -0.156948    -0.0481486  -0.00908035  -0.0482815    0.00767058   0.0860305   -0.160258     0.0795122    -0.0499435  -0.0312837   -0.00156536   0.0704239    0.048628    -0.226033     0.0229987   -0.00691845   0.0556175     0.12536     -0.000501022  -0.0255772 
 -0.174222    -0.0777021   -0.141792     0.0558056    -0.0608713    0.0330083  -0.0130069   -0.21198    -0.0242014    0.0675219   -0.113154    -0.11285     -0.0112055    0.00167913    0.0562226   0.205652    -0.0706307   -0.0402945    0.0451865   -0.0757374    0.090673    -0.0661712    0.214128      0.0554369   -0.0278512     0.226396  
  0.0273325    0.0944488    0.0625048   -0.0215858    -0.0151938    0.0287439   0.049969     0.0506934   0.035895    -0.0735296   -0.0401917    0.138568    -0.0725255   -0.0574888    -0.015065    0.0534748    0.0178241    0.0567816   -0.0495063    0.0276773    0.214287     0.0599053    0.163108      0.0842459    0.1125        0.045723  
 -0.0662517   -0.023615     0.0653082   -0.0810462    -0.0152542   -0.0341759   0.115982     0.0328016  -0.0385741   -0.0726364    0.032432     0.0338664    0.138995    -0.133159     -0.102086   -0.105769    -0.043866     0.0699987    0.0541815   -0.0876698    6.74867e-5  -0.0869745    0.0737856    -0.0041905    0.0974652     0.172517  
 -0.0782266    0.028207    -0.0672038    0.0589852    -0.0268262   -0.100723   -0.0109998    0.100312   -0.227143    -0.053293    -0.0450878   -0.120446    -0.287783     0.157443      0.0546774   0.0575677   -0.0321554    0.101303     0.0114458    0.0839626   -0.223185     0.142675    -0.0407767    -0.0275013    0.0151237    -0.0277555 
  0.0750415   -0.0591865    0.189338     0.0339413    -0.00859945   0.0773817  -0.00912298  -0.0779411   0.216768     0.0874815    0.0285288    0.0113284   -0.0930397   -0.0822855    -0.066173   -0.0811147   -0.098455    -0.0411344   -0.11055      0.0520894    0.0866639   -0.09826     -0.14129      -0.077683    -0.0574255    -0.00759595
 -0.109463     0.0222707   -0.0909866    0.0123467     0.00861514   0.191501    0.0182417    0.0213978  -0.025223    -0.0171533   -0.0191451    0.211493    -0.0735476    0.0474075     0.0277949  -0.107471    -0.0275576   -0.109221    -0.0406041    0.0690089    0.00385168  -0.0550358   -0.0743989     0.00272629  -0.0792609     0.0948978 
  0.00284401   0.065718    -0.0423008   -0.154041     -0.11591     -0.0231092  -0.0789856    0.0429343  -0.103078     0.148305    -0.127407     0.052639     0.20546     -0.075763     -0.0202649  -0.16482     -0.087773    -0.0724277    0.125802     0.0434394    0.0161426    0.0184722   -0.0504806     0.103831    -0.0754424     0.107712  kind diag, method split
0: avll = -1.396018769540826
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.396098
INFO: iteration 2, average log likelihood -1.396017
INFO: iteration 3, average log likelihood -1.395431
INFO: iteration 4, average log likelihood -1.389368
INFO: iteration 5, average log likelihood -1.375019
INFO: iteration 6, average log likelihood -1.369443
INFO: iteration 7, average log likelihood -1.367955
INFO: iteration 8, average log likelihood -1.367163
INFO: iteration 9, average log likelihood -1.366614
INFO: iteration 10, average log likelihood -1.366135
INFO: iteration 11, average log likelihood -1.365726
INFO: iteration 12, average log likelihood -1.365418
INFO: iteration 13, average log likelihood -1.365201
INFO: iteration 14, average log likelihood -1.365044
INFO: iteration 15, average log likelihood -1.364917
INFO: iteration 16, average log likelihood -1.364797
INFO: iteration 17, average log likelihood -1.364667
INFO: iteration 18, average log likelihood -1.364517
INFO: iteration 19, average log likelihood -1.364360
INFO: iteration 20, average log likelihood -1.364195
INFO: iteration 21, average log likelihood -1.364022
INFO: iteration 22, average log likelihood -1.363840
INFO: iteration 23, average log likelihood -1.363653
INFO: iteration 24, average log likelihood -1.363476
INFO: iteration 25, average log likelihood -1.363321
INFO: iteration 26, average log likelihood -1.363192
INFO: iteration 27, average log likelihood -1.363086
INFO: iteration 28, average log likelihood -1.362996
INFO: iteration 29, average log likelihood -1.362917
INFO: iteration 30, average log likelihood -1.362844
INFO: iteration 31, average log likelihood -1.362773
INFO: iteration 32, average log likelihood -1.362700
INFO: iteration 33, average log likelihood -1.362622
INFO: iteration 34, average log likelihood -1.362538
INFO: iteration 35, average log likelihood -1.362444
INFO: iteration 36, average log likelihood -1.362339
INFO: iteration 37, average log likelihood -1.362217
INFO: iteration 38, average log likelihood -1.362068
INFO: iteration 39, average log likelihood -1.361868
INFO: iteration 40, average log likelihood -1.361552
INFO: iteration 41, average log likelihood -1.360931
INFO: iteration 42, average log likelihood -1.360049
INFO: iteration 43, average log likelihood -1.359438
INFO: iteration 44, average log likelihood -1.359038
INFO: iteration 45, average log likelihood -1.358740
INFO: iteration 46, average log likelihood -1.358517
INFO: iteration 47, average log likelihood -1.358350
INFO: iteration 48, average log likelihood -1.358218
INFO: iteration 49, average log likelihood -1.358107
INFO: iteration 50, average log likelihood -1.358005
INFO: EM with 100000 data points 50 iterations avll -1.358005
952.4 data points per parameter
1: avll = [-1.3961,-1.39602,-1.39543,-1.38937,-1.37502,-1.36944,-1.36795,-1.36716,-1.36661,-1.36614,-1.36573,-1.36542,-1.3652,-1.36504,-1.36492,-1.3648,-1.36467,-1.36452,-1.36436,-1.3642,-1.36402,-1.36384,-1.36365,-1.36348,-1.36332,-1.36319,-1.36309,-1.363,-1.36292,-1.36284,-1.36277,-1.3627,-1.36262,-1.36254,-1.36244,-1.36234,-1.36222,-1.36207,-1.36187,-1.36155,-1.36093,-1.36005,-1.35944,-1.35904,-1.35874,-1.35852,-1.35835,-1.35822,-1.35811,-1.35801]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.358020
INFO: iteration 2, average log likelihood -1.357786
INFO: iteration 3, average log likelihood -1.357195
INFO: iteration 4, average log likelihood -1.353197
INFO: iteration 5, average log likelihood -1.341358
INFO: iteration 6, average log likelihood -1.333013
INFO: iteration 7, average log likelihood -1.329994
INFO: iteration 8, average log likelihood -1.328254
INFO: iteration 9, average log likelihood -1.326910
INFO: iteration 10, average log likelihood -1.325830
INFO: iteration 11, average log likelihood -1.324993
INFO: iteration 12, average log likelihood -1.324356
INFO: iteration 13, average log likelihood -1.323847
INFO: iteration 14, average log likelihood -1.323376
INFO: iteration 15, average log likelihood -1.322858
INFO: iteration 16, average log likelihood -1.322270
INFO: iteration 17, average log likelihood -1.321642
INFO: iteration 18, average log likelihood -1.321040
INFO: iteration 19, average log likelihood -1.320516
INFO: iteration 20, average log likelihood -1.320073
INFO: iteration 21, average log likelihood -1.319636
INFO: iteration 22, average log likelihood -1.319139
INFO: iteration 23, average log likelihood -1.318582
INFO: iteration 24, average log likelihood -1.318010
INFO: iteration 25, average log likelihood -1.317477
INFO: iteration 26, average log likelihood -1.317021
INFO: iteration 27, average log likelihood -1.316649
INFO: iteration 28, average log likelihood -1.316364
INFO: iteration 29, average log likelihood -1.316147
INFO: iteration 30, average log likelihood -1.315961
INFO: iteration 31, average log likelihood -1.315757
INFO: iteration 32, average log likelihood -1.315439
INFO: iteration 33, average log likelihood -1.314893
INFO: iteration 34, average log likelihood -1.314329
INFO: iteration 35, average log likelihood -1.313939
INFO: iteration 36, average log likelihood -1.313689
INFO: iteration 37, average log likelihood -1.313519
INFO: iteration 38, average log likelihood -1.313386
INFO: iteration 39, average log likelihood -1.313270
INFO: iteration 40, average log likelihood -1.313152
INFO: iteration 41, average log likelihood -1.313024
INFO: iteration 42, average log likelihood -1.312895
INFO: iteration 43, average log likelihood -1.312786
INFO: iteration 44, average log likelihood -1.312706
INFO: iteration 45, average log likelihood -1.312652
INFO: iteration 46, average log likelihood -1.312618
INFO: iteration 47, average log likelihood -1.312598
INFO: iteration 48, average log likelihood -1.312586
INFO: iteration 49, average log likelihood -1.312579
INFO: iteration 50, average log likelihood -1.312574
INFO: EM with 100000 data points 50 iterations avll -1.312574
473.9 data points per parameter
2: avll = [-1.35802,-1.35779,-1.3572,-1.3532,-1.34136,-1.33301,-1.32999,-1.32825,-1.32691,-1.32583,-1.32499,-1.32436,-1.32385,-1.32338,-1.32286,-1.32227,-1.32164,-1.32104,-1.32052,-1.32007,-1.31964,-1.31914,-1.31858,-1.31801,-1.31748,-1.31702,-1.31665,-1.31636,-1.31615,-1.31596,-1.31576,-1.31544,-1.31489,-1.31433,-1.31394,-1.31369,-1.31352,-1.31339,-1.31327,-1.31315,-1.31302,-1.3129,-1.31279,-1.31271,-1.31265,-1.31262,-1.3126,-1.31259,-1.31258,-1.31257]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.312733
INFO: iteration 2, average log likelihood -1.312580
INFO: iteration 3, average log likelihood -1.312124
INFO: iteration 4, average log likelihood -1.307909
INFO: iteration 5, average log likelihood -1.293696
INFO: iteration 6, average log likelihood -1.279842
INFO: iteration 7, average log likelihood -1.272729
INFO: iteration 8, average log likelihood -1.268492
INFO: iteration 9, average log likelihood -1.265534
INFO: iteration 10, average log likelihood -1.263376
INFO: iteration 11, average log likelihood -1.261689
INFO: iteration 12, average log likelihood -1.260168
WARNING: Variances had to be floored 7
INFO: iteration 13, average log likelihood -1.258444
INFO: iteration 14, average log likelihood -1.269375
INFO: iteration 15, average log likelihood -1.264235
INFO: iteration 16, average log likelihood -1.261856
INFO: iteration 17, average log likelihood -1.260663
INFO: iteration 18, average log likelihood -1.259817
INFO: iteration 19, average log likelihood -1.258968
INFO: iteration 20, average log likelihood -1.258067
INFO: iteration 21, average log likelihood -1.257079
WARNING: Variances had to be floored 7
INFO: iteration 22, average log likelihood -1.255743
INFO: iteration 23, average log likelihood -1.267455
INFO: iteration 24, average log likelihood -1.262767
INFO: iteration 25, average log likelihood -1.261137
INFO: iteration 26, average log likelihood -1.260558
INFO: iteration 27, average log likelihood -1.260180
INFO: iteration 28, average log likelihood -1.259798
INFO: iteration 29, average log likelihood -1.259315
INFO: iteration 30, average log likelihood -1.258763
INFO: iteration 31, average log likelihood -1.258237
INFO: iteration 32, average log likelihood -1.257660
INFO: iteration 33, average log likelihood -1.256764
WARNING: Variances had to be floored 7
INFO: iteration 34, average log likelihood -1.255415
INFO: iteration 35, average log likelihood -1.267460
INFO: iteration 36, average log likelihood -1.262755
INFO: iteration 37, average log likelihood -1.261115
INFO: iteration 38, average log likelihood -1.260535
INFO: iteration 39, average log likelihood -1.260155
INFO: iteration 40, average log likelihood -1.259765
INFO: iteration 41, average log likelihood -1.259277
INFO: iteration 42, average log likelihood -1.258728
INFO: iteration 43, average log likelihood -1.258212
INFO: iteration 44, average log likelihood -1.257634
INFO: iteration 45, average log likelihood -1.256725
WARNING: Variances had to be floored 7
INFO: iteration 46, average log likelihood -1.255367
INFO: iteration 47, average log likelihood -1.267463
INFO: iteration 48, average log likelihood -1.262755
INFO: iteration 49, average log likelihood -1.261112
INFO: iteration 50, average log likelihood -1.260531
INFO: EM with 100000 data points 50 iterations avll -1.260531
236.4 data points per parameter
3: avll = [-1.31273,-1.31258,-1.31212,-1.30791,-1.2937,-1.27984,-1.27273,-1.26849,-1.26553,-1.26338,-1.26169,-1.26017,-1.25844,-1.26937,-1.26423,-1.26186,-1.26066,-1.25982,-1.25897,-1.25807,-1.25708,-1.25574,-1.26745,-1.26277,-1.26114,-1.26056,-1.26018,-1.2598,-1.25932,-1.25876,-1.25824,-1.25766,-1.25676,-1.25542,-1.26746,-1.26275,-1.26111,-1.26054,-1.26015,-1.25976,-1.25928,-1.25873,-1.25821,-1.25763,-1.25672,-1.25537,-1.26746,-1.26276,-1.26111,-1.26053]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.260384
INFO: iteration 2, average log likelihood -1.259736
INFO: iteration 3, average log likelihood -1.258303
INFO: iteration 4, average log likelihood -1.247932
WARNING: Variances had to be floored 13
INFO: iteration 5, average log likelihood -1.217043
WARNING: Variances had to be floored 8 10 16
INFO: iteration 6, average log likelihood -1.194384
INFO: iteration 7, average log likelihood -1.204927
WARNING: Variances had to be floored 13 14
INFO: iteration 8, average log likelihood -1.181181
WARNING: Variances had to be floored 7 10 16
INFO: iteration 9, average log likelihood -1.184548
WARNING: Variances had to be floored 8
INFO: iteration 10, average log likelihood -1.204150
INFO: iteration 11, average log likelihood -1.190232
WARNING: Variances had to be floored 10 13 16
INFO: iteration 12, average log likelihood -1.170192
INFO: iteration 13, average log likelihood -1.190524
WARNING: Variances had to be floored 7 8
INFO: iteration 14, average log likelihood -1.174798
WARNING: Variances had to be floored 10 13 16
INFO: iteration 15, average log likelihood -1.177740
INFO: iteration 16, average log likelihood -1.196296
INFO: iteration 17, average log likelihood -1.175772
WARNING: Variances had to be floored 10
INFO: iteration 18, average log likelihood -1.163869
WARNING: Variances had to be floored 7 8 13 14 16
INFO: iteration 19, average log likelihood -1.170359
INFO: iteration 20, average log likelihood -1.198077
WARNING: Variances had to be floored 10
INFO: iteration 21, average log likelihood -1.184380
WARNING: Variances had to be floored 13 16
INFO: iteration 22, average log likelihood -1.183906
INFO: iteration 23, average log likelihood -1.180627
WARNING: Variances had to be floored 7 8 10
INFO: iteration 24, average log likelihood -1.164312
WARNING: Variances had to be floored 13 16
INFO: iteration 25, average log likelihood -1.183265
WARNING: Variances had to be floored 10 14
INFO: iteration 26, average log likelihood -1.182662
WARNING: Variances had to be floored 16
INFO: iteration 27, average log likelihood -1.187796
WARNING: Variances had to be floored 13
INFO: iteration 28, average log likelihood -1.178855
WARNING: Variances had to be floored 7 8 10 16
INFO: iteration 29, average log likelihood -1.167869
INFO: iteration 30, average log likelihood -1.190054
WARNING: Variances had to be floored 13 14 16
INFO: iteration 31, average log likelihood -1.172885
WARNING: Variances had to be floored 10
INFO: iteration 32, average log likelihood -1.183443
WARNING: Variances had to be floored 16
INFO: iteration 33, average log likelihood -1.184663
WARNING: Variances had to be floored 7 8 13
INFO: iteration 34, average log likelihood -1.177522
WARNING: Variances had to be floored 10 16
INFO: iteration 35, average log likelihood -1.177587
WARNING: Variances had to be floored 14
INFO: iteration 36, average log likelihood -1.185571
WARNING: Variances had to be floored 13 16
INFO: iteration 37, average log likelihood -1.178763
WARNING: Variances had to be floored 10
INFO: iteration 38, average log likelihood -1.178908
WARNING: Variances had to be floored 7 8 16
INFO: iteration 39, average log likelihood -1.173166
WARNING: Variances had to be floored 6 13
INFO: iteration 40, average log likelihood -1.178507
WARNING: Variances had to be floored 10 14 16
INFO: iteration 41, average log likelihood -1.184744
INFO: iteration 42, average log likelihood -1.198424
WARNING: Variances had to be floored 13 16
INFO: iteration 43, average log likelihood -1.177964
WARNING: Variances had to be floored 7 8 10
INFO: iteration 44, average log likelihood -1.174776
WARNING: Variances had to be floored 16
INFO: iteration 45, average log likelihood -1.184704
WARNING: Variances had to be floored 13 14
INFO: iteration 46, average log likelihood -1.178022
WARNING: Variances had to be floored 10 16
INFO: iteration 47, average log likelihood -1.178699
INFO: iteration 48, average log likelihood -1.189599
WARNING: Variances had to be floored 7 8 13 16
INFO: iteration 49, average log likelihood -1.172534
WARNING: Variances had to be floored 10
INFO: iteration 50, average log likelihood -1.182897
INFO: EM with 100000 data points 50 iterations avll -1.182897
118.1 data points per parameter
4: avll = [-1.26038,-1.25974,-1.2583,-1.24793,-1.21704,-1.19438,-1.20493,-1.18118,-1.18455,-1.20415,-1.19023,-1.17019,-1.19052,-1.1748,-1.17774,-1.1963,-1.17577,-1.16387,-1.17036,-1.19808,-1.18438,-1.18391,-1.18063,-1.16431,-1.18327,-1.18266,-1.1878,-1.17885,-1.16787,-1.19005,-1.17288,-1.18344,-1.18466,-1.17752,-1.17759,-1.18557,-1.17876,-1.17891,-1.17317,-1.17851,-1.18474,-1.19842,-1.17796,-1.17478,-1.1847,-1.17802,-1.1787,-1.1896,-1.17253,-1.1829]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 27 28 31 32
INFO: iteration 1, average log likelihood -1.180692
WARNING: Variances had to be floored 25 26 27 28 31 32
INFO: iteration 2, average log likelihood -1.166125
WARNING: Variances had to be floored 14 15 19 20 27 28 31 32
INFO: iteration 3, average log likelihood -1.159510
WARNING: Variances had to be floored 12 13 16 25 26 27 28 31 32
INFO: iteration 4, average log likelihood -1.153970
WARNING: Variances had to be floored 19 20 27 28 31 32
INFO: iteration 5, average log likelihood -1.131486
WARNING: Variances had to be floored 3 12 13 16 18 25 26 27 28 31 32
INFO: iteration 6, average log likelihood -1.100801
WARNING: Variances had to be floored 19 20 23 27 28 31 32
INFO: iteration 7, average log likelihood -1.106112
WARNING: Variances had to be floored 12 13 16 25 26 27 28 31 32
INFO: iteration 8, average log likelihood -1.090849
WARNING: Variances had to be floored 3 14 18 19 20 25 26 27 28 31 32
INFO: iteration 9, average log likelihood -1.088188
WARNING: Variances had to be floored 12 13 16 27 28 31 32
INFO: iteration 10, average log likelihood -1.095425
WARNING: Variances had to be floored 9 19 20 23 25 26 27 28 31 32
INFO: iteration 11, average log likelihood -1.077326
WARNING: Variances had to be floored 3 12 13 16 18 25 26 27 28 31 32
INFO: iteration 12, average log likelihood -1.083731
WARNING: Variances had to be floored 14 19 20 25 26 27 28 31 32
INFO: iteration 13, average log likelihood -1.088062
WARNING: Variances had to be floored 12 13 25 26 27 28 31 32
INFO: iteration 14, average log likelihood -1.081952
WARNING: Variances had to be floored 16 18 19 20 23 25 26 27 28 31 32
INFO: iteration 15, average log likelihood -1.067105
WARNING: Variances had to be floored 3 9 12 13 15 25 26 27 28 31 32
INFO: iteration 16, average log likelihood -1.080588
WARNING: Variances had to be floored 14 16 19 20 25 26 27 28 31 32
INFO: iteration 17, average log likelihood -1.092694
WARNING: Variances had to be floored 12 13 18 23 25 26 27 28 31 32
INFO: iteration 18, average log likelihood -1.096969
WARNING: Variances had to be floored 3 19 20 27 28 31 32
INFO: iteration 19, average log likelihood -1.096611
WARNING: Variances had to be floored 12 13 16 25 26 27 28 31 32
INFO: iteration 20, average log likelihood -1.076177
WARNING: Variances had to be floored 14 18 19 20 23 25 26 27 28 31 32
INFO: iteration 21, average log likelihood -1.072831
WARNING: Variances had to be floored 9 12 13 27 28 31 32
INFO: iteration 22, average log likelihood -1.087055
WARNING: Variances had to be floored 3 15 16 19 20 25 26 27 28 31 32
INFO: iteration 23, average log likelihood -1.076503
WARNING: Variances had to be floored 12 13 18 25 26 27 28 31 32
INFO: iteration 24, average log likelihood -1.091085
WARNING: Variances had to be floored 16 20 23 27 28 31 32
INFO: iteration 25, average log likelihood -1.086496
WARNING: Variances had to be floored 3 12 13 14 15 19 25 26 27 28 31 32
INFO: iteration 26, average log likelihood -1.074190
WARNING: Variances had to be floored 18 25 26 27 28 31 32
INFO: iteration 27, average log likelihood -1.105744
WARNING: Variances had to be floored 12 16 19 25 27 28 31 32
INFO: iteration 28, average log likelihood -1.085461
WARNING: Variances had to be floored 3 13 23 26 27 28 31 32
INFO: iteration 29, average log likelihood -1.083395
WARNING: Variances had to be floored 9 12 18 19 20 25 27 28 31 32
INFO: iteration 30, average log likelihood -1.072113
WARNING: Variances had to be floored 13 16 26 27 28 31 32
INFO: iteration 31, average log likelihood -1.095049
WARNING: Variances had to be floored 3 12 14 19 26 27 28 31 32
INFO: iteration 32, average log likelihood -1.081950
WARNING: Variances had to be floored 13 16 18 23 26 27 28 31 32
INFO: iteration 33, average log likelihood -1.090222
WARNING: Variances had to be floored 12 19 20 26 27 28 31 32
INFO: iteration 34, average log likelihood -1.084942
WARNING: Variances had to be floored 3 13 16 26 27 28 31 32
INFO: iteration 35, average log likelihood -1.086953
WARNING: Variances had to be floored 12 14 18 19 23 25 26 27 28 31 32
INFO: iteration 36, average log likelihood -1.073909
WARNING: Variances had to be floored 13 16 26 27 28 31 32
INFO: iteration 37, average log likelihood -1.096279
WARNING: Variances had to be floored 3 12 19 20 26 27 28 31 32
INFO: iteration 38, average log likelihood -1.072517
WARNING: Variances had to be floored 13 16 18 26 27 28 31 32
INFO: iteration 39, average log likelihood -1.082684
WARNING: Variances had to be floored 12 19 23 25 26 27 28 31 32
INFO: iteration 40, average log likelihood -1.071592
WARNING: Variances had to be floored 3 13 15 16 18 26 27 28 31 32
INFO: iteration 41, average log likelihood -1.086236
WARNING: Variances had to be floored 12 19 20 26 27 28 31 32
INFO: iteration 42, average log likelihood -1.088243
WARNING: Variances had to be floored 26 27 28 31 32
INFO: iteration 43, average log likelihood -1.085463
WARNING: Variances had to be floored 3 12 13 16 18 19 23 25 26 27 28 31 32
INFO: iteration 44, average log likelihood -1.050788
WARNING: Variances had to be floored 26 27 28 31 32
INFO: iteration 45, average log likelihood -1.100124
WARNING: Variances had to be floored 12 13 15 16 19 26 27 28 31 32
INFO: iteration 46, average log likelihood -1.062065
WARNING: Variances had to be floored 3 14 18 26 27 28 31 32
INFO: iteration 47, average log likelihood -1.084536
WARNING: Variances had to be floored 12 19 20 23 25 26 27 28 31 32
INFO: iteration 48, average log likelihood -1.077570
WARNING: Variances had to be floored 13 16 26 27 28 31 32
INFO: iteration 49, average log likelihood -1.094139
WARNING: Variances had to be floored 3 12 18 19 20 26 27 28 31 32
INFO: iteration 50, average log likelihood -1.076296
INFO: EM with 100000 data points 50 iterations avll -1.076296
59.0 data points per parameter
5: avll = [-1.18069,-1.16613,-1.15951,-1.15397,-1.13149,-1.1008,-1.10611,-1.09085,-1.08819,-1.09542,-1.07733,-1.08373,-1.08806,-1.08195,-1.0671,-1.08059,-1.09269,-1.09697,-1.09661,-1.07618,-1.07283,-1.08706,-1.0765,-1.09109,-1.0865,-1.07419,-1.10574,-1.08546,-1.08339,-1.07211,-1.09505,-1.08195,-1.09022,-1.08494,-1.08695,-1.07391,-1.09628,-1.07252,-1.08268,-1.07159,-1.08624,-1.08824,-1.08546,-1.05079,-1.10012,-1.06207,-1.08454,-1.07757,-1.09414,-1.0763]
[-1.39602,-1.3961,-1.39602,-1.39543,-1.38937,-1.37502,-1.36944,-1.36795,-1.36716,-1.36661,-1.36614,-1.36573,-1.36542,-1.3652,-1.36504,-1.36492,-1.3648,-1.36467,-1.36452,-1.36436,-1.3642,-1.36402,-1.36384,-1.36365,-1.36348,-1.36332,-1.36319,-1.36309,-1.363,-1.36292,-1.36284,-1.36277,-1.3627,-1.36262,-1.36254,-1.36244,-1.36234,-1.36222,-1.36207,-1.36187,-1.36155,-1.36093,-1.36005,-1.35944,-1.35904,-1.35874,-1.35852,-1.35835,-1.35822,-1.35811,-1.35801,-1.35802,-1.35779,-1.3572,-1.3532,-1.34136,-1.33301,-1.32999,-1.32825,-1.32691,-1.32583,-1.32499,-1.32436,-1.32385,-1.32338,-1.32286,-1.32227,-1.32164,-1.32104,-1.32052,-1.32007,-1.31964,-1.31914,-1.31858,-1.31801,-1.31748,-1.31702,-1.31665,-1.31636,-1.31615,-1.31596,-1.31576,-1.31544,-1.31489,-1.31433,-1.31394,-1.31369,-1.31352,-1.31339,-1.31327,-1.31315,-1.31302,-1.3129,-1.31279,-1.31271,-1.31265,-1.31262,-1.3126,-1.31259,-1.31258,-1.31257,-1.31273,-1.31258,-1.31212,-1.30791,-1.2937,-1.27984,-1.27273,-1.26849,-1.26553,-1.26338,-1.26169,-1.26017,-1.25844,-1.26937,-1.26423,-1.26186,-1.26066,-1.25982,-1.25897,-1.25807,-1.25708,-1.25574,-1.26745,-1.26277,-1.26114,-1.26056,-1.26018,-1.2598,-1.25932,-1.25876,-1.25824,-1.25766,-1.25676,-1.25542,-1.26746,-1.26275,-1.26111,-1.26054,-1.26015,-1.25976,-1.25928,-1.25873,-1.25821,-1.25763,-1.25672,-1.25537,-1.26746,-1.26276,-1.26111,-1.26053,-1.26038,-1.25974,-1.2583,-1.24793,-1.21704,-1.19438,-1.20493,-1.18118,-1.18455,-1.20415,-1.19023,-1.17019,-1.19052,-1.1748,-1.17774,-1.1963,-1.17577,-1.16387,-1.17036,-1.19808,-1.18438,-1.18391,-1.18063,-1.16431,-1.18327,-1.18266,-1.1878,-1.17885,-1.16787,-1.19005,-1.17288,-1.18344,-1.18466,-1.17752,-1.17759,-1.18557,-1.17876,-1.17891,-1.17317,-1.17851,-1.18474,-1.19842,-1.17796,-1.17478,-1.1847,-1.17802,-1.1787,-1.1896,-1.17253,-1.1829,-1.18069,-1.16613,-1.15951,-1.15397,-1.13149,-1.1008,-1.10611,-1.09085,-1.08819,-1.09542,-1.07733,-1.08373,-1.08806,-1.08195,-1.0671,-1.08059,-1.09269,-1.09697,-1.09661,-1.07618,-1.07283,-1.08706,-1.0765,-1.09109,-1.0865,-1.07419,-1.10574,-1.08546,-1.08339,-1.07211,-1.09505,-1.08195,-1.09022,-1.08494,-1.08695,-1.07391,-1.09628,-1.07252,-1.08268,-1.07159,-1.08624,-1.08824,-1.08546,-1.05079,-1.10012,-1.06207,-1.08454,-1.07757,-1.09414,-1.0763]
32×26 Array{Float64,2}:
 -0.0786973    0.0426993    -0.041722    -0.0610083    -0.04605      0.0974205   -0.0383636    0.0413534  -0.0600435     0.0410453   -0.12025      0.127641     0.0358903   -0.00179625  -0.000867736  -0.141165    -0.0616196   -0.0959527    0.0337402   0.0457933    0.00650916  -0.0112676  -0.0590725   0.0341315   -0.0136272    0.0771329 
  0.0206301   -0.0595165     0.0221183   -0.121073      0.0590402   -0.0234333   -0.0661447    0.118926   -0.0761375     0.0105439    0.138653    -0.0408563   -0.070278    -0.0521695   -0.0112605     0.0643595    0.0255743   -0.047382     0.0459883  -0.0754096    0.00481257  -0.0915665  -0.0638217   0.0327069    0.0769725    0.0806539 
 -0.141654     0.115668     -0.0642582    0.0416396     0.135801     0.0705604    0.137291    -0.118214    0.08826      -0.0316057   -0.122768    -0.104814     0.0159659    0.00162092  -0.1316        0.105891     0.0822895    0.00745783   0.0489203  -0.00653158   0.106522     0.0106049   0.0532502  -0.10884     -0.0335583   -0.0418021 
  0.0184284    0.00598282   -0.0661423   -0.0659685     0.103993    -0.0723138   -0.0470976   -0.0210203  -0.0621273    -0.0468807   -0.00926096  -0.0635353   -0.0957302    0.031929     0.0360019    -0.0195665   -0.0643118   -0.0329954   -0.0470142   0.0735815   -0.00834162  -0.174207    0.0768544   0.0193543    0.0055825   -0.0247022 
 -0.152402    -0.0100177    -0.200202    -0.128759      0.0189476   -0.21298     -0.0989422   -0.0104676   0.0442157    -0.0381725    0.12796     -0.380462    -0.105595     0.181261    -0.0243206    -0.658843     0.0614799   -0.10819     -0.114736    0.0486818    0.0155738   -0.0478201   0.128269   -0.20961      0.0869442   -0.0243068 
 -0.173943    -0.13117      -0.223332    -0.108766      0.102621    -0.188668    -0.0186576    0.0699791  -0.1046       -0.0323337   -0.0146124    0.005094    -0.080957     0.185631     0.0160344     0.608087     0.0335702   -0.101066    -0.278193    0.11977      0.0137954   -0.149855    0.0877502  -0.0812475    0.0621522    0.125159  
 -0.127863     0.0530796     0.0766348    0.264548      0.0851403   -0.0730069   -0.201063    -0.169722   -0.0619499    -0.279154     0.036583    -0.0242379   -0.0171743    0.0306631    0.0469574    -0.0691299   -0.107843    -0.0245014   -0.0197652   0.142094    -0.167687    -0.027668   -0.110544    0.267306    -0.112193    -0.0216693 
  0.0621417    0.166122      0.256177    -0.0133747     0.118192    -0.140393    -0.0248923   -0.094293   -0.183738      0.0655108   -0.00754116   0.0491496   -0.0550689   -0.0341927    0.0505801    -0.0653953    0.127915    -0.0108617    0.050221    0.0246107   -0.138096     0.104103    0.176121    0.00419129   0.165596    -0.113221  
  0.0340556   -0.000354455   0.12349     -0.140021     -0.0448428    0.0277983    0.0865164   -0.0123142  -0.091009     -0.0331123   -0.0563081    0.0508223    0.100797    -0.0844995   -0.0795851     0.064769    -0.0708768   -0.0202263    0.0457358  -0.00293933  -0.0434213   -0.0863072   0.0462558  -0.00356805   0.0333648    0.138951  
 -0.0878208    0.00346101    0.0554459    0.0297013    -0.0550219    0.00620082  -0.122005     0.0180293   0.00253227    0.0825152    0.0341503    0.0615952   -0.0657396   -0.0352679   -0.092041      0.0127118   -0.0105029   -0.0143922    0.0769933  -0.073286     0.11164     -0.0200855   0.0472405   0.0768341    0.0103365   -0.112749  
 -0.169246    -0.0813213    -0.136625     0.0372128    -0.0618988    0.0208006   -0.00849988  -0.226081    0.00385043    0.08699     -0.128636    -0.112912    -0.0173021    0.00807781   0.0721697     0.215574    -0.0695477   -0.0389623    0.0413522  -0.0748637    0.0938075   -0.0651472   0.199458    0.0611863   -0.0308235    0.221211  
 -0.13153      0.139922     -0.287858    -0.0461796    -0.036099    -0.0197051   -0.133628     0.150059   -0.0756126     0.0692911    0.0431306   -0.0181966   -0.066615    -0.00382789  -0.0408786    -0.0270973   -0.101195     0.0790202   -0.106576   -0.123233    -0.108263     0.028704    0.0197039  -0.0524003    0.00491812  -0.227672  
 -0.0123518    0.0509514     0.0405087    0.083695      0.00329374   0.0292373    0.0550476   -0.129571    0.0934925    -0.0522025    0.101484    -0.0893029   -0.180144     0.054636    -0.00311198    0.0232407    0.011161     0.174083    -0.0223873   0.0722336   -0.0576425   -0.0834303   0.040937   -0.0622857    0.0805466    0.0389414 
 -0.0169406    0.065351     -0.0672841    0.104265      0.0426281   -0.0154851    0.00934152   0.0459282  -0.090514      0.0342437    0.0812605   -0.181277    -0.125757    -0.0733075    0.0375913    -0.105448     0.0222971    0.121066     0.0369919  -0.0837897    0.0233738    0.222655    0.0464331   0.105044    -0.0734379   -0.0285078 
 -0.0736358   -0.00575334    0.0103667   -0.071157     -0.150451     0.0350281   -0.0165113   -0.146968    0.0908531     0.00295143  -0.0620478   -0.0252434    0.102963    -0.166875    -0.0636334     0.0173441   -0.107058     0.250064     0.0931114  -0.0177325   -0.0220578   -0.130585   -0.0826661  -0.0690958    0.0137925   -0.0392948 
  0.0393109    0.013752     -0.0441665    0.00153582    0.229933     0.149355    -0.16561      0.0777565  -0.0241227     0.0946581    0.00946042   0.0707874    0.00824749   0.103826    -0.0696183    -0.00115999  -0.10341      0.0118907   -0.212468    0.0085934    0.0631513    0.147905   -0.0936724   0.0276579   -0.219887     0.102394  
 -0.0754532    0.031438     -0.067448     0.0556321    -0.0429086   -0.0894776   -0.00706165   0.104906   -0.233258     -0.023778    -0.0557229   -0.110443    -0.272469     0.154751     0.073741      0.0513409   -0.00780149   0.0947225    0.0158614   0.127644    -0.211266     0.142133   -0.032625   -0.0313823    0.00444579  -0.0379684 
 -0.0258717   -0.0904117     0.045451    -0.233157      0.0588182    0.0317708   -0.0520801   -0.0469002  -0.118196     -0.0700052    0.00684784   0.130051     0.19495      0.038843    -0.0217868    -0.0326047   -0.00412383   0.0104626   -0.222576   -0.0855451   -0.0862145   -0.117083   -0.150034    0.0276945    0.133254    -0.0265317 
 -0.0337195    0.219835     -0.00824224   0.0149632    -0.0226422    0.170867     0.0722345   -0.0562549  -0.0976748    -0.375924     0.169009     0.00911036  -0.117313    -0.0465658    0.074626      0.0492338    0.222234    -0.0921126   -0.0689779  -0.104337    -0.186023    -0.165675   -0.167724   -0.10708     -0.0551098   -0.0591492 
 -0.0327523    0.242925     -0.00831685   0.114764     -0.0215032    0.092061    -0.282797    -0.0567056  -0.129452      0.0720602    0.1631       0.00655877  -0.126622    -0.0518791    0.0683728    -0.0357511    0.222235    -0.140871    -0.0686781  -0.0812078   -0.193275     0.129864   -0.166578   -0.0318898   -0.0544086   -0.0859711 
  0.0273147    0.058779      0.0307434   -0.0197154     0.0272178    0.0344256    0.0312911    0.0602375   0.000240814  -0.0724828   -0.0348695    0.145513    -0.0536724   -0.0842277   -0.000835741   0.0477418    0.026629     0.0194711   -0.0673095   0.0442992    0.212632     0.0576639   0.165319    0.0815695    0.0977182    0.0643289 
 -0.0455083   -0.0388114    -0.118989     0.0606161    -0.0267074    0.0400504    0.13306      0.0729388  -0.100562      0.00456256   0.0356596    0.0660864   -0.044669     0.0898765    0.0476525     0.0175047    0.189849     0.100436    -0.119351   -0.0455015   -0.0146395   -0.0107161   0.0704126  -0.00165811   0.092899     0.00932468
 -0.0396086    0.0653513    -0.0385773    0.0373919     0.0140868    0.0551432   -0.0424749    0.169919   -0.00846833    0.07152     -0.0699635   -0.0369801   -0.12543     -0.0426222   -0.0153045     0.142633    -0.107732     0.0763579   -0.0446811   0.0326381   -0.0277245   -0.039436   -0.0104385   0.0732175   -0.0283147    0.0647981 
  0.10977     -0.000398027   0.0210438    0.0142264    -0.104192     0.0301604    0.194413     0.0295648   0.0399697    -0.145336    -0.0725454    0.0394792    0.0431119    0.0238057   -0.0218331     0.0273552    0.0170307    0.177794    -0.190285    0.0872539   -0.182184    -0.0684107  -0.043123    0.00567869  -0.0328291    0.0560289 
  0.132917    -0.040075      0.231976     0.000585029   0.129451     0.0767921    0.0758229   -0.164243    0.208748      0.0657439    0.0240262    0.00999335  -0.0819866   -0.0988932   -0.0640501    -0.0561388   -0.0817154   -0.0253766   -0.121866    0.067586     0.0872527   -0.0973823  -0.244852   -0.163355    -0.0640566   -0.008686  
  0.0046345   -0.0714127     0.0550076   -0.0557591     0.134168     0.0587728    0.0580553    0.0539616  -0.00248714    0.00517573  -0.0222511    0.0121153    0.031925     0.0356834   -0.086516      0.0706779    0.0430447   -0.181492     0.0305174  -0.147536     0.0155987   -0.0602664   0.0316195  -0.117243    -0.084285    -0.0722237 
 -0.0464829   -0.00932426   -0.0633038    0.0792686    -0.289528    -0.141835     0.00333584  -0.0604859   0.117618      0.0825864    0.0161821   -0.0440714    0.0880393   -0.028388    -0.157188     -0.0536294    0.102819     0.0469348   -0.132372   -0.199707     0.0211046    0.0717229  -0.0419228   0.0433062   -0.232323    -0.09407   
 -0.215422     0.141885     -0.0762963    0.0810497    -0.244442    -0.147849    -0.147116    -0.0419617  -0.24219      -0.106372     0.0148328    0.284088     0.0877323   -0.0173829   -0.24704      -0.00530755   0.0957931    0.0259739    0.113868    0.0340496   -0.0895722    0.203946   -0.0477041   0.0498739   -0.220238    -0.0870415 
  0.0508877    0.0738167    -0.186587    -0.0163482     0.168151     0.224457    -0.0133126   -0.801938    0.0503209     0.0594777    0.0577786    0.0964927    0.0232517   -0.0250238   -0.0618809     0.0788591   -0.105969    -0.135137    -0.127204    0.0161826    0.148732     0.0612424  -0.0385593  -0.138647     0.0771554    0.103966  
 -0.00452419   0.0657996    -0.17646     -0.0226933     0.0933869    0.168473    -0.0893057    0.601608    0.0466375     0.164438    -0.114805     0.049625     0.0444483    0.00219926  -0.0919643     0.206312    -0.153496    -0.0498427    0.010877    0.193086     0.148409     0.063126    0.0293748  -0.0240138    0.0582993    0.120863  
  0.129953    -0.0291793    -0.0130924    0.0293784    -0.0250286   -0.00676862   0.0362968   -0.203489   -0.0536956     0.423439    -0.113994     0.235134    -0.00554602   0.0248094   -0.0334436     0.0614999    0.0894334   -0.707767     0.276739   -0.0838188    0.00346415   0.0763238  -0.026946    0.0339517   -0.0638876   -0.153599  
  0.166505    -0.0566228     0.13531      0.0299561    -0.0435154   -0.0137529    0.102503     0.254557   -0.0538831     0.137253    -0.099608     0.0035033   -0.0212468    0.0228985    0.0127        0.0772729    0.0980656    0.703635    -0.23623    -0.0628473    0.00236375  -0.359328   -0.0269413  -0.0226668   -0.056596    -0.102223  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 13 15 16 26 27 28 31 32
INFO: iteration 1, average log likelihood -1.077050
WARNING: Variances had to be floored 12 13 14 15 16 18 19 20 23 25 26 27 28 31 32
INFO: iteration 2, average log likelihood -1.044380
WARNING: Variances had to be floored 3 13 15 16 26 27 28 31 32
INFO: iteration 3, average log likelihood -1.071286
WARNING: Variances had to be floored 12 13 14 15 16 18 19 20 23 25 26 27 28 31 32
INFO: iteration 4, average log likelihood -1.046830
WARNING: Variances had to be floored 13 15 16 26 27 28 31 32
INFO: iteration 5, average log likelihood -1.072169
WARNING: Variances had to be floored 3 12 13 14 15 16 18 19 20 23 25 26 27 28 31 32
INFO: iteration 6, average log likelihood -1.042873
WARNING: Variances had to be floored 13 15 16 23 26 27 28 31 32
INFO: iteration 7, average log likelihood -1.075098
WARNING: Variances had to be floored 12 13 14 15 16 18 19 20 23 25 26 27 28 31 32
INFO: iteration 8, average log likelihood -1.047422
WARNING: Variances had to be floored 3 13 15 16 26 27 28 31 32
INFO: iteration 9, average log likelihood -1.071217
WARNING: Variances had to be floored 12 13 14 15 16 18 19 20 23 25 26 27 28 31 32
INFO: iteration 10, average log likelihood -1.046633
INFO: EM with 100000 data points 10 iterations avll -1.046633
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.384115e+05
      1       6.697541e+05      -1.686575e+05 |       32
      2       6.403641e+05      -2.938998e+04 |       32
      3       6.254634e+05      -1.490070e+04 |       32
      4       6.153570e+05      -1.010634e+04 |       32
      5       6.070333e+05      -8.323700e+03 |       32
      6       6.012532e+05      -5.780178e+03 |       32
      7       5.975664e+05      -3.686722e+03 |       32
      8       5.952230e+05      -2.343434e+03 |       32
      9       5.936003e+05      -1.622735e+03 |       32
     10       5.923796e+05      -1.220715e+03 |       32
     11       5.914855e+05      -8.940222e+02 |       32
     12       5.909123e+05      -5.732546e+02 |       32
     13       5.905184e+05      -3.938374e+02 |       32
     14       5.902360e+05      -2.824048e+02 |       32
     15       5.900500e+05      -1.860376e+02 |       32
     16       5.899236e+05      -1.264284e+02 |       32
     17       5.898322e+05      -9.136474e+01 |       32
     18       5.897597e+05      -7.249458e+01 |       32
     19       5.896794e+05      -8.034725e+01 |       32
     20       5.896141e+05      -6.522890e+01 |       32
     21       5.895587e+05      -5.545747e+01 |       32
     22       5.894892e+05      -6.951680e+01 |       32
     23       5.894041e+05      -8.502364e+01 |       32
     24       5.893117e+05      -9.241985e+01 |       32
     25       5.891727e+05      -1.390264e+02 |       32
     26       5.889804e+05      -1.923210e+02 |       32
     27       5.886927e+05      -2.877026e+02 |       32
     28       5.883167e+05      -3.759717e+02 |       32
     29       5.878791e+05      -4.376296e+02 |       32
     30       5.875222e+05      -3.568934e+02 |       32
     31       5.873079e+05      -2.143155e+02 |       32
     32       5.871955e+05      -1.123734e+02 |       32
     33       5.871471e+05      -4.834642e+01 |       32
     34       5.871220e+05      -2.518284e+01 |       31
     35       5.871037e+05      -1.829997e+01 |       30
     36       5.870918e+05      -1.189109e+01 |       29
     37       5.870817e+05      -1.004485e+01 |       26
     38       5.870724e+05      -9.352913e+00 |       28
     39       5.870644e+05      -7.979899e+00 |       29
     40       5.870574e+05      -6.981274e+00 |       23
     41       5.870518e+05      -5.604415e+00 |       25
     42       5.870472e+05      -4.576162e+00 |       24
     43       5.870414e+05      -5.828473e+00 |       28
     44       5.870345e+05      -6.850324e+00 |       29
     45       5.870244e+05      -1.009532e+01 |       26
     46       5.870076e+05      -1.681905e+01 |       29
     47       5.869798e+05      -2.785334e+01 |       32
     48       5.869404e+05      -3.936290e+01 |       32
     49       5.868937e+05      -4.673668e+01 |       31
     50       5.868333e+05      -6.034531e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 586833.3291172384)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.317728
INFO: iteration 2, average log likelihood -1.290712
INFO: iteration 3, average log likelihood -1.265032
INFO: iteration 4, average log likelihood -1.235801
INFO: iteration 5, average log likelihood -1.187507
WARNING: Variances had to be floored 23 31
INFO: iteration 6, average log likelihood -1.119643
WARNING: Variances had to be floored 5 13 18 28
INFO: iteration 7, average log likelihood -1.086465
WARNING: Variances had to be floored 6 10 15 21 25 29
INFO: iteration 8, average log likelihood -1.076681
WARNING: Variances had to be floored 8 30
INFO: iteration 9, average log likelihood -1.117256
WARNING: Variances had to be floored 23 31
INFO: iteration 10, average log likelihood -1.084855
WARNING: Variances had to be floored 5 13 16 27 28
INFO: iteration 11, average log likelihood -1.070255
WARNING: Variances had to be floored 6 7 15 18 21
INFO: iteration 12, average log likelihood -1.088119
WARNING: Variances had to be floored 10 29
INFO: iteration 13, average log likelihood -1.100710
WARNING: Variances had to be floored 5 8 23 30 31
INFO: iteration 14, average log likelihood -1.069414
WARNING: Variances had to be floored 13 25 28
INFO: iteration 15, average log likelihood -1.096029
WARNING: Variances had to be floored 6 15 16 29
INFO: iteration 16, average log likelihood -1.089211
WARNING: Variances had to be floored 7 10 18 27
INFO: iteration 17, average log likelihood -1.091768
WARNING: Variances had to be floored 5 21 23 31
INFO: iteration 18, average log likelihood -1.079180
WARNING: Variances had to be floored 13 28
INFO: iteration 19, average log likelihood -1.105633
WARNING: Variances had to be floored 8 15 25 29 30
INFO: iteration 20, average log likelihood -1.067207
WARNING: Variances had to be floored 7 16 31
INFO: iteration 21, average log likelihood -1.088181
WARNING: Variances had to be floored 5 10 18 23 27
INFO: iteration 22, average log likelihood -1.085632
WARNING: Variances had to be floored 13
INFO: iteration 23, average log likelihood -1.113783
WARNING: Variances had to be floored 21 28 29 31
INFO: iteration 24, average log likelihood -1.072014
WARNING: Variances had to be floored 6 8 15 23 25 30
INFO: iteration 25, average log likelihood -1.066301
WARNING: Variances had to be floored 5 7 16
INFO: iteration 26, average log likelihood -1.092461
WARNING: Variances had to be floored 10 13 29
INFO: iteration 27, average log likelihood -1.096237
WARNING: Variances had to be floored 18 27 31
INFO: iteration 28, average log likelihood -1.086567
WARNING: Variances had to be floored 5 15 21 23 28
INFO: iteration 29, average log likelihood -1.067998
WARNING: Variances had to be floored 7 30
INFO: iteration 30, average log likelihood -1.101860
WARNING: Variances had to be floored 6 8 13 16 29
INFO: iteration 31, average log likelihood -1.085116
WARNING: Variances had to be floored 10 25 31
INFO: iteration 32, average log likelihood -1.089720
WARNING: Variances had to be floored 5 15 18 23
INFO: iteration 33, average log likelihood -1.083140
WARNING: Variances had to be floored 7 28 29
INFO: iteration 34, average log likelihood -1.097429
WARNING: Variances had to be floored 8 13 16 21 27 30 31
INFO: iteration 35, average log likelihood -1.079230
INFO: iteration 36, average log likelihood -1.122358
WARNING: Variances had to be floored 6 10 15 23
INFO: iteration 37, average log likelihood -1.064614
WARNING: Variances had to be floored 5 18 25 29 31
INFO: iteration 38, average log likelihood -1.063902
WARNING: Variances had to be floored 7 13 28
INFO: iteration 39, average log likelihood -1.105705
WARNING: Variances had to be floored 8 16 30
INFO: iteration 40, average log likelihood -1.088881
WARNING: Variances had to be floored 6 10 15 23 31
INFO: iteration 41, average log likelihood -1.073015
WARNING: Variances had to be floored 5 18 25 29
INFO: iteration 42, average log likelihood -1.103123
WARNING: Variances had to be floored 13 27
INFO: iteration 43, average log likelihood -1.105978
WARNING: Variances had to be floored 7 28
INFO: iteration 44, average log likelihood -1.070443
WARNING: Variances had to be floored 6 8 10 16 18 21 23 30 31
INFO: iteration 45, average log likelihood -1.040159
WARNING: Variances had to be floored 5 15 25 29
INFO: iteration 46, average log likelihood -1.111700
WARNING: Variances had to be floored 13
INFO: iteration 47, average log likelihood -1.131438
INFO: iteration 48, average log likelihood -1.092613
WARNING: Variances had to be floored 5 6 7 10 23 27 28 29 31
INFO: iteration 49, average log likelihood -1.031452
WARNING: Variances had to be floored 8 15 16 18 25
INFO: iteration 50, average log likelihood -1.098619
INFO: EM with 100000 data points 50 iterations avll -1.098619
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0454188   -0.0330607   -0.102216     0.0576357   -0.0272405    0.0341999    0.126552     0.0665103  -0.0929325     0.00802653    0.0344548    0.0656217   -0.0467059    0.0789945    0.0392835    0.0223979    0.189222     0.107821    -0.113313    -0.0396597   -0.0217616   -0.00772797   0.0713617   0.000967809   0.0807726     0.00907685
  0.0272914    0.0624028    0.030364    -0.0258131    0.025073     0.035599     0.031024     0.0617831   0.000654863  -0.0725733    -0.0350016    0.145504    -0.0582743   -0.0825547   -0.0037254    0.0470807    0.0304853    0.0217235   -0.0659789    0.047254     0.218051     0.0581107    0.166812    0.0816278     0.0995443     0.0633068 
 -0.00746281  -0.0619586    0.0562512   -0.174981     0.0179478    0.0315159   -0.0314522    0.183266   -0.101307      0.0771873     0.0507783   -0.0754501   -0.220201    -0.110772    -0.190256     0.0839459    0.0400173   -0.00132146   0.122527    -0.0125897    0.026041    -0.106183    -0.0735374  -0.0254301     0.150678      0.120258  
  0.00717494   0.077822     0.0389506    0.0315402    0.0829871   -0.0867168   -0.0443314   -0.0175043  -0.130445      0.0468591     0.0473103   -0.0491546   -0.0458592    0.041057    -0.0250035   -0.0324       0.00744558  -0.0532962   -0.00173383   0.05811     -0.120781     0.0315408    0.0963706   0.0270823     0.0568518    -0.0563342 
  0.158675    -0.0449775    0.0615567    0.0368663   -0.0550601   -0.0246671    0.0960883    0.0328609  -0.0414072     0.30376      -0.0955393    0.150809    -0.00499152   0.0161036   -0.0197008    0.0629112    0.0977132   -0.00400526   0.0128092   -0.0568871   -0.0141214   -0.141798    -0.0369701  -0.00375952   -0.0720048    -0.143686  
 -0.00045053   0.0366519    0.0740533    0.0558714   -0.0736098    0.0161914    0.0919874   -0.139027    0.152557     -0.0540393     0.080604    -0.0972511   -0.185737     0.179641    -0.0450778   -0.0220514    0.00505212   0.139249     0.0165519    0.0720139   -0.0295492   -0.106671     0.0664195  -0.0379204     0.0414779     0.0378269 
 -0.189556     0.0791224   -0.027648     0.0698953   -0.335696    -0.155517    -0.0563371   -0.0524815  -0.0662943    -0.015354      0.0605709    0.138419     0.0689134   -0.0207218   -0.216577    -0.0572915    0.138416     0.0430809    0.00860173  -0.0804983   -0.0283678    0.115149    -0.0466357   0.0428025    -0.199383     -0.057355  
 -0.0193657    0.0671858   -0.0679765    0.0997054    0.0427761   -0.0165421    0.00548986   0.0464267  -0.0746163     0.0344492     0.0467893   -0.146509    -0.105235    -0.0563312    0.0332713   -0.097264     0.0254728    0.101021     0.0242883   -0.0883398    0.0255502    0.224468     0.0456902   0.103997     -0.0737874    -0.0368947 
 -0.169356    -0.0813358   -0.137118     0.0368518   -0.0616511    0.0206393   -0.00892421  -0.225595    0.00246843    0.0876341    -0.129825    -0.112968    -0.0169819    0.00844141   0.0726382    0.21503     -0.0693658   -0.0386886    0.0414141   -0.0746303    0.0938436   -0.0651107    0.199457    0.0609859    -0.0315468     0.220109  
  0.00320867  -0.0413873    0.146264    -0.065941     0.0470451   -0.00196632   0.156366     0.0382511  -0.00629439   -0.0581038     0.0539887    0.0605771    0.111389    -0.218142    -0.0977765   -0.0958898   -0.0571943    0.0609272    0.0151844   -0.0740148    0.0186578   -0.10705      0.0829811   0.00964283    0.0707717     0.211904  
  0.109669     0.00112065   0.0263288    0.0200846   -0.0940647    0.0105733    0.199297     0.0345885   0.0317544    -0.13513      -0.0706932    0.0396739    0.0480391    0.0180273   -0.0230489    0.044995     0.0157874    0.164894    -0.190911     0.0903575   -0.181253    -0.0702744   -0.0431826   0.00446064   -0.0331681     0.0706291 
 -0.164779     0.0173142   -0.0988869    0.00742877   0.00461189   0.191721     0.0102752    0.0402931  -0.0506058    -0.033716     -0.0511275    0.205363    -0.0576585    0.0467784    0.015399    -0.115043    -0.0289491   -0.0959195   -0.0214939    0.0626406   -0.00667159  -0.0502758   -0.0405743  -0.0213579    -0.0473984     0.075036  
  0.00101449  -0.0719884    0.0539352   -0.0484472    0.115339     0.0551244    0.0575306    0.052733   -0.00306154    0.00902663   -0.0204981    0.0149579    0.0320632    0.0323072   -0.0859443    0.0696021    0.0415918   -0.175735     0.0240262   -0.145678     0.0126453   -0.0573612    0.0285761  -0.110818     -0.0864663    -0.0699165 
 -0.16238     -0.0689181   -0.210754    -0.118399     0.055576    -0.201931    -0.0605508    0.0263358  -0.0277237    -0.0353375     0.0592921   -0.194758    -0.0913881    0.182945    -0.00655985  -0.050966     0.0480075   -0.103846    -0.195293     0.0827705    0.0148322   -0.0941534    0.10792    -0.14266       0.070833      0.0485848 
  0.0391862    0.0163798   -0.0451699    0.00228652   0.224783     0.148947    -0.16761      0.0705171  -0.0202544     0.0965614     0.0118196    0.0655369   -0.00327689   0.102082    -0.0667381   -0.00157456  -0.104233     0.020422    -0.211813     0.0124469    0.0621464    0.139434    -0.0890866   0.0250589    -0.212137      0.107733  
 -0.142552     0.118116    -0.0652798    0.0430056    0.135667     0.0721196    0.140758    -0.117519    0.0884964    -0.0306798    -0.123054    -0.105138     0.0209418    0.00746983  -0.136279     0.107681     0.0811996    0.00466904   0.0477595   -0.00461629   0.107348     0.0114113    0.0523508  -0.111624     -0.0379543    -0.0415701 
  0.10257      0.0167206    0.184667    -0.198517    -0.0763732    0.100965     0.0605143   -0.054251   -0.164417      0.000545833  -0.150261     0.06796      0.0689098   -0.00993335  -0.0532877    0.211035    -0.0949485   -0.103034     0.048534     0.0818579   -0.102118    -0.0806968    0.0319381  -0.0132682    -0.0287289     0.0986461 
 -0.0137007    0.0527134    0.0390156    0.0882712    0.00508017   0.0289853    0.0541726   -0.132923    0.0913203    -0.0493049     0.0989265   -0.0947864   -0.164754     0.0538417    0.00211126   0.0259457    0.00855048   0.171975    -0.0204032    0.0751534   -0.0589691   -0.0748831    0.0359474  -0.0595402     0.0681325     0.0374262 
 -0.0522605    0.13534     -0.0397178    0.0600545   -0.036142     0.030048    -0.046142     0.0195141  -0.170357     -0.123018      0.0559397   -0.048157    -0.202143     0.0488231    0.0754152    0.0324433    0.118151    -0.014126    -0.0309254    0.0132355   -0.198742     0.0428283   -0.105844   -0.062442     -0.0300394    -0.0642491 
  0.0558992    0.0065445   -0.00920914  -0.203637     0.149608    -0.146959    -0.0517512   -0.088      -0.0179894    -0.142546     -0.11832      0.0154235   -0.133616    -0.0434923    0.131842    -0.0224041   -0.0147075    0.0295809   -0.0359019    0.0405136    0.0915636   -0.260188     0.112336   -0.00925192    0.00578099   -0.0470569 
 -0.122241     0.127547    -0.262117    -0.0424674   -0.0366126   -0.0115199   -0.137832     0.147139   -0.0752119     0.0843478     0.0389102   -0.0142356   -0.0643107   -0.00510034  -0.0363666   -0.0157297   -0.0824371    0.0814989   -0.100741    -0.120968    -0.0949664    0.0245322    0.0151167  -0.0394853     0.000679733  -0.217681  
 -0.126449     0.0371447    0.0244699    0.0230136   -0.0709645   -0.0823582   -0.0737965    0.0984453  -0.0362522     0.167326      0.0299362    0.0647627    0.0549039   -0.133157    -0.136058     0.0468575   -0.00881118  -0.0427987    0.164552     0.00733571   0.169439    -0.0339055    0.0880643   0.0809664     0.0591985    -0.204816  
 -0.155467     0.177729    -0.175949    -0.019588    -0.0681548   -0.0374502   -0.108444     0.101418   -0.0750224     0.0385552     0.0533713    0.00236935  -0.041633     0.0228324   -0.0684006   -0.0408702   -0.0881962    0.083155    -0.0938446   -0.120696    -0.0853753    0.0422869    0.0174275  -0.0395037    -0.0384142    -0.172755  
 -0.127757     0.0530065    0.076776     0.264282     0.0846899   -0.0734712   -0.200703    -0.169697   -0.0619383    -0.278927      0.0367042   -0.0214811   -0.0169265    0.030398     0.046756    -0.0684925   -0.106571    -0.0244709   -0.0192618    0.142791    -0.165753    -0.0269957   -0.109787    0.266197     -0.112418     -0.0223391 
 -0.0798929   -0.0114555    0.0429185    0.0596652   -0.0555833    0.0957866   -0.175337    -0.0477832  -0.0115692    -0.0491243     0.0314068    0.0721317   -0.158538     0.0478208   -0.0496891   -0.00949514   0.00604173   0.0764871    0.0377426   -0.225647     0.0237987   -0.00679291   0.0591911   0.11474      -0.0111864    -0.0256201 
  0.0241231    0.0692481   -0.180662    -0.0186212    0.128681     0.190244    -0.0492213   -0.108429    0.0467538     0.113802     -0.0319498    0.0685583    0.0302926   -0.00884467  -0.0744191    0.14543     -0.125337    -0.0867002   -0.0599934    0.104268     0.145977     0.0626483   -0.0044338  -0.0799839     0.0644386     0.107332  
 -0.0628141   -0.046257     0.0193612   -0.0810498   -0.180426     0.0326823   -0.00801042  -0.150384    0.118326     -0.0275471    -0.0419863   -0.0145752    0.15054     -0.235691    -0.0635707   -0.00449081  -0.108644     0.201128     0.0943135   -0.0185189   -0.0246877   -0.121649    -0.121827   -0.0532744     0.00209886   -0.0227353 
  0.17261     -0.0997978    0.226159     0.0532663    0.0445585    0.0608462    0.0316662   -0.125664    0.133604      0.103819      0.100897     0.0197798   -0.0566499   -0.0623667   -0.0697472   -0.0308051   -0.059635    -0.0409254   -0.135394     0.0252312    0.076646    -0.0868347   -0.173787   -0.0615892    -0.0776075     0.00306699
 -0.0790946    0.0822836   -0.0786335    0.0171038   -0.020397     0.0550246   -0.0521905    0.144736   -0.0449225     0.0762771    -0.090459    -0.0485095   -0.145981    -0.0316615   -0.0102991    0.165358    -0.125071     0.105013    -0.0214777    0.0251626   -0.0452772   -0.0400652   -0.0124206   0.0588189    -0.0166283     0.0533175 
 -0.0253299   -0.0871034    0.0399233   -0.224731     0.0533944    0.0299396   -0.0463255   -0.0473607  -0.118678     -0.0630312    -0.00021249   0.141947     0.189882     0.0396305   -0.0238726   -0.0289616    0.00138297   0.0149107   -0.214042    -0.090731    -0.0817779   -0.116122    -0.147142    0.0297678     0.122836     -0.0321852 
  0.00346436   0.0623875    0.0174386   -0.171519    -0.122786    -0.00180983  -0.141119     0.0376679  -0.0811639     0.145112     -0.300863     0.0452264    0.188122    -0.0313576   -0.0341744   -0.189959    -0.123167    -0.106514     0.142127     0.0401531    0.0146506    0.0124447   -0.0845088   0.0940985     0.0596003     0.096903  
  0.0487718   -0.0286043   -0.02178     -0.0848522    0.109138    -0.0879964   -0.0984182    0.085082   -0.0798399    -0.0761359     0.216967    -0.0140334    0.082831     0.00280626   0.166162     0.0443639    0.0290979   -0.0698958   -0.023132    -0.140765    -0.00764028  -0.0778228   -0.0538295   0.0879647     0.00637625    0.0538422 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 13 21 30
INFO: iteration 1, average log likelihood -1.122792
WARNING: Variances had to be floored 5 13 21 30
INFO: iteration 2, average log likelihood -1.082402
WARNING: Variances had to be floored 6 13 21 29 30 31
INFO: iteration 3, average log likelihood -1.043454
WARNING: Variances had to be floored 5 7 8 10 13 15 16 18 21 25 28 30
INFO: iteration 4, average log likelihood -1.022301
WARNING: Variances had to be floored 13 21 29 30 31
INFO: iteration 5, average log likelihood -1.100776
WARNING: Variances had to be floored 5 13 19 21 27 30
INFO: iteration 6, average log likelihood -1.069644
WARNING: Variances had to be floored 6 13 21 29 30
INFO: iteration 7, average log likelihood -1.044186
WARNING: Variances had to be floored 5 7 8 10 13 15 16 18 19 21 25 28 30 31
INFO: iteration 8, average log likelihood -0.997315
WARNING: Variances had to be floored 13 21 29 30
INFO: iteration 9, average log likelihood -1.108253
WARNING: Variances had to be floored 5 13 19 21 30
INFO: iteration 10, average log likelihood -1.065278
INFO: EM with 100000 data points 10 iterations avll -1.065278
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.121332    0.033459    -0.0182592    0.0302209   -0.255575      0.0497607    -0.0881383  -0.0992017    -0.0839247   -0.0503658    0.0665222   -0.131358    -0.0661546    0.0210606   -0.0460081     0.106486    -0.149461     0.14439      -0.0542063  -0.134627     0.0841341   -0.00460537   0.127621     -0.251626     0.013517    -0.0110572 
  0.0943849  -0.0228842   -0.00862989   0.02111      0.14739       0.185706     -0.0892747  -0.165479     -0.14683     -0.146328    -0.182015    -0.0475076    0.0947287   -0.218887    -0.0775636    -0.104723     0.102384     0.0901694    -0.139593   -0.125136    -0.0888471   -0.181723     0.108026      0.0741236    0.0879017    0.146115  
  0.127254   -0.125672     0.160639    -0.0227977   -0.00321797    0.158776      0.0641837  -0.0724095    -0.0568681    0.0542285   -0.0210866   -0.00967508  -0.0274591   -0.0386085   -0.0220407    -0.0878694    0.11674      0.130685     -0.067111    0.0675511    0.0286792   -0.152187     0.0312981     0.0576797   -0.0169835    0.0117145 
  0.120146   -0.215688    -0.261845    -0.0697919    0.183401     -0.020409     -0.116409    0.0717133    -0.0138577    0.0710788   -0.0862727    0.0418161   -0.0404351    0.139923    -0.131607      0.036208     0.148369    -0.0107044     0.0184011   0.167105     0.0577522    0.0635048   -0.0313201    -0.0844659   -0.0511255   -0.103856  
 -0.142726    0.141847    -0.0551288    0.0302046   -0.116818     -0.0264439    -0.0392863   0.117416     -0.100003     0.0553623   -0.0765065   -0.120162     0.15504      0.240269     0.099829     -0.00682132   0.0696003    0.20707       0.126506    0.0889759   -0.113156    -0.0328967    0.166558     -0.183592     0.13307     -0.204326  
 -0.0846326  -0.0616753    0.231388     0.083806    -0.187455      0.021404     -0.0802357  -0.120991      0.0584162    0.0278284    0.0284411    0.00178145   0.0818599   -0.0885588   -0.0589549    -0.00410594  -0.039369    -0.0269747     0.0958203  -0.0282754    0.0296574   -0.0260169    0.00244666    0.0234877   -0.127566    -0.0150121 
  0.0496054  -0.0824543   -0.0753943    0.0904264   -0.152175      0.0205632     0.187792   -0.0295434    -0.00843464   0.0181545    0.15525     -0.0212046   -0.137326    -0.035848     0.0777681    -0.0121873   -0.0528093   -0.0663323    -0.0566475  -0.00542145  -0.0547937   -0.0988483   -0.0428412    -0.0370129    0.0586957   -0.0898108 
 -0.100215   -0.0525315   -0.0656043    0.15612     -0.0835365     0.0769354     0.0157064  -0.0479972    -0.0815109    0.034768    -0.144216     0.136103     0.0919172   -0.120055     0.195705     -0.0897534    0.0162107   -0.0560152     0.212817   -0.0425529    0.0543405   -0.0319652   -0.12948       0.0594585   -0.0232382    0.0478107 
 -0.0303473  -0.120328     0.0824108    0.139275     0.0963372    -0.0151827     0.130135   -0.196662      0.0744607    0.0392936    0.184288    -0.0194822    0.0921857    0.079482    -0.0164037    -0.168298    -0.127557     0.13059      -0.126723   -0.00448073   0.0872179   -0.0715147   -0.0400267    -0.210268     0.0117276   -0.140403  
 -0.0124568  -0.0296989    0.00819353  -0.00284635   0.115438      0.000921091   0.0976661   0.126241     -0.112078    -0.110956     0.0441928   -0.0755434    0.0543565   -0.0136668   -0.0948478    -0.0329884    0.0827407    0.110543     -0.201767    0.0685784   -0.00455547   0.037798    -0.0946658    -0.0704179   -0.0444799    0.0823473 
 -0.0730365  -0.169613     0.0117639    0.0347464   -0.0709927     0.0385814     0.130445   -0.0361133    -0.0690544   -0.1501       0.0708519    0.0726226    0.0252212    0.0280074    0.098136     -0.134604    -0.0467       0.0486722    -0.264568   -0.0351727    0.134948     0.215577     0.157993      0.0761602    0.0854767    0.129162  
 -0.0367544   0.286662    -0.020485     0.00680795  -0.0666716    -0.163106     -0.042802   -0.0689578     0.088045    -0.0757797    0.0760012    0.0194998   -0.0316274   -0.0454101   -0.100002     -0.160531    -0.143137     0.13368      -0.0837411   0.110101    -0.171222    -0.14254      0.0129096     0.117832    -0.0565388    0.0526486 
 -0.0754247   0.249171    -0.0244454   -0.0696396   -0.0496717    -0.120858      0.014525   -0.0219827     0.237592    -0.048394     0.171462     0.154832     0.108998     0.025447     0.109337     -0.201115     0.0726092   -0.108551     -0.232024    0.189174     0.112111    -0.00765269   0.0190161     0.0213097   -0.148444     0.0735695 
  0.0356114  -0.0700813    0.0436165   -0.0122066    0.0412306     0.0801968    -0.0773656  -0.104813      0.0349419   -0.123732     0.0977742   -0.0611718   -0.0585463   -0.020073     0.0396459    -0.0696015    0.150515    -0.0697723    -0.0955392  -0.0940453   -0.113972     0.0400143   -0.0373046     0.0453883   -0.0398575    0.0164375 
  0.0502962  -0.0395957    0.0443448   -0.109377     0.0748746     0.01468      -0.104386    0.0555074     0.167921    -0.0231892    0.062593     0.0607199   -0.143861     0.0862797   -0.0962102     0.0363845   -0.00440294   0.0836336    -0.0830308  -0.0879852    0.00443565   0.0607787    0.130162     -0.0499088   -0.129398     0.00358779
 -0.147811    0.0556901   -0.084297     0.0968196   -0.0946122     0.0324104     0.0365152  -0.0226714    -0.171193    -0.0834659   -0.0890541    0.104165    -0.0144761    0.109817    -0.000337457  -0.0932795   -0.0312044    0.127159      0.115592    0.0120862    0.0116114    0.0959452   -0.145277      0.0787602   -0.285551    -0.171668  
  0.022106   -0.0521766   -0.174054     0.066052     0.0918321     0.127449      0.119014    0.056905      0.0548018    0.031322    -0.0559544   -0.0401656    0.0393846   -0.0269995   -0.0454711    -0.0142455    0.046597    -0.0267008     0.0165065   0.0575755    0.0270405   -0.0395004   -0.12584      -0.249725    -0.0819888   -0.185506  
  0.130887    0.0415602   -0.103844     0.106494    -0.0432567     0.0822414     0.138716    0.190266     -0.0125765    0.191157    -0.0208187    0.0682271    0.00995984  -0.0848516    0.111412     -0.0507539   -0.20541     -0.0301325     0.139518   -0.0337543   -0.152418     0.0797427   -0.0106091     0.155134     0.0791732    0.0119826 
  0.141339   -0.0947791   -0.00701428  -0.0452441   -0.00582573    0.0858176    -0.0316424  -0.126938      0.101136     0.0995071    0.0737479   -0.0840076   -0.107888    -0.0233694   -0.0239105    -0.0568033   -0.113327    -0.0607587    -0.0865833  -0.0487732   -0.0412936    0.081795     0.0108481     0.0877357   -0.0504833   -0.00816518
 -0.102292   -0.0811956   -0.014455     0.216409     0.236816     -0.041461     -0.0469937   0.0685069    -0.11384     -0.0975821    0.123205    -0.035221    -0.00325392   0.0522981    0.0283639     0.0324661   -0.0143545    0.13556       0.0953733  -0.00597294  -0.133228    -0.062521    -0.000477479   0.0693544    0.057019     0.0423717 
 -0.113593   -0.0961122    0.0177678    0.0515638    0.0372193    -0.084986      0.0400104   0.0501068     0.0983569   -0.0682224   -0.0425236    0.0264339    0.186617    -0.0545873    0.0547998    -0.138928    -0.0130516    0.0463055     0.153217   -0.128227     0.0736327   -0.145441     0.0318086    -0.144737    -0.179529     0.0770751 
 -0.102715   -0.0299492    0.0570488    0.0670909    0.0569445     0.0797512    -0.132494    0.000214239   0.0445749    0.0394457   -0.0312146    0.191784    -0.00326511   0.0233896    0.126631     -0.0331734   -0.171094     0.0601848     0.0773796   0.0278697   -0.11627     -0.0321575    0.0177531     0.0248096    0.0625905    0.0907588 
 -0.0554889  -0.174937    -0.13373      0.169354     0.0252344     0.145698      0.0416738   0.109347     -0.0804113   -0.0416236   -0.0316011   -0.0864906   -0.103053     0.093985     0.118747      0.0588417   -0.112993     0.000611848   0.0164443  -0.0321754    0.0481541   -0.0513222    0.00871676    0.00241626  -0.108827    -0.0632964 
 -0.13328     0.0315356    0.249665     0.0987801   -0.000107269   0.0397041     0.0331733   0.136856     -0.020102    -0.0381207    0.126062    -0.0626573    0.216348    -0.146917    -0.0683658     0.0198875    0.140533    -0.000854042  -0.0965428  -0.228093    -0.0308084   -0.0147971    0.212005     -0.138201     0.142106     0.117872  
 -0.0550753   0.102865     0.213922     0.116652     0.0327081     0.0685237    -0.0219756   0.0235892    -0.0591738    0.00129643  -0.00562494  -0.0404977   -0.0167444   -0.00734143  -0.00281021    0.141323    -0.0685794    0.0630691     0.0863558  -0.0382429    0.0565873   -0.0285473   -0.139966      0.14713      0.105417    -0.0151197 
  0.177643   -0.15373     -0.076531    -0.0321843    0.192153      0.11152       0.101939   -0.0772866    -0.169629     0.0459037    0.0479754   -0.153407    -0.0661126    0.140614     0.136319     -0.0817305    0.0792648    0.0529328     0.0696815  -0.0358803   -0.0363735    0.016377    -0.0981791    -0.0207335   -0.0187957    0.0920023 
 -0.0791392  -0.0536051   -0.0396243   -0.0532672   -0.0492659    -0.0327627    -0.0456598   0.203472      0.0598723    0.0798385    0.0801387   -0.0604062   -0.0799613   -0.155462    -0.206511     -0.0414433    0.0787073   -0.0951645    -0.0236004  -0.0607534    0.13579     -0.189848    -0.114803     -0.10124      0.100439    -0.0842101 
  0.045063    0.0457518   -0.0621356    0.189877    -0.0565773     0.179823      0.104593    0.00875293    0.0714783   -0.103801     0.0195152    0.016025     0.0441838    0.0171453    0.109554      0.158517     0.0163517   -0.00693598    0.121719    0.045758    -0.131609    -0.0683524   -0.087825      0.0879746   -0.0581128    0.173357  
 -0.0857233   0.118161    -0.22337     -0.148748     0.121691      0.0910387     0.16956     0.0198394     0.0147819    0.00534467  -0.205991     0.183821     0.0897964    0.257313    -0.0816522     0.143985     0.0626183    0.0169832     0.122156    0.0177948   -0.0495918    0.0653815    0.0397794    -0.0154765    0.00440709   0.0269831 
 -0.0451931   0.00532495   0.0809439   -0.0278381   -0.0436581     0.0562664    -0.107847    0.24685       0.171039     0.122757     0.221341    -0.00764726   0.205614     0.123316    -0.174383      0.0134568   -0.112135     0.150809      0.0720539  -0.0413017    0.099856     0.0305371   -0.0333848    -0.166734     0.0199857    0.0892876 
  0.228432   -0.0877464    0.156271    -0.00346347   0.0116353    -0.241004      0.200732   -0.139488      0.0903196    0.0441524   -0.0438051    0.105611    -0.0475008   -0.0251362    0.0843277    -0.0541672    0.0136781    0.145164     -0.092529   -0.00492048   0.0521055   -0.0865046   -0.00337889    0.0664917   -0.0223829   -0.244171  
 -0.132131   -0.214277    -0.0382645    0.0314253    0.0238388     0.017406     -0.124455   -0.120639     -0.203101    -0.0427529    0.00924455  -0.0813167    0.045988    -0.0440092    0.143403      0.0832158   -0.0449549   -0.0370401    -0.270048   -0.00101914   0.0310413    0.213838     0.0390123     0.0844475   -0.0111384   -0.0880843 kind full, method split
0: avll = -1.4155249835994685
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.415544
INFO: iteration 2, average log likelihood -1.415488
INFO: iteration 3, average log likelihood -1.415450
INFO: iteration 4, average log likelihood -1.415404
INFO: iteration 5, average log likelihood -1.415346
INFO: iteration 6, average log likelihood -1.415274
INFO: iteration 7, average log likelihood -1.415188
INFO: iteration 8, average log likelihood -1.415093
INFO: iteration 9, average log likelihood -1.414998
INFO: iteration 10, average log likelihood -1.414911
INFO: iteration 11, average log likelihood -1.414836
INFO: iteration 12, average log likelihood -1.414770
INFO: iteration 13, average log likelihood -1.414702
INFO: iteration 14, average log likelihood -1.414616
INFO: iteration 15, average log likelihood -1.414478
INFO: iteration 16, average log likelihood -1.414233
INFO: iteration 17, average log likelihood -1.413800
INFO: iteration 18, average log likelihood -1.413107
INFO: iteration 19, average log likelihood -1.412201
INFO: iteration 20, average log likelihood -1.411324
INFO: iteration 21, average log likelihood -1.410705
INFO: iteration 22, average log likelihood -1.410361
INFO: iteration 23, average log likelihood -1.410196
INFO: iteration 24, average log likelihood -1.410120
INFO: iteration 25, average log likelihood -1.410087
INFO: iteration 26, average log likelihood -1.410071
INFO: iteration 27, average log likelihood -1.410064
INFO: iteration 28, average log likelihood -1.410060
INFO: iteration 29, average log likelihood -1.410058
INFO: iteration 30, average log likelihood -1.410057
INFO: iteration 31, average log likelihood -1.410056
INFO: iteration 32, average log likelihood -1.410055
INFO: iteration 33, average log likelihood -1.410055
INFO: iteration 34, average log likelihood -1.410054
INFO: iteration 35, average log likelihood -1.410054
INFO: iteration 36, average log likelihood -1.410054
INFO: iteration 37, average log likelihood -1.410053
INFO: iteration 38, average log likelihood -1.410053
INFO: iteration 39, average log likelihood -1.410053
INFO: iteration 40, average log likelihood -1.410052
INFO: iteration 41, average log likelihood -1.410052
INFO: iteration 42, average log likelihood -1.410052
INFO: iteration 43, average log likelihood -1.410052
INFO: iteration 44, average log likelihood -1.410052
INFO: iteration 45, average log likelihood -1.410051
INFO: iteration 46, average log likelihood -1.410051
INFO: iteration 47, average log likelihood -1.410051
INFO: iteration 48, average log likelihood -1.410051
INFO: iteration 49, average log likelihood -1.410051
INFO: iteration 50, average log likelihood -1.410051
INFO: EM with 100000 data points 50 iterations avll -1.410051
952.4 data points per parameter
1: avll = [-1.41554,-1.41549,-1.41545,-1.4154,-1.41535,-1.41527,-1.41519,-1.41509,-1.415,-1.41491,-1.41484,-1.41477,-1.4147,-1.41462,-1.41448,-1.41423,-1.4138,-1.41311,-1.4122,-1.41132,-1.4107,-1.41036,-1.4102,-1.41012,-1.41009,-1.41007,-1.41006,-1.41006,-1.41006,-1.41006,-1.41006,-1.41006,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.410070
INFO: iteration 2, average log likelihood -1.410012
INFO: iteration 3, average log likelihood -1.409972
INFO: iteration 4, average log likelihood -1.409923
INFO: iteration 5, average log likelihood -1.409861
INFO: iteration 6, average log likelihood -1.409784
INFO: iteration 7, average log likelihood -1.409693
INFO: iteration 8, average log likelihood -1.409594
INFO: iteration 9, average log likelihood -1.409496
INFO: iteration 10, average log likelihood -1.409406
INFO: iteration 11, average log likelihood -1.409329
INFO: iteration 12, average log likelihood -1.409265
INFO: iteration 13, average log likelihood -1.409212
INFO: iteration 14, average log likelihood -1.409166
INFO: iteration 15, average log likelihood -1.409126
INFO: iteration 16, average log likelihood -1.409091
INFO: iteration 17, average log likelihood -1.409060
INFO: iteration 18, average log likelihood -1.409032
INFO: iteration 19, average log likelihood -1.409008
INFO: iteration 20, average log likelihood -1.408986
INFO: iteration 21, average log likelihood -1.408966
INFO: iteration 22, average log likelihood -1.408948
INFO: iteration 23, average log likelihood -1.408931
INFO: iteration 24, average log likelihood -1.408916
INFO: iteration 25, average log likelihood -1.408902
INFO: iteration 26, average log likelihood -1.408889
INFO: iteration 27, average log likelihood -1.408876
INFO: iteration 28, average log likelihood -1.408865
INFO: iteration 29, average log likelihood -1.408854
INFO: iteration 30, average log likelihood -1.408844
INFO: iteration 31, average log likelihood -1.408835
INFO: iteration 32, average log likelihood -1.408826
INFO: iteration 33, average log likelihood -1.408818
INFO: iteration 34, average log likelihood -1.408811
INFO: iteration 35, average log likelihood -1.408804
INFO: iteration 36, average log likelihood -1.408797
INFO: iteration 37, average log likelihood -1.408791
INFO: iteration 38, average log likelihood -1.408786
INFO: iteration 39, average log likelihood -1.408781
INFO: iteration 40, average log likelihood -1.408777
INFO: iteration 41, average log likelihood -1.408773
INFO: iteration 42, average log likelihood -1.408769
INFO: iteration 43, average log likelihood -1.408766
INFO: iteration 44, average log likelihood -1.408763
INFO: iteration 45, average log likelihood -1.408760
INFO: iteration 46, average log likelihood -1.408757
INFO: iteration 47, average log likelihood -1.408755
INFO: iteration 48, average log likelihood -1.408753
INFO: iteration 49, average log likelihood -1.408751
INFO: iteration 50, average log likelihood -1.408750
INFO: EM with 100000 data points 50 iterations avll -1.408750
473.9 data points per parameter
2: avll = [-1.41007,-1.41001,-1.40997,-1.40992,-1.40986,-1.40978,-1.40969,-1.40959,-1.4095,-1.40941,-1.40933,-1.40927,-1.40921,-1.40917,-1.40913,-1.40909,-1.40906,-1.40903,-1.40901,-1.40899,-1.40897,-1.40895,-1.40893,-1.40892,-1.4089,-1.40889,-1.40888,-1.40886,-1.40885,-1.40884,-1.40883,-1.40883,-1.40882,-1.40881,-1.4088,-1.4088,-1.40879,-1.40879,-1.40878,-1.40878,-1.40877,-1.40877,-1.40877,-1.40876,-1.40876,-1.40876,-1.40876,-1.40875,-1.40875,-1.40875]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.408763
INFO: iteration 2, average log likelihood -1.408713
INFO: iteration 3, average log likelihood -1.408679
INFO: iteration 4, average log likelihood -1.408642
INFO: iteration 5, average log likelihood -1.408599
INFO: iteration 6, average log likelihood -1.408548
INFO: iteration 7, average log likelihood -1.408487
INFO: iteration 8, average log likelihood -1.408415
INFO: iteration 9, average log likelihood -1.408334
INFO: iteration 10, average log likelihood -1.408247
INFO: iteration 11, average log likelihood -1.408156
INFO: iteration 12, average log likelihood -1.408066
INFO: iteration 13, average log likelihood -1.407979
INFO: iteration 14, average log likelihood -1.407896
INFO: iteration 15, average log likelihood -1.407820
INFO: iteration 16, average log likelihood -1.407750
INFO: iteration 17, average log likelihood -1.407687
INFO: iteration 18, average log likelihood -1.407630
INFO: iteration 19, average log likelihood -1.407580
INFO: iteration 20, average log likelihood -1.407537
INFO: iteration 21, average log likelihood -1.407501
INFO: iteration 22, average log likelihood -1.407470
INFO: iteration 23, average log likelihood -1.407444
INFO: iteration 24, average log likelihood -1.407421
INFO: iteration 25, average log likelihood -1.407402
INFO: iteration 26, average log likelihood -1.407386
INFO: iteration 27, average log likelihood -1.407372
INFO: iteration 28, average log likelihood -1.407359
INFO: iteration 29, average log likelihood -1.407347
INFO: iteration 30, average log likelihood -1.407336
INFO: iteration 31, average log likelihood -1.407327
INFO: iteration 32, average log likelihood -1.407317
INFO: iteration 33, average log likelihood -1.407309
INFO: iteration 34, average log likelihood -1.407301
INFO: iteration 35, average log likelihood -1.407293
INFO: iteration 36, average log likelihood -1.407285
INFO: iteration 37, average log likelihood -1.407278
INFO: iteration 38, average log likelihood -1.407270
INFO: iteration 39, average log likelihood -1.407263
INFO: iteration 40, average log likelihood -1.407256
INFO: iteration 41, average log likelihood -1.407249
INFO: iteration 42, average log likelihood -1.407242
INFO: iteration 43, average log likelihood -1.407235
INFO: iteration 44, average log likelihood -1.407228
INFO: iteration 45, average log likelihood -1.407221
INFO: iteration 46, average log likelihood -1.407214
INFO: iteration 47, average log likelihood -1.407207
INFO: iteration 48, average log likelihood -1.407200
INFO: iteration 49, average log likelihood -1.407192
INFO: iteration 50, average log likelihood -1.407185
INFO: EM with 100000 data points 50 iterations avll -1.407185
236.4 data points per parameter
3: avll = [-1.40876,-1.40871,-1.40868,-1.40864,-1.4086,-1.40855,-1.40849,-1.40841,-1.40833,-1.40825,-1.40816,-1.40807,-1.40798,-1.4079,-1.40782,-1.40775,-1.40769,-1.40763,-1.40758,-1.40754,-1.4075,-1.40747,-1.40744,-1.40742,-1.4074,-1.40739,-1.40737,-1.40736,-1.40735,-1.40734,-1.40733,-1.40732,-1.40731,-1.4073,-1.40729,-1.40729,-1.40728,-1.40727,-1.40726,-1.40726,-1.40725,-1.40724,-1.40723,-1.40723,-1.40722,-1.40721,-1.40721,-1.4072,-1.40719,-1.40719]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.407187
INFO: iteration 2, average log likelihood -1.407130
INFO: iteration 3, average log likelihood -1.407078
INFO: iteration 4, average log likelihood -1.407018
INFO: iteration 5, average log likelihood -1.406945
INFO: iteration 6, average log likelihood -1.406857
INFO: iteration 7, average log likelihood -1.406754
INFO: iteration 8, average log likelihood -1.406638
INFO: iteration 9, average log likelihood -1.406515
INFO: iteration 10, average log likelihood -1.406392
INFO: iteration 11, average log likelihood -1.406274
INFO: iteration 12, average log likelihood -1.406163
INFO: iteration 13, average log likelihood -1.406064
INFO: iteration 14, average log likelihood -1.405975
INFO: iteration 15, average log likelihood -1.405897
INFO: iteration 16, average log likelihood -1.405831
INFO: iteration 17, average log likelihood -1.405773
INFO: iteration 18, average log likelihood -1.405724
INFO: iteration 19, average log likelihood -1.405683
INFO: iteration 20, average log likelihood -1.405647
INFO: iteration 21, average log likelihood -1.405616
INFO: iteration 22, average log likelihood -1.405588
INFO: iteration 23, average log likelihood -1.405564
INFO: iteration 24, average log likelihood -1.405543
INFO: iteration 25, average log likelihood -1.405524
INFO: iteration 26, average log likelihood -1.405506
INFO: iteration 27, average log likelihood -1.405490
INFO: iteration 28, average log likelihood -1.405475
INFO: iteration 29, average log likelihood -1.405461
INFO: iteration 30, average log likelihood -1.405448
INFO: iteration 31, average log likelihood -1.405436
INFO: iteration 32, average log likelihood -1.405424
INFO: iteration 33, average log likelihood -1.405412
INFO: iteration 34, average log likelihood -1.405401
INFO: iteration 35, average log likelihood -1.405390
INFO: iteration 36, average log likelihood -1.405380
INFO: iteration 37, average log likelihood -1.405369
INFO: iteration 38, average log likelihood -1.405359
INFO: iteration 39, average log likelihood -1.405350
INFO: iteration 40, average log likelihood -1.405340
INFO: iteration 41, average log likelihood -1.405331
INFO: iteration 42, average log likelihood -1.405322
INFO: iteration 43, average log likelihood -1.405313
INFO: iteration 44, average log likelihood -1.405305
INFO: iteration 45, average log likelihood -1.405297
INFO: iteration 46, average log likelihood -1.405289
INFO: iteration 47, average log likelihood -1.405281
INFO: iteration 48, average log likelihood -1.405274
INFO: iteration 49, average log likelihood -1.405267
INFO: iteration 50, average log likelihood -1.405260
INFO: EM with 100000 data points 50 iterations avll -1.405260
118.1 data points per parameter
4: avll = [-1.40719,-1.40713,-1.40708,-1.40702,-1.40695,-1.40686,-1.40675,-1.40664,-1.40652,-1.40639,-1.40627,-1.40616,-1.40606,-1.40597,-1.4059,-1.40583,-1.40577,-1.40572,-1.40568,-1.40565,-1.40562,-1.40559,-1.40556,-1.40554,-1.40552,-1.40551,-1.40549,-1.40548,-1.40546,-1.40545,-1.40544,-1.40542,-1.40541,-1.4054,-1.40539,-1.40538,-1.40537,-1.40536,-1.40535,-1.40534,-1.40533,-1.40532,-1.40531,-1.4053,-1.4053,-1.40529,-1.40528,-1.40527,-1.40527,-1.40526]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.405261
INFO: iteration 2, average log likelihood -1.405200
INFO: iteration 3, average log likelihood -1.405141
INFO: iteration 4, average log likelihood -1.405069
INFO: iteration 5, average log likelihood -1.404976
INFO: iteration 6, average log likelihood -1.404858
INFO: iteration 7, average log likelihood -1.404714
INFO: iteration 8, average log likelihood -1.404551
INFO: iteration 9, average log likelihood -1.404382
INFO: iteration 10, average log likelihood -1.404217
INFO: iteration 11, average log likelihood -1.404063
INFO: iteration 12, average log likelihood -1.403925
INFO: iteration 13, average log likelihood -1.403802
INFO: iteration 14, average log likelihood -1.403695
INFO: iteration 15, average log likelihood -1.403600
INFO: iteration 16, average log likelihood -1.403518
INFO: iteration 17, average log likelihood -1.403446
INFO: iteration 18, average log likelihood -1.403383
INFO: iteration 19, average log likelihood -1.403327
INFO: iteration 20, average log likelihood -1.403277
INFO: iteration 21, average log likelihood -1.403233
INFO: iteration 22, average log likelihood -1.403193
INFO: iteration 23, average log likelihood -1.403158
INFO: iteration 24, average log likelihood -1.403125
INFO: iteration 25, average log likelihood -1.403095
INFO: iteration 26, average log likelihood -1.403068
INFO: iteration 27, average log likelihood -1.403042
INFO: iteration 28, average log likelihood -1.403019
INFO: iteration 29, average log likelihood -1.402997
INFO: iteration 30, average log likelihood -1.402976
INFO: iteration 31, average log likelihood -1.402956
INFO: iteration 32, average log likelihood -1.402937
INFO: iteration 33, average log likelihood -1.402919
INFO: iteration 34, average log likelihood -1.402902
INFO: iteration 35, average log likelihood -1.402885
INFO: iteration 36, average log likelihood -1.402869
INFO: iteration 37, average log likelihood -1.402854
INFO: iteration 38, average log likelihood -1.402839
INFO: iteration 39, average log likelihood -1.402824
INFO: iteration 40, average log likelihood -1.402810
INFO: iteration 41, average log likelihood -1.402796
INFO: iteration 42, average log likelihood -1.402782
INFO: iteration 43, average log likelihood -1.402769
INFO: iteration 44, average log likelihood -1.402756
INFO: iteration 45, average log likelihood -1.402743
INFO: iteration 46, average log likelihood -1.402730
INFO: iteration 47, average log likelihood -1.402718
INFO: iteration 48, average log likelihood -1.402706
INFO: iteration 49, average log likelihood -1.402695
INFO: iteration 50, average log likelihood -1.402683
INFO: EM with 100000 data points 50 iterations avll -1.402683
59.0 data points per parameter
5: avll = [-1.40526,-1.4052,-1.40514,-1.40507,-1.40498,-1.40486,-1.40471,-1.40455,-1.40438,-1.40422,-1.40406,-1.40392,-1.4038,-1.40369,-1.4036,-1.40352,-1.40345,-1.40338,-1.40333,-1.40328,-1.40323,-1.40319,-1.40316,-1.40313,-1.4031,-1.40307,-1.40304,-1.40302,-1.403,-1.40298,-1.40296,-1.40294,-1.40292,-1.4029,-1.40289,-1.40287,-1.40285,-1.40284,-1.40282,-1.40281,-1.4028,-1.40278,-1.40277,-1.40276,-1.40274,-1.40273,-1.40272,-1.40271,-1.40269,-1.40268]
[-1.41552,-1.41554,-1.41549,-1.41545,-1.4154,-1.41535,-1.41527,-1.41519,-1.41509,-1.415,-1.41491,-1.41484,-1.41477,-1.4147,-1.41462,-1.41448,-1.41423,-1.4138,-1.41311,-1.4122,-1.41132,-1.4107,-1.41036,-1.4102,-1.41012,-1.41009,-1.41007,-1.41006,-1.41006,-1.41006,-1.41006,-1.41006,-1.41006,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41005,-1.41007,-1.41001,-1.40997,-1.40992,-1.40986,-1.40978,-1.40969,-1.40959,-1.4095,-1.40941,-1.40933,-1.40927,-1.40921,-1.40917,-1.40913,-1.40909,-1.40906,-1.40903,-1.40901,-1.40899,-1.40897,-1.40895,-1.40893,-1.40892,-1.4089,-1.40889,-1.40888,-1.40886,-1.40885,-1.40884,-1.40883,-1.40883,-1.40882,-1.40881,-1.4088,-1.4088,-1.40879,-1.40879,-1.40878,-1.40878,-1.40877,-1.40877,-1.40877,-1.40876,-1.40876,-1.40876,-1.40876,-1.40875,-1.40875,-1.40875,-1.40876,-1.40871,-1.40868,-1.40864,-1.4086,-1.40855,-1.40849,-1.40841,-1.40833,-1.40825,-1.40816,-1.40807,-1.40798,-1.4079,-1.40782,-1.40775,-1.40769,-1.40763,-1.40758,-1.40754,-1.4075,-1.40747,-1.40744,-1.40742,-1.4074,-1.40739,-1.40737,-1.40736,-1.40735,-1.40734,-1.40733,-1.40732,-1.40731,-1.4073,-1.40729,-1.40729,-1.40728,-1.40727,-1.40726,-1.40726,-1.40725,-1.40724,-1.40723,-1.40723,-1.40722,-1.40721,-1.40721,-1.4072,-1.40719,-1.40719,-1.40719,-1.40713,-1.40708,-1.40702,-1.40695,-1.40686,-1.40675,-1.40664,-1.40652,-1.40639,-1.40627,-1.40616,-1.40606,-1.40597,-1.4059,-1.40583,-1.40577,-1.40572,-1.40568,-1.40565,-1.40562,-1.40559,-1.40556,-1.40554,-1.40552,-1.40551,-1.40549,-1.40548,-1.40546,-1.40545,-1.40544,-1.40542,-1.40541,-1.4054,-1.40539,-1.40538,-1.40537,-1.40536,-1.40535,-1.40534,-1.40533,-1.40532,-1.40531,-1.4053,-1.4053,-1.40529,-1.40528,-1.40527,-1.40527,-1.40526,-1.40526,-1.4052,-1.40514,-1.40507,-1.40498,-1.40486,-1.40471,-1.40455,-1.40438,-1.40422,-1.40406,-1.40392,-1.4038,-1.40369,-1.4036,-1.40352,-1.40345,-1.40338,-1.40333,-1.40328,-1.40323,-1.40319,-1.40316,-1.40313,-1.4031,-1.40307,-1.40304,-1.40302,-1.403,-1.40298,-1.40296,-1.40294,-1.40292,-1.4029,-1.40289,-1.40287,-1.40285,-1.40284,-1.40282,-1.40281,-1.4028,-1.40278,-1.40277,-1.40276,-1.40274,-1.40273,-1.40272,-1.40271,-1.40269,-1.40268]
32×26 Array{Float64,2}:
 -0.512411    0.319814    -0.151238   -0.128831     0.356987    -0.129706    0.300834   -0.74699      0.319707     0.155439    0.00922783  -0.000139739  -0.0455838   0.215019   -0.0529996   -0.899204    0.142276   -0.00835803   0.248831    0.174496    -0.00602458   0.269539   -0.347257    0.708405     -0.850675    0.287958  
 -0.556393   -0.630005    -0.379784   -0.00548291   0.194716     0.446113    0.245953    0.0959498   -0.0231866    0.280068    0.0676392    0.106393     -0.0831888   0.222434    0.0453407   -0.0458765   0.488969    0.737272     0.104959   -0.0643973    0.287038    -0.0117472   0.0159646  -0.153853     -0.359112   -0.455231  
  0.476054   -0.415443     0.214828   -0.307765    -0.800334     0.62532     0.0228556  -0.511724    -0.13304      0.0888333   0.133681    -0.225831      0.223741    0.0316176  -0.777347    -0.186471    0.70243     0.0328922    0.0257214  -0.282136     0.0719309    0.203378    0.150015    0.570338     -0.531236    0.196574  
 -0.226928   -0.495918     0.0359303   0.028109     0.354549     0.239295   -0.0719654   0.0649359    0.147484     0.657129   -0.1249       0.71861       0.490306   -0.199859   -0.428997    -0.0862622   0.281213   -0.0410226    0.0539414   0.0207503    0.666415     0.518122   -0.653526   -0.149834     -0.315275    0.12795   
  0.48624    -0.620525    -0.454839   -0.0616079   -0.052063     0.381106   -0.0877989   0.181222    -0.00126116  -0.115591   -0.337672     0.0918317    -0.288509   -0.408226   -0.247281     0.642826    0.25116     0.0623148   -0.188364    0.6049       0.343719    -0.226696   -0.308578   -0.249187      0.842202   -0.576886  
  0.387807   -0.928107    -0.406118   -0.504168     0.357301     0.61545     0.0636133   0.31458     -0.649519     0.0777903  -0.641147    -0.312735      1.05502    -0.024585   -0.187307     0.0964889   0.44562     0.562722    -0.0160331   0.472406    -0.210635     0.109226   -0.494752   -0.725532      0.412001   -0.211015  
 -0.178513    0.368451     0.158343   -0.594915     0.551431     0.479592    0.450191    0.145399     0.0400144    0.482861   -0.230895     0.323598      0.339406   -0.0209787   0.211624     0.536506   -0.0511684   0.567012    -0.188871    0.307699    -0.688283    -0.171319    0.529436   -0.416709      0.859819   -0.386823  
 -0.291555    0.140553     0.178452    0.246624     0.128643     0.0100055   0.217738    0.403138     0.479815     0.812589   -0.471643    -0.137252      0.176435   -0.239592    0.977931     0.628986   -0.639319    0.396847    -0.285794    0.814105     0.0814309   -0.233247   -0.0307844  -0.184529      0.202471   -0.152195  
  0.44077     0.253633    -0.0925103  -0.247061     0.218967    -0.828148   -0.559409    0.127574     0.138295    -0.218492   -0.284361     0.187873     -0.110784    0.191271   -0.13109     -0.275612    0.0941965  -0.0439332    0.125443   -0.534768     0.0391107   -0.167416   -0.0929112   0.0868717     0.322032   -1.20804   
  0.100743   -0.0367246    0.505977    0.0867421   -0.0916833   -0.424519   -1.17419    -0.347683     0.405792    -0.0556657  -0.178921     0.326538      0.141907    0.540617    0.0267311   -0.18223     0.288812   -0.224801     0.0567028   0.0120238   -0.301092    -0.541155   -0.144941   -0.219483     -0.0304358   0.0640107 
  0.381103    0.335463    -0.621566    0.10254     -0.192259    -0.013067    0.0112962   0.207675    -0.00784405   0.18504     0.310215    -0.18084      -0.327416    0.396702    0.192924    -0.060618   -0.0229748  -0.0644029    0.783154    0.0883442    0.00558887   0.0831492   0.397246    0.446657     -0.290507   -0.242988  
  0.560772   -0.0115864    0.223976   -0.137523    -0.00598481  -0.207677    0.338455    0.535176     0.301836    -0.306749   -0.39824      0.213649      0.401268    0.279979    0.293246    -0.0568657  -0.212778   -0.228408     0.97013    -0.0228928   -0.0149371    0.256918   -0.079542    0.498425     -0.453436    0.230189  
  0.090392   -0.00391931   0.864246   -0.421866    -0.382337     0.330234   -0.32734     0.567496    -0.554735     0.0225097   0.332193    -0.497776     -0.163687    0.273822   -0.0589357    0.454684   -0.0359214  -0.0935467   -0.375216    0.300793    -0.426013     0.136307    1.56777    -0.0238395     0.280191    0.398108  
  0.111622    0.309306     0.250979    0.452932    -0.209534     0.0090191   0.0173027  -0.0890466   -0.0981357   -0.291433    0.180081    -0.158698     -0.181049   -0.1281     -0.113556    -0.123393   -0.145896   -0.378141    -0.014193   -0.0773589   -0.104096    -0.181741    0.154448    0.000702996  -0.0102137   0.490588  
  0.205865    0.232032    -0.168623    0.0945021   -0.147278    -0.348703   -0.358048    0.231729    -0.171752     0.0289084   0.166652    -0.254915      0.252707    0.307872    1.03545     -0.0363239  -1.08586     0.223897    -0.0990335  -0.724569    -0.114814     0.0184182   0.0839809   0.0569015     0.111276    0.213407  
 -0.172822    0.0430929    0.735851    0.0219257    0.0454169   -0.493417    0.655464    0.591855     0.0795261   -0.4582     -0.156635     0.322621      0.22926     0.0282411  -0.183647     0.196488   -0.758342    0.296653    -0.66019    -0.833427     0.171904     0.0349567   0.0148257  -0.468179     -0.309036    0.20471   
 -0.641795   -0.175567    -0.0676016   0.15516     -0.209719     0.0640565   0.207562   -0.0910701    0.142456    -1.04921     0.100427    -0.117182     -0.244794   -0.18157    -0.584928    -0.128816    0.650017    0.247567     0.0674525  -0.206369     0.0301181   -0.0871249   0.112858   -0.257291      0.26669    -0.182965  
  0.31963     0.191588     0.254779    0.147414    -0.0145516    0.0197873  -0.223974    0.15866     -0.557617    -0.845544    0.165216     0.302361     -0.0107945  -0.0233802  -0.71271     -0.592313    0.162468   -0.309453     0.157292   -0.463291    -0.018986     0.309275    0.0168519   0.0305922     0.367613    0.3459    
 -0.194478   -0.241853    -0.717476   -0.148968     0.1464      -0.0273187   0.0112601  -0.405404    -0.172715     0.154082   -0.0273984   -0.0875715    -0.368992   -0.0721086  -0.15658     -0.119363    0.33167     0.27505     -0.0879905   0.0166109    0.24143     -0.237992   -0.17172    -0.139964     -0.0589524  -0.664394  
 -0.0906654  -0.342354     0.099884   -0.313825     0.14077     -0.0934645  -0.0097411   0.209915     0.172908     0.369459   -0.167024     0.0123406    -0.162928    0.103251   -0.0232242    0.0634439  -0.0639037   0.42017     -0.292872    0.181555     0.113789     0.214964   -0.331798    0.0730815     0.0946224  -0.818622  
 -0.159352    0.121737    -0.274785    0.210961     0.338367    -0.210929    0.235973   -0.691197     0.102702    -0.493014   -0.364798     0.0563437     0.611374   -0.785433    0.233729     0.174501   -0.362754   -0.255648    -0.431365    0.120372    -0.220566    -0.202348   -1.04135    -0.236747      0.0421418  -0.0822125 
  0.0211776   0.516378     0.0151727   0.291838     0.355605    -0.226547    0.187619    0.11598     -0.150272    -0.113915   -0.139053     0.166835      0.240802   -0.297143    0.248409    -0.13357    -0.350085    0.0331442    0.108848   -0.338581    -0.0829389   -0.544414    0.271945   -0.391102      0.0788031   0.00680082
  0.262437   -0.104356    -0.0423241   0.179182    -0.446592    -0.131971   -0.280383   -0.340522     0.270809    -0.260188    0.0955592   -0.0212252     0.124549    0.217664    0.335078     0.0827874  -0.339004   -0.200074    -0.121712    0.0511095   -0.110614    -0.23261    -0.0884386   0.194095      0.423397    0.101311  
  0.47382    -0.199123     0.0730022   0.113661    -0.357035    -0.04854    -0.157525    0.82381      0.140975    -0.578816   -0.204353    -0.134268     -0.289652   -0.160177    0.00841752   0.271192   -0.116829   -0.110065     0.0707767   0.109478     0.0903057    0.270809   -0.172974   -0.00718017    0.566104   -0.259629  
 -0.0744829  -0.191067    -0.131112    0.605935    -0.200505     0.127766   -0.31424    -0.435376    -0.254771     0.218687    0.552446    -0.277231     -0.287246   -0.272883   -0.046326    -0.0774443  -0.0147183   0.0694651   -1.08657     0.106321    -0.0581897   -0.114445   -0.0489939  -0.421204      0.589637   -0.0655374 
 -0.292541    0.0359974   -0.414459    0.413905    -0.0555726    0.483936    1.08618     0.35621     -0.527998     0.0215642   0.552615    -0.248029     -0.497264   -0.718213    0.0765739    0.0625529  -0.122984    0.128261    -0.281408    0.021205    -0.00480896   0.403774    0.174498    0.135112     -0.394656    0.46861   
 -0.643729   -0.290726     0.0931584   0.413425    -0.245389     0.405636    0.32534    -0.24536      0.0684854    0.0331423   0.492979    -0.295366      0.159936    0.292018    0.274963     0.0407393  -0.132067   -0.00725208   0.197172    0.00381918   0.025105    -0.161826    0.298197   -0.341893     -0.436947    0.949717  
 -0.165703    0.380411     0.161706   -0.125057     0.0875191    0.3373      0.0428381  -0.584318    -0.0818406    0.535095    0.280581     0.0381468     0.168268   -0.123966    0.0204006    0.308599    0.172302   -0.557333    -0.0335908   0.44146     -0.223382    -0.259942    0.168812   -0.105038     -0.339748    0.82824   
  0.190051   -0.160324    -0.188168   -0.207877     0.0287789    0.193251    0.187088    0.00580467  -0.277832    -0.0266152  -0.0435553    0.0126599     0.0782599   0.151542   -0.189015     0.113716    0.124536    0.111512     0.211085   -0.180235     0.114396     0.10103     0.216723   -0.0566795    -0.0195866  -0.0263677 
 -0.140441   -0.0461179    0.180233   -0.0246422    0.0603098    0.0893437   0.0321548  -0.0301848    0.0846262    0.108508    0.0427697    0.0542113     0.124491   -0.113676    0.0317389    0.0435264  -0.0451907  -0.0225987   -0.153837    0.213428    -0.0564775   -0.041836   -0.197948   -0.124318     -0.0828582   0.191631  
 -0.205844    0.152244     0.506632    0.0619407    0.0749285   -0.107381   -0.250821    0.0980487   -0.135722     0.217146    0.264489    -0.193633      0.264907    0.170604    0.266587    -0.383567   -0.312584    0.261621    -0.468134   -0.413146    -0.196835     0.212546    0.36653     0.345378     -0.391507    0.261845  
  0.0985232   0.170405     0.155584    0.290355     0.0840149   -0.161967   -0.153214    0.259859     0.148807     0.0421031   0.0594657   -0.0114492    -0.355816    0.175857   -0.161933    -0.43659     0.2448     -0.141645     0.255666   -0.148065     0.00876625   0.0626274   0.100033    0.311695     -0.472684   -0.0875575 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.402672
INFO: iteration 2, average log likelihood -1.402661
INFO: iteration 3, average log likelihood -1.402651
INFO: iteration 4, average log likelihood -1.402640
INFO: iteration 5, average log likelihood -1.402630
INFO: iteration 6, average log likelihood -1.402620
INFO: iteration 7, average log likelihood -1.402611
INFO: iteration 8, average log likelihood -1.402601
INFO: iteration 9, average log likelihood -1.402592
INFO: iteration 10, average log likelihood -1.402583
INFO: EM with 100000 data points 10 iterations avll -1.402583
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.037039e+05
      1       6.916234e+05      -2.120805e+05 |       32
      2       6.797114e+05      -1.191204e+04 |       32
      3       6.750364e+05      -4.674933e+03 |       32
      4       6.724449e+05      -2.591520e+03 |       32
      5       6.707579e+05      -1.687026e+03 |       32
      6       6.695721e+05      -1.185782e+03 |       32
      7       6.687128e+05      -8.593013e+02 |       32
      8       6.680446e+05      -6.682563e+02 |       32
      9       6.674601e+05      -5.844169e+02 |       32
     10       6.669647e+05      -4.954579e+02 |       32
     11       6.665501e+05      -4.145994e+02 |       32
     12       6.661821e+05      -3.680306e+02 |       32
     13       6.658528e+05      -3.292917e+02 |       32
     14       6.655842e+05      -2.685648e+02 |       32
     15       6.653325e+05      -2.516515e+02 |       32
     16       6.650873e+05      -2.452151e+02 |       32
     17       6.648620e+05      -2.253519e+02 |       32
     18       6.646555e+05      -2.064734e+02 |       32
     19       6.644662e+05      -1.893383e+02 |       32
     20       6.642992e+05      -1.669223e+02 |       32
     21       6.641451e+05      -1.541865e+02 |       32
     22       6.639976e+05      -1.474498e+02 |       32
     23       6.638709e+05      -1.266719e+02 |       32
     24       6.637582e+05      -1.127435e+02 |       32
     25       6.636633e+05      -9.487210e+01 |       32
     26       6.635626e+05      -1.007366e+02 |       32
     27       6.634626e+05      -9.993983e+01 |       32
     28       6.633701e+05      -9.250988e+01 |       32
     29       6.632803e+05      -8.979154e+01 |       32
     30       6.631885e+05      -9.181239e+01 |       32
     31       6.631084e+05      -8.013467e+01 |       32
     32       6.630217e+05      -8.669171e+01 |       32
     33       6.629527e+05      -6.905314e+01 |       32
     34       6.628802e+05      -7.245784e+01 |       32
     35       6.628087e+05      -7.147365e+01 |       32
     36       6.627486e+05      -6.007334e+01 |       32
     37       6.626979e+05      -5.079554e+01 |       32
     38       6.626533e+05      -4.451942e+01 |       32
     39       6.626096e+05      -4.368707e+01 |       32
     40       6.625691e+05      -4.059663e+01 |       32
     41       6.625312e+05      -3.787540e+01 |       32
     42       6.624954e+05      -3.581663e+01 |       32
     43       6.624661e+05      -2.927757e+01 |       32
     44       6.624408e+05      -2.528917e+01 |       32
     45       6.624184e+05      -2.237368e+01 |       32
     46       6.623981e+05      -2.032708e+01 |       32
     47       6.623759e+05      -2.219138e+01 |       32
     48       6.623481e+05      -2.779689e+01 |       32
     49       6.623195e+05      -2.855596e+01 |       32
     50       6.622953e+05      -2.424961e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 662295.2978011124)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.415164
INFO: iteration 2, average log likelihood -1.409977
INFO: iteration 3, average log likelihood -1.408584
INFO: iteration 4, average log likelihood -1.407544
INFO: iteration 5, average log likelihood -1.406416
INFO: iteration 6, average log likelihood -1.405337
INFO: iteration 7, average log likelihood -1.404581
INFO: iteration 8, average log likelihood -1.404160
INFO: iteration 9, average log likelihood -1.403929
INFO: iteration 10, average log likelihood -1.403784
INFO: iteration 11, average log likelihood -1.403679
INFO: iteration 12, average log likelihood -1.403596
INFO: iteration 13, average log likelihood -1.403526
INFO: iteration 14, average log likelihood -1.403465
INFO: iteration 15, average log likelihood -1.403411
INFO: iteration 16, average log likelihood -1.403363
INFO: iteration 17, average log likelihood -1.403319
INFO: iteration 18, average log likelihood -1.403278
INFO: iteration 19, average log likelihood -1.403240
INFO: iteration 20, average log likelihood -1.403205
INFO: iteration 21, average log likelihood -1.403172
INFO: iteration 22, average log likelihood -1.403141
INFO: iteration 23, average log likelihood -1.403111
INFO: iteration 24, average log likelihood -1.403083
INFO: iteration 25, average log likelihood -1.403057
INFO: iteration 26, average log likelihood -1.403032
INFO: iteration 27, average log likelihood -1.403007
INFO: iteration 28, average log likelihood -1.402984
INFO: iteration 29, average log likelihood -1.402962
INFO: iteration 30, average log likelihood -1.402941
INFO: iteration 31, average log likelihood -1.402921
INFO: iteration 32, average log likelihood -1.402901
INFO: iteration 33, average log likelihood -1.402882
INFO: iteration 34, average log likelihood -1.402864
INFO: iteration 35, average log likelihood -1.402847
INFO: iteration 36, average log likelihood -1.402830
INFO: iteration 37, average log likelihood -1.402814
INFO: iteration 38, average log likelihood -1.402798
INFO: iteration 39, average log likelihood -1.402782
INFO: iteration 40, average log likelihood -1.402767
INFO: iteration 41, average log likelihood -1.402753
INFO: iteration 42, average log likelihood -1.402739
INFO: iteration 43, average log likelihood -1.402726
INFO: iteration 44, average log likelihood -1.402713
INFO: iteration 45, average log likelihood -1.402700
INFO: iteration 46, average log likelihood -1.402688
INFO: iteration 47, average log likelihood -1.402677
INFO: iteration 48, average log likelihood -1.402666
INFO: iteration 49, average log likelihood -1.402655
INFO: iteration 50, average log likelihood -1.402644
INFO: EM with 100000 data points 50 iterations avll -1.402644
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.377082   -0.572132     0.146823    -0.227021     0.0362022   -0.794319     0.194206     0.45321       0.24104     -0.239735   -0.505945    -0.0178696    0.275926     0.31063     0.112629     0.0627616   -0.319089    0.665526    -0.311503    -0.633306    0.249013     0.306704   -0.368417    -0.478317   -0.399034   -0.634188 
  0.0578402   0.0275877    0.283634     0.225636    -0.491895     0.416323    -0.534503    -0.098114     -0.0161784   -0.252572    0.561186    -0.502558    -0.320832     0.0733117  -0.15892      0.221099     0.0394257  -0.599977    -0.501143     0.296955   -0.283427    -0.0682506   0.256818    -0.228324    0.763135    0.407359 
 -0.158814    0.0556473    0.729161    -0.0673915   -0.137848     0.565786     0.251933     0.326338     -0.375353     0.33751     0.411929    -0.20857      0.126998     0.182883   -0.0792658    0.136234    -0.0498639   0.0508619   -0.304706    -0.0453897  -0.0475152    0.337666    0.790193    -0.0442338  -0.234885    0.740255 
  0.168646   -0.364208    -0.232549     0.564758     0.208477     0.483244     0.340659     0.458292      0.282217    -0.0790757  -0.906225    -0.128894    -0.360463     0.331536    0.266943    -1.12345      0.452318   -0.66282      0.844586     0.387839    0.693135     0.445699   -0.284945     0.526752   -0.476056   -0.0536487
 -0.2172      0.152841    -0.0219338   -0.375034     0.213207     0.335744     0.355128     0.158709      0.248612     0.552659   -0.343678     0.104866     0.210866    -0.227203    0.58403      0.783248    -0.401377    0.640012    -0.243129     0.451418   -0.102991    -0.286508    0.288145    -0.142109    0.504181   -0.295711 
  0.17031     0.595882     0.18753      0.766889    -0.0155218   -0.326215     0.255665     0.216589     -0.161664    -0.431795   -0.157119    -0.153891    -0.0197139   -0.220537    0.356226    -0.430008    -0.684729   -0.0623647    0.391034    -0.499098   -0.162448    -0.137839    0.126394     0.0469433  -0.21798     0.425189 
  0.0500042  -0.284277     0.038805    -0.342239    -0.354925     0.432793     0.107342    -0.500284     -0.0754972    0.404781    0.236046    -0.163078     0.160223     0.0437846  -0.309349     0.126988     0.509685    0.0180409   -0.0578815    0.264814   -0.122763     0.245059   -0.0577448    0.158118   -0.346843    0.248764 
  0.586687    0.0184437   -0.0612906   -0.0746118   -0.18996     -0.166408    -0.179714     0.558596     -0.11678     -0.784689   -0.146346     0.0986532   -0.0668113   -0.0548037  -0.199817    -0.204014    -0.38454     0.175898    -0.25123     -0.352025    0.103274     0.230216   -0.0437575    0.249386    0.881687   -0.527917 
 -0.354447   -0.011357    -0.449226     0.477797    -0.169724     0.378193     0.779544     0.0772542    -0.465332    -0.0561172   0.720477    -0.202375    -0.364233    -0.668291    0.215209     0.259623    -0.159271   -0.0638963   -0.362604     0.07353    -0.0942       0.239167    0.179572    -0.0506626  -0.377918    0.728696 
 -0.31238    -0.305767    -0.467178    -0.139111     0.558101    -0.00487282   0.269671     0.0587456    -0.0166493    0.594165   -0.105421     0.764856     0.511655    -0.251044    0.0283007   -0.220343     0.193849    0.402887     0.372176    -0.150369    0.449743     0.337586   -0.586567     0.133612   -0.760803   -0.0533485
  0.379401   -0.174572    -0.344108     0.185894    -0.39237     -0.185538    -0.285677     0.704958      0.251463     0.121428    0.278032    -0.0565641   -0.501425     0.577797    0.0403402    0.250642     0.285256    0.203482     0.708708    -0.117283    0.0417486    0.100992    0.170899     0.185137   -0.198283   -0.551239 
 -0.0423359   0.139446    -0.0388212    0.375866    -0.184565     0.0197534    0.0438073   -0.0394263    -0.160559    -0.441135    0.377414    -0.0968549   -0.127977    -0.0110167  -0.20212     -0.24461      0.21355    -0.215305     0.125041    -0.341938   -0.132568    -0.0586737   0.123039    -0.0643785   0.0842665   0.22028  
  0.244855    0.0722955    0.111464     0.0842025   -0.0509318   -0.254011    -0.38439      0.0511386     0.129205    -0.267133   -0.055696     0.0837233    0.00787757   0.173933    0.0417758   -0.041593    -0.168378   -0.104558     0.0502736   -0.171593   -0.0262725   -0.231066   -0.0731657    0.0033135   0.215794   -0.119263 
 -0.605202   -0.401903    -0.497717     0.171925     0.0645781    0.584593     0.308785    -0.453352     -0.144888    -0.0889059  -0.0313885   -0.151283    -0.242867    -0.143032   -0.211871    -0.165267     0.294318    0.644831    -0.232283     0.118723    0.218825    -0.460263    0.00557364  -0.525755    0.293124   -0.561706 
 -0.188603   -0.39944      0.194261     0.038349    -0.0464908    0.196898     0.041464     0.315327      0.0711729   -0.659302    0.0177101    0.35927     -0.0017533   -0.261962   -0.997885    -0.0851921    0.909836    0.167977    -0.134952    -0.290281    0.310385     0.279204   -0.0489658   -0.235678    0.0745649   0.0284493
 -0.698366   -0.00389101   0.00966202   0.20616      0.0775582    0.187186     0.152879    -0.539597      0.444542     0.294437    0.539024    -0.197893     0.171275     0.541683    0.511512    -0.194766    -0.152136   -0.0456588    0.506325     0.0900168  -0.0801585   -0.620406    0.0265585   -0.118294   -0.703076    0.732775 
  0.745903   -0.998593    -0.715014    -0.551201     0.281598     0.510826    -0.184161     0.558855     -1.10343      0.274789   -0.615998    -0.544892     0.546706    -0.0706918   0.0847927    0.432432     0.25864     0.285006    -0.189975     0.140887    0.0189976    0.191487   -0.129996    -0.598542    0.733816   -0.29142  
  0.440733    0.0870147    0.373766    -0.208247    -0.402815    -0.728249     0.414762     0.1181       -0.162745    -0.577174   -0.188959    -0.312612     0.0991439   -0.447446    0.618025     0.846507    -0.288357   -0.262079    -0.0129838    0.271067   -0.760368    -0.410823    0.197008    -0.584955   -0.155502    0.0290277
  0.51848     0.0323018    0.257591    -0.553615     0.301155     0.154328     0.0379664    0.471185      0.323677    -0.113077   -0.626162     0.27904      0.508376     0.293809    0.047602     0.20842      0.15199    -0.173729     1.05094      0.527067   -0.124418     0.0721577   0.00818241   0.243597    0.337604   -0.271601 
  0.509889    0.144067     0.0850295    0.12979     -0.271954    -0.346287    -0.121461     0.236809      0.137455    -0.0119945   0.0574328    0.0494338    0.209265     0.322766    0.420446    -0.00309706  -0.516898   -0.414645     0.26608     -0.0333608  -0.033812     0.315724   -0.0816073    0.511038   -0.22319     0.432411 
  0.177644    0.228846     0.382082    -0.195334     0.0056904    0.128174    -0.18467     -0.49524      -0.273538    -0.0456159  -0.239078     0.612307     0.940816     0.064687   -0.356333    -0.00283648  -0.105214   -0.557239    -0.00384052  -0.403446    0.0744594   -0.21546    -0.0591374   -0.45668    -0.0706762   0.709052 
  0.2607     -0.29846     -0.218235     0.665578     0.238413    -0.0255136    0.0191407   -0.160459     -0.102769     0.156756   -0.233243     0.235483     0.0252049   -0.974113    0.0194203    0.298913     0.0307575  -0.0673036   -0.71655      0.539678    0.319022    -0.0287475  -0.738884    -0.498109    0.37695    -0.264754 
  0.144284    0.0290179    0.19592      0.0653257    0.207833    -0.132134    -0.161635     0.30079       0.0123668    0.231912   -0.00148294   0.0569482   -0.0717461    0.404261   -0.144839    -0.463663     0.11323     0.301412     0.048315    -0.435556    0.0306188    0.116283    0.340399     0.242985   -0.557108   -0.122005 
  0.0505156  -0.175791    -0.0167564   -0.066714    -0.0918566   -0.371331    -0.704986    -0.726611      0.449088    -0.315407   -0.280185     0.0698092    0.0160997    0.31666     0.160763    -0.0313878    0.149042   -0.206141    -0.0899563    0.0233802  -0.0124346   -0.464521   -0.304159     0.053932    0.270319   -0.225486 
  0.124651    0.603242    -0.242103    -0.0307107    0.446826    -0.917541    -0.120649    -0.157929     -0.179258     0.0282613  -0.00873032   0.140399    -0.342407    -0.37524     0.00732061  -0.253215     0.0157513  -0.322068     0.134407    -0.337428    0.0163943   -0.545483   -0.0142739   -0.125714    0.116441   -0.785107 
 -0.400637    0.243674     0.276982    -0.00253978   0.337139     0.368178     0.495694     0.0617899    -0.0382442    0.570885   -0.00578837  -0.393075    -0.203389    -0.220788   -0.417271    -0.501011    -0.0734993  -0.122617    -0.181864     0.416412    0.00196961   0.653798   -0.248359     0.322418   -0.393858   -0.107241 
 -0.144613    0.120548     0.158073     0.0542802    0.20546      0.153563     0.183919     0.000586006   0.0143335    0.0850374  -0.0569409    0.0643485    0.253579    -0.135179    0.205854     0.129514    -0.251388   -0.00814328  -0.0539462    0.0896584  -0.00882004  -0.2299      0.121791    -0.198878   -0.0717253   0.306851 
 -0.419486    0.0513259    0.23463      0.246218     0.00261064  -0.250742    -0.441069    -0.157199     -0.131883     0.144296    0.474681    -0.0217485    0.248128    -0.197713    0.573786    -0.329629    -0.598824    0.569262    -0.915023    -0.485482   -0.335172    -0.109336    0.162998     0.0140897   0.191802   -0.0875922
 -0.0971475  -0.05029      0.564254     0.368842     0.37476     -0.143204    -0.244342     0.330027      0.521146     0.654087    0.00497296   0.290605     0.194371     0.36822     0.3438       0.0792198   -0.104782    0.115933    -0.540662     0.500731   -0.562151    -0.128656   -0.42699     -0.786213    0.311469   -0.268631 
  0.0120215  -0.301219    -0.224393    -0.150823     0.0109125    0.0149874   -0.00631263   0.0955978     0.00128249   0.066158   -0.110251     0.00977376  -0.186681    -0.0270261  -0.0744427    0.0679831    0.0368666   0.244428    -0.085461     0.111282    0.149847     0.0780119  -0.203494    -0.0272546   0.149654   -0.483717 
 -0.102971   -0.0207358   -0.384336    -0.134721    -0.0794771    0.0574912   -0.0922382   -0.913014      0.0870135    0.215685    0.264427     0.0556068   -0.197428     0.110293   -0.32664     -0.403179     0.539439   -0.120904     0.0222804    0.292189   -0.139768     0.140418   -0.112306     0.535031   -0.572438    0.0650658
  0.0607952   0.104883    -0.0988572   -0.632993    -0.133838     0.419733     0.236529    -0.0376817    -0.526231    -0.698448   -0.313568    -0.647459    -0.0582352    0.244016   -0.458649    -0.121054     0.191957    0.169324     0.573824    -0.450766    0.423259    -0.0564839   0.479589     0.854799   -0.501994    0.096608 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.402634
INFO: iteration 2, average log likelihood -1.402624
INFO: iteration 3, average log likelihood -1.402614
INFO: iteration 4, average log likelihood -1.402605
INFO: iteration 5, average log likelihood -1.402596
INFO: iteration 6, average log likelihood -1.402587
INFO: iteration 7, average log likelihood -1.402578
INFO: iteration 8, average log likelihood -1.402569
INFO: iteration 9, average log likelihood -1.402561
INFO: iteration 10, average log likelihood -1.402552
INFO: EM with 100000 data points 10 iterations avll -1.402552
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
