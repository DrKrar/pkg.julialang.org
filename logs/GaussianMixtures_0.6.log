>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.6.6
INFO: Installing JLD v0.6.5
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.1
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.2.0
INFO: Installing StaticArrays v0.0.11
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1181
Commit e90a2e4 (2016-11-04 01:59 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-100-generic #147-Ubuntu SMP Tue Oct 18 16:48:51 UTC 2016 x86_64 x86_64
Memory: 2.939289093017578 GB (677.67578125 MB free)
Uptime: 24481.0 sec
Load Avg:  0.9736328125  1.01123046875  1.037109375
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3500 MHz    1497944 s         72 s     165960 s     530653 s         39 s
#2  3500 MHz     636832 s       6622 s      81390 s    1642272 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.3
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.6.6
 - JLD                           0.6.5
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.1
 - Rmath                         0.1.4
 - SHA                           0.2.1
 - ScikitLearnBase               0.2.0
 - StaticArrays                  0.0.11
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-3.3984752552924864e6,[74014.5,25985.5],
[9986.69 -25250.3 18733.9; -9576.72 24739.3 -18414.3],

Array{Float64,2}[
[72451.4 4872.78 -3539.68; 4872.78 61871.6 7981.51; -3539.68 7981.51 66380.2],

[27122.9 -4792.71 3310.9; -4792.71 38213.1 -8624.85; 3310.9 -8624.85 33801.7]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.288192e+03
      1       9.522302e+02      -3.359622e+02 |        5
      2       9.056916e+02      -4.653865e+01 |        3
      3       8.869230e+02      -1.876858e+01 |        2
      4       8.473146e+02      -3.960839e+01 |        2
      5       8.450597e+02      -2.254916e+00 |        2
      6       8.205029e+02      -2.455683e+01 |        2
      7       8.167235e+02      -3.779375e+00 |        0
      8       8.167235e+02       0.000000e+00 |        0
K-means converged with 8 iterations (objv = 816.7234999416246)
INFO: K-means with 272 data points using 8 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.057246
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.806795
INFO: iteration 2, lowerbound -3.675446
INFO: iteration 3, lowerbound -3.535208
INFO: iteration 4, lowerbound -3.382372
INFO: iteration 5, lowerbound -3.243366
INFO: iteration 6, lowerbound -3.141801
INFO: dropping number of Gaussions to 7
INFO: iteration 7, lowerbound -3.078726
INFO: dropping number of Gaussions to 5
INFO: iteration 8, lowerbound -3.027468
INFO: iteration 9, lowerbound -2.980925
INFO: dropping number of Gaussions to 4
INFO: iteration 10, lowerbound -2.924498
INFO: iteration 11, lowerbound -2.854409
INFO: iteration 12, lowerbound -2.781746
INFO: iteration 13, lowerbound -2.713531
INFO: iteration 14, lowerbound -2.654612
INFO: iteration 15, lowerbound -2.601901
INFO: iteration 16, lowerbound -2.551019
INFO: iteration 17, lowerbound -2.501738
INFO: dropping number of Gaussions to 3
INFO: iteration 18, lowerbound -2.451087
INFO: iteration 19, lowerbound -2.399248
INFO: iteration 20, lowerbound -2.357937
INFO: iteration 21, lowerbound -2.327521
INFO: iteration 22, lowerbound -2.310423
INFO: iteration 23, lowerbound -2.308347
INFO: dropping number of Gaussions to 2
INFO: iteration 24, lowerbound -2.302916
INFO: iteration 25, lowerbound -2.299259
INFO: iteration 26, lowerbound -2.299256
INFO: iteration 27, lowerbound -2.299254
INFO: iteration 28, lowerbound -2.299254
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Fri 04 Nov 2016 11:52:25 AM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Fri 04 Nov 2016 11:52:26 AM UTC: K-means with 272 data points using 8 iterations
11.3 data points per parameter
,Fri 04 Nov 2016 11:52:28 AM UTC: EM with 272 data points 0 iterations avll -2.057246
5.8 data points per parameter
,Fri 04 Nov 2016 11:52:28 AM UTC: GMM converted to Variational GMM
,Fri 04 Nov 2016 11:52:31 AM UTC: iteration 1, lowerbound -3.806795
,Fri 04 Nov 2016 11:52:31 AM UTC: iteration 2, lowerbound -3.675446
,Fri 04 Nov 2016 11:52:31 AM UTC: iteration 3, lowerbound -3.535208
,Fri 04 Nov 2016 11:52:31 AM UTC: iteration 4, lowerbound -3.382372
,Fri 04 Nov 2016 11:52:31 AM UTC: iteration 5, lowerbound -3.243366
,Fri 04 Nov 2016 11:52:31 AM UTC: iteration 6, lowerbound -3.141801
,Fri 04 Nov 2016 11:52:31 AM UTC: dropping number of Gaussions to 7
,Fri 04 Nov 2016 11:52:31 AM UTC: iteration 7, lowerbound -3.078726
,Fri 04 Nov 2016 11:52:31 AM UTC: dropping number of Gaussions to 5
,Fri 04 Nov 2016 11:52:31 AM UTC: iteration 8, lowerbound -3.027468
,Fri 04 Nov 2016 11:52:31 AM UTC: iteration 9, lowerbound -2.980925
,Fri 04 Nov 2016 11:52:31 AM UTC: dropping number of Gaussions to 4
,Fri 04 Nov 2016 11:52:31 AM UTC: iteration 10, lowerbound -2.924498
,Fri 04 Nov 2016 11:52:32 AM UTC: iteration 11, lowerbound -2.854409
,Fri 04 Nov 2016 11:52:32 AM UTC: iteration 12, lowerbound -2.781746
,Fri 04 Nov 2016 11:52:32 AM UTC: iteration 13, lowerbound -2.713531
,Fri 04 Nov 2016 11:52:32 AM UTC: iteration 14, lowerbound -2.654612
,Fri 04 Nov 2016 11:52:32 AM UTC: iteration 15, lowerbound -2.601901
,Fri 04 Nov 2016 11:52:32 AM UTC: iteration 16, lowerbound -2.551019
,Fri 04 Nov 2016 11:52:32 AM UTC: iteration 17, lowerbound -2.501738
,Fri 04 Nov 2016 11:52:32 AM UTC: dropping number of Gaussions to 3
,Fri 04 Nov 2016 11:52:32 AM UTC: iteration 18, lowerbound -2.451087
,Fri 04 Nov 2016 11:52:32 AM UTC: iteration 19, lowerbound -2.399248
,Fri 04 Nov 2016 11:52:32 AM UTC: iteration 20, lowerbound -2.357937
,Fri 04 Nov 2016 11:52:32 AM UTC: iteration 21, lowerbound -2.327521
,Fri 04 Nov 2016 11:52:32 AM UTC: iteration 22, lowerbound -2.310423
,Fri 04 Nov 2016 11:52:32 AM UTC: iteration 23, lowerbound -2.308347
,Fri 04 Nov 2016 11:52:32 AM UTC: dropping number of Gaussions to 2
,Fri 04 Nov 2016 11:52:32 AM UTC: iteration 24, lowerbound -2.302916
,Fri 04 Nov 2016 11:52:32 AM UTC: iteration 25, lowerbound -2.299259
,Fri 04 Nov 2016 11:52:33 AM UTC: iteration 26, lowerbound -2.299256
,Fri 04 Nov 2016 11:52:33 AM UTC: iteration 27, lowerbound -2.299254
,Fri 04 Nov 2016 11:52:33 AM UTC: iteration 28, lowerbound -2.299254
,Fri 04 Nov 2016 11:52:33 AM UTC: iteration 29, lowerbound -2.299253
,Fri 04 Nov 2016 11:52:33 AM UTC: iteration 30, lowerbound -2.299253
,Fri 04 Nov 2016 11:52:33 AM UTC: iteration 31, lowerbound -2.299253
,Fri 04 Nov 2016 11:52:33 AM UTC: iteration 32, lowerbound -2.299253
,Fri 04 Nov 2016 11:52:33 AM UTC: iteration 33, lowerbound -2.299253
,Fri 04 Nov 2016 11:52:33 AM UTC: iteration 34, lowerbound -2.299253
,Fri 04 Nov 2016 11:52:33 AM UTC: iteration 35, lowerbound -2.299253
,Fri 04 Nov 2016 11:52:33 AM UTC: iteration 36, lowerbound -2.299253
,Fri 04 Nov 2016 11:52:33 AM UTC: iteration 37, lowerbound -2.299253
,Fri 04 Nov 2016 11:52:33 AM UTC: iteration 38, lowerbound -2.299253
,Fri 04 Nov 2016 11:52:34 AM UTC: iteration 39, lowerbound -2.299253
,Fri 04 Nov 2016 11:52:34 AM UTC: iteration 40, lowerbound -2.299253
,Fri 04 Nov 2016 11:52:34 AM UTC: iteration 41, lowerbound -2.299253
,Fri 04 Nov 2016 11:52:34 AM UTC: iteration 42, lowerbound -2.299253
,Fri 04 Nov 2016 11:52:34 AM UTC: iteration 43, lowerbound -2.299253
,Fri 04 Nov 2016 11:52:34 AM UTC: iteration 44, lowerbound -2.299253
,Fri 04 Nov 2016 11:52:34 AM UTC: iteration 45, lowerbound -2.299253
,Fri 04 Nov 2016 11:52:34 AM UTC: iteration 46, lowerbound -2.299253
,Fri 04 Nov 2016 11:52:34 AM UTC: iteration 47, lowerbound -2.299253
,Fri 04 Nov 2016 11:52:34 AM UTC: iteration 48, lowerbound -2.299253
,Fri 04 Nov 2016 11:52:34 AM UTC: iteration 49, lowerbound -2.299253
,Fri 04 Nov 2016 11:52:34 AM UTC: iteration 50, lowerbound -2.299253
,Fri 04 Nov 2016 11:52:34 AM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.045,95.9549]
β = [178.045,95.9549]
m = [4.2503 79.2869; 2.00023 53.852]
ν = [180.045,97.9549]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.184042 -0.00764405; 0.0 0.00858171],

[0.375876 -0.00895312; 0.0 0.0127487]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 99999.99999999999
avll from stats: -0.9810428850493367
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9810428850493363
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9810428850493363
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.00000000003
avll from stats: -0.9709904002447306
avll from llpg:  -0.9709904002447305
avll direct:     -0.9709904002447304
sum posterior: 100000.0
32×26 Array{Float64,2}:
  0.0908812  -0.053807     -0.056733    -0.0825877    0.0312155   -0.0395785  -0.104264    -0.0577601    0.0369741   -0.152313     0.0883553   -0.104356    -0.0809448   -0.000755929   0.0568205    0.066765     0.0206314     0.167364     0.0340893  -0.0698745    0.101876    -0.0393035   -0.0892751   -0.0709655  -0.0787072    0.226233  
 -0.0557898   0.0267226    -0.0300716   -0.0366324   -0.0337911    0.209913    0.0397742    0.0696237    0.013647     0.0515326    0.185592     0.0213045   -0.157563     0.0212926    -0.0963095   -0.0782912    0.101283     -0.0794221    0.0093963  -0.149351    -0.0210556   -0.0791594   -0.131396    -0.148327   -0.0482897    0.127028  
  0.0626597  -0.110196      0.025599    -0.152823     0.0379803    0.152599    0.0133665    0.0848224    0.0206632   -0.00485735  -0.136931     0.127682     0.0163161    0.0570224    -0.139589    -0.244322    -0.0601355    -0.130062     0.159082    0.012889    -0.0234337   -0.0962467   -0.0574028    0.0777621  -0.135743     0.00712559
 -0.238765    0.147248      0.0324807   -0.050734    -0.090073    -0.0737346   0.112436    -0.0160604   -0.174675     0.0597594   -0.081211    -0.0149979   -0.178707     0.0424216    -0.0154762    0.104059    -0.117747      0.0370321   -0.016558    0.077129    -0.172625     0.0341809   -0.0410431   -0.0854423   0.0351235    0.122515  
  0.149083    0.0671909     0.0873381   -0.0205709   -0.0815004   -0.096605   -0.00625325  -0.0561293   -0.00792812   0.0686336    0.0335168   -0.143682    -0.0228009   -0.0519538     0.29445     -0.0809447   -0.0601528    -0.0391108   -0.0673645   0.0770375   -0.146193     0.111244     0.0865637    0.0924451  -0.123958     0.100255  
  0.110406   -0.130358      0.0760659   -0.0589502    0.0860021   -0.0318537  -0.0448399    0.106174    -0.0132632   -0.0617086   -0.0484377    0.0590097   -0.0349238    0.0763608     0.113842    -0.147895     0.0384639     0.289027    -0.0783157   0.0653397    0.124723     0.264569    -0.0311027   -0.0358656  -0.0206716    0.131748  
  0.0127143   0.0103793     0.0751245   -0.17014      0.104691     0.283373   -0.0579046   -0.0761932   -0.0350865   -0.168215    -0.157016    -0.00923364   0.037742    -0.0553316    -0.0930043    0.00274383   0.0824328    -0.0921385    0.0193575   0.0233285   -0.113296     0.0258265    0.0367666    0.0275153   0.027819     0.3194    
  0.0833496  -0.0770782    -0.045564     0.00417685   0.0256895   -0.169795    0.0864878   -0.133394    -0.0923354   -0.0179976   -0.111389     0.00763517  -0.029926     0.0593098     0.145259    -0.00768637  -0.114143      0.0386803   -0.0609717  -0.10507      0.148459    -0.121812    -0.0256174   -0.0124768  -0.0103671   -0.0766038 
 -0.0801314   0.0621716    -0.0199394   -0.00495828  -0.0656246   -0.0874786  -0.0635899    0.29787      0.0843161    0.0722462   -0.128553    -0.178103    -0.0412063   -0.0214884     0.0230915   -0.0366634   -0.0433314     0.00400407   0.100848    0.074581     0.019486    -0.0843354    0.140443    -0.0396742  -0.042471    -0.0186403 
 -0.0784423  -0.00868564   -0.0144781    0.00302558  -0.0759444    0.0349874   0.0120203   -0.118337    -0.12597     -0.0553346    0.112744    -0.077437     0.0499284   -0.18009       0.0923774    0.0411119    0.0988996     0.104565     0.0044489   0.217021    -0.0778251   -0.0936248    0.0976468    0.232259   -0.0460007    0.136462  
  0.0981495   0.0766108     0.0392779   -0.132466    -0.0439668    0.100074   -0.175042     0.0517613   -0.0888812    0.0250072    0.0525234    0.135543    -0.0105037   -0.0413092    -0.138985     0.0503943   -0.0143906    -0.072423    -0.135296    0.0171011   -0.0719918    0.155337     0.0143893   -0.0440583  -0.1596       0.00593907
  0.132778    0.200147     -0.1625      -0.0704451   -0.0879484   -0.0713593   0.112508     0.107035    -0.121814    -0.0158402    0.0457841   -0.0394975   -0.0237704   -0.0420554     0.11908     -0.124807     0.00280761   -0.0321979   -0.0601287   0.0951156   -0.0927632   -0.136631     0.14133      0.0612454  -0.0406446   -0.0317331 
 -0.0975255  -0.170407     -0.105742     0.0041781    0.0409456    0.0517822  -0.0329552    0.112033    -0.122374     0.0945484    0.123961    -0.07373     -0.067128    -0.0803089    -0.0412161    0.0346594   -0.0294869     0.247931     0.0639631  -0.171429     0.0549343    0.154291    -0.148815    -0.0153831   0.116659    -0.0162648 
  0.196494    0.225517     -0.0333058    0.104013    -0.0498668    0.0471341   0.152414    -0.165372     0.102126    -0.233304    -0.05451     -0.0626554   -0.0966344   -0.111062      0.0914955   -0.0504965    0.102688     -0.0204618   -0.0959999  -0.117122     0.0853676   -0.0703866   -0.00717973  -0.065786    0.0891552    0.0157713 
  0.0595455  -0.0210186    -0.167959    -0.0515513   -0.115652     0.101399    0.12798     -0.0462364    0.0205116   -0.0965662    0.0887263    0.104739     0.0563768    0.0622484    -0.0973601    0.125846    -0.0808093     0.156908     0.106484   -0.230824    -0.100989     0.0884904   -0.0224569    0.14591     0.197292     0.0575256 
 -0.0835926  -0.0314635    -0.0659849    0.0261455   -0.00790563   0.198847   -0.11258      0.0574286   -0.0332289    0.0794204    0.319781    -0.0969087    0.234562     0.0214388     0.0543129   -0.126224    -0.0167647     0.0879206   -0.0269045  -0.0644123   -0.0843288   -0.0050519    0.21626      0.0785823   0.0556948    0.229201  
 -0.0481092  -0.190921     -0.021544     0.113869    -0.0611561    0.0414114   0.0161092   -0.037653     0.183187    -0.0242729    0.0163039    0.0193974   -0.0505347    0.0911599     0.0314534   -0.10317      0.136518     -0.0807388    0.0662938   0.0505428   -0.271286    -0.165115    -0.0768143   -0.128898    0.128628     0.0940484 
  0.004135    0.0761869    -0.164034     0.0499074    0.0204174    0.190223   -0.0855552   -0.0776494   -0.142664    -0.0176513   -0.0775815   -0.0431225   -0.107876     0.0229882    -0.0363715    0.0134284   -0.0972884    -0.0801615    0.0865626   0.130562    -0.103684    -0.125893    -0.019678     0.13282     0.0927438   -0.0388039 
  0.0458464   0.0449608    -0.0508484   -0.03365     -0.0709698    0.176947   -0.00817404   0.113085     0.0152361   -0.0312046   -0.138711    -0.101169     0.120236     0.100215      0.0687323   -0.203894     0.0353647    -0.0675686    0.0343087   0.0626885    0.174156     0.0346201   -0.0323264    0.0486493   0.011254    -0.0203108 
  0.128054    0.0606106     0.171785     0.0491129    0.0927251    0.0938918  -0.0167865   -0.0635443    0.207481     0.16581      0.0502369    0.103269     0.205245     0.0168036    -0.0103994    0.143635    -0.12885      -0.119972    -0.211188    0.185713     0.0239456   -0.0634672    0.0215942    0.355429   -0.149746     0.129298  
  0.197984    0.0565879     0.0424557    0.0723678    0.0525231    0.11711    -0.00164411  -0.00858394  -0.146601     0.0228248   -0.013938    -0.030658     0.055412    -0.175154      0.00865949  -0.0581876   -0.117444      0.0748697    0.0347155  -0.104747    -0.0409187    0.0145759   -0.208923     0.058414   -0.00420266  -0.114837  
  0.0965815  -0.0882858     0.0351959   -0.0873667    0.144962     0.0586593   0.0838012   -0.0443307   -0.142795     0.0525809    0.0245964    0.129446     0.147243     0.0477868    -0.132208    -0.0158082   -0.000134321   0.10715      0.0655924  -0.00601679   0.00770545  -0.00138068   0.204396    -0.0684327   0.0697485   -0.103711  
  0.0625521  -0.0224838    -0.0258192    0.0873888   -0.036585     0.0223377   0.259615    -0.0147718   -0.0990712   -0.0961178   -0.0595443   -0.0571265    0.116362    -0.0291925     0.0708541   -0.130955    -0.0473725    -0.0894075   -0.130969    0.188788     0.0193293    0.0916592   -0.0997386    0.133938   -0.0288995    0.0013293 
 -0.079694    0.129016      0.102897    -0.0393296   -0.0971402    0.0867098   0.162743     0.0370711    0.0723901   -0.0637498   -0.0600385   -0.111316    -0.12423     -0.0281795     0.0377575    0.0247629    0.0698118     0.106791    -0.0278735   0.216246     0.00952596   0.051028     0.133293     0.0474585  -0.174817     0.0043912 
 -0.0397594  -0.208013     -0.107453     0.140617     0.0360006    0.0119174  -0.0995852    0.0611519   -0.00226008  -0.235359     0.0145636   -0.0710228   -0.0730883   -0.0247247     0.0187145   -0.0096753   -0.0661987    -0.0717592    0.126321   -0.108192     0.0418857    0.0288558    0.101077     0.0146783  -0.0598766    0.0788801 
  0.0645963  -0.0592327     0.158638     0.0167192    0.0693313    0.154218    0.0543739   -0.146532     0.0630645   -0.0982073    0.0592093    0.142       -0.0432342    0.0248073    -0.0728832   -0.124392     0.131301     -0.0278746    0.151733    0.0473601   -0.103079     0.130578     0.0259366    0.0669907   0.105093    -0.178875  
  0.0280272   0.0439321    -0.032497     0.00232634   0.0438707   -0.125388    0.0822505   -0.103293    -0.0944218    0.014862     0.173963    -0.0957716   -0.0287059    0.0796355    -0.153005    -0.0540021    0.0873925     0.052573     0.0925182   0.0437734   -0.0450613    0.11375      0.0834612   -0.0997416   0.0459314   -0.107603  
 -0.0999581  -0.0244331     0.200235     0.061733    -0.0677086    0.0992511   0.0649834   -0.0223025    0.0228502   -0.0152183   -0.0770366   -0.0559019    0.163792     0.177571     -0.0262535   -0.158439     0.0822007     0.112577     0.0679197  -0.128587     0.110593    -0.0558991    0.0906505    0.0754742  -0.0467815   -0.0671927 
 -0.0155478  -0.0127709     0.0471708   -0.124039     0.0521672   -0.0385796  -0.211808     0.0875342    0.0999834    0.064014    -0.0869223   -0.151387     0.00937987  -0.0813601     0.00281902  -0.0113449    0.115312     -0.110307     0.0473404   0.12386      0.0807919   -0.07109     -0.0542348   -0.0266599  -0.107939     0.0243551 
  0.118129   -0.138372      0.135112    -0.0151669    0.184205     0.077835    0.053675    -0.0285246    0.0346955    0.206394     0.00315079   0.0604799    0.0742781    0.0649961    -0.0512339    0.105248    -0.0633924    -0.0911878    0.140634    0.0950155    0.143254     0.0594315    0.253772    -0.0133043   0.0169966    0.130989  
  0.0680704   0.0341625     0.0748532   -0.0272834    0.21341      0.063762    0.145703    -0.0798616    0.00390658   0.0180117    0.0225533   -0.0193815   -0.0320712   -0.0470466     0.0378602    0.0207094    0.0327794    -0.0782785    0.0147968  -0.0151604   -0.111923     0.0456947    0.0151534    0.0134673   0.0559115    0.321592  
  0.121145   -0.000373183  -0.00544465   0.0330746    0.00127305  -0.0499394   0.109457     0.0090409    0.0106205   -0.211121     0.0811085   -0.0508471   -0.0524935   -0.317968      0.0692738   -0.139427    -0.133769      0.0602486    0.0379424   0.132851    -0.0165103    0.0375819   -0.0192585   -0.147031    0.0954634   -0.0873222 kind diag, method split
0: avll = -1.4248829010932045
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.424967
INFO: iteration 2, average log likelihood -1.424894
INFO: iteration 3, average log likelihood -1.424375
INFO: iteration 4, average log likelihood -1.417275
INFO: iteration 5, average log likelihood -1.396430
INFO: iteration 6, average log likelihood -1.389759
INFO: iteration 7, average log likelihood -1.389183
INFO: iteration 8, average log likelihood -1.389000
INFO: iteration 9, average log likelihood -1.388900
INFO: iteration 10, average log likelihood -1.388833
INFO: iteration 11, average log likelihood -1.388782
INFO: iteration 12, average log likelihood -1.388738
INFO: iteration 13, average log likelihood -1.388693
INFO: iteration 14, average log likelihood -1.388646
INFO: iteration 15, average log likelihood -1.388597
INFO: iteration 16, average log likelihood -1.388547
INFO: iteration 17, average log likelihood -1.388498
INFO: iteration 18, average log likelihood -1.388448
INFO: iteration 19, average log likelihood -1.388396
INFO: iteration 20, average log likelihood -1.388343
INFO: iteration 21, average log likelihood -1.388288
INFO: iteration 22, average log likelihood -1.388233
INFO: iteration 23, average log likelihood -1.388175
INFO: iteration 24, average log likelihood -1.388115
INFO: iteration 25, average log likelihood -1.388055
INFO: iteration 26, average log likelihood -1.387992
INFO: iteration 27, average log likelihood -1.387928
INFO: iteration 28, average log likelihood -1.387857
INFO: iteration 29, average log likelihood -1.387767
INFO: iteration 30, average log likelihood -1.387651
INFO: iteration 31, average log likelihood -1.387506
INFO: iteration 32, average log likelihood -1.387356
INFO: iteration 33, average log likelihood -1.387229
INFO: iteration 34, average log likelihood -1.387123
INFO: iteration 35, average log likelihood -1.387033
INFO: iteration 36, average log likelihood -1.386955
INFO: iteration 37, average log likelihood -1.386880
INFO: iteration 38, average log likelihood -1.386810
INFO: iteration 39, average log likelihood -1.386747
INFO: iteration 40, average log likelihood -1.386691
INFO: iteration 41, average log likelihood -1.386643
INFO: iteration 42, average log likelihood -1.386602
INFO: iteration 43, average log likelihood -1.386566
INFO: iteration 44, average log likelihood -1.386532
INFO: iteration 45, average log likelihood -1.386503
INFO: iteration 46, average log likelihood -1.386481
INFO: iteration 47, average log likelihood -1.386467
INFO: iteration 48, average log likelihood -1.386458
INFO: iteration 49, average log likelihood -1.386452
INFO: iteration 50, average log likelihood -1.386448
INFO: EM with 100000 data points 50 iterations avll -1.386448
952.4 data points per parameter
1: avll = [-1.42497,-1.42489,-1.42437,-1.41727,-1.39643,-1.38976,-1.38918,-1.389,-1.3889,-1.38883,-1.38878,-1.38874,-1.38869,-1.38865,-1.3886,-1.38855,-1.3885,-1.38845,-1.3884,-1.38834,-1.38829,-1.38823,-1.38817,-1.38812,-1.38805,-1.38799,-1.38793,-1.38786,-1.38777,-1.38765,-1.38751,-1.38736,-1.38723,-1.38712,-1.38703,-1.38695,-1.38688,-1.38681,-1.38675,-1.38669,-1.38664,-1.3866,-1.38657,-1.38653,-1.3865,-1.38648,-1.38647,-1.38646,-1.38645,-1.38645]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.386608
INFO: iteration 2, average log likelihood -1.386454
INFO: iteration 3, average log likelihood -1.385890
INFO: iteration 4, average log likelihood -1.380485
INFO: iteration 5, average log likelihood -1.364355
INFO: iteration 6, average log likelihood -1.354575
INFO: iteration 7, average log likelihood -1.351959
INFO: iteration 8, average log likelihood -1.350684
INFO: iteration 9, average log likelihood -1.349651
INFO: iteration 10, average log likelihood -1.348765
INFO: iteration 11, average log likelihood -1.348070
INFO: iteration 12, average log likelihood -1.347591
INFO: iteration 13, average log likelihood -1.347252
INFO: iteration 14, average log likelihood -1.346982
INFO: iteration 15, average log likelihood -1.346747
INFO: iteration 16, average log likelihood -1.346524
INFO: iteration 17, average log likelihood -1.346302
INFO: iteration 18, average log likelihood -1.346067
INFO: iteration 19, average log likelihood -1.345821
INFO: iteration 20, average log likelihood -1.345554
INFO: iteration 21, average log likelihood -1.345252
INFO: iteration 22, average log likelihood -1.344926
INFO: iteration 23, average log likelihood -1.344613
INFO: iteration 24, average log likelihood -1.344319
INFO: iteration 25, average log likelihood -1.344038
INFO: iteration 26, average log likelihood -1.343769
INFO: iteration 27, average log likelihood -1.343506
INFO: iteration 28, average log likelihood -1.343253
INFO: iteration 29, average log likelihood -1.342997
INFO: iteration 30, average log likelihood -1.342739
INFO: iteration 31, average log likelihood -1.342487
INFO: iteration 32, average log likelihood -1.342251
INFO: iteration 33, average log likelihood -1.342045
INFO: iteration 34, average log likelihood -1.341870
INFO: iteration 35, average log likelihood -1.341721
INFO: iteration 36, average log likelihood -1.341592
INFO: iteration 37, average log likelihood -1.341478
INFO: iteration 38, average log likelihood -1.341376
INFO: iteration 39, average log likelihood -1.341287
INFO: iteration 40, average log likelihood -1.341210
INFO: iteration 41, average log likelihood -1.341145
INFO: iteration 42, average log likelihood -1.341092
INFO: iteration 43, average log likelihood -1.341050
INFO: iteration 44, average log likelihood -1.341018
INFO: iteration 45, average log likelihood -1.340994
INFO: iteration 46, average log likelihood -1.340976
INFO: iteration 47, average log likelihood -1.340964
INFO: iteration 48, average log likelihood -1.340955
INFO: iteration 49, average log likelihood -1.340950
INFO: iteration 50, average log likelihood -1.340946
INFO: EM with 100000 data points 50 iterations avll -1.340946
473.9 data points per parameter
2: avll = [-1.38661,-1.38645,-1.38589,-1.38049,-1.36435,-1.35458,-1.35196,-1.35068,-1.34965,-1.34877,-1.34807,-1.34759,-1.34725,-1.34698,-1.34675,-1.34652,-1.3463,-1.34607,-1.34582,-1.34555,-1.34525,-1.34493,-1.34461,-1.34432,-1.34404,-1.34377,-1.34351,-1.34325,-1.343,-1.34274,-1.34249,-1.34225,-1.34204,-1.34187,-1.34172,-1.34159,-1.34148,-1.34138,-1.34129,-1.34121,-1.34114,-1.34109,-1.34105,-1.34102,-1.34099,-1.34098,-1.34096,-1.34096,-1.34095,-1.34095]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.341134
INFO: iteration 2, average log likelihood -1.340937
INFO: iteration 3, average log likelihood -1.340040
INFO: iteration 4, average log likelihood -1.331000
INFO: iteration 5, average log likelihood -1.309189
INFO: iteration 6, average log likelihood -1.296833
INFO: iteration 7, average log likelihood -1.291310
INFO: iteration 8, average log likelihood -1.287534
INFO: iteration 9, average log likelihood -1.284105
WARNING: Variances had to be floored 2
INFO: iteration 10, average log likelihood -1.280447
INFO: iteration 11, average log likelihood -1.292537
INFO: iteration 12, average log likelihood -1.284875
INFO: iteration 13, average log likelihood -1.281409
INFO: iteration 14, average log likelihood -1.277367
INFO: iteration 15, average log likelihood -1.272120
WARNING: Variances had to be floored 1
INFO: iteration 16, average log likelihood -1.269183
INFO: iteration 17, average log likelihood -1.282609
INFO: iteration 18, average log likelihood -1.274870
INFO: iteration 19, average log likelihood -1.271548
INFO: iteration 20, average log likelihood -1.269172
WARNING: Variances had to be floored 1
INFO: iteration 21, average log likelihood -1.267372
INFO: iteration 22, average log likelihood -1.280877
INFO: iteration 23, average log likelihood -1.273886
INFO: iteration 24, average log likelihood -1.271110
INFO: iteration 25, average log likelihood -1.269057
INFO: iteration 26, average log likelihood -1.267464
WARNING: Variances had to be floored 1
INFO: iteration 27, average log likelihood -1.266132
INFO: iteration 28, average log likelihood -1.280829
INFO: iteration 29, average log likelihood -1.273821
INFO: iteration 30, average log likelihood -1.271048
INFO: iteration 31, average log likelihood -1.268992
INFO: iteration 32, average log likelihood -1.267386
WARNING: Variances had to be floored 1
INFO: iteration 33, average log likelihood -1.266082
INFO: iteration 34, average log likelihood -1.280825
INFO: iteration 35, average log likelihood -1.273817
INFO: iteration 36, average log likelihood -1.271050
INFO: iteration 37, average log likelihood -1.268995
INFO: iteration 38, average log likelihood -1.267391
WARNING: Variances had to be floored 1
INFO: iteration 39, average log likelihood -1.266085
INFO: iteration 40, average log likelihood -1.280824
INFO: iteration 41, average log likelihood -1.273817
INFO: iteration 42, average log likelihood -1.271052
INFO: iteration 43, average log likelihood -1.268999
INFO: iteration 44, average log likelihood -1.267396
WARNING: Variances had to be floored 1
INFO: iteration 45, average log likelihood -1.266089
INFO: iteration 46, average log likelihood -1.280823
INFO: iteration 47, average log likelihood -1.273817
INFO: iteration 48, average log likelihood -1.271053
INFO: iteration 49, average log likelihood -1.269001
INFO: iteration 50, average log likelihood -1.267399
INFO: EM with 100000 data points 50 iterations avll -1.267399
236.4 data points per parameter
3: avll = [-1.34113,-1.34094,-1.34004,-1.331,-1.30919,-1.29683,-1.29131,-1.28753,-1.2841,-1.28045,-1.29254,-1.28487,-1.28141,-1.27737,-1.27212,-1.26918,-1.28261,-1.27487,-1.27155,-1.26917,-1.26737,-1.28088,-1.27389,-1.27111,-1.26906,-1.26746,-1.26613,-1.28083,-1.27382,-1.27105,-1.26899,-1.26739,-1.26608,-1.28082,-1.27382,-1.27105,-1.269,-1.26739,-1.26609,-1.28082,-1.27382,-1.27105,-1.269,-1.2674,-1.26609,-1.28082,-1.27382,-1.27105,-1.269,-1.2674]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 2
INFO: iteration 1, average log likelihood -1.266336
WARNING: Variances had to be floored 1 2
INFO: iteration 2, average log likelihood -1.266027
WARNING: Variances had to be floored 1 2
INFO: iteration 3, average log likelihood -1.263632
WARNING: Variances had to be floored 1 2
INFO: iteration 4, average log likelihood -1.240971
WARNING: Variances had to be floored 1 2 3 4
INFO: iteration 5, average log likelihood -1.203561
WARNING: Variances had to be floored 1 2
INFO: iteration 6, average log likelihood -1.205307
WARNING: Variances had to be floored 1 2
INFO: iteration 7, average log likelihood -1.190065
WARNING: Variances had to be floored 1 2 3 4
INFO: iteration 8, average log likelihood -1.178551
WARNING: Variances had to be floored 1 2
INFO: iteration 9, average log likelihood -1.197907
WARNING: Variances had to be floored 1 2 3
INFO: iteration 10, average log likelihood -1.186304
WARNING: Variances had to be floored 1 2 4
INFO: iteration 11, average log likelihood -1.185546
WARNING: Variances had to be floored 1 2 3
INFO: iteration 12, average log likelihood -1.187600
WARNING: Variances had to be floored 1 2
INFO: iteration 13, average log likelihood -1.186142
WARNING: Variances had to be floored 1 2 3 4
INFO: iteration 14, average log likelihood -1.174189
WARNING: Variances had to be floored 1 2
INFO: iteration 15, average log likelihood -1.192747
WARNING: Variances had to be floored 1 2 3
INFO: iteration 16, average log likelihood -1.181460
WARNING: Variances had to be floored 1 2 4
INFO: iteration 17, average log likelihood -1.181329
WARNING: Variances had to be floored 1 2 3
INFO: iteration 18, average log likelihood -1.184309
WARNING: Variances had to be floored 1 2
INFO: iteration 19, average log likelihood -1.183631
WARNING: Variances had to be floored 1 2 3 4
INFO: iteration 20, average log likelihood -1.172325
WARNING: Variances had to be floored 1 2
INFO: iteration 21, average log likelihood -1.191545
WARNING: Variances had to be floored 1 2 3
INFO: iteration 22, average log likelihood -1.180685
WARNING: Variances had to be floored 1 2 4
INFO: iteration 23, average log likelihood -1.180875
WARNING: Variances had to be floored 1 2 3
INFO: iteration 24, average log likelihood -1.183989
WARNING: Variances had to be floored 1 2
INFO: iteration 25, average log likelihood -1.183352
WARNING: Variances had to be floored 1 2 3 4
INFO: iteration 26, average log likelihood -1.172089
WARNING: Variances had to be floored 1 2
INFO: iteration 27, average log likelihood -1.191340
WARNING: Variances had to be floored 1 2 3
INFO: iteration 28, average log likelihood -1.180468
WARNING: Variances had to be floored 1 2 4
INFO: iteration 29, average log likelihood -1.180660
WARNING: Variances had to be floored 1 2 3
INFO: iteration 30, average log likelihood -1.183703
WARNING: Variances had to be floored 1 2
INFO: iteration 31, average log likelihood -1.183030
WARNING: Variances had to be floored 1 2 3 4
INFO: iteration 32, average log likelihood -1.171704
WARNING: Variances had to be floored 1 2
INFO: iteration 33, average log likelihood -1.190645
WARNING: Variances had to be floored 1 2 3
INFO: iteration 34, average log likelihood -1.179650
WARNING: Variances had to be floored 1 2 4
INFO: iteration 35, average log likelihood -1.179600
WARNING: Variances had to be floored 1 2 3
INFO: iteration 36, average log likelihood -1.182479
WARNING: Variances had to be floored 1 2
INFO: iteration 37, average log likelihood -1.181843
WARNING: Variances had to be floored 1 2 3 4
INFO: iteration 38, average log likelihood -1.170592
WARNING: Variances had to be floored 1 2
INFO: iteration 39, average log likelihood -1.189701
WARNING: Variances had to be floored 1 2 3
INFO: iteration 40, average log likelihood -1.178913
WARNING: Variances had to be floored 1 2 4
INFO: iteration 41, average log likelihood -1.179129
WARNING: Variances had to be floored 1 2 3
INFO: iteration 42, average log likelihood -1.182170
WARNING: Variances had to be floored 1 2
INFO: iteration 43, average log likelihood -1.181575
WARNING: Variances had to be floored 1 2 3 4
INFO: iteration 44, average log likelihood -1.170272
WARNING: Variances had to be floored 1 2
INFO: iteration 45, average log likelihood -1.189455
WARNING: Variances had to be floored 1 2 3
INFO: iteration 46, average log likelihood -1.178473
WARNING: Variances had to be floored 1 2 4
INFO: iteration 47, average log likelihood -1.178533
WARNING: Variances had to be floored 1 2 3
INFO: iteration 48, average log likelihood -1.181207
WARNING: Variances had to be floored 1 2
INFO: iteration 49, average log likelihood -1.180142
WARNING: Variances had to be floored 1 2 3 4
INFO: iteration 50, average log likelihood -1.168336
INFO: EM with 100000 data points 50 iterations avll -1.168336
118.1 data points per parameter
4: avll = [-1.26634,-1.26603,-1.26363,-1.24097,-1.20356,-1.20531,-1.19007,-1.17855,-1.19791,-1.1863,-1.18555,-1.1876,-1.18614,-1.17419,-1.19275,-1.18146,-1.18133,-1.18431,-1.18363,-1.17233,-1.19154,-1.18068,-1.18088,-1.18399,-1.18335,-1.17209,-1.19134,-1.18047,-1.18066,-1.1837,-1.18303,-1.1717,-1.19065,-1.17965,-1.1796,-1.18248,-1.18184,-1.17059,-1.1897,-1.17891,-1.17913,-1.18217,-1.18157,-1.17027,-1.18946,-1.17847,-1.17853,-1.18121,-1.18014,-1.16834]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 2 3 4
INFO: iteration 1, average log likelihood -1.187708
WARNING: Variances had to be floored 1 2 3 4 5
INFO: iteration 2, average log likelihood -1.176305
WARNING: Variances had to be floored 1 2 3 4 6 7 8 9
INFO: iteration 3, average log likelihood -1.164052
WARNING: Variances had to be floored 1 2 3 4 9
INFO: iteration 4, average log likelihood -1.165913
WARNING: Variances had to be floored 1 2 3 4 6 9 18
INFO: iteration 5, average log likelihood -1.117321
WARNING: Variances had to be floored 1 2 3 4 7 8 9 11 30
INFO: iteration 6, average log likelihood -1.095955
WARNING: Variances had to be floored 1 2 3 4 6 9 18 29
INFO: iteration 7, average log likelihood -1.093781
WARNING: Variances had to be floored 1 2 3 4 7 8 9 30
INFO: iteration 8, average log likelihood -1.099120
WARNING: Variances had to be floored 1 2 3 4 6 9 11 14 18
INFO: iteration 9, average log likelihood -1.076961
WARNING: Variances had to be floored 1 2 3 4 7 8 9 29 30
INFO: iteration 10, average log likelihood -1.093175
WARNING: Variances had to be floored 1 2 3 4 6 9 18
INFO: iteration 11, average log likelihood -1.096402
WARNING: Variances had to be floored 1 2 3 4 7 8 9 11 30
INFO: iteration 12, average log likelihood -1.086038
WARNING: Variances had to be floored 1 2 3 4 6 9 18 29
INFO: iteration 13, average log likelihood -1.084791
WARNING: Variances had to be floored 1 2 3 4 7 8 9 14 30
INFO: iteration 14, average log likelihood -1.093800
WARNING: Variances had to be floored 1 2 3 4 6 9 11 18
INFO: iteration 15, average log likelihood -1.085324
WARNING: Variances had to be floored 1 2 3 4 7 8 9 29 30
INFO: iteration 16, average log likelihood -1.087971
WARNING: Variances had to be floored 1 2 3 4 6 9 18
INFO: iteration 17, average log likelihood -1.093495
WARNING: Variances had to be floored 1 2 3 4 7 8 9 14 30
INFO: iteration 18, average log likelihood -1.083579
WARNING: Variances had to be floored 1 2 3 4 6 9 11 18 29
INFO: iteration 19, average log likelihood -1.077570
WARNING: Variances had to be floored 1 2 3 4 7 8 9 30
INFO: iteration 20, average log likelihood -1.101618
WARNING: Variances had to be floored 1 2 3 4 6 9 18
INFO: iteration 21, average log likelihood -1.082755
WARNING: Variances had to be floored 1 2 3 4 7 8 9 14 29 30
INFO: iteration 22, average log likelihood -1.076343
WARNING: Variances had to be floored 1 2 3 4 6 9 11 18
INFO: iteration 23, average log likelihood -1.094072
WARNING: Variances had to be floored 1 2 3 4 7 8 9 30
INFO: iteration 24, average log likelihood -1.092856
WARNING: Variances had to be floored 1 2 3 4 6 9 18 29
INFO: iteration 25, average log likelihood -1.077779
WARNING: Variances had to be floored 1 2 3 4 7 8 9 14 30
INFO: iteration 26, average log likelihood -1.090424
WARNING: Variances had to be floored 1 2 3 4 6 9 18
INFO: iteration 27, average log likelihood -1.086178
WARNING: Variances had to be floored 1 2 3 4 7 8 9 29 30
INFO: iteration 28, average log likelihood -1.080141
WARNING: Variances had to be floored 1 2 3 4 6 9 11 18
INFO: iteration 29, average log likelihood -1.087966
WARNING: Variances had to be floored 1 2 3 4 7 8 9 14 30
INFO: iteration 30, average log likelihood -1.088296
WARNING: Variances had to be floored 1 2 3 4 6 9 18 29
INFO: iteration 31, average log likelihood -1.083112
WARNING: Variances had to be floored 1 2 3 4 7 8 9 30
INFO: iteration 32, average log likelihood -1.095103
WARNING: Variances had to be floored 1 2 3 4 6 9 18
INFO: iteration 33, average log likelihood -1.081139
WARNING: Variances had to be floored 1 2 3 4 7 8 9 29 30
INFO: iteration 34, average log likelihood -1.076743
WARNING: Variances had to be floored 1 2 3 4 6 9 11 14 18
INFO: iteration 35, average log likelihood -1.083723
WARNING: Variances had to be floored 1 2 3 4 7 8 9 30
INFO: iteration 36, average log likelihood -1.095306
WARNING: Variances had to be floored 1 2 3 4 6 9 18 29
INFO: iteration 37, average log likelihood -1.079507
WARNING: Variances had to be floored 1 2 3 4 7 8 9 30
INFO: iteration 38, average log likelihood -1.093554
WARNING: Variances had to be floored 1 2 3 4 6 9 18
INFO: iteration 39, average log likelihood -1.079919
WARNING: Variances had to be floored 1 2 3 4 7 8 9 29 30
INFO: iteration 40, average log likelihood -1.075560
WARNING: Variances had to be floored 1 2 3 4 6 9 14 15 18
INFO: iteration 41, average log likelihood -1.083161
WARNING: Variances had to be floored 1 2 3 4 7 8 9 11 30
INFO: iteration 42, average log likelihood -1.092290
WARNING: Variances had to be floored 1 2 3 4 6 9 18 29
INFO: iteration 43, average log likelihood -1.088529
WARNING: Variances had to be floored 1 2 3 4 7 8 9 30
INFO: iteration 44, average log likelihood -1.098124
WARNING: Variances had to be floored 1 2 3 4 6 9 18
INFO: iteration 45, average log likelihood -1.083399
WARNING: Variances had to be floored 1 2 3 4 7 8 9 29 30
INFO: iteration 46, average log likelihood -1.079676
WARNING: Variances had to be floored 1 2 3 4 6 9 11 18
INFO: iteration 47, average log likelihood -1.088007
WARNING: Variances had to be floored 1 2 3 4 7 8 9 14 30
INFO: iteration 48, average log likelihood -1.088827
WARNING: Variances had to be floored 1 2 3 4 6 9 18 29
INFO: iteration 49, average log likelihood -1.083282
WARNING: Variances had to be floored 1 2 3 4 7 8 9 30
INFO: iteration 50, average log likelihood -1.094466
INFO: EM with 100000 data points 50 iterations avll -1.094466
59.0 data points per parameter
5: avll = [-1.18771,-1.1763,-1.16405,-1.16591,-1.11732,-1.09595,-1.09378,-1.09912,-1.07696,-1.09318,-1.0964,-1.08604,-1.08479,-1.0938,-1.08532,-1.08797,-1.0935,-1.08358,-1.07757,-1.10162,-1.08275,-1.07634,-1.09407,-1.09286,-1.07778,-1.09042,-1.08618,-1.08014,-1.08797,-1.0883,-1.08311,-1.0951,-1.08114,-1.07674,-1.08372,-1.09531,-1.07951,-1.09355,-1.07992,-1.07556,-1.08316,-1.09229,-1.08853,-1.09812,-1.0834,-1.07968,-1.08801,-1.08883,-1.08328,-1.09447]
[-1.42488,-1.42497,-1.42489,-1.42437,-1.41727,-1.39643,-1.38976,-1.38918,-1.389,-1.3889,-1.38883,-1.38878,-1.38874,-1.38869,-1.38865,-1.3886,-1.38855,-1.3885,-1.38845,-1.3884,-1.38834,-1.38829,-1.38823,-1.38817,-1.38812,-1.38805,-1.38799,-1.38793,-1.38786,-1.38777,-1.38765,-1.38751,-1.38736,-1.38723,-1.38712,-1.38703,-1.38695,-1.38688,-1.38681,-1.38675,-1.38669,-1.38664,-1.3866,-1.38657,-1.38653,-1.3865,-1.38648,-1.38647,-1.38646,-1.38645,-1.38645,-1.38661,-1.38645,-1.38589,-1.38049,-1.36435,-1.35458,-1.35196,-1.35068,-1.34965,-1.34877,-1.34807,-1.34759,-1.34725,-1.34698,-1.34675,-1.34652,-1.3463,-1.34607,-1.34582,-1.34555,-1.34525,-1.34493,-1.34461,-1.34432,-1.34404,-1.34377,-1.34351,-1.34325,-1.343,-1.34274,-1.34249,-1.34225,-1.34204,-1.34187,-1.34172,-1.34159,-1.34148,-1.34138,-1.34129,-1.34121,-1.34114,-1.34109,-1.34105,-1.34102,-1.34099,-1.34098,-1.34096,-1.34096,-1.34095,-1.34095,-1.34113,-1.34094,-1.34004,-1.331,-1.30919,-1.29683,-1.29131,-1.28753,-1.2841,-1.28045,-1.29254,-1.28487,-1.28141,-1.27737,-1.27212,-1.26918,-1.28261,-1.27487,-1.27155,-1.26917,-1.26737,-1.28088,-1.27389,-1.27111,-1.26906,-1.26746,-1.26613,-1.28083,-1.27382,-1.27105,-1.26899,-1.26739,-1.26608,-1.28082,-1.27382,-1.27105,-1.269,-1.26739,-1.26609,-1.28082,-1.27382,-1.27105,-1.269,-1.2674,-1.26609,-1.28082,-1.27382,-1.27105,-1.269,-1.2674,-1.26634,-1.26603,-1.26363,-1.24097,-1.20356,-1.20531,-1.19007,-1.17855,-1.19791,-1.1863,-1.18555,-1.1876,-1.18614,-1.17419,-1.19275,-1.18146,-1.18133,-1.18431,-1.18363,-1.17233,-1.19154,-1.18068,-1.18088,-1.18399,-1.18335,-1.17209,-1.19134,-1.18047,-1.18066,-1.1837,-1.18303,-1.1717,-1.19065,-1.17965,-1.1796,-1.18248,-1.18184,-1.17059,-1.1897,-1.17891,-1.17913,-1.18217,-1.18157,-1.17027,-1.18946,-1.17847,-1.17853,-1.18121,-1.18014,-1.16834,-1.18771,-1.1763,-1.16405,-1.16591,-1.11732,-1.09595,-1.09378,-1.09912,-1.07696,-1.09318,-1.0964,-1.08604,-1.08479,-1.0938,-1.08532,-1.08797,-1.0935,-1.08358,-1.07757,-1.10162,-1.08275,-1.07634,-1.09407,-1.09286,-1.07778,-1.09042,-1.08618,-1.08014,-1.08797,-1.0883,-1.08311,-1.0951,-1.08114,-1.07674,-1.08372,-1.09531,-1.07951,-1.09355,-1.07992,-1.07556,-1.08316,-1.09229,-1.08853,-1.09812,-1.0834,-1.07968,-1.08801,-1.08883,-1.08328,-1.09447]
32×26 Array{Float64,2}:
 -0.0811102    0.0608625    0.373574      0.594586     0.355965    -1.59336    -0.0235853   0.300027   -0.0725073    0.0366345    -0.128167    -0.170138    -0.0696048    0.125038     0.00539684  -0.0247511    0.113672      0.0324078   -0.0081362    0.0624325    0.00784187   0.0528858     0.103465    -0.0414079    0.0129751    0.0235892  
 -0.0799629    0.0540409    0.0912277     0.103098    -0.0277815    0.468       0.151336    0.291892   -0.0612557   -0.18191      -0.12894     -0.145837    -0.0494539    0.612509    -0.357915    -0.0408476    0.0748763     0.00236237   0.147291     0.0842572   -0.127102    -0.340495      0.126541    -0.0377049    0.0094657   -0.0393601  
 -0.0800502    0.0644168   -0.176951     -0.14796     -0.500008    -0.193648   -0.0905936   0.304403    0.271625     0.418503     -0.128678    -0.190532    -0.0387821   -0.407005     0.389662    -0.0291004   -0.373473      0.00745853   0.293855     0.077055    -0.0183527    0.00547376    0.161032    -0.0386       0.116129     0.00390146 
 -0.079706     0.0621565   -0.487071     -0.364479     0.00251283   0.432709   -0.209539    0.293648    0.239697     0.077656     -0.128584    -0.204229    -0.0432526   -0.21882      0.074302    -0.0365433    0.042825     -0.0239794   -0.0973021    0.0763224    0.170869     0.0643192     0.159767    -0.0496046   -0.182421    -0.049702   
 -0.0239591   -0.197548    -0.00814311    0.121025    -0.0242926    0.0305614   0.0299441  -0.0438817   0.182376     0.0176095     0.00248184   0.00789666  -0.0555125    0.0739361    0.0273826   -0.0799162    0.134961     -0.10781      0.0666196    0.0469696   -0.281757    -0.166626     -0.0766929   -0.125818     0.124599     0.0845015  
  0.0335406    0.0170517   -0.0206556     0.0081648    0.0403362   -0.189572    0.0833123  -0.0988714  -0.0911385    0.0531338     0.180523    -0.0886392   -0.0246694    0.0635904   -0.162137    -0.0350688    0.0744163     0.0244238    0.0916863    0.0399491   -0.0877013    0.108516      0.0826889   -0.0941603    0.0445099   -0.109425   
  0.0850469   -0.0256212   -0.0254885     0.046873    -0.0607487    0.0174204   0.259638    0.0357619   1.14363     -0.173471     -0.108763    -0.0555172    0.122383    -0.00773645   0.0601867   -0.126216    -0.0403959    -0.0893931   -0.139804     0.189752     0.0116677    0.094871     -0.105923     0.132692    -0.0301205    0.00448294 
  0.0850877   -0.0432007   -0.0189688     0.0980435    0.0593864    0.0246719   0.259248   -0.0385302  -1.40754     -0.146806     -0.019522    -0.0559034    0.111271    -0.0723444    0.106943    -0.124362    -0.0414746    -0.088972    -0.126567     0.188756     0.0517305    0.0912016    -0.124345     0.132937    -0.0223575   -0.00144633 
 -0.18361     -0.00541029   0.00139137    0.00624175  -0.0748615    0.0167194   0.0183271  -0.117559   -0.125107    -0.0322369     0.109567    -0.085971     0.0466776   -0.16108      0.0897887    0.0402361    0.0988154     0.121006    -0.0458944    0.207459    -0.0790104   -0.0821205     0.101117     0.246041    -0.0464304    0.134915   
  0.161556     0.0650921    0.0819386    -0.011588    -0.127391    -0.0989754  -0.019968   -0.0554344  -0.00723616   0.0557822     0.0586554   -0.17351      0.0218408   -0.0448879    0.294182    -0.0946416   -0.0632995    -0.0481501   -0.0858671    0.0892654   -0.136874     0.110613      0.077926     0.099436    -0.123236     0.112864   
  0.0522104   -0.106246    -0.0703025     0.00700717   0.024182    -0.163589    0.0663468  -0.0950666  -0.111563     0.000185546  -0.0878008   -0.0152822   -0.0316802    0.0467231    0.119093    -0.00387159  -0.125175      0.0927858   -0.0808126   -0.135656     0.145279    -0.0555636    -0.0469353   -0.00992455   0.0066245   -0.0798496  
  0.0258713    0.00930992   0.0668391    -0.168628     0.111764     0.282162   -0.0561764  -0.0531395  -0.0524266   -0.167135     -0.128555    -0.00614264   0.0256573   -0.0599103   -0.092342    -0.00152879   0.0765716    -0.0918879    0.0191878    0.0221218   -0.107533     0.0446832     0.0445359   -0.0062849    0.0274304    0.317052   
 -0.0400141   -0.2028      -0.104576      0.141664     0.0308201    0.0133668  -0.130842    0.062121    0.00746122  -0.237361      0.0092559   -0.0637821   -0.0801219    0.0210816    0.00696152  -0.0134346   -0.0630086    -0.0742911    0.131271    -0.114018     0.0583031    0.0330256     0.0847755   -0.00759575  -0.0728431    0.0832578  
 -0.075314    -0.0262267    0.163332      0.0199698   -0.0431374    0.112168    0.0433579   0.0221702   0.018531    -0.0148582    -0.0842613   -0.0147944    0.139501     0.122909    -0.063178    -0.175611     0.0566976     0.073049     0.0875055   -0.0721395    0.0558063   -0.0938756     0.034625     0.0728157   -0.0808848   -0.0491551  
  0.138303     0.021856     0.0335563    -0.0216043    0.118968     0.127771    0.0425164  -0.0238805  -0.0525902    0.0208895    -0.00842807   0.0192227    0.0182794   -0.0285088    0.0133858   -0.0649946   -0.0470243    -0.0409157    0.0575402   -0.039536    -0.072224     0.0317322    -0.0914385    0.0446856    0.0159833    0.0890688  
 -0.040331     0.0414599   -0.0153759    -0.0334937   -0.0507676    0.129865    0.0549068   0.096728    0.0257539   -0.0274337    -0.0610513   -0.096271    -0.033672    -0.0215445    0.0251244   -0.0390452    0.0484266     0.0735779   -0.00420362   0.0986778    0.100581     0.0546188    -0.00360319   0.0236854   -0.0694662    0.00146725 
  0.119383     0.19494     -0.166323     -0.0718463   -0.138002    -0.0635374   0.104725    0.0924033  -0.129309    -0.0121173     0.0742182   -0.0280469   -0.0250871   -0.0400396    0.119867    -0.0836928    0.000687846  -0.0324602   -0.0584256    0.0927074   -0.0904187   -0.135619      0.145774     0.059175    -0.045192    -0.0367254  
  0.0892171   -0.118663     0.00584816   -0.114705     0.160786     0.0722578   0.0842783  -0.0213625  -0.132981     0.0342943     0.0246381    0.122517     0.141625     0.0515692   -0.130358    -0.0154343    0.00708669    0.140898     0.0662329   -0.00727799   0.00327005  -0.00562133    0.19699     -0.0507978    0.0814106   -0.0928539  
 -0.0744822   -0.0406987   -0.103552      0.00780863   0.00236729   0.221179   -0.103791    0.0115362  -1.19628      0.0253444     0.305705    -0.100973     0.218452     0.0378448   -0.0178195   -0.212153    -0.0159166     0.0885871    0.111918    -0.0584886   -0.100131     0.0439366     0.219269     0.0743707    0.150781     0.220603   
 -0.0771681   -0.0302285   -0.0502941     0.0615735   -0.0319213    0.21481    -0.125709    0.0323105   1.22921      0.0751554     0.283184    -0.131694     0.305885     0.0132912    0.0668802   -0.0790119   -0.0187139     0.0577848   -0.128175    -0.0664101   -0.0948624    0.00448449    0.214449     0.0676403   -0.00399245   0.234847   
  0.100232    -0.191833     0.167882     -0.0176838    0.153618     0.0892917   0.052825   -0.0301819  -0.0136843    0.0930531     0.0405978    0.0341579    0.0364335    0.0656451   -0.722209     0.111879    -0.0851682    -0.100642     0.175822    -0.0145036    0.161547     0.0582231     0.250001    -0.059751     0.01512      0.141452   
  0.125298    -0.105001     0.0965291    -0.0167317    0.196696     0.0722751   0.0681888  -0.0245      0.102464     0.375974     -0.0428588    0.0844748    0.1342       0.0629671    0.782782     0.102135    -0.0670995    -0.0859959    0.178654     0.2669       0.133034     0.0592663     0.259767     0.0311726    0.018474     0.113264   
  0.212989     0.328672    -0.0582452     0.134046    -0.0577345    0.0631123   0.147494   -0.0613491   0.0905304   -0.182852     -0.412565    -0.0699197   -0.141552    -0.113297     0.0526851    0.0697007    0.0771315    -0.0482012   -0.0718753   -0.0389127    0.0164815    0.000785893  -0.0635417   -0.152916     0.0814709    0.0193802  
  0.190934     0.156578    -0.0479838     0.0850304   -0.0381448    0.0279697   0.153522   -0.317411    0.11069     -0.276489      0.256276    -0.0592368   -0.0974077   -0.109342     0.125443    -0.144362     0.177921      0.0389819   -0.114261    -0.183826     0.140221    -0.0433545     0.00944284  -0.023087     0.0993592   -0.000175107
 -0.0643353    0.0574901   -0.000241254  -0.0039804   -0.0254959   -0.0613969   0.0983279  -0.0114246  -0.0816577   -0.063757      0.00325366  -0.0275348   -0.116302    -0.140259     0.0207136   -0.0161926   -0.145988      0.0576774    0.0140499    0.0938702   -0.0904972    0.0478953    -0.0531085   -0.122819     0.0730499    0.0129876  
  0.0841519   -0.0700175   -0.014383     -0.0617166    0.0211271   -0.0400796  -0.105282   -0.0617817   0.0462506   -0.150107      0.1003      -0.104047    -0.0815675   -0.0413759    0.063475     0.0552941    0.0146089     0.183148     0.0117438   -0.093881     0.108764    -0.0424254    -0.0962919   -0.0763853   -0.0767635    0.169849   
 -0.0109447   -0.00350574   0.0548887    -0.123758     0.0661967   -0.0456544  -0.238301    0.0868091   0.108813     0.0677012    -0.0957986   -0.133443     0.0531232   -0.0984406   -0.0176291   -0.0121757    0.124924     -0.118116     0.0528275    0.0909674    0.091486    -0.0776045    -0.0542137   -0.0243745   -0.12033      0.0212573  
  0.0518615    0.0178926    0.000227249  -0.00723292  -0.00285441   0.130853    0.0595444  -0.0207769   0.0801537    0.0582625     0.105015     0.0817985    0.039249     0.0353536   -0.0747419    0.0819483   -0.0462933    -0.0297261   -0.0366541   -0.0440415   -0.0290241   -0.00944604   -0.0410835    0.151738    -0.00493932   0.0911814  
  0.00499358   0.0582716   -0.152826      0.0502365    0.00790474   0.197214   -0.0795796  -0.0793696  -0.192691    -0.0197991    -0.072652    -0.0554177   -0.101008    -0.00680459  -0.0232656    0.00821876  -0.102593     -0.0707664    0.083118     0.127188    -0.102364    -0.126011     -0.0220328    0.128047     0.0554806   -0.0407431  
  0.106897    -0.133789     0.0769612    -0.0524634    0.0863226   -0.0310052  -0.0367693   0.13002    -0.0150769   -0.0547187    -0.0587369   -0.0698555   -0.026007     0.089459     0.118361    -0.130512     0.0171999     0.28906     -0.105642     0.0492795    0.124585     0.265587     -0.0208625   -0.04038     -0.00766519   0.156829   
  0.0658243    0.0652442    0.0383825    -0.130928    -0.019801     0.0747148  -0.170283    0.0254429  -0.0937319    0.0305243     0.0865508    0.13597      0.00926644  -0.0361859   -0.145249     0.0433278   -0.0401358    -0.059563    -0.121564    -0.0204899   -0.0870689    0.139502      0.0166615   -0.038992    -0.173266    -0.0041084  
  0.0560964   -0.0582818    0.11271       0.0217838    0.0692686    0.161801    0.0457405  -0.163165    0.0401436   -0.101242      0.0515569    0.184215     0.0230738    0.0325134   -0.0629612   -0.140418     0.131889     -0.0152627    0.1544       0.0556244   -0.116428     0.141679      0.0202727    0.0664136    0.040986    -0.165543   INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 2 3 4 6 9 18
INFO: iteration 1, average log likelihood -1.079690
WARNING: Variances had to be floored 1 2 3 4 6 7 8 9 11 18 29 30
INFO: iteration 2, average log likelihood -1.055067
WARNING: Variances had to be floored 1 2 3 4 6 9 11 18
INFO: iteration 3, average log likelihood -1.077407
WARNING: Variances had to be floored 1 2 3 4 6 7 8 9 11 14 18 29 30
INFO: iteration 4, average log likelihood -1.055609
WARNING: Variances had to be floored 1 2 3 4 6 9 11 18
INFO: iteration 5, average log likelihood -1.079276
WARNING: Variances had to be floored 1 2 3 4 6 7 8 9 11 18 29 30
INFO: iteration 6, average log likelihood -1.057826
WARNING: Variances had to be floored 1 2 3 4 6 9 11 14 18
INFO: iteration 7, average log likelihood -1.074923
WARNING: Variances had to be floored 1 2 3 4 6 7 8 9 11 18 29 30
INFO: iteration 8, average log likelihood -1.059590
WARNING: Variances had to be floored 1 2 3 4 6 9 11 18
INFO: iteration 9, average log likelihood -1.077017
WARNING: Variances had to be floored 1 2 3 4 6 7 8 9 11 14 18 29 30
INFO: iteration 10, average log likelihood -1.055077
INFO: EM with 100000 data points 10 iterations avll -1.055077
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.110809e+05
      1       6.946254e+05      -2.164555e+05 |       32
      2       6.589893e+05      -3.563619e+04 |       32
      3       6.415436e+05      -1.744563e+04 |       32
      4       6.313735e+05      -1.017009e+04 |       32
      5       6.260737e+05      -5.299786e+03 |       32
      6       6.235389e+05      -2.534849e+03 |       32
      7       6.221915e+05      -1.347380e+03 |       32
      8       6.212457e+05      -9.458203e+02 |       32
      9       6.204334e+05      -8.123082e+02 |       32
     10       6.195250e+05      -9.084055e+02 |       32
     11       6.185338e+05      -9.911499e+02 |       32
     12       6.176857e+05      -8.481797e+02 |       32
     13       6.170530e+05      -6.326377e+02 |       32
     14       6.165351e+05      -5.179377e+02 |       32
     15       6.160409e+05      -4.941735e+02 |       32
     16       6.156382e+05      -4.026842e+02 |       32
     17       6.152887e+05      -3.495366e+02 |       32
     18       6.150897e+05      -1.990075e+02 |       32
     19       6.149728e+05      -1.168940e+02 |       32
     20       6.148597e+05      -1.130569e+02 |       32
     21       6.147235e+05      -1.361990e+02 |       32
     22       6.145978e+05      -1.256960e+02 |       32
     23       6.144925e+05      -1.053391e+02 |       32
     24       6.144107e+05      -8.181749e+01 |       32
     25       6.143455e+05      -6.512622e+01 |       32
     26       6.142935e+05      -5.201135e+01 |       32
     27       6.142470e+05      -4.655231e+01 |       32
     28       6.142085e+05      -3.845514e+01 |       30
     29       6.141767e+05      -3.180369e+01 |       30
     30       6.141416e+05      -3.511208e+01 |       32
     31       6.141075e+05      -3.415225e+01 |       31
     32       6.140710e+05      -3.641662e+01 |       32
     33       6.140447e+05      -2.635538e+01 |       31
     34       6.140263e+05      -1.843280e+01 |       31
     35       6.140095e+05      -1.674965e+01 |       32
     36       6.139955e+05      -1.403711e+01 |       31
     37       6.139855e+05      -1.000986e+01 |       29
     38       6.139779e+05      -7.601968e+00 |       28
     39       6.139718e+05      -6.095499e+00 |       27
     40       6.139660e+05      -5.777070e+00 |       22
     41       6.139621e+05      -3.902817e+00 |       25
     42       6.139592e+05      -2.841360e+00 |       23
     43       6.139574e+05      -1.886944e+00 |       17
     44       6.139564e+05      -9.507404e-01 |        9
     45       6.139560e+05      -3.731778e-01 |       16
     46       6.139553e+05      -7.078485e-01 |       10
     47       6.139550e+05      -2.993232e-01 |        5
     48       6.139549e+05      -1.503734e-01 |        2
     49       6.139548e+05      -4.123070e-02 |        4
     50       6.139547e+05      -9.855190e-02 |        2
K-means terminated without convergence after 50 iterations (objv = 613954.7310208442)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.329378
INFO: iteration 2, average log likelihood -1.299133
INFO: iteration 3, average log likelihood -1.265362
INFO: iteration 4, average log likelihood -1.223810
WARNING: Variances had to be floored 24
INFO: iteration 5, average log likelihood -1.180629
INFO: iteration 6, average log likelihood -1.159478
WARNING: Variances had to be floored 2 10 11 32
INFO: iteration 7, average log likelihood -1.104766
WARNING: Variances had to be floored 17 28
INFO: iteration 8, average log likelihood -1.110594
WARNING: Variances had to be floored 1 13 24 26 27
INFO: iteration 9, average log likelihood -1.097732
WARNING: Variances had to be floored 3
INFO: iteration 10, average log likelihood -1.107119
WARNING: Variances had to be floored 2 10 11 20 32
INFO: iteration 11, average log likelihood -1.066113
WARNING: Variances had to be floored 17 28
INFO: iteration 12, average log likelihood -1.094245
WARNING: Variances had to be floored 1 19 24 26
INFO: iteration 13, average log likelihood -1.074778
WARNING: Variances had to be floored 2 3 13 27
INFO: iteration 14, average log likelihood -1.075894
WARNING: Variances had to be floored 10 11 20 32
INFO: iteration 15, average log likelihood -1.077691
WARNING: Variances had to be floored 17 28
INFO: iteration 16, average log likelihood -1.093770
WARNING: Variances had to be floored 1 19 24
INFO: iteration 17, average log likelihood -1.082602
WARNING: Variances had to be floored 2 3 11 13 27 32
INFO: iteration 18, average log likelihood -1.075898
WARNING: Variances had to be floored 10
INFO: iteration 19, average log likelihood -1.097190
WARNING: Variances had to be floored 17 20 24 28
INFO: iteration 20, average log likelihood -1.066540
WARNING: Variances had to be floored 1 11 19 26
INFO: iteration 21, average log likelihood -1.098394
WARNING: Variances had to be floored 2 3 13
INFO: iteration 22, average log likelihood -1.101563
WARNING: Variances had to be floored 10 32
INFO: iteration 23, average log likelihood -1.085680
WARNING: Variances had to be floored 11 17 24 28
INFO: iteration 24, average log likelihood -1.050851
WARNING: Variances had to be floored 1 2 19 20 26
INFO: iteration 25, average log likelihood -1.086710
WARNING: Variances had to be floored 3 13 32
INFO: iteration 26, average log likelihood -1.099992
WARNING: Variances had to be floored 10 11
INFO: iteration 27, average log likelihood -1.074669
WARNING: Variances had to be floored 2 17 24 28
INFO: iteration 28, average log likelihood -1.065556
WARNING: Variances had to be floored 1 3 13 19 26
INFO: iteration 29, average log likelihood -1.082857
WARNING: Variances had to be floored 11 32
INFO: iteration 30, average log likelihood -1.097519
WARNING: Variances had to be floored 2 10 20
INFO: iteration 31, average log likelihood -1.061449
WARNING: Variances had to be floored 17 19 24 28
INFO: iteration 32, average log likelihood -1.065702
WARNING: Variances had to be floored 1 3 11 13 26
INFO: iteration 33, average log likelihood -1.091630
WARNING: Variances had to be floored 2 32
INFO: iteration 34, average log likelihood -1.105469
WARNING: Variances had to be floored 10 17 20 28
INFO: iteration 35, average log likelihood -1.060875
WARNING: Variances had to be floored 11 19 24
INFO: iteration 36, average log likelihood -1.081845
WARNING: Variances had to be floored 1 2 3 26
INFO: iteration 37, average log likelihood -1.090069
WARNING: Variances had to be floored 13 17 28 32
INFO: iteration 38, average log likelihood -1.076781
WARNING: Variances had to be floored 10 11 24
INFO: iteration 39, average log likelihood -1.071052
WARNING: Variances had to be floored 19 20
INFO: iteration 40, average log likelihood -1.088601
WARNING: Variances had to be floored 1 2 3 13 17 26 28
INFO: iteration 41, average log likelihood -1.056279
WARNING: Variances had to be floored 11 24 32
INFO: iteration 42, average log likelihood -1.108363
WARNING: Variances had to be floored 10
INFO: iteration 43, average log likelihood -1.102898
WARNING: Variances had to be floored 19 28
INFO: iteration 44, average log likelihood -1.058599
WARNING: Variances had to be floored 1 2 3 11 13 17 20 24 26
INFO: iteration 45, average log likelihood -1.035760
INFO: iteration 46, average log likelihood -1.147994
WARNING: Variances had to be floored 10 32
INFO: iteration 47, average log likelihood -1.089732
WARNING: Variances had to be floored 2 11 19 24 28
INFO: iteration 48, average log likelihood -1.049706
WARNING: Variances had to be floored 1 3 13 17 26
INFO: iteration 49, average log likelihood -1.081098
WARNING: Variances had to be floored 20 32
INFO: iteration 50, average log likelihood -1.100205
INFO: EM with 100000 data points 50 iterations avll -1.100205
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.194843     0.115482     0.0671209     0.133701     0.0729939    0.117901    -0.0498407   -0.0248572    -0.11466      0.00979489   0.0364307   -0.0539045    0.0918681   -0.142493     0.0568351   -0.0847706   -0.107518     0.124692     0.0189529   -0.0906081   -0.0435343    0.0423105   -0.203262     0.0415081    0.0625435   -0.102632  
  0.104671    -0.146402     0.0771995    -0.0478052    0.0804856   -0.0303993   -0.0249488    0.104836     -0.0194197   -0.0495473   -0.0498161   -0.0640645   -0.0248438    0.082707     0.109409    -0.125907     0.00506488   0.264652    -0.101523     0.0511938    0.123966     0.25627     -0.0159311   -0.0400954   -0.0127077    0.127577  
  0.112104    -0.15164     -0.0922782     0.0221481    0.0117269   -0.22737      0.100201    -0.155315     -0.0765863   -0.0169174   -0.194334    -0.0150252   -0.00852991   0.048761     0.119652    -0.0479724   -0.125555     0.0674425   -0.178951    -0.168337     0.153335    -0.215532    -0.0276594   -0.0200615    0.00609395  -0.100798  
 -0.0958152   -0.220465    -0.123511      0.0136224    0.0306227    0.05044     -0.0213952    0.111865     -0.103892     0.0814715    0.101176    -0.0707021   -0.0438014   -0.071367    -0.0441721    0.0478131   -0.0229234    0.249421     0.0678721   -0.170436     0.0700855    0.151514    -0.142398    -0.0542604    0.0641346   -0.0335803 
  0.112504    -0.148626     0.133996     -0.0174453    0.173775     0.0808606    0.0600434   -0.0276764     0.0412691    0.226443     0.00161594   0.0577079    0.0827232    0.064412    -0.00932619   0.107018    -0.0779043   -0.0931405    0.177367     0.118231     0.1481       0.0586984    0.254806    -0.0177905    0.0164849    0.127605  
  0.0845287   -0.0695172   -0.0202844    -0.0609334    0.0201572   -0.0406144   -0.104753    -0.0648059     0.0519902   -0.152825     0.103486    -0.102883    -0.0815383   -0.040144     0.0580906    0.0546261    0.0184876    0.18363      0.0090079   -0.0927651    0.10654     -0.0439916   -0.095983    -0.0769701   -0.077202     0.170332  
  0.200786     0.24364     -0.0545957     0.108978    -0.0464214    0.0463638    0.151066    -0.197451      0.102394    -0.232523    -0.0622192   -0.0652689   -0.11994     -0.111047     0.0896502   -0.0403676    0.130612    -0.00431036  -0.0941517   -0.112341     0.0815717   -0.0200606   -0.0239859   -0.0866484    0.0897782    0.00833796
 -0.074272    -0.0365146   -0.0776912     0.0307342   -0.0150107    0.216658    -0.112573     0.0269716    -0.0341493    0.0483852    0.296134    -0.114321     0.256244     0.0253589    0.024276    -0.144697    -0.0177133    0.0720901   -0.00344151  -0.0603081   -0.0980798    0.0231976    0.216803     0.0713159    0.075512     0.227053  
 -0.0360752   -0.195627    -0.108648      0.1134       0.0203835    0.00934011  -0.186366     0.10887       0.0527896   -0.274691     0.0183026   -0.0674978   -0.0694021   -0.277836     0.0027651   -0.0138218   -0.0812151   -0.0912968    0.131735    -0.164001     0.0798349    0.0647881    0.0705543   -0.0287164   -0.0746873    0.0930285 
  0.0684675   -0.0813057   -0.0168374    -0.153037     0.14503      0.0317849    0.0847639   -0.0508634    -0.110978     0.0808661    0.037888     0.121518     0.0940075    0.0302483   -0.128346    -0.0143742   -0.00516441   2.6786       0.0614448   -0.00131714   0.00256857  -0.00234957   0.215995    -0.0882413    0.0640754   -0.0930935 
  0.0788551   -0.0378101   -0.0236212     0.0675328   -0.00364188   0.0187733    0.25519      0.000982303  -0.0580343   -0.15328     -0.0617617   -0.059366     0.109566    -0.0370568    0.0790462   -0.126447    -0.0406148   -0.0878313   -0.127063     0.184493     0.0307583    0.0906073   -0.106994     0.126922    -0.0236803    5.84304e-5
 -0.0127236   -0.00606306   0.0542757    -0.123452     0.0661486   -0.0476524   -0.237394     0.0865968     0.10852      0.067069    -0.0993045   -0.131745     0.0510519   -0.0985086   -0.0173411   -0.0123928    0.122965    -0.117234     0.0544597    0.0900115    0.0926842   -0.07909     -0.0561716   -0.0240359   -0.121002     0.0217452 
  0.0507407   -0.104394     0.0545129    -0.225291     0.0429362    0.147088    -0.0118814    0.0896286     2.53232e-5   0.00162968  -0.191835     0.191837     0.0223048    0.0546777   -0.139708    -0.16561     -0.0567996   -0.143858     0.153414     0.00292062  -0.0408313   -0.124473    -0.0806675    0.0880394   -0.186997    -0.0278629 
 -0.0570157   -0.215353    -0.0410513     0.20698      0.0468929    0.0144675   -0.0317844   -0.0605606    -0.0546315   -0.149872    -0.0262582   -0.060241    -0.0834692    0.47085      0.00208147  -0.0359849   -0.0307578   -0.0155728    0.13096     -0.0792247    0.054596    -0.025574     0.138427     0.0428661   -0.0512547    0.0517634 
  0.112633     0.0654246    0.177167      0.0460895    0.0875595    0.09577     -0.0139742   -0.059815      0.204118     0.174023     0.0518108    0.102463     0.201452     0.0512686   -0.0378152    0.149822    -0.107304    -0.121042    -0.211856     0.204993     0.0237794   -0.0596469    0.0182706    0.360839    -0.151822     0.131511  
  0.140133     0.12007     -0.0353326    -0.0395125   -0.129471    -0.0872118    0.0454165    0.00787085   -0.064849     0.0199231    0.0611967   -0.0963256   -0.00430451  -0.0355578    0.207149    -0.0858308   -0.0360445   -0.0318291   -0.0742365    0.0833425   -0.102031    -0.0112681    0.10828      0.0747798   -0.090491     0.0391404 
  0.060346     0.0828731   -0.0404788     0.00731501   0.0382856   -0.27651      0.0917809   -0.100454     -0.0952045    0.0874277    0.193002    -0.111513    -0.0110827    0.0867952   -0.141116    -0.0502246    0.0784447    0.0115986    0.0914257    0.0433576   -0.108408     0.118071     0.0719048   -0.110227     0.0546738   -0.101715  
  0.0751303    0.0923336    0.0524619    -0.120417    -0.0143449    0.01675     -0.12975      0.000725379  -0.0945408    0.0294205    0.086506     0.138473     0.00396019  -0.0265588   -0.122136     0.0393864   -0.0462713   -0.0540463   -0.111275    -0.022476    -0.0698443    0.138266     0.0228864   -0.0395423   -0.153209    -0.0224366 
 -0.166814    -0.011357    -0.0302251     0.00861135  -0.0556707   -0.0300692    0.0284278   -0.119978     -0.118755    -0.0308651    0.106593    -0.0960926    0.0356587   -0.128511     0.0915884    0.035689     0.075823     0.105093    -0.0537093    0.195435    -0.0483212   -0.0709554    0.0893018    0.208804    -0.0427794    0.114261  
  0.00450059   0.0650953   -0.14862       0.052297     0.014678     0.1962      -0.0823145   -0.0836495    -0.198419    -0.0176584   -0.0753363   -0.0565347   -0.107034    -0.00773476  -0.0240311    0.0174956   -0.114045    -0.0690925    0.0877707    0.132297    -0.102202    -0.12657     -0.0194016    0.130927     0.0714706   -0.0437578 
 -0.0549879    0.0197087   -0.0307787    -0.0382444   -0.032651     0.215029     0.0353266    0.0640095     0.0036807    0.059239     0.186746     0.0239176   -0.177129     0.0105549   -0.0931359   -0.0625418    0.102267    -0.0777743    0.0130882   -0.117523    -0.0276045   -0.0750728   -0.137919    -0.147102    -0.0582495    0.115404  
  0.0279738    0.0101212    0.0682169    -0.166624     0.113801     0.280978    -0.0512146   -0.0540951    -0.0523511   -0.16869     -0.130916    -0.0055586    0.027206    -0.0592123   -0.0901925   -0.00295842   0.074493    -0.0920151    0.0186356    0.0229023   -0.103608     0.0403692    0.0475017   -0.00444322   0.0270351    0.314828  
 -0.0561421    0.0675697    0.000254306  -0.00614534  -0.0282136   -0.0629113    0.103152    -0.0137678    -0.0808563   -0.0672127    0.00472231  -0.0259054   -0.115683    -0.144427     0.0188244   -0.0217074   -0.143877     0.0495217    0.0151455    0.102474    -0.0924615    0.0461874   -0.0487216   -0.120062     0.074434     0.0162464 
 -0.0780885    0.0666519   -0.0607938     0.012541    -0.0690235   -0.16035     -0.0498736    0.28032       0.106484     0.0905424   -0.121955    -0.166447    -0.0462894    0.0115934    0.0310904   -0.0369581   -0.0432371    0.0075429    0.0904565    0.0743692    0.0152739   -0.041886     0.134822    -0.038293    -0.0160518   -0.0152147 
  0.0600942    0.0403516    0.0808971    -0.0226879    0.191379     0.0667114    0.141087    -0.080097     -0.0143643    0.0270957    0.0371698   -0.00583216  -0.0223673    0.0242329    0.0287762    0.00571799   0.0372354   -0.0652591    0.031572    -0.0113082   -0.110892     0.0349831    0.00814476   0.0270813    0.0476818    0.254249  
 -0.0904872   -0.00235514   0.207458      0.107152    -0.115198     0.0981699    0.0676321    0.017452      0.0131958   -0.0098857   -0.0526411   -0.115917     0.168245     0.160488    -0.0147845   -0.14131      0.0567669    0.0908987    0.0593569   -0.134612     0.095785    -0.120932     0.10684      0.0679177   -0.0677095   -0.0574497 
  0.0618276    0.0414822   -0.0398149    -0.028968    -0.0603922    0.158904    -0.00115913   0.102792      0.0135878   -0.0415147   -0.137567    -0.0947913    0.127251     0.083207     0.049346    -0.188943     0.0469717   -0.0644445    0.048368     0.0553194    0.169698     0.0468025   -0.0301361    0.0506495    0.015891    -0.0323233 
 -0.0715188    0.142819     0.087199     -0.0480922   -0.0746113    0.11943      0.169827     0.00146167    0.0556581   -0.0632155   -0.100393    -0.114487    -0.124655    -0.0519288    0.0297207    0.00967569   0.0682296    0.122937    -0.0739897    0.230608     0.0162454    0.0378953    0.114421     0.045993    -0.189024     0.00924539
 -0.022934    -0.207316    -0.00843972    0.119186    -0.0223071    0.0283832    0.0307159   -0.0465234     0.18029      0.0214725    0.0073456    0.00641687  -0.0562337    0.0736865    0.0263017   -0.0862978    0.139632    -0.103539     0.0683959    0.0487278   -0.280536    -0.159775    -0.0767528   -0.126849     0.123856     0.08227   
  0.0882935   -0.017536    -0.175537     -0.0391571   -0.0818177    0.103696     0.166876    -0.0444256     0.0212122   -0.103671     0.0907963    0.116092     0.0567107    0.0591646   -0.0730064    0.134658    -0.0951135    0.119518     0.121225    -0.229991    -0.0981024    0.0993175   -0.0148093    0.181008     0.198793     0.0237666 
  0.058359    -0.064376     0.120324      0.0207005    0.0701194    0.167971     0.0496873   -0.178326      0.0347917   -0.106455     0.0539593    0.199041     0.0202372    0.0419351   -0.0618352   -0.141778     0.136133    -0.0210848    0.15657      0.0678096   -0.119015     0.144077     0.0234449    0.0736918    0.0338947   -0.172767  
  0.0909548   -0.12842      0.0225785    -0.0917459    0.171486     0.0859412    0.0837758   -0.0136819    -0.141625     0.0206105    0.0260422    0.128574     0.161581     0.0626525   -0.132311    -0.0163897    0.00654759  -1.07241      0.0677014   -0.00738352   0.00253108  -0.00777138   0.194668    -0.0177361    0.0836262   -0.0936984 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 10 11 28
INFO: iteration 1, average log likelihood -1.074135
WARNING: Variances had to be floored 1 2 3 10 11 17 19 24 28
INFO: iteration 2, average log likelihood -1.025140
WARNING: Variances had to be floored 2 10 11 13 24 26 28
INFO: iteration 3, average log likelihood -1.038483
WARNING: Variances had to be floored 1 2 10 11 17 19 20 24 28
INFO: iteration 4, average log likelihood -1.024521
WARNING: Variances had to be floored 3 10 11 19 28
INFO: iteration 5, average log likelihood -1.046000
WARNING: Variances had to be floored 1 2 10 11 13 17 24 26 28
INFO: iteration 6, average log likelihood -1.021594
WARNING: Variances had to be floored 2 3 10 11 19 24 28
INFO: iteration 7, average log likelihood -1.030932
WARNING: Variances had to be floored 1 2 10 11 17 20 24 28
INFO: iteration 8, average log likelihood -1.021440
WARNING: Variances had to be floored 3 10 11 13 19 26 28
INFO: iteration 9, average log likelihood -1.032131
WARNING: Variances had to be floored 1 2 10 11 17 24 28
INFO: iteration 10, average log likelihood -1.032174
INFO: EM with 100000 data points 10 iterations avll -1.032174
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.214931     0.0301733   -0.170543     0.0536847   -0.158025    -0.0851521    0.0939356    0.133989    -0.14969      0.0683317   -0.0305312    0.01061      0.062387   -0.125134    0.140346     0.091521    -0.0310066    -0.0323399   -0.0846762   -0.199463     0.0682187    0.012335      0.134334      0.0862411   0.0116889   -0.298486   
 -0.0141553    0.0898116    0.0745517    0.0565952   -0.0164736    0.00105854  -0.00978443   0.0285806    0.00821965   0.0926577   -0.0730983   -0.0347747    0.123404    0.041773   -0.138076     0.187992    -0.0129485    -0.0889843    0.0967403   -0.0566018    9.65815e-5   0.0598569     0.158384      0.148325    0.0472207    0.101011   
 -0.0767308    0.10862     -0.0488659   -0.0049142    0.0417198    0.208835    -0.112086    -0.275651     0.0996333    0.0511729    0.232577     0.0743718    0.130777    0.0340486   0.202534    -0.122207    -0.0678481    -0.030513     0.00499763   0.157668    -0.090857    -0.0514847    -0.117425      0.0493561   0.00713924   0.0292431  
  0.0200884    0.0788964    0.0324793    0.148735    -0.0497954    0.0461948    0.152618     0.027003     0.200925     0.0395571    0.141146    -0.0573391   -0.0134763   0.0427682   0.147214     0.299993    -0.0733211     0.0894601    0.0389326   -0.169604    -0.187878    -0.106146      0.0103903    -0.0438817  -0.0774732    0.0801435  
 -0.0943299    0.086739     0.0238432    0.148572    -0.0801244   -0.117674     0.0386422   -0.0905869    0.123054     0.0464741    0.140027     0.00465619   0.0616959   0.0534635  -0.0285934   -0.00960743   0.00569987   -0.187038    -0.208956     0.0157583    0.0388741   -0.014283      0.0983599     0.0999362   0.0352496    0.0763414  
 -0.0629473    0.121491     0.039202     0.00307835   0.0667345    0.00380314  -0.0390068   -0.249704    -0.0716738    0.0171911    0.0508172    0.0612871   -0.0646875   0.0648567   0.00864914   0.017083    -0.0616323    -0.0180869    0.0479793    0.0446209   -0.0127284    0.221248     -0.276651     -0.240045    0.0297619   -0.208218   
 -0.0769503   -0.185654     0.130622     0.0452852    0.166044    -0.21094     -0.11132     -0.0184875   -0.121794     0.118066     0.023063     0.053901    -0.059841   -0.0746658  -0.12773     -0.123671    -0.01789       0.0120666    0.134975    -0.0114118    0.0434145   -0.082989      0.0760295     0.0213626  -0.0440732   -0.0748926  
  0.082928    -0.0514274    0.0412107    0.0434705   -0.111924    -0.0183097    0.0625472    0.151266    -0.100892     0.0183254   -0.208172    -0.121388    -0.0276682   0.0978379   0.26153     -0.0420763    0.0847108     0.141264    -0.148996    -0.00139525   0.0444322    0.121469      0.12712      -0.0495129   0.0755905    0.0920297  
  0.0680628   -0.055088    -0.104623    -0.0164442    0.050898    -0.0375269   -0.00878399   0.135387    -0.124234     0.0478556   -0.129078    -0.138585    -0.09304    -0.0135194  -0.124089     0.0523748    0.182618      0.162614    -0.138378    -0.166168     0.0761361   -0.164237      0.0326395     0.159976    0.0724669   -0.137992   
 -0.144702     0.079717     0.220981    -0.144616     0.0693465    0.0810387   -0.0298559   -0.215771    -0.0351866    0.0407348    0.189046    -0.0886891   -0.0654844  -0.134833    0.186236    -0.062874    -0.08138      -0.069578     0.0897125    0.0532253   -0.206684     0.0654584    -0.0647471    -0.149999   -0.107541    -0.00279324 
 -0.041852     0.009262    -0.067956     0.147909     0.0693904    0.0582542    0.0327486    0.0608001    0.0296658    0.0140888   -0.00374784  -0.0102693    0.120635   -0.118523   -0.0732827   -0.217821     0.0295293     0.130114     0.127037     0.139148     0.142854    -0.0598679    -0.0781303    -0.0218536  -0.209817     0.00510251 
  0.125326    -0.0266028    0.0974227    0.0233429    0.142561    -0.0401941   -0.0926646    0.01118      0.0393048    0.107681    -0.0235004   -0.16451     -0.070632    0.0757688   0.0166294    0.139114     0.00271969   -0.100904     0.122619     0.019175    -0.0656141    0.0761383    -0.0626814    -0.057896   -0.0168366   -0.0132552  
  0.207028     0.212376    -0.0763087    0.0155273    0.167816    -0.0356712    0.00336171  -0.109109    -0.0139467   -0.0342658    0.0292451   -0.108244    -0.0893919  -0.016546    0.137076    -0.160649    -0.255916     -0.042102    -0.2382      -0.0366538   -0.155387    -0.0315833     0.129513      0.121864   -0.17297     -0.256201   
 -0.0313895   -0.0731141   -0.0501815    0.117593    -0.07208      0.142601     0.0141796   -0.0249428    0.0283914    0.00480305   0.0788821   -0.0049257   -0.0143284   0.105038   -0.0163054   -0.0757775    0.0484931    -0.0102707    0.0708342    0.0696578    0.108914    -0.196102     -0.0162585    -0.0464691  -0.00367913  -0.102206   
  0.0286495    0.0438499    0.00410303  -0.112247    -0.0780326   -0.0477656    0.124103    -0.0641284   -0.0899557   -0.0668673   -0.0202623   -0.0205326   -0.0136362   0.030335    0.128357    -0.0275115    0.0717417    -0.0573372   -0.0874632    0.0607395    0.169564    -0.118927     -0.00839086   -0.102582   -0.0245142   -0.0064932  
 -0.128852     0.0345158   -0.0215645   -0.182047     0.0337951   -0.00618269   0.138393    -0.130761    -0.0996696   -0.0182837   -0.0333025   -0.0330835   -0.0628291   0.0132327  -0.0388474    0.0964466   -0.02236       0.0446444   -0.0594434    0.0557706   -0.209099    -0.0988871    -0.282492      0.0665999  -0.110418    -0.155114   
  0.0915561   -0.018832    -0.0922565    0.143064    -0.0258877   -0.039481    -0.0041902    0.386113     0.0716256    0.0589836    0.0658718   -0.0190743    0.0673401   0.118985   -0.0924853    0.0328235    0.0478822     0.0452996    0.00115458   0.0598267   -0.0933629   -0.0305217    -0.0704169     0.0484408   0.0292066    0.0386476  
  0.0296836   -0.0754897    0.162912     0.142252     0.107248    -0.0896429    0.198859    -0.116256    -0.189964    -0.10973     -0.03598      0.10753     -0.141063    0.155046    0.0399535    0.0138182    0.000894902   0.321253     0.0350186    0.127647    -0.111649     0.112636     -0.0172612    -0.0975162  -0.0301732    0.000103137
  0.01243      0.00646727  -0.0118763    0.0730904   -0.0520348    0.101473     0.158091    -0.151192     0.00604696  -0.0171318   -0.0157883    0.0532083    0.0132505  -0.180043   -0.0377957   -0.110396     0.0138621     0.162758     0.223111    -0.0211702   -0.146991    -0.00755664   -0.000945804  -0.0651182  -0.190191    -0.0447384  
 -0.00817243   0.0150691    0.0840862   -0.251591    -0.0786829   -0.149618     0.0145528    0.0402776   -0.0121401    0.0965084   -0.0895221    0.0123684    0.161771    0.0831699   0.0346937   -0.00982777   0.0245034    -0.00437289  -0.0551832   -0.101871     0.0299497    0.0641738    -0.0572877     0.161153   -0.0615847   -0.153992   
  0.0647553    0.02765      0.0419371   -0.131888     0.0980412    0.106458    -0.209504     0.0480766   -0.0129306   -0.0151815    0.0832984   -0.0765008    0.155624    0.0434398  -0.0445285    0.119026    -0.0449357     0.10204     -0.0865632   -0.0297669   -0.109689    -0.055066     -0.0331758     0.102942    0.0119193   -0.144067   
 -0.00819723   0.14148      0.113785    -0.118796    -0.0509439   -0.0136978    0.143323     0.0123639   -0.149435    -0.0554861   -0.152084    -0.0123232    0.108882   -0.0381447  -0.0543375   -0.151037     0.113618      0.172066    -0.153804    -0.0257851   -0.0437789    0.0971395    -0.0471076     0.134351   -0.107661    -0.0291771  
  0.043457    -0.0935868   -0.0831513   -0.10144      0.00164805  -0.0497031    0.0422814    0.124589     0.126608    -0.0689736    0.0590696   -0.05567     -0.0301432  -0.0655248  -0.00380404   0.0757576    0.293611      0.143815     0.0689252    0.0434382   -0.0539966    0.000461929   0.215871     -0.0967351   0.131902     0.065787   
  0.117683     0.0803918    0.129754    -0.0477113   -0.00765187  -0.0252879    0.0976973   -0.144796    -0.198985     0.0211483    0.0272241   -0.0226939   -0.0368225   0.0875893   0.0953133    0.0880574    0.121051     -0.0284314   -0.0889699    0.00740126   0.0945638   -0.0149389    -0.14531       0.0464249  -0.205008    -0.0202453  
  0.0460104   -0.0274924    0.122898     0.0346759    0.106421     0.0336379   -0.271452    -0.210696    -0.0604856    0.100462    -0.0508997    0.122396     0.0440725   0.120356   -0.0730939   -0.00179012   0.113439      0.00853421   0.088276    -0.0578253   -0.02713      0.091639     -0.109787     -0.0528644  -0.06674      0.0403423  
 -0.00598644   0.0561748    0.098989    -0.0795209   -0.0343403    0.0282465   -0.0514316   -0.039003     0.0168446    0.0414011   -0.0837386    0.0170736    0.105591    0.144205   -0.0451159    0.0689691   -0.00574035   -0.113963    -0.0133409    0.00367453   0.0497258    0.125419     -0.150828      0.0906076  -0.115858     0.0630179  
 -0.142297    -0.178054     0.0370025    0.0857852    0.0590307   -0.0410117    0.0451784   -0.0317109   -0.0624583   -0.160009    -0.0343464   -0.0271661    0.13455    -0.274784    0.133016    -0.115167     0.008064      0.0282143    0.0030502   -0.0339286   -0.11556      0.10501       0.139144     -0.0288285  -0.0232262   -0.023806   
 -0.0919286    0.0820113   -0.138067     0.0302232   -0.0613831   -0.134624     0.034904    -0.0141127   -0.0301673    0.0957902   -0.0776113    0.0128511    0.0918553  -0.0714516   0.245013     0.192376     0.0790897    -0.090486     0.0597876    0.0272329    0.0841369   -0.0388129     0.0966378     0.116903   -0.146297    -0.181251   
  0.00621747  -0.00533524   0.158767    -0.0350616   -0.0926396   -0.229635    -0.125027     0.211875     0.206112    -0.272892    -0.0486133   -0.0468047    0.150412   -0.194173   -0.0914418    0.00572141   0.00765811    0.0472541   -0.014199     0.097191     0.204836    -0.0429157    -0.0962638     0.0374959   0.0894852   -0.126696   
  0.0317648   -0.114188     0.0519237    0.0603693   -0.0731079    0.0522217   -0.0359468   -0.0763925   -0.056866    -0.130784    -0.0315742    0.060825    -0.201832    0.126251   -0.0672925   -0.0354513    0.0312435     0.157139     0.0671135   -0.0200799    0.159176     0.0317652     0.0583201     0.0269508   0.100306     0.085622   
  0.00322115  -0.0389606    0.0181739    0.02907      0.127556     0.0109781   -0.100398    -0.0144965    0.114227     0.0665187    0.0739481   -0.0359948    0.0667158  -0.15479     0.0514418    0.0476255    0.101513      0.00773861   0.0428187   -0.131296     0.00162401  -0.085021      0.0647862     0.0802686  -0.0223085   -0.0222038  
  0.0421477   -0.0765123    0.053682    -0.127863    -0.0643969   -0.0202377    0.029576    -0.00273612  -0.109598    -0.0286215    0.0316392   -0.0552461   -0.073082   -0.0575857   0.0946876   -0.0530812   -0.0963837    -0.0890063   -0.10355     -0.0470959   -0.145888    -0.00249119   -0.00339347   -0.025711   -0.157785    -0.112982   kind full, method split
0: avll = -1.4257266481445579
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.425746
INFO: iteration 2, average log likelihood -1.425685
INFO: iteration 3, average log likelihood -1.425639
INFO: iteration 4, average log likelihood -1.425584
INFO: iteration 5, average log likelihood -1.425518
INFO: iteration 6, average log likelihood -1.425442
INFO: iteration 7, average log likelihood -1.425359
INFO: iteration 8, average log likelihood -1.425275
INFO: iteration 9, average log likelihood -1.425186
INFO: iteration 10, average log likelihood -1.425078
INFO: iteration 11, average log likelihood -1.424908
INFO: iteration 12, average log likelihood -1.424598
INFO: iteration 13, average log likelihood -1.424038
INFO: iteration 14, average log likelihood -1.423175
INFO: iteration 15, average log likelihood -1.422173
INFO: iteration 16, average log likelihood -1.421361
INFO: iteration 17, average log likelihood -1.420894
INFO: iteration 18, average log likelihood -1.420679
INFO: iteration 19, average log likelihood -1.420590
INFO: iteration 20, average log likelihood -1.420552
INFO: iteration 21, average log likelihood -1.420537
INFO: iteration 22, average log likelihood -1.420530
INFO: iteration 23, average log likelihood -1.420527
INFO: iteration 24, average log likelihood -1.420525
INFO: iteration 25, average log likelihood -1.420524
INFO: iteration 26, average log likelihood -1.420524
INFO: iteration 27, average log likelihood -1.420523
INFO: iteration 28, average log likelihood -1.420523
INFO: iteration 29, average log likelihood -1.420523
INFO: iteration 30, average log likelihood -1.420522
INFO: iteration 31, average log likelihood -1.420522
INFO: iteration 32, average log likelihood -1.420522
INFO: iteration 33, average log likelihood -1.420522
INFO: iteration 34, average log likelihood -1.420521
INFO: iteration 35, average log likelihood -1.420521
INFO: iteration 36, average log likelihood -1.420521
INFO: iteration 37, average log likelihood -1.420521
INFO: iteration 38, average log likelihood -1.420521
INFO: iteration 39, average log likelihood -1.420521
INFO: iteration 40, average log likelihood -1.420521
INFO: iteration 41, average log likelihood -1.420520
INFO: iteration 42, average log likelihood -1.420520
INFO: iteration 43, average log likelihood -1.420520
INFO: iteration 44, average log likelihood -1.420520
INFO: iteration 45, average log likelihood -1.420520
INFO: iteration 46, average log likelihood -1.420520
INFO: iteration 47, average log likelihood -1.420520
INFO: iteration 48, average log likelihood -1.420520
INFO: iteration 49, average log likelihood -1.420520
INFO: iteration 50, average log likelihood -1.420520
INFO: EM with 100000 data points 50 iterations avll -1.420520
952.4 data points per parameter
1: avll = [-1.42575,-1.42569,-1.42564,-1.42558,-1.42552,-1.42544,-1.42536,-1.42527,-1.42519,-1.42508,-1.42491,-1.4246,-1.42404,-1.42318,-1.42217,-1.42136,-1.42089,-1.42068,-1.42059,-1.42055,-1.42054,-1.42053,-1.42053,-1.42053,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.420539
INFO: iteration 2, average log likelihood -1.420476
INFO: iteration 3, average log likelihood -1.420427
INFO: iteration 4, average log likelihood -1.420369
INFO: iteration 5, average log likelihood -1.420298
INFO: iteration 6, average log likelihood -1.420219
INFO: iteration 7, average log likelihood -1.420135
INFO: iteration 8, average log likelihood -1.420057
INFO: iteration 9, average log likelihood -1.419989
INFO: iteration 10, average log likelihood -1.419934
INFO: iteration 11, average log likelihood -1.419891
INFO: iteration 12, average log likelihood -1.419855
INFO: iteration 13, average log likelihood -1.419824
INFO: iteration 14, average log likelihood -1.419795
INFO: iteration 15, average log likelihood -1.419766
INFO: iteration 16, average log likelihood -1.419737
INFO: iteration 17, average log likelihood -1.419706
INFO: iteration 18, average log likelihood -1.419675
INFO: iteration 19, average log likelihood -1.419642
INFO: iteration 20, average log likelihood -1.419610
INFO: iteration 21, average log likelihood -1.419578
INFO: iteration 22, average log likelihood -1.419548
INFO: iteration 23, average log likelihood -1.419521
INFO: iteration 24, average log likelihood -1.419497
INFO: iteration 25, average log likelihood -1.419477
INFO: iteration 26, average log likelihood -1.419460
INFO: iteration 27, average log likelihood -1.419446
INFO: iteration 28, average log likelihood -1.419435
INFO: iteration 29, average log likelihood -1.419427
INFO: iteration 30, average log likelihood -1.419420
INFO: iteration 31, average log likelihood -1.419414
INFO: iteration 32, average log likelihood -1.419410
INFO: iteration 33, average log likelihood -1.419406
INFO: iteration 34, average log likelihood -1.419403
INFO: iteration 35, average log likelihood -1.419401
INFO: iteration 36, average log likelihood -1.419399
INFO: iteration 37, average log likelihood -1.419397
INFO: iteration 38, average log likelihood -1.419395
INFO: iteration 39, average log likelihood -1.419394
INFO: iteration 40, average log likelihood -1.419393
INFO: iteration 41, average log likelihood -1.419392
INFO: iteration 42, average log likelihood -1.419391
INFO: iteration 43, average log likelihood -1.419390
INFO: iteration 44, average log likelihood -1.419390
INFO: iteration 45, average log likelihood -1.419389
INFO: iteration 46, average log likelihood -1.419388
INFO: iteration 47, average log likelihood -1.419388
INFO: iteration 48, average log likelihood -1.419387
INFO: iteration 49, average log likelihood -1.419387
INFO: iteration 50, average log likelihood -1.419386
INFO: EM with 100000 data points 50 iterations avll -1.419386
473.9 data points per parameter
2: avll = [-1.42054,-1.42048,-1.42043,-1.42037,-1.4203,-1.42022,-1.42014,-1.42006,-1.41999,-1.41993,-1.41989,-1.41986,-1.41982,-1.41979,-1.41977,-1.41974,-1.41971,-1.41967,-1.41964,-1.41961,-1.41958,-1.41955,-1.41952,-1.4195,-1.41948,-1.41946,-1.41945,-1.41944,-1.41943,-1.41942,-1.41941,-1.41941,-1.41941,-1.4194,-1.4194,-1.4194,-1.4194,-1.4194,-1.41939,-1.41939,-1.41939,-1.41939,-1.41939,-1.41939,-1.41939,-1.41939,-1.41939,-1.41939,-1.41939,-1.41939]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.419396
INFO: iteration 2, average log likelihood -1.419344
INFO: iteration 3, average log likelihood -1.419298
INFO: iteration 4, average log likelihood -1.419246
INFO: iteration 5, average log likelihood -1.419184
INFO: iteration 6, average log likelihood -1.419109
INFO: iteration 7, average log likelihood -1.419021
INFO: iteration 8, average log likelihood -1.418925
INFO: iteration 9, average log likelihood -1.418824
INFO: iteration 10, average log likelihood -1.418726
INFO: iteration 11, average log likelihood -1.418634
INFO: iteration 12, average log likelihood -1.418551
INFO: iteration 13, average log likelihood -1.418480
INFO: iteration 14, average log likelihood -1.418421
INFO: iteration 15, average log likelihood -1.418372
INFO: iteration 16, average log likelihood -1.418332
INFO: iteration 17, average log likelihood -1.418300
INFO: iteration 18, average log likelihood -1.418274
INFO: iteration 19, average log likelihood -1.418252
INFO: iteration 20, average log likelihood -1.418233
INFO: iteration 21, average log likelihood -1.418217
INFO: iteration 22, average log likelihood -1.418202
INFO: iteration 23, average log likelihood -1.418189
INFO: iteration 24, average log likelihood -1.418177
INFO: iteration 25, average log likelihood -1.418165
INFO: iteration 26, average log likelihood -1.418154
INFO: iteration 27, average log likelihood -1.418144
INFO: iteration 28, average log likelihood -1.418133
INFO: iteration 29, average log likelihood -1.418123
INFO: iteration 30, average log likelihood -1.418114
INFO: iteration 31, average log likelihood -1.418104
INFO: iteration 32, average log likelihood -1.418095
INFO: iteration 33, average log likelihood -1.418086
INFO: iteration 34, average log likelihood -1.418078
INFO: iteration 35, average log likelihood -1.418069
INFO: iteration 36, average log likelihood -1.418061
INFO: iteration 37, average log likelihood -1.418053
INFO: iteration 38, average log likelihood -1.418046
INFO: iteration 39, average log likelihood -1.418039
INFO: iteration 40, average log likelihood -1.418032
INFO: iteration 41, average log likelihood -1.418025
INFO: iteration 42, average log likelihood -1.418018
INFO: iteration 43, average log likelihood -1.418012
INFO: iteration 44, average log likelihood -1.418006
INFO: iteration 45, average log likelihood -1.418000
INFO: iteration 46, average log likelihood -1.417995
INFO: iteration 47, average log likelihood -1.417989
INFO: iteration 48, average log likelihood -1.417984
INFO: iteration 49, average log likelihood -1.417979
INFO: iteration 50, average log likelihood -1.417974
INFO: EM with 100000 data points 50 iterations avll -1.417974
236.4 data points per parameter
3: avll = [-1.4194,-1.41934,-1.4193,-1.41925,-1.41918,-1.41911,-1.41902,-1.41892,-1.41882,-1.41873,-1.41863,-1.41855,-1.41848,-1.41842,-1.41837,-1.41833,-1.4183,-1.41827,-1.41825,-1.41823,-1.41822,-1.4182,-1.41819,-1.41818,-1.41817,-1.41815,-1.41814,-1.41813,-1.41812,-1.41811,-1.4181,-1.4181,-1.41809,-1.41808,-1.41807,-1.41806,-1.41805,-1.41805,-1.41804,-1.41803,-1.41802,-1.41802,-1.41801,-1.41801,-1.418,-1.41799,-1.41799,-1.41798,-1.41798,-1.41797]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.417979
INFO: iteration 2, average log likelihood -1.417923
INFO: iteration 3, average log likelihood -1.417873
INFO: iteration 4, average log likelihood -1.417817
INFO: iteration 5, average log likelihood -1.417749
INFO: iteration 6, average log likelihood -1.417667
INFO: iteration 7, average log likelihood -1.417570
INFO: iteration 8, average log likelihood -1.417461
INFO: iteration 9, average log likelihood -1.417346
INFO: iteration 10, average log likelihood -1.417231
INFO: iteration 11, average log likelihood -1.417122
INFO: iteration 12, average log likelihood -1.417021
INFO: iteration 13, average log likelihood -1.416932
INFO: iteration 14, average log likelihood -1.416852
INFO: iteration 15, average log likelihood -1.416783
INFO: iteration 16, average log likelihood -1.416723
INFO: iteration 17, average log likelihood -1.416671
INFO: iteration 18, average log likelihood -1.416626
INFO: iteration 19, average log likelihood -1.416587
INFO: iteration 20, average log likelihood -1.416552
INFO: iteration 21, average log likelihood -1.416521
INFO: iteration 22, average log likelihood -1.416493
INFO: iteration 23, average log likelihood -1.416467
INFO: iteration 24, average log likelihood -1.416442
INFO: iteration 25, average log likelihood -1.416419
INFO: iteration 26, average log likelihood -1.416397
INFO: iteration 27, average log likelihood -1.416375
INFO: iteration 28, average log likelihood -1.416355
INFO: iteration 29, average log likelihood -1.416335
INFO: iteration 30, average log likelihood -1.416316
INFO: iteration 31, average log likelihood -1.416298
INFO: iteration 32, average log likelihood -1.416281
INFO: iteration 33, average log likelihood -1.416265
INFO: iteration 34, average log likelihood -1.416249
INFO: iteration 35, average log likelihood -1.416234
INFO: iteration 36, average log likelihood -1.416220
INFO: iteration 37, average log likelihood -1.416206
INFO: iteration 38, average log likelihood -1.416193
INFO: iteration 39, average log likelihood -1.416181
INFO: iteration 40, average log likelihood -1.416169
INFO: iteration 41, average log likelihood -1.416158
INFO: iteration 42, average log likelihood -1.416147
INFO: iteration 43, average log likelihood -1.416136
INFO: iteration 44, average log likelihood -1.416126
INFO: iteration 45, average log likelihood -1.416116
INFO: iteration 46, average log likelihood -1.416107
INFO: iteration 47, average log likelihood -1.416098
INFO: iteration 48, average log likelihood -1.416089
INFO: iteration 49, average log likelihood -1.416080
INFO: iteration 50, average log likelihood -1.416072
INFO: EM with 100000 data points 50 iterations avll -1.416072
118.1 data points per parameter
4: avll = [-1.41798,-1.41792,-1.41787,-1.41782,-1.41775,-1.41767,-1.41757,-1.41746,-1.41735,-1.41723,-1.41712,-1.41702,-1.41693,-1.41685,-1.41678,-1.41672,-1.41667,-1.41663,-1.41659,-1.41655,-1.41652,-1.41649,-1.41647,-1.41644,-1.41642,-1.4164,-1.41638,-1.41635,-1.41634,-1.41632,-1.4163,-1.41628,-1.41626,-1.41625,-1.41623,-1.41622,-1.41621,-1.41619,-1.41618,-1.41617,-1.41616,-1.41615,-1.41614,-1.41613,-1.41612,-1.41611,-1.4161,-1.41609,-1.41608,-1.41607]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.416072
INFO: iteration 2, average log likelihood -1.416011
INFO: iteration 3, average log likelihood -1.415953
INFO: iteration 4, average log likelihood -1.415885
INFO: iteration 5, average log likelihood -1.415802
INFO: iteration 6, average log likelihood -1.415702
INFO: iteration 7, average log likelihood -1.415584
INFO: iteration 8, average log likelihood -1.415454
INFO: iteration 9, average log likelihood -1.415315
INFO: iteration 10, average log likelihood -1.415174
INFO: iteration 11, average log likelihood -1.415036
INFO: iteration 12, average log likelihood -1.414905
INFO: iteration 13, average log likelihood -1.414783
INFO: iteration 14, average log likelihood -1.414673
INFO: iteration 15, average log likelihood -1.414575
INFO: iteration 16, average log likelihood -1.414488
INFO: iteration 17, average log likelihood -1.414410
INFO: iteration 18, average log likelihood -1.414341
INFO: iteration 19, average log likelihood -1.414279
INFO: iteration 20, average log likelihood -1.414223
INFO: iteration 21, average log likelihood -1.414173
INFO: iteration 22, average log likelihood -1.414126
INFO: iteration 23, average log likelihood -1.414083
INFO: iteration 24, average log likelihood -1.414043
INFO: iteration 25, average log likelihood -1.414005
INFO: iteration 26, average log likelihood -1.413969
INFO: iteration 27, average log likelihood -1.413934
INFO: iteration 28, average log likelihood -1.413901
INFO: iteration 29, average log likelihood -1.413869
INFO: iteration 30, average log likelihood -1.413838
INFO: iteration 31, average log likelihood -1.413807
INFO: iteration 32, average log likelihood -1.413778
INFO: iteration 33, average log likelihood -1.413749
INFO: iteration 34, average log likelihood -1.413721
INFO: iteration 35, average log likelihood -1.413693
INFO: iteration 36, average log likelihood -1.413666
INFO: iteration 37, average log likelihood -1.413640
INFO: iteration 38, average log likelihood -1.413615
INFO: iteration 39, average log likelihood -1.413590
INFO: iteration 40, average log likelihood -1.413566
INFO: iteration 41, average log likelihood -1.413543
INFO: iteration 42, average log likelihood -1.413520
INFO: iteration 43, average log likelihood -1.413499
INFO: iteration 44, average log likelihood -1.413478
INFO: iteration 45, average log likelihood -1.413458
INFO: iteration 46, average log likelihood -1.413439
INFO: iteration 47, average log likelihood -1.413421
INFO: iteration 48, average log likelihood -1.413403
INFO: iteration 49, average log likelihood -1.413386
INFO: iteration 50, average log likelihood -1.413370
INFO: EM with 100000 data points 50 iterations avll -1.413370
59.0 data points per parameter
5: avll = [-1.41607,-1.41601,-1.41595,-1.41588,-1.4158,-1.4157,-1.41558,-1.41545,-1.41532,-1.41517,-1.41504,-1.4149,-1.41478,-1.41467,-1.41458,-1.41449,-1.41441,-1.41434,-1.41428,-1.41422,-1.41417,-1.41413,-1.41408,-1.41404,-1.414,-1.41397,-1.41393,-1.4139,-1.41387,-1.41384,-1.41381,-1.41378,-1.41375,-1.41372,-1.41369,-1.41367,-1.41364,-1.41361,-1.41359,-1.41357,-1.41354,-1.41352,-1.4135,-1.41348,-1.41346,-1.41344,-1.41342,-1.4134,-1.41339,-1.41337]
[-1.42573,-1.42575,-1.42569,-1.42564,-1.42558,-1.42552,-1.42544,-1.42536,-1.42527,-1.42519,-1.42508,-1.42491,-1.4246,-1.42404,-1.42318,-1.42217,-1.42136,-1.42089,-1.42068,-1.42059,-1.42055,-1.42054,-1.42053,-1.42053,-1.42053,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42052,-1.42054,-1.42048,-1.42043,-1.42037,-1.4203,-1.42022,-1.42014,-1.42006,-1.41999,-1.41993,-1.41989,-1.41986,-1.41982,-1.41979,-1.41977,-1.41974,-1.41971,-1.41967,-1.41964,-1.41961,-1.41958,-1.41955,-1.41952,-1.4195,-1.41948,-1.41946,-1.41945,-1.41944,-1.41943,-1.41942,-1.41941,-1.41941,-1.41941,-1.4194,-1.4194,-1.4194,-1.4194,-1.4194,-1.41939,-1.41939,-1.41939,-1.41939,-1.41939,-1.41939,-1.41939,-1.41939,-1.41939,-1.41939,-1.41939,-1.41939,-1.4194,-1.41934,-1.4193,-1.41925,-1.41918,-1.41911,-1.41902,-1.41892,-1.41882,-1.41873,-1.41863,-1.41855,-1.41848,-1.41842,-1.41837,-1.41833,-1.4183,-1.41827,-1.41825,-1.41823,-1.41822,-1.4182,-1.41819,-1.41818,-1.41817,-1.41815,-1.41814,-1.41813,-1.41812,-1.41811,-1.4181,-1.4181,-1.41809,-1.41808,-1.41807,-1.41806,-1.41805,-1.41805,-1.41804,-1.41803,-1.41802,-1.41802,-1.41801,-1.41801,-1.418,-1.41799,-1.41799,-1.41798,-1.41798,-1.41797,-1.41798,-1.41792,-1.41787,-1.41782,-1.41775,-1.41767,-1.41757,-1.41746,-1.41735,-1.41723,-1.41712,-1.41702,-1.41693,-1.41685,-1.41678,-1.41672,-1.41667,-1.41663,-1.41659,-1.41655,-1.41652,-1.41649,-1.41647,-1.41644,-1.41642,-1.4164,-1.41638,-1.41635,-1.41634,-1.41632,-1.4163,-1.41628,-1.41626,-1.41625,-1.41623,-1.41622,-1.41621,-1.41619,-1.41618,-1.41617,-1.41616,-1.41615,-1.41614,-1.41613,-1.41612,-1.41611,-1.4161,-1.41609,-1.41608,-1.41607,-1.41607,-1.41601,-1.41595,-1.41588,-1.4158,-1.4157,-1.41558,-1.41545,-1.41532,-1.41517,-1.41504,-1.4149,-1.41478,-1.41467,-1.41458,-1.41449,-1.41441,-1.41434,-1.41428,-1.41422,-1.41417,-1.41413,-1.41408,-1.41404,-1.414,-1.41397,-1.41393,-1.4139,-1.41387,-1.41384,-1.41381,-1.41378,-1.41375,-1.41372,-1.41369,-1.41367,-1.41364,-1.41361,-1.41359,-1.41357,-1.41354,-1.41352,-1.4135,-1.41348,-1.41346,-1.41344,-1.41342,-1.4134,-1.41339,-1.41337]
32×26 Array{Float64,2}:
 -0.0669285  -0.537199    0.0261889   -0.782994    -0.172405    -0.153589    0.0678409   -0.404496   -0.38977    -0.137085    0.212785    -0.0549054  -0.427713    -0.130777    0.259485     0.280758   -0.246932    0.391317    -0.346093    0.236959    -0.386739    0.361444    -0.151024   -0.430976    0.0636121    0.0681428 
 -0.218423    0.0606919  -0.153238     0.370763    -0.471346    -0.124817   -0.591304    -0.148623   -0.0254135  -0.266773   -0.04778      0.152932    0.28211     -0.148028    0.559492     0.0754141  -0.0535794  -0.0657011   -0.403583    0.212044    -0.259932    0.259564    -0.31331    -0.315599   -0.214919     0.102031  
 -0.13448     0.0486117   0.042408    -0.00311147  -0.00605657  -0.0949474   0.00944739   0.0310938  -0.0979104  -0.0405328   0.0759004   -0.0840208   0.0321337   -0.0224191   0.183841    -0.0108108   0.105387    0.00879     -0.0804312   0.0194212    0.112246    0.0405306    0.0597995  -0.0136923  -0.00675917   0.124312  
  0.186184    0.153039   -0.111647    -0.0710789    0.211427    -0.014236   -0.203168    -0.27001     0.13971     0.145341    0.0103695    0.088332   -0.148084     0.0503592   0.00436463   0.333295   -0.0236194   0.153669     0.188335   -0.234506    -0.177553   -0.140408     0.226058   -0.0759166  -0.0993211   -0.4279    
  0.212579    0.1899     -0.0995637    0.117327    -0.380855     0.678659    0.449423    -0.107807   -0.0228938  -0.101171   -0.509238    -0.427912    0.372165    -0.187064   -0.0659505    0.0786008  -0.551609   -0.378225     0.323076   -0.484793    -0.0509712   0.11934     -0.3418      0.333505   -0.21596     -0.153283  
 -0.121238   -0.148194   -0.230241    -0.0547763   -0.0582366    0.346631    0.0753269   -0.267856   -0.169199    0.158171   -0.0878933   -0.063824    0.211648     0.24673    -0.564946    -0.495693   -0.178436    0.0635788   -0.0191484   0.437397    -0.454678   -0.147752    -0.239283    0.0697636  -0.127269    -0.160293  
  0.213226   -0.117894    0.227455     0.345188    -0.375955     0.200617    0.492637     0.0871484   0.198712    0.170706    0.0610591    0.469956   -0.0730292    0.301234   -0.399325     0.212633    0.118445    0.124229    -0.191694   -0.230834     0.0326263   0.279224    -0.278011    0.263183    0.674279     0.0459838 
 -0.270825    0.0188716   0.0655505    0.544847     0.422081     0.222539   -0.0678667   -0.342053   -0.0957401   0.540908   -0.270663    -0.404038    0.0354311    0.432801   -0.342012     0.264098    0.366339   -0.278337    -0.427369   -0.476322    -0.0241025  -0.0277466   -0.568857    0.425137    0.0647767    0.292422  
 -0.0926259   0.370417    0.554436    -0.181009     0.278861     0.0135636  -0.13128      0.0311738  -0.0862545  -0.360128   -0.147678    -0.0428101  -0.177133    -0.192718    0.125987    -0.338579    0.141733   -0.785309     0.801875   -0.108787     0.249711    0.085415    -0.428497    0.136795   -0.501122     0.361094  
  0.0509999   0.444035    0.134697    -0.189978     0.520569     0.2131     -0.543006     0.430878    0.255005   -0.346904    0.0184547   -0.0831983   0.05916      0.892044   -0.427643     0.0285581   0.107644   -0.686624    -0.0597638   0.68226     -0.351697    0.219354    -0.486539    0.552854   -0.202517    -0.385966  
  0.0964943  -0.188876    0.707707    -0.887339    -0.125063     0.321438   -0.286224     0.35788     0.266267   -0.0642023  -0.188585     0.0263855   0.00508415   0.249592    0.0905768   -0.377441    0.516478    0.514103     0.3151     -0.12909      0.0467993  -0.306973     0.126452   -0.345143   -0.67514     -0.205744  
  0.283605   -0.1679      0.612776    -0.514357     0.272956    -0.228801   -0.155354     0.0313879  -0.180772    0.0409791  -0.202251     0.872101    0.146267     0.16435    -0.0205786   -0.405009   -0.215788    0.348599     0.439591    0.09993      0.0342595   0.159009    -0.200165    0.0835452   0.157488    -0.504193  
 -0.218965    0.100374   -0.261395     0.372771    -0.259297     0.342471    0.546002     0.750868   -0.203321    0.351294   -0.0157475   -0.155678   -0.0461803    0.625181    0.302537    -0.269757    0.398496    0.15199     -0.197426    0.0375866    0.046612   -0.503702     0.0524804  -0.0571611  -0.12         0.066395  
 -0.462448    0.209043    0.00920607   0.0211292    0.169709    -0.13264    -0.0718239    0.294983   -0.476634   -0.415194    0.363341    -0.472209   -0.0610537    0.123753    0.0501183   -0.745814   -0.0155724   0.426306    -0.549297    0.5155       0.137594   -0.260722     0.0298015   0.0499119  -0.128856     0.233242  
  0.141683   -0.1624      0.460067    -0.119681     0.298783    -0.0382158   0.46036      0.565929    0.453748   -0.126672    0.0142389   -0.106447   -0.171319    -0.211295   -0.536362    -0.361049    0.0117157   0.171285     0.367877    0.260421     0.298789   -0.30113      0.145518    0.385206    0.412698     0.0573218 
  0.284913    0.0833977  -0.356674     0.194988     0.0522276   -0.599341    0.22634      0.0773852  -0.0456998  -0.238086    0.0837375   -0.234789    0.366539    -0.54782    -0.181987    -0.168416   -0.276142   -0.541258     0.0150734   0.194712     0.424736    0.357174     0.0116229   0.310365    0.176371     0.272334  
  0.122981   -0.146525    0.471006     0.103243     0.132581     0.291083   -0.0938215   -0.413534    0.197268    0.496264   -0.103032     0.169095   -0.342023     0.239145    0.189868     0.191957    0.633998   -0.451176     0.58754    -0.541252    -0.529186    0.463934    -0.783862   -0.425506   -0.271031    -0.341512  
 -0.100148    0.241502    0.0822052   -0.206872    -0.123277     0.126531   -0.268574    -0.321191    0.165466    0.0245801   0.106613     0.227806   -0.627631     0.354111   -0.048622     0.451005    0.476689    1.03574     -0.132315   -0.248582    -0.666214   -0.138888     0.301859   -0.35363     0.299088    -0.00130758
  0.34543     0.158253   -0.473009     0.176433     0.09158      0.0926226   0.253217    -0.65775    -0.0952089   0.215261   -0.26398      0.142971   -0.00628056   0.239387   -0.265479     0.586609   -0.347573   -0.159913     0.152446   -0.302203    -0.138635   -0.526153     0.383198    0.375687   -0.204757    -0.674181  
  0.255661   -0.344875    0.104022     0.34599      0.189674     0.416595    0.545886    -0.0599721  -0.235104    0.0101102  -0.464426    -0.0304667  -0.371681    -0.486114   -0.215605    -0.2222     -0.595724    0.76372     -0.249442    0.120464    -0.58994    -0.417549    -0.230468    0.475469   -0.258965    -0.221671  
  0.544098    0.282648    0.307821     0.082232    -0.106281    -0.153065   -0.0413246    0.107357    0.534929   -0.382757   -0.378482    -0.177078    0.106983    -0.820596    0.283316     0.690591   -0.177813    0.0720979    0.0133547  -0.37433      0.0966312   0.164296     0.331144    0.175422   -0.134265    -0.177782  
  0.0216279  -0.606796    0.761488     0.0610264   -0.101386    -0.340051    0.42789      0.27796     0.203339    0.136775   -0.00324162  -0.035156   -0.411419    -0.243958    0.247443     0.331483    0.0934579   0.409241    -0.100306   -0.334243     1.01653    -0.181314     0.535537    0.484349   -0.0818723    0.607868  
  0.114694   -0.0390438   0.143609     0.131608     0.043956     0.240954    0.298141     0.0347955   0.116578    0.314518   -0.139714    -0.264015   -0.102621    -0.0259366  -0.103914     0.0204139   0.212854   -0.00601302   0.14286    -0.4523       0.142273   -0.00600088  -0.171637    0.196219   -0.124179     0.107531  
  0.2182     -0.153308    0.485255     0.307015    -0.569633     0.0520734   0.0725739    0.32021     0.225909   -0.175818    0.196829     0.66367     0.151068    -0.0700672   0.050842    -0.470068    0.263512   -0.268707     0.176061    0.585807     0.284664    0.184751    -0.444525    0.103566    0.347329     0.288987  
  0.0495388   0.0599069  -0.415804    -0.189488    -0.0331346   -0.352965    0.10551     -0.452652   -0.471074    0.32188     0.540618    -0.157697   -0.0234581   -0.460895    0.328142    -0.225689    0.342308    0.09891      0.240553    0.130339     0.327709   -0.270726     0.637717   -0.561559   -0.184794     0.0895661 
 -0.661552    0.352345   -0.0200172   -0.0874757   -0.00732245  -0.642839   -0.418897     0.072439    0.459125    0.172766    0.779635     0.0136939   0.433732     0.303155    0.0921553    0.142421    0.719325   -0.374387     0.218702   -0.107295     0.493986    0.287196     0.218639   -0.286437    0.041423     0.0562549 
 -0.492841   -0.182033   -0.65457      0.660899    -0.0287814   -0.310516    0.336655     0.0228118  -0.661019    0.243331    0.173131    -0.287015   -0.0925523    0.0849145   0.232847     0.549692    0.203574   -0.286561    -0.680391   -0.00418673   0.16844     0.442338     0.255218   -0.298465    0.351771     0.404992  
 -0.928929   -0.252144   -0.0748991    0.185395     0.260939    -0.235481   -0.492158    -0.427582   -0.328227    0.42655    -0.488768    -0.279746    0.630437    -0.0896334   0.705126     0.390197   -0.372404   -0.193987    -0.422054   -0.431696    -0.0350507  -0.247018     0.372665   -0.171632   -0.34605     -0.213275  
 -0.257712    0.510469   -0.613603    -0.304163     0.192029     0.264713   -0.791775    -0.914622   -0.653508   -0.0692431   0.202873    -0.105428    0.558101    -0.0764979   0.431953     0.0653592   0.0642544  -0.423965     0.224346    0.357405    -0.817193    0.275413    -0.234414   -0.621552   -0.329366    -0.685275  
 -0.138308   -0.0321335  -0.577823     0.0228741    0.0685001   -0.164103   -0.353439     1.06724     0.221348   -1.08026     0.0607068   -0.13666     0.613644    -0.161921    0.796995    -0.68076    -0.577893    0.129369     0.287083   -0.407784    -0.516462    0.51649     -0.231965   -0.136514   -0.491488    -0.294999  
  0.649193    0.42423    -0.107607    -0.99223     -0.0404049   -0.76144    -0.26667      0.473092    0.26896    -0.604841    0.372883     0.13607     0.164481    -0.484989   -0.225217    -0.0940763  -0.343068    0.174946     0.16472     0.412447    -0.083633    0.233568     0.51595    -0.0334696   0.0699563   -0.345582  
 -0.444391   -0.0812864  -0.415102     0.160196     0.151059    -0.274231   -0.155566     0.0718822  -0.295438   -0.270106    0.0406903   -0.21015     0.512903     0.165156   -0.331574    -0.493943   -0.449986    0.106869    -0.70814     0.805078    -0.110362   -0.279207     0.187062    0.127635    0.264717    -0.196314  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.413355
INFO: iteration 2, average log likelihood -1.413339
INFO: iteration 3, average log likelihood -1.413325
INFO: iteration 4, average log likelihood -1.413311
INFO: iteration 5, average log likelihood -1.413297
INFO: iteration 6, average log likelihood -1.413284
INFO: iteration 7, average log likelihood -1.413271
INFO: iteration 8, average log likelihood -1.413258
INFO: iteration 9, average log likelihood -1.413246
INFO: iteration 10, average log likelihood -1.413234
INFO: EM with 100000 data points 10 iterations avll -1.413234
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.588286e+05
      1       7.033376e+05      -2.554909e+05 |       32
      2       6.923787e+05      -1.095889e+04 |       32
      3       6.882201e+05      -4.158623e+03 |       32
      4       6.859640e+05      -2.256089e+03 |       32
      5       6.845018e+05      -1.462235e+03 |       32
      6       6.834132e+05      -1.088586e+03 |       32
      7       6.826256e+05      -7.876287e+02 |       32
      8       6.820025e+05      -6.230118e+02 |       32
      9       6.814922e+05      -5.103862e+02 |       32
     10       6.810531e+05      -4.390487e+02 |       32
     11       6.806418e+05      -4.113597e+02 |       32
     12       6.802511e+05      -3.906431e+02 |       32
     13       6.798743e+05      -3.768403e+02 |       32
     14       6.795620e+05      -3.122238e+02 |       32
     15       6.793123e+05      -2.497477e+02 |       32
     16       6.790688e+05      -2.434737e+02 |       32
     17       6.788432e+05      -2.256652e+02 |       32
     18       6.786412e+05      -2.019582e+02 |       32
     19       6.784419e+05      -1.993007e+02 |       32
     20       6.782532e+05      -1.887012e+02 |       32
     21       6.780987e+05      -1.545166e+02 |       32
     22       6.779766e+05      -1.221134e+02 |       32
     23       6.778604e+05      -1.161717e+02 |       32
     24       6.777617e+05      -9.867775e+01 |       32
     25       6.776711e+05      -9.066341e+01 |       32
     26       6.775870e+05      -8.406580e+01 |       32
     27       6.775153e+05      -7.169800e+01 |       32
     28       6.774450e+05      -7.030850e+01 |       32
     29       6.773795e+05      -6.547120e+01 |       32
     30       6.773251e+05      -5.442664e+01 |       32
     31       6.772747e+05      -5.034012e+01 |       32
     32       6.772274e+05      -4.734826e+01 |       32
     33       6.771823e+05      -4.512590e+01 |       32
     34       6.771407e+05      -4.152975e+01 |       32
     35       6.771031e+05      -3.761472e+01 |       32
     36       6.770706e+05      -3.257455e+01 |       32
     37       6.770418e+05      -2.871720e+01 |       32
     38       6.770141e+05      -2.777024e+01 |       32
     39       6.769892e+05      -2.483935e+01 |       32
     40       6.769672e+05      -2.202937e+01 |       32
     41       6.769426e+05      -2.460363e+01 |       32
     42       6.769154e+05      -2.723320e+01 |       32
     43       6.768904e+05      -2.496233e+01 |       32
     44       6.768681e+05      -2.227246e+01 |       32
     45       6.768450e+05      -2.312756e+01 |       32
     46       6.768230e+05      -2.198075e+01 |       32
     47       6.767988e+05      -2.418409e+01 |       32
     48       6.767732e+05      -2.566652e+01 |       32
     49       6.767474e+05      -2.573545e+01 |       32
     50       6.767176e+05      -2.985317e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 676717.5764427911)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.425156
INFO: iteration 2, average log likelihood -1.420152
INFO: iteration 3, average log likelihood -1.418897
INFO: iteration 4, average log likelihood -1.418052
INFO: iteration 5, average log likelihood -1.417146
INFO: iteration 6, average log likelihood -1.416183
INFO: iteration 7, average log likelihood -1.415381
INFO: iteration 8, average log likelihood -1.414866
INFO: iteration 9, average log likelihood -1.414568
INFO: iteration 10, average log likelihood -1.414386
INFO: iteration 11, average log likelihood -1.414259
INFO: iteration 12, average log likelihood -1.414161
INFO: iteration 13, average log likelihood -1.414082
INFO: iteration 14, average log likelihood -1.414015
INFO: iteration 15, average log likelihood -1.413956
INFO: iteration 16, average log likelihood -1.413905
INFO: iteration 17, average log likelihood -1.413859
INFO: iteration 18, average log likelihood -1.413818
INFO: iteration 19, average log likelihood -1.413781
INFO: iteration 20, average log likelihood -1.413747
INFO: iteration 21, average log likelihood -1.413715
INFO: iteration 22, average log likelihood -1.413685
INFO: iteration 23, average log likelihood -1.413658
INFO: iteration 24, average log likelihood -1.413632
INFO: iteration 25, average log likelihood -1.413607
INFO: iteration 26, average log likelihood -1.413584
INFO: iteration 27, average log likelihood -1.413562
INFO: iteration 28, average log likelihood -1.413541
INFO: iteration 29, average log likelihood -1.413521
INFO: iteration 30, average log likelihood -1.413502
INFO: iteration 31, average log likelihood -1.413484
INFO: iteration 32, average log likelihood -1.413466
INFO: iteration 33, average log likelihood -1.413450
INFO: iteration 34, average log likelihood -1.413433
INFO: iteration 35, average log likelihood -1.413418
INFO: iteration 36, average log likelihood -1.413402
INFO: iteration 37, average log likelihood -1.413388
INFO: iteration 38, average log likelihood -1.413373
INFO: iteration 39, average log likelihood -1.413359
INFO: iteration 40, average log likelihood -1.413345
INFO: iteration 41, average log likelihood -1.413332
INFO: iteration 42, average log likelihood -1.413319
INFO: iteration 43, average log likelihood -1.413306
INFO: iteration 44, average log likelihood -1.413293
INFO: iteration 45, average log likelihood -1.413280
INFO: iteration 46, average log likelihood -1.413268
INFO: iteration 47, average log likelihood -1.413256
INFO: iteration 48, average log likelihood -1.413245
INFO: iteration 49, average log likelihood -1.413233
INFO: iteration 50, average log likelihood -1.413222
INFO: EM with 100000 data points 50 iterations avll -1.413222
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.120495   -0.0537102     0.51415     0.0060913    0.0127823   -0.704373   -0.323274    0.420029      0.175615   -0.416082    0.0159058     0.598665     0.20937      0.054574   -0.0122735  -0.679502     0.0553099   -0.259136    0.408092     0.828811     0.266924   -0.0653867  -0.349273    0.0213839   -0.336589     0.125765  
  0.271666   -0.35333       0.71065    -0.0853291    0.102318    -0.016403    0.561077    0.702751      0.55702     0.0569348  -0.0210403     0.105201    -0.387527     0.0308803  -0.232537   -0.00161752   0.230561     0.367206    0.143874    -0.0792032    0.595077   -0.251888    0.288257    0.579315     0.220917     0.157359  
  0.0465195   0.350355      0.129875   -0.305998     0.473201     0.278592   -0.479265    0.318724      0.205391   -0.23775     0.0898543    -0.0313638   -0.0168424    0.884924   -0.505446   -0.022138     0.26314     -0.495467   -0.0499238    0.783693    -0.517022    0.271734   -0.423378    0.475532    -0.120986    -0.389478  
 -0.204297    0.0800214     0.19741     0.336925     0.153733     0.0755479   0.0890123  -0.129139     -0.0377219   0.0937139   0.0260851    -0.206945    -0.160416    -0.12103    -0.169439    0.0171313    0.203102    -0.226226   -0.0669863    0.00614456   0.0908045   0.0237725  -0.11716     0.279382    -0.0032601    0.450722  
  0.0242308   0.494919     -0.300701    0.272366     0.327454     0.0466534  -0.157066   -0.0250431     0.354519   -0.267383   -0.0309293     0.0671513   -0.224523    -0.305555    0.63916     0.699592    -0.0592876   -0.13786     0.347895    -0.146951    -0.39268    -0.413237    0.885134    0.420889    -0.827372    -0.710959  
  0.147792   -0.424618      0.436547   -0.331485     0.136854    -0.146478    0.264165    0.430141     -0.351637    0.0582757  -0.214236     -0.207327     0.415026    -0.11732    -0.677943   -0.679753    -0.310691     0.0199777  -0.0859915   -0.111032     0.127467    0.304879   -0.635716   -0.137118     0.921271     0.0655794 
 -0.0903767   0.399076     -0.133531    0.626336     0.306244     0.283005   -0.0155014  -0.169174     -0.0173729   0.538624   -0.348138     -0.230296     0.110113     0.696863   -0.336193    0.248561     0.338444    -0.177881   -0.30352     -0.709715     0.192889   -0.145866   -0.29059     0.581516     0.125507    -0.157778  
  0.144571    0.0474324     0.239721   -0.429061     0.16024     -0.452755   -0.155743   -0.287667     -0.194312   -0.0208426   0.211727      0.00178793  -0.237776    -0.119688    0.413024    0.150243     0.318999    -0.0419199   0.32134     -0.431782     0.675925   -0.0772655   0.37227    -0.463809    -0.0537324    0.0617305 
 -0.202728   -0.0411357    -0.441823   -0.103601    -0.349217    -0.159568    0.0990822  -0.258301     -0.410555   -0.0485197   0.324505     -0.0558147   -0.279952     0.412008    0.0685285   0.339207     0.0810185    0.178992   -0.40989      0.0330011   -0.325887    0.189847   -0.0886757  -0.451742     0.376707     0.15074   
  0.266797   -0.567215      0.35257    -0.274703     0.435284     0.257417    0.286068   -0.168052     -0.615405   -0.0652476  -0.443875      0.128115    -0.602904    -0.523718    0.306639   -0.474175    -0.593276     0.906757   -0.0889852    0.373154    -0.407369   -0.204162   -0.30468    -0.0218351   -0.301227    -0.289684  
 -0.108451   -0.0107273    -0.238369   -0.316675    -0.0756782   -0.012362   -0.180719   -0.00925053   -0.260728   -0.248465    0.117217     -0.384303     0.28215     -0.139487    0.0484607  -0.468848    -0.224926    -0.205613    0.0181824    0.33223     -0.105842   -0.0848437  -0.130097   -0.187218    -0.683652     0.00431386
  0.596329    0.318926     -0.0434642  -0.567623     0.227808    -0.72394    -0.491819    0.291549      0.424982   -0.508003    0.0132985     0.274831     0.253434    -0.146787   -0.174215    0.0307406   -0.340579     0.422284    0.108455     0.0204205   -0.226868   -0.112626    0.459325    0.18787      0.309006    -0.660072  
 -0.571444    0.144597      0.213786    0.0355275   -0.0876712   -0.409028   -0.301225    0.23027       0.487094    0.30459     0.624364     -0.155431     0.304468     0.293649    0.0562318   0.027895     0.824934    -0.203931    0.172206    -0.135469     0.4986      0.353076    0.0769475  -0.232321     0.0662812    0.324365  
 -0.732859   -0.0200536    -0.252049    0.489726     0.138679    -0.558379   -0.229422   -0.11209      -0.579299    0.201956   -0.124897     -0.0808103    0.44277     -0.239451    0.832091    0.518546    -0.0489269   -0.367076   -0.743399     0.12017      0.208206    0.176856    0.466691   -0.439687    -0.0172085    0.145844  
  0.329269   -0.0114452    -0.272333    0.672461    -0.118525    -0.635037    0.191699   -0.0630556     0.23202    -0.149176    0.129467      0.158238     0.233074    -0.461402   -0.229079    0.182387    -0.0330845   -0.593923    0.0362065    0.210324     0.525864    0.466832   -0.0698861   0.665505     0.659568     0.215741  
  0.119178    0.141572     -0.247635   -0.46077     -0.241474    -0.500743    0.047208    0.0921336     0.0520274  -0.284894    0.797749      0.0108358   -0.022956    -0.856815    0.119366   -0.28611     -0.0248242    0.128258    0.0718377    0.81347      0.0832783   0.367697    0.440415   -0.585268    -0.101314     0.043896  
 -0.397862   -0.41196      -0.777177    0.118613     0.467973    -0.234559   -0.100713   -0.662298     -0.277663    0.604128   -0.205367     -0.531773     0.0867464   -0.0514191  -0.0706179   0.378318    -0.180624     0.066063   -0.153082    -0.572747    -0.337497   -0.277383    0.229687   -0.0823428   -0.481929    -0.15172   
  0.18697     0.351784      0.0797503  -0.163978     0.112213     0.373957    0.152048    0.232048      0.249774   -0.234148   -0.304422     -0.264211     0.521169    -0.156221   -0.0948973  -0.408877    -0.469319    -0.688408    0.919489    -0.287209     0.286587   -0.0574198  -0.380401    0.402694    -0.494759    -0.217681  
 -0.49406     0.210585     -0.331921    0.00764502   0.210685     0.0113812  -0.334469    0.0712388    -0.506293   -0.594853    0.13697      -0.306631     0.349444     0.211465    0.0327063  -0.529889    -0.285769     0.115108   -0.434306     0.601166    -0.356925   -0.0831153   0.0598864  -0.0994433    0.0487337   -0.0812471 
  0.0989244   0.328084     -0.543724   -0.277359     0.0593336    0.171612   -0.562454   -0.916665     -0.399828    0.25959     0.15148       0.163901     0.399219     0.0995704   0.374739    0.196216     0.184228    -0.435433    0.249814     0.0168764   -0.674132    0.249645   -0.325308   -0.715535    -0.316349    -0.985127  
  0.329184   -0.13279       0.135196    0.2843      -0.191746     0.2195      0.387812    0.117386      0.142137    0.196966   -0.180352     -0.00545643  -0.156612    -0.115362   -0.0557965   0.105237    -0.0363183    0.0796359   0.00741718  -0.238126     0.0718399  -0.121678   -0.10622     0.341649    -0.109036    -0.0230432 
 -0.219251   -0.0314854     0.621526    0.0687653    0.201116     0.435866   -0.120853   -0.219094     -0.226206   -0.0117665  -0.153774      0.0829604   -0.400111     0.296825    0.525373   -0.0642194    0.623639    -0.3188      0.369067    -0.540353    -0.415213    0.454482   -0.826618   -0.0315455   -0.516297     0.167899  
  0.049507   -0.31108       0.0399211  -0.211918    -0.308433    -0.0925212   0.185739   -0.477346     -0.286324    0.0687988   0.170764      0.671193    -0.530546     0.176785   -0.0515043   0.0991701    0.333802     0.739779   -0.277393    -0.0301349   -0.448398    0.24639    -0.083218   -0.289948     0.585893     0.172691  
  0.207955   -0.069715     -0.0888587   0.247152    -0.0892081    0.531581    0.407969   -0.200571      0.0415664   0.11081    -0.338911      0.0799926    0.00586794   0.134076   -0.541585    0.075916    -0.544069     0.1799      0.0029265   -0.0481112   -0.5103     -0.233651   -0.15113     0.473788     0.0404576   -0.408881  
  0.260222   -0.146599      0.689367   -0.0427681    0.00969377  -0.179064   -0.342363   -0.0886432     0.775044    0.332416   -0.351758      0.463699    -0.21764     -0.167075   -0.149098    0.633723    -0.149108    -0.0906705   0.45554     -0.514036    -0.492285    0.676717   -0.503009   -0.401078     0.0099223   -0.361181  
  0.0843048  -0.342804      0.316288   -0.18794     -0.180878    -0.419905    0.191204    0.0611345    -0.0909338  -0.734445   -0.296812     -0.317084     0.01956     -0.716456    0.0373434   0.207236    -0.592887     0.456903   -0.51065     -0.250655     0.145077    0.14673     0.424006    0.797881    -0.366313     0.372234  
  0.39705     0.437595      0.0491398   0.178296    -0.612092     0.303591    0.225956   -0.121809      0.228675   -0.209407   -0.389652     -0.311918     0.23467     -0.593219    0.540276    0.572141    -0.333344    -0.114052   -0.110307    -0.605417     0.150781    0.165654   -0.0882829  -0.176971    -0.118398    -0.0199496 
 -0.0414155  -0.236145      0.230502    0.371377    -0.784103     0.829707   -0.0295691   0.170049     -0.186751    0.163105    0.00074244    0.0726258    0.382245     0.0969719   0.486569   -0.209072     0.190613    -0.295784    0.0023242    0.439246     0.116081    0.297736   -0.202563    0.0622647    0.353825     0.243863  
 -0.239875    0.119951     -0.403895    0.433115    -0.0367107   -0.0433902   0.485104    0.620839     -0.2492      0.209497    0.144615     -0.216077     0.0911219    0.337662    0.171868   -0.541929     0.238642     0.227427   -0.33482      0.240777     0.311497   -0.57482     0.137649    0.114601    -0.00726179   0.198376  
  0.139576    0.0944847     0.387177    0.0472305    0.0710624    0.564323    0.518419   -1.29989      -0.252995    1.0577      0.0582968     0.117769    -0.259971    -0.0960525  -0.68916     0.415856     0.332452    -0.0355782   0.215988     0.504081     0.135096   -0.95021     0.297607    0.0322755   -0.0690958    0.0126126 
  0.0433069   0.0443941     0.0355703  -0.0973885    0.0658364   -0.0700908  -0.09439    -0.000317058   0.0616976   0.0124372  -0.000356879   0.101383     0.0896583    0.0219096   0.0720607   0.0220138    0.00170763   0.131326    0.0781421   -0.0375762   -0.0348651   0.0087645   0.066251   -0.00337297  -0.0115296   -0.205902  
 -0.249881    0.000352512   0.463071   -0.534648    -0.154887     0.671799   -0.188336    0.196347      0.235448    0.097705   -0.0983348    -0.0221191   -0.248638     0.395905    0.160801   -0.0860711    0.676775     0.958861    0.0566433   -0.172584    -0.355099   -0.446317    0.248172   -0.523764    -0.360008    -0.177937  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.413212
INFO: iteration 2, average log likelihood -1.413201
INFO: iteration 3, average log likelihood -1.413191
INFO: iteration 4, average log likelihood -1.413181
INFO: iteration 5, average log likelihood -1.413171
INFO: iteration 6, average log likelihood -1.413162
INFO: iteration 7, average log likelihood -1.413153
INFO: iteration 8, average log likelihood -1.413144
INFO: iteration 9, average log likelihood -1.413135
INFO: iteration 10, average log likelihood -1.413127
INFO: EM with 100000 data points 10 iterations avll -1.413127
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
