>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.7.0
INFO: Installing JLD v0.6.6
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.1
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.3.0
INFO: Installing ScikitLearnBase v0.2.1
INFO: Installing StaticArrays v0.1.0
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
WARNING: takebuf_string(b) is deprecated, use String(take!(b)) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in takebuf_string(::Base.AbstractIOBuffer{Array{UInt8,1}}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/BinDeps/src/dependencies.jl:887 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::SubString{String}) at ./sysimg.jl:14
 in evalfile(::SubString{String}, ::Array{String,1}) at ./loading.jl:572 (repeats 2 times)
 in cd(::##2#4, ::String) at ./file.jl:69
 in (::##1#3)(::IOStream) at ./none:13
 in open(::##1#3, ::String, ::String) at ./iostream.jl:152
 in eval(::Module, ::Any) at ./boot.jl:236
 in process_options(::Base.JLOptions) at ./client.jl:248
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/Rmath/deps/build.jl, in expression starting on line 39
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1307
Commit 03c2464 (2016-11-24 20:28 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-101-generic #148-Ubuntu SMP Thu Oct 20 22:08:32 UTC 2016 x86_64 x86_64
Memory: 2.939281463623047 GB (655.3515625 MB free)
Uptime: 26402.0 sec
Load Avg:  1.10302734375  1.02978515625  1.0078125
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3499 MHz    1610307 s        176 s     133206 s     626226 s         44 s
#2  3499 MHz     771007 s       6418 s      83655 s    1697527 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.4
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.7.0
 - JLD                           0.6.6
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.1
 - Rmath                         0.1.4
 - SHA                           0.3.0
 - ScikitLearnBase               0.2.1
 - StaticArrays                  0.1.0
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: takebuf_string(b) is deprecated, use String(take!(b)) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in takebuf_string(::Base.AbstractIOBuffer{Array{UInt8,1}}) at ./deprecated.jl:50
 in #_write#17(::Array{Any,1}, ::Function, ::JLD.JldFile, ::String, ::Array{Any,1}, ::JLD.JldWriteSession) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:587
 in #write#14(::Array{Any,1}, ::Function, ::JLD.JldFile, ::String, ::Array{Any,1}, ::JLD.JldWriteSession) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:529
 in #jldopen#9(::Bool, ::Bool, ::Bool, ::Function, ::String, ::Bool, ::Bool, ::Bool, ::Bool, ::Bool) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:198
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::String, ::Bool, ::Bool, ::Bool, ::Bool, ::Bool) at ./<missing>:0
 in #jldopen#10(::Bool, ::Bool, ::Bool, ::Function, ::String, ::String) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:253
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::String, ::String) at ./<missing>:0
 in #jldopen#11(::Array{Any,1}, ::Function, ::JLD.##34#35{String,Array{Float64,2},Tuple{}}, ::String, ::Vararg{String,N}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:263
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::Function, ::String, ::String) at ./<missing>:0
 in #save#33(::Bool, ::Bool, ::Function, ::FileIO.File{FileIO.DataFormat{:JLD}}, ::String, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:1217
 in save(::FileIO.File{FileIO.DataFormat{:JLD}}, ::String, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:1214
 in #save#14(::Array{Any,1}, ::Function, ::String, ::String, ::Vararg{Any,N}) at /home/vagrant/.julia/v0.6/FileIO/src/loadsave.jl:54
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:8 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-1.9230436482035853e6,[97852.5,2147.5],
[-2213.1 2820.29 -3374.87; 2368.49 -2798.03 3598.47],

Array{Float64,2}[
[94777.0 2445.79 -3616.16; 2445.79 94208.0 4311.85; -3616.16 4311.85 92184.4],

[4454.76 -2451.12 3422.78; -2451.12 5216.35 -4076.25; 3422.78 -4076.25 7139.09]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.712209e+03
      1       1.387723e+03      -3.244864e+02 |        7
      2       1.292680e+03      -9.504304e+01 |        5
      3       1.209768e+03      -8.291107e+01 |        3
      4       1.159653e+03      -5.011583e+01 |        3
      5       1.113240e+03      -4.641245e+01 |        3
      6       1.066886e+03      -4.635430e+01 |        2
      7       1.011643e+03      -5.524254e+01 |        0
      8       1.011643e+03       0.000000e+00 |        0
K-means converged with 8 iterations (objv = 1011.643375377158)
INFO: K-means with 272 data points using 8 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.071032
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.862560
INFO: iteration 2, lowerbound -3.761696
INFO: iteration 3, lowerbound -3.638012
INFO: iteration 4, lowerbound -3.460443
INFO: iteration 5, lowerbound -3.235199
INFO: iteration 6, lowerbound -3.001199
INFO: dropping number of Gaussions to 7
INFO: iteration 7, lowerbound -2.796316
INFO: iteration 8, lowerbound -2.640895
INFO: dropping number of Gaussions to 6
INFO: iteration 9, lowerbound -2.528782
INFO: dropping number of Gaussions to 5
INFO: iteration 10, lowerbound -2.446425
INFO: dropping number of Gaussions to 4
INFO: iteration 11, lowerbound -2.383367
INFO: iteration 12, lowerbound -2.346006
INFO: dropping number of Gaussions to 3
INFO: iteration 13, lowerbound -2.318834
INFO: iteration 14, lowerbound -2.307497
INFO: dropping number of Gaussions to 2
INFO: iteration 15, lowerbound -2.302945
INFO: iteration 16, lowerbound -2.299263
INFO: iteration 17, lowerbound -2.299258
INFO: iteration 18, lowerbound -2.299255
INFO: iteration 19, lowerbound -2.299254
INFO: iteration 20, lowerbound -2.299253
INFO: iteration 21, lowerbound -2.299253
INFO: iteration 22, lowerbound -2.299253
INFO: iteration 23, lowerbound -2.299253
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Sat 26 Nov 2016 12:50:15 PM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Sat 26 Nov 2016 12:50:16 PM UTC: K-means with 272 data points using 8 iterations
11.3 data points per parameter
,Sat 26 Nov 2016 12:50:18 PM UTC: EM with 272 data points 0 iterations avll -2.071032
5.8 data points per parameter
,Sat 26 Nov 2016 12:50:18 PM UTC: GMM converted to Variational GMM
,Sat 26 Nov 2016 12:50:21 PM UTC: iteration 1, lowerbound -3.862560
,Sat 26 Nov 2016 12:50:21 PM UTC: iteration 2, lowerbound -3.761696
,Sat 26 Nov 2016 12:50:21 PM UTC: iteration 3, lowerbound -3.638012
,Sat 26 Nov 2016 12:50:21 PM UTC: iteration 4, lowerbound -3.460443
,Sat 26 Nov 2016 12:50:21 PM UTC: iteration 5, lowerbound -3.235199
,Sat 26 Nov 2016 12:50:21 PM UTC: iteration 6, lowerbound -3.001199
,Sat 26 Nov 2016 12:50:21 PM UTC: dropping number of Gaussions to 7
,Sat 26 Nov 2016 12:50:21 PM UTC: iteration 7, lowerbound -2.796316
,Sat 26 Nov 2016 12:50:21 PM UTC: iteration 8, lowerbound -2.640895
,Sat 26 Nov 2016 12:50:21 PM UTC: dropping number of Gaussions to 6
,Sat 26 Nov 2016 12:50:21 PM UTC: iteration 9, lowerbound -2.528782
,Sat 26 Nov 2016 12:50:22 PM UTC: dropping number of Gaussions to 5
,Sat 26 Nov 2016 12:50:22 PM UTC: iteration 10, lowerbound -2.446425
,Sat 26 Nov 2016 12:50:22 PM UTC: dropping number of Gaussions to 4
,Sat 26 Nov 2016 12:50:22 PM UTC: iteration 11, lowerbound -2.383367
,Sat 26 Nov 2016 12:50:22 PM UTC: iteration 12, lowerbound -2.346006
,Sat 26 Nov 2016 12:50:22 PM UTC: dropping number of Gaussions to 3
,Sat 26 Nov 2016 12:50:22 PM UTC: iteration 13, lowerbound -2.318834
,Sat 26 Nov 2016 12:50:22 PM UTC: iteration 14, lowerbound -2.307497
,Sat 26 Nov 2016 12:50:22 PM UTC: dropping number of Gaussions to 2
,Sat 26 Nov 2016 12:50:22 PM UTC: iteration 15, lowerbound -2.302945
,Sat 26 Nov 2016 12:50:22 PM UTC: iteration 16, lowerbound -2.299263
,Sat 26 Nov 2016 12:50:22 PM UTC: iteration 17, lowerbound -2.299258
,Sat 26 Nov 2016 12:50:22 PM UTC: iteration 18, lowerbound -2.299255
,Sat 26 Nov 2016 12:50:22 PM UTC: iteration 19, lowerbound -2.299254
,Sat 26 Nov 2016 12:50:22 PM UTC: iteration 20, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:22 PM UTC: iteration 21, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:22 PM UTC: iteration 22, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:23 PM UTC: iteration 23, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:23 PM UTC: iteration 24, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:23 PM UTC: iteration 25, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:23 PM UTC: iteration 26, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:23 PM UTC: iteration 27, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:23 PM UTC: iteration 28, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:23 PM UTC: iteration 29, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:23 PM UTC: iteration 30, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:23 PM UTC: iteration 31, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:23 PM UTC: iteration 32, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:23 PM UTC: iteration 33, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:23 PM UTC: iteration 34, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:23 PM UTC: iteration 35, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:23 PM UTC: iteration 36, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:23 PM UTC: iteration 37, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:24 PM UTC: iteration 38, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:24 PM UTC: iteration 39, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:24 PM UTC: iteration 40, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:24 PM UTC: iteration 41, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:24 PM UTC: iteration 42, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:24 PM UTC: iteration 43, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:24 PM UTC: iteration 44, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:24 PM UTC: iteration 45, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:24 PM UTC: iteration 46, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:24 PM UTC: iteration 47, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:24 PM UTC: iteration 48, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:24 PM UTC: iteration 49, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:24 PM UTC: iteration 50, lowerbound -2.299253
,Sat 26 Nov 2016 12:50:24 PM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.9549,178.045]
β = [95.9549,178.045]
m = [2.00023 53.852; 4.2503 79.2869]
ν = [97.9549,180.045]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.375876 -0.00895312; 0.0 0.0127487],

[0.184042 -0.00764405; 0.0 0.00858171]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 99999.99999999999
avll from stats: -0.9830668437739168
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9830668437739225
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9830668437739226
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -0.9767075046117353
avll from llpg:  -0.9767075046117352
avll direct:     -0.9767075046117351
sum posterior: 100000.0
32×26 Array{Float64,2}:
 -0.130748    0.0550627    0.0131005   -0.0275924     0.0234723  -0.182531    -0.190734    -0.175257     0.0211042   -0.0430696     0.103932     0.0540997    0.0229425    -0.134961    -0.00589049  -0.182007    -0.0286893    0.031495     0.0823001    0.141574    -0.0391099   -0.217965    -0.0778734    -0.0244895   -0.03079    -0.00341514
  0.167305    0.00524606   0.102203     0.0247495     0.0530396  -0.0617003   -0.0309563    0.0703077    0.00440349  -0.0811052     0.212523    -0.136281     0.0325244     0.0801752   -0.201763     0.157954     0.0345388    0.155965     0.00953345  -0.0338697   -0.0690096   -0.0127816    0.0432708     0.0215871   -0.0306726  -0.0196813 
 -0.0545239  -0.037838     0.00108249  -0.124674     -0.0215171  -0.0512355   -0.136617    -0.0791626   -0.0068914    0.000251381  -0.13363     -0.311733     0.165821      0.146926    -0.076578     0.147196     0.0322916    0.161662    -0.0558906    0.0144192   -0.0493642    0.00206359  -0.0126755     0.155753    -0.191407    0.0349806 
  0.138166   -0.0858464    0.0681594    0.170057     -0.0558962  -0.0298652   -0.177727     0.0366179    0.0200061   -0.11809      -0.00495495  -0.182495     0.182222     -0.115395     0.0470698    0.0584386   -0.08449     -0.0390322   -0.0931786    0.0511656    0.0183033    0.0136631    0.0680812     0.0528274    0.0321384   0.0683521 
 -0.0149409  -0.0992926   -0.112711    -0.0501966    -0.0340572   0.142528    -0.218929     0.0703751    0.0547444    0.0316123     0.097703     0.0138217   -0.0745886    -0.119642    -0.151051     0.0402499   -0.150636    -0.0800749   -0.0277793   -0.0697032    0.120756     0.105481    -0.0590967     0.00394507  -0.139928    0.0797978 
  0.0425659   0.030358    -0.017206     0.020696      0.175333   -0.134417    -0.121879     0.0188333   -0.274899     0.138005     -0.0728856    0.02813      0.0227695     0.0357412   -0.112071     0.00118153  -0.0446994    0.0728373   -0.103805     0.0852816    0.100786     0.0199282   -0.00318939   -0.00851358   0.108376    0.0556807 
 -0.0593942   0.128734    -0.0928766    0.188996      0.0924807  -0.0450899   -0.0289248   -0.0893225    0.0334987   -0.11757       0.00510586  -0.00522402   0.032031      0.0735057   -0.0592046    0.0834239    0.12922      0.0239822   -0.0320973   -0.139707    -0.0240047   -0.0883077   -0.0709251    -0.0958648    0.0108357  -0.0708177 
 -0.0153182   0.00269341   0.0159484    0.0435977     0.0924902  -0.0623681    0.0449018   -0.0539623   -0.134445    -0.0841413     0.157546    -0.00886375  -0.0474331    -0.118737    -0.206982     0.0318293   -0.0845566   -0.0635027   -0.0943307   -0.155541    -0.180155     0.0261076    0.000790296   0.129081     0.018354   -0.11893   
  0.0689073  -0.0940602   -0.0649757    0.0961355    -0.025737    0.0238994    0.124328     0.011224    -0.214884    -0.132303      0.0484912   -0.0966391    0.0399938    -0.12411     -0.130539     0.237216    -0.108341     0.248998     0.0962406   -0.157934     0.194891    -0.0648171   -0.0118402    -0.0394444   -0.199975    0.110067  
 -0.103204    0.106385     0.0974444   -0.0317284    -0.127399    0.144196    -0.0800788   -0.00468246  -0.10905     -0.0303671    -0.0633039    0.0559329   -0.0588436    -0.117466    -0.0292603    0.0190506    0.0989432   -0.00656847   0.155838     0.1034       0.0226764   -0.0123821   -0.0871796    -0.0479137   -0.0312824  -0.0191007 
  0.144795   -0.0735516   -0.0245209    0.115233     -0.143267    0.0277384   -0.0588667   -0.00245481  -0.0301139    0.0282062     0.233616    -0.0870928    0.0181574     0.0475768   -0.212061    -0.0796726    0.124184    -0.00659031   0.0395339   -0.129595     0.106595    -0.0700014    0.0509047    -0.0935859    0.0709044  -0.0451116 
 -0.0676696   0.10999      0.0107519    0.000643601  -0.0786267   0.113156     0.0464519    0.12854      0.0754102   -0.210602      0.0291107   -0.151247     0.0169833     0.108361     0.0715668   -0.0279324    0.0485625    0.0826105    0.0575214   -0.0259392    0.156012     0.0280603    0.0938277    -0.222813     0.0303688   0.0339406 
  0.128581   -0.0608643   -0.124977     0.127797     -0.0215008  -0.0409633    0.034505    -0.027307     0.0484766   -0.0912739    -0.0503995   -0.0187811    0.00592635   -0.0193463    0.147826     0.0389864    0.0206248   -0.0702357    0.0725289    0.201604     0.0150144   -0.0474693   -0.00375745    0.0882314    0.0449972  -0.166033  
  0.157961   -0.0172854    0.0225168    0.0327901     0.0442925   0.0821516    0.0605575    0.0143862    0.00974344  -0.0113653     0.191516    -0.147543    -0.126534     -0.108632    -0.100296     0.0303294   -0.00309127   0.0109976   -0.0756451   -0.122552     0.0944371   -0.081402     0.0622822     0.169085     0.0502669   0.0416046 
  0.0728394  -0.167334     0.091025     0.130056      0.0692049  -0.0697112   -0.031951     0.0432788   -0.112471    -0.0761819     0.0959721    0.0734665    0.070226     -0.162779     0.177453    -0.0454636   -0.178748     0.0866266   -0.0658659    0.0657518    0.118962     0.0445487   -0.0659542    -0.00835965   0.0769814  -0.12133   
 -0.0226071   0.0811045    0.0554498   -0.103349     -0.133978   -0.160248     0.0350834   -0.0322096    0.0446688    0.104163      0.00300645  -0.0704576    0.0815324    -0.0245434    0.131314    -0.106647     0.00626197   0.147412     0.090777     0.00144442  -0.0142155   -0.201784     0.00562636    0.0487394    0.0937141   0.142269  
  0.0479387  -0.119902     0.0449922   -0.0430627    -0.0184571  -0.0706859   -0.053721     0.00664774   0.147067    -0.0600678    -0.060813    -0.0377636    0.126136     -0.156337    -0.108262    -0.0380072    0.0912713    0.0788447    0.0407023   -0.0234566   -0.188455     0.212573     0.169666      0.0227344   -0.118482    0.0959161 
  0.247974   -0.00903916  -0.0997234    0.100223     -0.126412    0.0700282    0.0738115    0.154287     0.139712    -0.0972523    -0.0840763    0.19623      0.107761      0.0197186    0.0128906   -0.0384953    0.0683109    0.0173603    0.00212541   0.126063     0.00256871   0.139178    -0.0611098    -0.0154322    0.103397    0.0312353 
 -0.034858   -0.0248214   -0.0303699   -0.0722486     0.0105139  -0.0114414    0.0285291    0.0458153   -0.0358996   -0.201569     -0.0343795    0.0603547    0.0651459    -0.0265896    0.100987     0.00365604  -0.107232     0.147725    -0.181411     0.070999     0.0136766   -0.0196206    0.0775343    -0.0913609   -0.109194   -0.0338289 
 -0.0646717   0.04804     -0.0879469   -0.050869      0.0553253  -0.0190553    0.320677     0.0419618   -0.00515997  -0.029632      0.0764304   -0.124015     0.0194219     0.0256327   -0.0511369   -0.0663418   -0.0527259    0.0582436    0.00913186   0.0584806   -0.0570187    0.0680603    0.0168094     0.166382     0.0645592   0.120695  
  0.0815265  -0.0763203   -0.0548473    0.0283671     0.0765616  -0.0324836   -0.0757019    0.0522702   -0.0604052   -0.0162772     0.0187632    0.125745    -0.000863589   0.0796375   -0.109341    -0.0232855    0.0968513   -0.152228    -0.0817254   -0.014671     0.110227    -0.0653277   -0.026608      0.0937354   -0.132958    0.0671509 
  0.0263604  -0.0965527    0.127494    -0.200703      0.050809    0.0887857    0.10186      0.00297928  -0.185464     0.115434     -0.136233    -0.112186     0.0985904    -0.125218    -0.135525     0.148173     0.0193711    0.0371384   -0.105295     0.0660777   -0.0720591    0.0226982   -0.0939758     0.0593764   -0.0121808   0.0238829 
  0.185915    0.0778805   -0.0314155   -0.112274     -0.224838    0.00866695   0.0591889    0.0377138   -0.17634      0.0432852    -0.168547     0.172452     0.088163     -0.141825    -0.0541185    0.0353482   -0.0112672   -0.123673    -0.042896    -0.10866     -0.0796293    0.0293608   -0.00461952   -0.153382     0.115377    0.0727933 
  0.0841821   0.0171209    0.0106865    0.00926594    0.0306808   0.0305369   -0.012588    -0.0107597   -0.0495842   -0.0386188    -0.0476553    0.034226    -0.0893703     0.0595836    0.0403758    0.0578986    0.0659946   -0.207698     0.0696675   -0.0804424    0.0106277   -0.0192463    0.0158527    -0.100259     0.0893939   0.0689707 
  0.0182128   0.0376053    0.0217551   -0.092628     -0.0954682  -0.124518    -0.00613163   0.0951508    0.077873    -0.0779665    -0.0352645   -0.184123    -0.0872064    -0.00281686   0.0423341    0.0788078   -0.0362624   -0.0105075    0.23416     -0.0670807   -0.0564786   -0.142228    -0.151899     -0.0826167   -0.0909267   0.0366812 
 -0.0268113   0.132598     0.132473     0.138427      0.0842997  -0.128602     0.0497697    0.012445     0.131456     0.0301026    -0.00487874  -0.0257138   -0.130812     -0.154588    -0.207417    -0.0754588    0.0568581    0.0529872   -0.0219571   -0.0855924   -0.155206     0.088362    -0.133003      0.203416     0.0127611   0.187076  
 -0.0215439   0.0056934    0.119904    -0.0721753     0.0457726  -0.0190074   -0.16877     -0.0548217    0.103056    -0.0115678     0.112694     0.0860174    0.0885826    -0.0403537    0.0243536    0.196122     0.0509244    0.154476     0.123937    -0.00368935  -0.0290713    0.202078    -0.0194315    -0.00207475  -0.0728046   0.0705243 
 -0.103661   -0.0584902   -0.124221     0.147825     -0.0655686   0.13495      0.104182     0.0565571    0.00500602  -0.0458756    -0.0559502    0.127042    -0.0923512     0.149994    -0.0276816   -0.0617381   -0.0247253   -0.125937    -0.0683078   -0.189273    -0.0984022    0.0351533    0.0652731     0.0231388   -0.0623082   0.151807  
  0.0214656   0.0823252   -0.0329429   -0.0907307    -0.152754    0.0748964   -0.144688    -0.30711      0.0525565    0.0285378     0.0170676   -0.0946287   -0.0818781     0.115419     0.0145307   -0.0602983   -0.0310244    0.130858     0.067015     0.0276545    0.0498521   -0.118224     0.0456622     0.0116461   -0.0426221   0.0869429 
  0.203617    0.00515081  -0.138554    -0.185637      0.107567   -0.0749492    0.198229     0.0229751   -0.112831    -0.0354591     0.045783     0.0805369   -0.1006        0.0833518   -0.0792487    0.13391     -0.037447    -0.0580297    0.0417737    0.0598616   -0.0488659   -0.141422     0.135234      0.0167949   -0.0127909  -0.212043  
 -0.0794208  -0.00159418   0.152607    -0.0566423    -0.0316199   0.0475771   -0.0707884    0.180576     0.118825    -0.0734113     0.112098     0.207165    -0.0603489    -0.0942252   -0.149999     0.00109772  -0.0383982   -0.0854981    0.016521     0.0601526   -0.0783254    0.202446     0.0772871    -0.0308646   -0.116975   -0.0932086 
  0.0216661  -0.100317    -0.00440458  -0.078313     -0.0973701  -0.0643962   -0.0390572   -0.0221812    0.106727     0.0248479    -0.0449318    0.00275609   0.0896347     0.0676103   -0.0818658   -0.136581     0.00983434   0.138164     0.139849     0.0767078   -0.0287674    0.051318     0.119317      0.0398246    0.0321041   0.168295  kind diag, method split
0: avll = -1.3941673360266795
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.394232
INFO: iteration 2, average log likelihood -1.394164
INFO: iteration 3, average log likelihood -1.393615
INFO: iteration 4, average log likelihood -1.386962
INFO: iteration 5, average log likelihood -1.368584
INFO: iteration 6, average log likelihood -1.361291
INFO: iteration 7, average log likelihood -1.360208
INFO: iteration 8, average log likelihood -1.359804
INFO: iteration 9, average log likelihood -1.359580
INFO: iteration 10, average log likelihood -1.359443
INFO: iteration 11, average log likelihood -1.359357
INFO: iteration 12, average log likelihood -1.359301
INFO: iteration 13, average log likelihood -1.359261
INFO: iteration 14, average log likelihood -1.359231
INFO: iteration 15, average log likelihood -1.359208
INFO: iteration 16, average log likelihood -1.359188
INFO: iteration 17, average log likelihood -1.359169
INFO: iteration 18, average log likelihood -1.359151
INFO: iteration 19, average log likelihood -1.359135
INFO: iteration 20, average log likelihood -1.359120
INFO: iteration 21, average log likelihood -1.359105
INFO: iteration 22, average log likelihood -1.359092
INFO: iteration 23, average log likelihood -1.359079
INFO: iteration 24, average log likelihood -1.359068
INFO: iteration 25, average log likelihood -1.359057
INFO: iteration 26, average log likelihood -1.359046
INFO: iteration 27, average log likelihood -1.359036
INFO: iteration 28, average log likelihood -1.359026
INFO: iteration 29, average log likelihood -1.359017
INFO: iteration 30, average log likelihood -1.359009
INFO: iteration 31, average log likelihood -1.359002
INFO: iteration 32, average log likelihood -1.358995
INFO: iteration 33, average log likelihood -1.358989
INFO: iteration 34, average log likelihood -1.358983
INFO: iteration 35, average log likelihood -1.358978
INFO: iteration 36, average log likelihood -1.358973
INFO: iteration 37, average log likelihood -1.358969
INFO: iteration 38, average log likelihood -1.358966
INFO: iteration 39, average log likelihood -1.358962
INFO: iteration 40, average log likelihood -1.358960
INFO: iteration 41, average log likelihood -1.358957
INFO: iteration 42, average log likelihood -1.358955
INFO: iteration 43, average log likelihood -1.358953
INFO: iteration 44, average log likelihood -1.358951
INFO: iteration 45, average log likelihood -1.358950
INFO: iteration 46, average log likelihood -1.358948
INFO: iteration 47, average log likelihood -1.358947
INFO: iteration 48, average log likelihood -1.358946
INFO: iteration 49, average log likelihood -1.358945
INFO: iteration 50, average log likelihood -1.358945
INFO: EM with 100000 data points 50 iterations avll -1.358945
952.4 data points per parameter
1: avll = [-1.39423,-1.39416,-1.39361,-1.38696,-1.36858,-1.36129,-1.36021,-1.3598,-1.35958,-1.35944,-1.35936,-1.3593,-1.35926,-1.35923,-1.35921,-1.35919,-1.35917,-1.35915,-1.35913,-1.35912,-1.35911,-1.35909,-1.35908,-1.35907,-1.35906,-1.35905,-1.35904,-1.35903,-1.35902,-1.35901,-1.359,-1.35899,-1.35899,-1.35898,-1.35898,-1.35897,-1.35897,-1.35897,-1.35896,-1.35896,-1.35896,-1.35895,-1.35895,-1.35895,-1.35895,-1.35895,-1.35895,-1.35895,-1.35895,-1.35894]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.359062
INFO: iteration 2, average log likelihood -1.358963
INFO: iteration 3, average log likelihood -1.358722
INFO: iteration 4, average log likelihood -1.356410
INFO: iteration 5, average log likelihood -1.346581
INFO: iteration 6, average log likelihood -1.334681
INFO: iteration 7, average log likelihood -1.328366
INFO: iteration 8, average log likelihood -1.324844
INFO: iteration 9, average log likelihood -1.322529
INFO: iteration 10, average log likelihood -1.320839
INFO: iteration 11, average log likelihood -1.319531
INFO: iteration 12, average log likelihood -1.318616
INFO: iteration 13, average log likelihood -1.317939
INFO: iteration 14, average log likelihood -1.317370
INFO: iteration 15, average log likelihood -1.316852
INFO: iteration 16, average log likelihood -1.316352
INFO: iteration 17, average log likelihood -1.315855
INFO: iteration 18, average log likelihood -1.315370
INFO: iteration 19, average log likelihood -1.314910
INFO: iteration 20, average log likelihood -1.314460
INFO: iteration 21, average log likelihood -1.313993
INFO: iteration 22, average log likelihood -1.313492
INFO: iteration 23, average log likelihood -1.312958
INFO: iteration 24, average log likelihood -1.312374
INFO: iteration 25, average log likelihood -1.311814
INFO: iteration 26, average log likelihood -1.311358
INFO: iteration 27, average log likelihood -1.310986
INFO: iteration 28, average log likelihood -1.310648
INFO: iteration 29, average log likelihood -1.310292
INFO: iteration 30, average log likelihood -1.309856
INFO: iteration 31, average log likelihood -1.309337
INFO: iteration 32, average log likelihood -1.308904
INFO: iteration 33, average log likelihood -1.308642
INFO: iteration 34, average log likelihood -1.308493
INFO: iteration 35, average log likelihood -1.308402
INFO: iteration 36, average log likelihood -1.308338
INFO: iteration 37, average log likelihood -1.308286
INFO: iteration 38, average log likelihood -1.308235
INFO: iteration 39, average log likelihood -1.308182
INFO: iteration 40, average log likelihood -1.308116
INFO: iteration 41, average log likelihood -1.308027
INFO: iteration 42, average log likelihood -1.307903
INFO: iteration 43, average log likelihood -1.307725
INFO: iteration 44, average log likelihood -1.307470
INFO: iteration 45, average log likelihood -1.307159
INFO: iteration 46, average log likelihood -1.306946
INFO: iteration 47, average log likelihood -1.306880
INFO: iteration 48, average log likelihood -1.306847
INFO: iteration 49, average log likelihood -1.306824
INFO: iteration 50, average log likelihood -1.306809
INFO: EM with 100000 data points 50 iterations avll -1.306809
473.9 data points per parameter
2: avll = [-1.35906,-1.35896,-1.35872,-1.35641,-1.34658,-1.33468,-1.32837,-1.32484,-1.32253,-1.32084,-1.31953,-1.31862,-1.31794,-1.31737,-1.31685,-1.31635,-1.31585,-1.31537,-1.31491,-1.31446,-1.31399,-1.31349,-1.31296,-1.31237,-1.31181,-1.31136,-1.31099,-1.31065,-1.31029,-1.30986,-1.30934,-1.3089,-1.30864,-1.30849,-1.3084,-1.30834,-1.30829,-1.30824,-1.30818,-1.30812,-1.30803,-1.3079,-1.30772,-1.30747,-1.30716,-1.30695,-1.30688,-1.30685,-1.30682,-1.30681]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.306939
INFO: iteration 2, average log likelihood -1.306805
INFO: iteration 3, average log likelihood -1.306534
INFO: iteration 4, average log likelihood -1.303813
INFO: iteration 5, average log likelihood -1.289755
INFO: iteration 6, average log likelihood -1.272823
INFO: iteration 7, average log likelihood -1.265286
INFO: iteration 8, average log likelihood -1.261299
INFO: iteration 9, average log likelihood -1.258754
INFO: iteration 10, average log likelihood -1.256870
INFO: iteration 11, average log likelihood -1.255213
INFO: iteration 12, average log likelihood -1.253642
INFO: iteration 13, average log likelihood -1.252405
INFO: iteration 14, average log likelihood -1.251588
INFO: iteration 15, average log likelihood -1.251022
INFO: iteration 16, average log likelihood -1.250540
INFO: iteration 17, average log likelihood -1.249962
INFO: iteration 18, average log likelihood -1.249165
INFO: iteration 19, average log likelihood -1.248252
INFO: iteration 20, average log likelihood -1.247496
INFO: iteration 21, average log likelihood -1.246815
INFO: iteration 22, average log likelihood -1.245920
INFO: iteration 23, average log likelihood -1.244686
INFO: iteration 24, average log likelihood -1.243683
INFO: iteration 25, average log likelihood -1.243287
INFO: iteration 26, average log likelihood -1.243075
INFO: iteration 27, average log likelihood -1.242843
INFO: iteration 28, average log likelihood -1.242546
INFO: iteration 29, average log likelihood -1.242162
INFO: iteration 30, average log likelihood -1.241681
INFO: iteration 31, average log likelihood -1.241167
INFO: iteration 32, average log likelihood -1.240774
INFO: iteration 33, average log likelihood -1.240522
INFO: iteration 34, average log likelihood -1.240360
INFO: iteration 35, average log likelihood -1.240258
INFO: iteration 36, average log likelihood -1.240193
INFO: iteration 37, average log likelihood -1.240152
INFO: iteration 38, average log likelihood -1.240125
INFO: iteration 39, average log likelihood -1.240108
INFO: iteration 40, average log likelihood -1.240096
INFO: iteration 41, average log likelihood -1.240088
INFO: iteration 42, average log likelihood -1.240083
INFO: iteration 43, average log likelihood -1.240080
INFO: iteration 44, average log likelihood -1.240077
INFO: iteration 45, average log likelihood -1.240075
INFO: iteration 46, average log likelihood -1.240073
INFO: iteration 47, average log likelihood -1.240071
INFO: iteration 48, average log likelihood -1.240070
INFO: iteration 49, average log likelihood -1.240068
INFO: iteration 50, average log likelihood -1.240066
INFO: EM with 100000 data points 50 iterations avll -1.240066
236.4 data points per parameter
3: avll = [-1.30694,-1.30681,-1.30653,-1.30381,-1.28975,-1.27282,-1.26529,-1.2613,-1.25875,-1.25687,-1.25521,-1.25364,-1.2524,-1.25159,-1.25102,-1.25054,-1.24996,-1.24916,-1.24825,-1.2475,-1.24681,-1.24592,-1.24469,-1.24368,-1.24329,-1.24308,-1.24284,-1.24255,-1.24216,-1.24168,-1.24117,-1.24077,-1.24052,-1.24036,-1.24026,-1.24019,-1.24015,-1.24013,-1.24011,-1.2401,-1.24009,-1.24008,-1.24008,-1.24008,-1.24007,-1.24007,-1.24007,-1.24007,-1.24007,-1.24007]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.240254
INFO: iteration 2, average log likelihood -1.240060
INFO: iteration 3, average log likelihood -1.239391
INFO: iteration 4, average log likelihood -1.228349
WARNING: Variances had to be floored 1
INFO: iteration 5, average log likelihood -1.184082
WARNING: Variances had to be floored 2 4
INFO: iteration 6, average log likelihood -1.165161
WARNING: Variances had to be floored 1 11
INFO: iteration 7, average log likelihood -1.164338
WARNING: Variances had to be floored 2
INFO: iteration 8, average log likelihood -1.164499
WARNING: Variances had to be floored 1
INFO: iteration 9, average log likelihood -1.152819
WARNING: Variances had to be floored 4 8 11
INFO: iteration 10, average log likelihood -1.149179
WARNING: Variances had to be floored 1 2
INFO: iteration 11, average log likelihood -1.158906
WARNING: Variances had to be floored 11
INFO: iteration 12, average log likelihood -1.163609
WARNING: Variances had to be floored 1
INFO: iteration 13, average log likelihood -1.157077
WARNING: Variances had to be floored 2 4
INFO: iteration 14, average log likelihood -1.145726
WARNING: Variances had to be floored 1 8 11
INFO: iteration 15, average log likelihood -1.149012
WARNING: Variances had to be floored 2
INFO: iteration 16, average log likelihood -1.163682
WARNING: Variances had to be floored 1 11
INFO: iteration 17, average log likelihood -1.149666
WARNING: Variances had to be floored 4
INFO: iteration 18, average log likelihood -1.154000
WARNING: Variances had to be floored 1 2
INFO: iteration 19, average log likelihood -1.139927
WARNING: Variances had to be floored 8 11
INFO: iteration 20, average log likelihood -1.153238
WARNING: Variances had to be floored 1
INFO: iteration 21, average log likelihood -1.162367
WARNING: Variances had to be floored 2 4
INFO: iteration 22, average log likelihood -1.148457
WARNING: Variances had to be floored 1 11
INFO: iteration 23, average log likelihood -1.151292
WARNING: Variances had to be floored 2
INFO: iteration 24, average log likelihood -1.154187
WARNING: Variances had to be floored 1 8 11
INFO: iteration 25, average log likelihood -1.144001
WARNING: Variances had to be floored 4
INFO: iteration 26, average log likelihood -1.160951
WARNING: Variances had to be floored 1 2
INFO: iteration 27, average log likelihood -1.143380
WARNING: Variances had to be floored 11
INFO: iteration 28, average log likelihood -1.155863
WARNING: Variances had to be floored 1
INFO: iteration 29, average log likelihood -1.153195
WARNING: Variances had to be floored 2 4 8
INFO: iteration 30, average log likelihood -1.143068
WARNING: Variances had to be floored 1 11
INFO: iteration 31, average log likelihood -1.158411
WARNING: Variances had to be floored 2
INFO: iteration 32, average log likelihood -1.157943
WARNING: Variances had to be floored 1
INFO: iteration 33, average log likelihood -1.146875
WARNING: Variances had to be floored 4 11
INFO: iteration 34, average log likelihood -1.144322
WARNING: Variances had to be floored 1 2 8
INFO: iteration 35, average log likelihood -1.143876
WARNING: Variances had to be floored 11
INFO: iteration 36, average log likelihood -1.165075
WARNING: Variances had to be floored 1
INFO: iteration 37, average log likelihood -1.157120
WARNING: Variances had to be floored 2 4
INFO: iteration 38, average log likelihood -1.145991
WARNING: Variances had to be floored 1 11
INFO: iteration 39, average log likelihood -1.149459
WARNING: Variances had to be floored 2 8
INFO: iteration 40, average log likelihood -1.152781
WARNING: Variances had to be floored 1
INFO: iteration 41, average log likelihood -1.154211
WARNING: Variances had to be floored 4 11
INFO: iteration 42, average log likelihood -1.148224
WARNING: Variances had to be floored 1 2
INFO: iteration 43, average log likelihood -1.146792
WARNING: Variances had to be floored 11
INFO: iteration 44, average log likelihood -1.156106
WARNING: Variances had to be floored 1 8
INFO: iteration 45, average log likelihood -1.151961
WARNING: Variances had to be floored 2 4
INFO: iteration 46, average log likelihood -1.153364
WARNING: Variances had to be floored 1 11
INFO: iteration 47, average log likelihood -1.153349
WARNING: Variances had to be floored 2
INFO: iteration 48, average log likelihood -1.155645
WARNING: Variances had to be floored 1
INFO: iteration 49, average log likelihood -1.145244
WARNING: Variances had to be floored 4 8 11
INFO: iteration 50, average log likelihood -1.143071
INFO: EM with 100000 data points 50 iterations avll -1.143071
118.1 data points per parameter
4: avll = [-1.24025,-1.24006,-1.23939,-1.22835,-1.18408,-1.16516,-1.16434,-1.1645,-1.15282,-1.14918,-1.15891,-1.16361,-1.15708,-1.14573,-1.14901,-1.16368,-1.14967,-1.154,-1.13993,-1.15324,-1.16237,-1.14846,-1.15129,-1.15419,-1.144,-1.16095,-1.14338,-1.15586,-1.15319,-1.14307,-1.15841,-1.15794,-1.14687,-1.14432,-1.14388,-1.16507,-1.15712,-1.14599,-1.14946,-1.15278,-1.15421,-1.14822,-1.14679,-1.15611,-1.15196,-1.15336,-1.15335,-1.15565,-1.14524,-1.14307]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 2 3 4 10
INFO: iteration 1, average log likelihood -1.154250
WARNING: Variances had to be floored 1 2 3 4 10 22
INFO: iteration 2, average log likelihood -1.138682
WARNING: Variances had to be floored 1 2 3 4 8 10 21
INFO: iteration 3, average log likelihood -1.132987
WARNING: Variances had to be floored 1 2 3 4 7 10 16
INFO: iteration 4, average log likelihood -1.127811
WARNING: Variances had to be floored 1 2 3 4 10 15 20 21
INFO: iteration 5, average log likelihood -1.100063
WARNING: Variances had to be floored 1 2 3 4 7 10 14 22
INFO: iteration 6, average log likelihood -1.092417
WARNING: Variances had to be floored 1 2 3 4 10 20 21
INFO: iteration 7, average log likelihood -1.089141
WARNING: Variances had to be floored 1 2 3 4 10 14 15
INFO: iteration 8, average log likelihood -1.072623
WARNING: Variances had to be floored 1 2 3 4 7 8 10 20 21 22
INFO: iteration 9, average log likelihood -1.053890
WARNING: Variances had to be floored 1 2 3 4 10 14
INFO: iteration 10, average log likelihood -1.090697
WARNING: Variances had to be floored 1 2 3 4 10 15 20 21
INFO: iteration 11, average log likelihood -1.065299
WARNING: Variances had to be floored 1 2 3 4 7 10 14
INFO: iteration 12, average log likelihood -1.062865
WARNING: Variances had to be floored 1 2 3 4 8 10 20 21 22
INFO: iteration 13, average log likelihood -1.044248
WARNING: Variances had to be floored 1 2 3 4 7 10 14 15
INFO: iteration 14, average log likelihood -1.075436
WARNING: Variances had to be floored 1 2 3 4 10 20 21
INFO: iteration 15, average log likelihood -1.073175
WARNING: Variances had to be floored 1 2 3 4 7 10 14
INFO: iteration 16, average log likelihood -1.054355
WARNING: Variances had to be floored 1 2 3 4 8 10 15 20 21 22
INFO: iteration 17, average log likelihood -1.039360
WARNING: Variances had to be floored 1 2 3 4 7 10 14
INFO: iteration 18, average log likelihood -1.084550
WARNING: Variances had to be floored 1 2 3 4 10 20 21
INFO: iteration 19, average log likelihood -1.066184
WARNING: Variances had to be floored 1 2 3 4 7 10 14 15
INFO: iteration 20, average log likelihood -1.050047
WARNING: Variances had to be floored 1 2 3 4 8 10 20 21 22
INFO: iteration 21, average log likelihood -1.049310
WARNING: Variances had to be floored 1 2 3 4 7 10 14
INFO: iteration 22, average log likelihood -1.078944
WARNING: Variances had to be floored 1 2 3 4 10 15 20 21
INFO: iteration 23, average log likelihood -1.063430
WARNING: Variances had to be floored 1 2 3 4 7 10 14
INFO: iteration 24, average log likelihood -1.059636
WARNING: Variances had to be floored 1 2 3 4 8 10 20 21 22
INFO: iteration 25, average log likelihood -1.042146
WARNING: Variances had to be floored 1 2 3 4 7 10 14 15
INFO: iteration 26, average log likelihood -1.074193
WARNING: Variances had to be floored 1 2 3 4 10 20 21
INFO: iteration 27, average log likelihood -1.072421
WARNING: Variances had to be floored 1 2 3 4 7 10 14
INFO: iteration 28, average log likelihood -1.053978
WARNING: Variances had to be floored 1 2 3 4 8 10 15 20 21 22
INFO: iteration 29, average log likelihood -1.039282
WARNING: Variances had to be floored 1 2 3 4 7 10 14
INFO: iteration 30, average log likelihood -1.084518
WARNING: Variances had to be floored 1 2 3 4 10 20 21
INFO: iteration 31, average log likelihood -1.066185
WARNING: Variances had to be floored 1 2 3 4 7 10 14 15
INFO: iteration 32, average log likelihood -1.050066
WARNING: Variances had to be floored 1 2 3 4 8 10 20 21 22
INFO: iteration 33, average log likelihood -1.049298
WARNING: Variances had to be floored 1 2 3 4 7 10 14
INFO: iteration 34, average log likelihood -1.078937
WARNING: Variances had to be floored 1 2 3 4 9 10 15 20 21
INFO: iteration 35, average log likelihood -1.063423
WARNING: Variances had to be floored 1 2 3 4 7 9 10 14
INFO: iteration 36, average log likelihood -1.059633
WARNING: Variances had to be floored 1 2 3 4 8 9 10 20 21 22
INFO: iteration 37, average log likelihood -1.042139
WARNING: Variances had to be floored 1 2 3 4 7 10 14 15
INFO: iteration 38, average log likelihood -1.074188
WARNING: Variances had to be floored 1 2 3 4 9 10 20 21
INFO: iteration 39, average log likelihood -1.072413
WARNING: Variances had to be floored 1 2 3 4 7 9 10 14
INFO: iteration 40, average log likelihood -1.053973
WARNING: Variances had to be floored 1 2 3 4 8 9 10 15 20 21 22
INFO: iteration 41, average log likelihood -1.039275
WARNING: Variances had to be floored 1 2 3 4 7 9 10 14
INFO: iteration 42, average log likelihood -1.084513
WARNING: Variances had to be floored 1 2 3 4 9 10 20 21
INFO: iteration 43, average log likelihood -1.066179
WARNING: Variances had to be floored 1 2 3 4 7 9 10 14 15
INFO: iteration 44, average log likelihood -1.050061
WARNING: Variances had to be floored 1 2 3 4 8 9 10 20 21 22
INFO: iteration 45, average log likelihood -1.049291
WARNING: Variances had to be floored 1 2 3 4 7 9 10 14
INFO: iteration 46, average log likelihood -1.078932
WARNING: Variances had to be floored 1 2 3 4 9 10 15 20 21
INFO: iteration 47, average log likelihood -1.063417
WARNING: Variances had to be floored 1 2 3 4 7 9 10 14
INFO: iteration 48, average log likelihood -1.059625
WARNING: Variances had to be floored 1 2 3 4 8 9 10 20 21 22
INFO: iteration 49, average log likelihood -1.042130
WARNING: Variances had to be floored 1 2 3 4 7 9 10 14 15
INFO: iteration 50, average log likelihood -1.074182
INFO: EM with 100000 data points 50 iterations avll -1.074182
59.0 data points per parameter
5: avll = [-1.15425,-1.13868,-1.13299,-1.12781,-1.10006,-1.09242,-1.08914,-1.07262,-1.05389,-1.0907,-1.0653,-1.06286,-1.04425,-1.07544,-1.07318,-1.05435,-1.03936,-1.08455,-1.06618,-1.05005,-1.04931,-1.07894,-1.06343,-1.05964,-1.04215,-1.07419,-1.07242,-1.05398,-1.03928,-1.08452,-1.06619,-1.05007,-1.0493,-1.07894,-1.06342,-1.05963,-1.04214,-1.07419,-1.07241,-1.05397,-1.03927,-1.08451,-1.06618,-1.05006,-1.04929,-1.07893,-1.06342,-1.05963,-1.04213,-1.07418]
[-1.39417,-1.39423,-1.39416,-1.39361,-1.38696,-1.36858,-1.36129,-1.36021,-1.3598,-1.35958,-1.35944,-1.35936,-1.3593,-1.35926,-1.35923,-1.35921,-1.35919,-1.35917,-1.35915,-1.35913,-1.35912,-1.35911,-1.35909,-1.35908,-1.35907,-1.35906,-1.35905,-1.35904,-1.35903,-1.35902,-1.35901,-1.359,-1.35899,-1.35899,-1.35898,-1.35898,-1.35897,-1.35897,-1.35897,-1.35896,-1.35896,-1.35896,-1.35895,-1.35895,-1.35895,-1.35895,-1.35895,-1.35895,-1.35895,-1.35895,-1.35894,-1.35906,-1.35896,-1.35872,-1.35641,-1.34658,-1.33468,-1.32837,-1.32484,-1.32253,-1.32084,-1.31953,-1.31862,-1.31794,-1.31737,-1.31685,-1.31635,-1.31585,-1.31537,-1.31491,-1.31446,-1.31399,-1.31349,-1.31296,-1.31237,-1.31181,-1.31136,-1.31099,-1.31065,-1.31029,-1.30986,-1.30934,-1.3089,-1.30864,-1.30849,-1.3084,-1.30834,-1.30829,-1.30824,-1.30818,-1.30812,-1.30803,-1.3079,-1.30772,-1.30747,-1.30716,-1.30695,-1.30688,-1.30685,-1.30682,-1.30681,-1.30694,-1.30681,-1.30653,-1.30381,-1.28975,-1.27282,-1.26529,-1.2613,-1.25875,-1.25687,-1.25521,-1.25364,-1.2524,-1.25159,-1.25102,-1.25054,-1.24996,-1.24916,-1.24825,-1.2475,-1.24681,-1.24592,-1.24469,-1.24368,-1.24329,-1.24308,-1.24284,-1.24255,-1.24216,-1.24168,-1.24117,-1.24077,-1.24052,-1.24036,-1.24026,-1.24019,-1.24015,-1.24013,-1.24011,-1.2401,-1.24009,-1.24008,-1.24008,-1.24008,-1.24007,-1.24007,-1.24007,-1.24007,-1.24007,-1.24007,-1.24025,-1.24006,-1.23939,-1.22835,-1.18408,-1.16516,-1.16434,-1.1645,-1.15282,-1.14918,-1.15891,-1.16361,-1.15708,-1.14573,-1.14901,-1.16368,-1.14967,-1.154,-1.13993,-1.15324,-1.16237,-1.14846,-1.15129,-1.15419,-1.144,-1.16095,-1.14338,-1.15586,-1.15319,-1.14307,-1.15841,-1.15794,-1.14687,-1.14432,-1.14388,-1.16507,-1.15712,-1.14599,-1.14946,-1.15278,-1.15421,-1.14822,-1.14679,-1.15611,-1.15196,-1.15336,-1.15335,-1.15565,-1.14524,-1.14307,-1.15425,-1.13868,-1.13299,-1.12781,-1.10006,-1.09242,-1.08914,-1.07262,-1.05389,-1.0907,-1.0653,-1.06286,-1.04425,-1.07544,-1.07318,-1.05435,-1.03936,-1.08455,-1.06618,-1.05005,-1.04931,-1.07894,-1.06343,-1.05964,-1.04215,-1.07419,-1.07242,-1.05398,-1.03928,-1.08452,-1.06619,-1.05007,-1.0493,-1.07894,-1.06342,-1.05963,-1.04214,-1.07419,-1.07241,-1.05397,-1.03927,-1.08451,-1.06618,-1.05006,-1.04929,-1.07893,-1.06342,-1.05963,-1.04213,-1.07418]
32×26 Array{Float64,2}:
 -0.187246     0.064428     0.0139663   -0.0242528    0.00594197  -0.135819     0.00765355   0.245454     0.0748724   -0.0941761   -0.0452321  -0.130017    -0.0871997   -0.00151093   0.0449292    0.0743095   -0.0755187     0.0617389    0.265232    -0.0665346   -0.0812928    -0.141698    -0.171596    -0.0911131    -0.0812116   0.0319804 
  0.13751      0.0425897    0.0269359   -0.117877    -0.206247    -0.121192     0.02696     -0.0451557    0.0770614   -0.0733357   -0.0524625  -0.248367    -0.0872843   -0.00463047   0.0392063    0.0766163   -0.00586082   -0.0517297    0.247762    -0.0662229   -0.029338     -0.141283    -0.165841    -0.086289     -0.0984577   0.0468737 
  0.17146     -0.329372     0.103707    -0.0812719    0.093377    -0.0336114   -0.0609176    0.0696635   -0.0529312   -0.162032     0.287739   -0.136664     0.029844    -0.432519    -0.230064     0.227905     0.03552       0.155593    -0.0946293   -0.0967678   -0.0830393    -0.019529     0.0711938   -0.0637047    -0.0289795  -0.00396363
  0.180341     0.247497     0.100549     0.0382653    0.00780077  -0.106289    -0.00783389   0.071741     0.0541046   -0.0201397    0.143416   -0.136293     0.0285844    0.487499    -0.1864       0.0702338    0.0323198     0.155303     0.156215     0.0272979   -0.0616526    -0.0142345    0.0134214    0.0401268    -0.0247777  -0.02992   
  0.0457377   -0.117798     0.0418446   -0.511164    -0.0284593   -0.11066     -0.0581575    0.0410943    0.158605     0.00893377  -0.0389982  -0.0392039    0.132458    -0.179766    -0.0986403   -0.048541     0.160268     -0.00687149   0.0124486    0.026951    -0.201819      0.212164     0.128411     0.0316241    -0.09002     0.0957804 
  0.0512657   -0.120493     0.0441912    0.4749       0.00728578  -0.0327327   -0.0572581   -0.0263325    0.133888    -0.111934    -0.0845807  -0.0309411    0.126507    -0.13311     -0.131731     0.015556     0.0608358     0.255        0.0857928   -0.0395874   -0.187537      0.158943     0.214475     0.0260402    -0.151125    0.0943498 
 -0.0658284    0.0411722   -0.087083    -0.0596258    0.091444     0.0283281    0.325231     0.0442839   -0.0010099   -0.0249156    0.0740996  -0.124613     0.00450031   0.0435101   -0.0621666   -0.0899223   -0.0498257     0.0715012    0.0259585    0.0852101   -0.054038      0.0220378    0.0250042    0.165914      0.0795691   0.123141  
  0.0859088   -0.0368454   -0.0367566    0.031579     0.0512331    0.085848    -0.0522943    0.033149    -0.0644138   -0.0275201    0.100635    0.0549419   -0.0897821    0.0712173   -0.110225     0.00539763   0.0987917    -0.0949196   -0.0943157   -0.057788     0.139183     -0.0666672    0.010025     0.115162     -0.1335      0.0714457 
  0.205867     0.071557    -0.132823    -0.116668    -0.243721     0.0119173    0.0562161    0.0397614   -0.330725     0.020565    -0.152225    0.154302     0.0866863   -0.136542    -0.123474     0.222951     0.0203545     0.0281671   -0.0972158   -0.0757706   -0.0156878    -0.304245     0.0267076   -0.107206      0.118229    0.0651276 
  0.177572     0.081927    -0.0303967   -0.112277    -0.20192      0.00673      0.0596607    0.0388996    0.0163501    0.0561646   -0.197658    0.199434     0.0879966   -0.139676    -0.00610274  -0.105267    -0.0529096    -0.265533     0.120362    -0.132354    -0.171263      0.440835    -0.0362774   -0.202562      0.118186    0.0814159 
 -0.0707385    0.172511     0.0390053   -0.0377769   -0.144183     0.136447    -0.0912537   -0.0636959   -0.0234797   -0.0245195   -0.0563776   0.0494022   -0.0597618   -0.0831303   -0.154101     0.0650507    0.162873     -0.0409605    0.185994     0.092024     0.00136599    0.156508    -0.0740357   -0.63502       0.0814982  -0.0226553 
 -0.13533      0.0219268    0.148584    -0.0355982   -0.12032      0.143174    -0.0920536    0.110913    -0.210431    -0.0368436   -0.0531841   0.0540851   -0.0567035   -0.168934     0.109339     0.020432     0.0370707     0.0494418    0.11203      0.115992     0.0306392    -0.176176    -0.0925764    0.562176     -0.156613   -0.017342  
 -0.0636425    0.113374     0.00339293   0.00629356  -0.086549     0.100546     0.0287117    0.141206     0.039934    -0.216509     0.0410534  -0.138623     0.0218416    0.10821      0.0649165   -0.0305759    0.0475194     0.0803014    0.0502403   -0.0263085    0.156317      0.0272362    0.0913861   -0.226373      0.0541189   0.0584049 
 -0.0100776   -0.00236796   0.124946    -0.0718448    0.045112     0.00345969  -0.180683    -0.013846     0.108231     0.00754786   0.115845    0.0440826    0.0861693   -0.0371124    0.00105829   0.195078     0.0560104     0.154851     0.125397    -0.00386658  -0.0255567     0.191893    -0.0177939   -0.00146884   -0.0905888   0.067666  
  0.0129534   -0.122705     0.119929    -0.189629     0.0498969    0.0708982    0.0438232    0.0237572   -0.211612     0.123137    -0.143789   -0.118297     0.104461    -0.126122    -0.200087     0.148454    -0.00341506    0.0282044   -0.0792481    0.0662566   -0.0469394     0.0207844   -0.0765193    0.0611668     0.0302946   0.0238104 
 -0.020066     0.00564997  -0.0976123    0.0797225    0.0650119    0.0465985   -0.0956134    0.00133624   0.040582    -0.0319054    0.0587172   0.00672847  -0.0167571   -0.0218617   -0.0726581    0.0555889   -0.00531231   -0.0260595   -0.0306654   -0.110022     0.0548588     0.00573573  -0.0641747   -0.0428957    -0.0418803   0.0194257 
 -0.0723786    0.00328069   0.132862    -0.0598659   -0.0304015    0.0487003   -0.0771172    0.120265     0.129019    -0.0757736    0.111893    0.198521    -0.0617277   -0.094939    -0.132571     0.00218502  -0.122357     -0.0737974    0.00360108   0.08144     -0.0686184     0.151521     0.0656779   -0.0293559    -0.116144   -0.0954558 
 -0.0401626   -0.0362565    0.0287885   -0.0622337   -0.0242309   -0.053435    -0.144709    -0.086447    -0.00427178  -0.00365966  -0.137554   -0.312907     0.162841     0.178373    -0.0736134    0.150893     0.0321795     0.158596    -0.0555664   -0.0123916   -0.0503368     0.0168453    0.00817122   0.173593     -0.195963    0.012971  
 -0.0539607   -0.0191543    0.0124344    0.0908886    0.108026    -0.065237     0.00998981  -0.0617651   -0.169373    -0.0767844    0.161512   -0.00608347  -0.0504147   -0.121842    -0.21571      0.0347077   -0.124173     -0.065984    -0.0923245   -0.127195    -0.171343      0.0311587    0.00328317   0.137549      0.0152566  -0.125843  
  0.0526039   -0.153731     0.0848637    0.142751     0.0707387   -0.0731354   -0.0173668    0.030047    -0.0849645   -0.0718153    0.111317    0.0592746    0.0673536   -0.11814      0.164451    -0.0461816   -0.134673      0.0864493   -0.0676467    0.100976     0.158202      0.0955917   -0.0746481    0.00137669    0.0776888  -0.106272  
 -0.032709     0.136723     0.13024      0.191965     0.0780721   -0.133373     0.0382945    0.0129298    0.128592     0.0259515   -0.0137427  -0.034077    -0.137357    -0.126931    -0.207528    -0.095834    -0.000697097   0.047474    -0.0213434   -0.0827371   -0.178395      0.104403    -0.12133      0.192106      0.0125885   0.192963  
  0.161917    -0.023064     0.00971117   0.00678644   0.0438538    0.0705542    0.0514338    0.0103383    0.0179773    0.00286272   0.10725    -0.109692    -0.153022    -0.105183    -0.096177     0.0111487   -0.0488697     0.022069    -0.0641315   -0.134126     0.0830985    -0.101674    -0.0117191    0.150814      0.0342831   0.0961889 
 -0.11349     -0.0517423   -0.122576     0.114063    -0.0817177    0.133953     0.100267    -0.0114998   -0.494429    -0.0430842    0.0540972   0.134632    -0.098729     0.147513     0.0441346    0.0719595   -0.0794094    -0.134711    -0.0914786   -0.190808    -0.0890089     0.00898968   0.066142     0.0554047    -0.0573667   0.422884  
 -0.12913     -0.0612664   -0.120148     0.162045    -0.0657405    0.167642     0.121296     0.0989621    0.30662     -0.0495092   -0.145467    0.131315    -0.0838176    0.152755    -0.121341    -0.172197     0.0594486    -0.125005    -0.0430253   -0.194427    -0.109211      0.0421018   -0.0261159    0.0294625    -0.104603    0.0508507 
 -0.00633435   0.0723019    0.00212884  -0.0905545   -0.137917    -0.0415641   -0.055532    -0.13167      0.0263608    0.0699055    0.0172181  -0.0725458   -0.00609241   0.0289715    0.0613036   -0.0290216   -0.00722024    0.123325     0.0586218    0.00833838   0.0256193    -0.139779     0.025907     0.040935      0.0308588   0.105269  
  0.0427848   -0.0714542   -0.0693646    0.00716174   0.0114838    0.0210577    0.0734713    0.0286055   -0.105577    -0.171932     0.0147415  -0.0353571    0.035094    -0.0765884   -0.0192692    0.0870503   -0.104314      0.173381    -0.046887    -0.0410538    0.110591     -0.0510548    0.0293342   -0.0395514    -0.153055    0.0388597 
  0.20236     -0.00334243  -0.139122    -0.207717     0.0941686   -0.0798668    0.182252     0.0337268   -0.0898771   -0.0435759    0.0381069   0.072268    -0.110467     0.0266501   -0.0898822    0.138145    -0.0469398    -0.0603742    0.0339546    0.0623729   -0.0484761    -0.0865003    0.135781     0.0209866    -0.0136187  -0.186663  
  0.131982    -0.0550527   -0.0713133    0.151798    -0.0787699   -0.015604    -0.00216717   0.0109399    0.0251677   -0.0108865    0.0720969  -0.0401815    0.0155025    0.0247785   -0.023691     0.00345993   0.0636827    -0.0380012    0.0575987    0.044924     0.080974     -0.0546078    0.0244169   -0.000936137   0.0717765  -0.129154  
  0.0942347   -0.0384259    0.0141564    0.106165     0.0228226   -0.0663534   -0.140126     0.041102    -0.107636    -0.00111662  -0.0145551  -0.0519718    0.0945096   -0.0213977   -0.0547741    0.0159759   -0.0438338    -0.0419998   -0.0987863    0.052557     0.0916322     0.0109085    0.0250454    0.0409871     0.0370953   0.0617615 
  0.223543     0.00543808  -0.0996815    0.100168    -0.108743     0.0674531    0.0418238    0.153878     0.11829     -0.0966024   -0.0848      0.180637     0.103988     0.0218264   -0.00620834  -0.0227118    0.0768837     0.0218617    0.0129772    0.16072     -0.000986437   0.139616    -0.0405273   -0.0150367     0.139067    0.0365527 
  0.0328301   -0.112328    -0.0433693   -0.0714657   -0.0772285   -0.0719373   -0.0500241   -0.00352421   0.0941252    0.0122887   -0.0257312   0.00523295   0.0685423    0.0728421   -0.0606761   -0.12166     -0.0036724     0.103515     0.115078     0.0716999   -0.011494      0.00119156   0.10776      0.0549042     0.0141426   0.136027  
 -0.0174451    0.0324554    0.021032    -0.00640482   0.0453186   -0.0693657   -0.106743    -0.0801529   -0.018779    -0.0408419    0.0364077   0.0976636   -0.0224458   -0.0501697    0.025801    -0.041463     0.0229407    -0.0811715    0.0687727    0.0262679   -0.0118015    -0.124885    -0.0676415   -0.0624057     0.0454716   0.0308318 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 2 3 4 9 10 20 21
INFO: iteration 1, average log likelihood -1.072406
WARNING: Variances had to be floored 1 2 3 4 7 9 10 14 20 21
INFO: iteration 2, average log likelihood -1.042364
WARNING: Variances had to be floored 1 2 3 4 8 9 10 15 20 21 22
INFO: iteration 3, average log likelihood -1.040402
WARNING: Variances had to be floored 1 2 3 4 7 9 10 14 20 21
INFO: iteration 4, average log likelihood -1.061983
WARNING: Variances had to be floored 1 2 3 4 9 10 20 21
INFO: iteration 5, average log likelihood -1.049803
WARNING: Variances had to be floored 1 2 3 4 7 8 9 10 14 15 20 21 22
INFO: iteration 6, average log likelihood -1.030768
WARNING: Variances had to be floored 1 2 3 4 9 10 20 21
INFO: iteration 7, average log likelihood -1.071389
WARNING: Variances had to be floored 1 2 3 4 7 9 10 14 20 21
INFO: iteration 8, average log likelihood -1.041915
WARNING: Variances had to be floored 1 2 3 4 8 9 10 15 20 21 22
INFO: iteration 9, average log likelihood -1.040796
WARNING: Variances had to be floored 1 2 3 4 7 9 10 14 20 21
INFO: iteration 10, average log likelihood -1.061992
INFO: EM with 100000 data points 10 iterations avll -1.061992
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.800687e+05
      1       6.609684e+05      -2.191003e+05 |       32
      2       6.331877e+05      -2.778073e+04 |       32
      3       6.169440e+05      -1.624370e+04 |       32
      4       6.063171e+05      -1.062683e+04 |       32
      5       5.991763e+05      -7.140868e+03 |       32
      6       5.946269e+05      -4.549419e+03 |       32
      7       5.914167e+05      -3.210141e+03 |       32
      8       5.893983e+05      -2.018404e+03 |       32
      9       5.885125e+05      -8.857997e+02 |       32
     10       5.881180e+05      -3.945497e+02 |       32
     11       5.878790e+05      -2.390027e+02 |       32
     12       5.877022e+05      -1.767865e+02 |       32
     13       5.875439e+05      -1.582337e+02 |       32
     14       5.873571e+05      -1.868755e+02 |       32
     15       5.871325e+05      -2.245465e+02 |       32
     16       5.868438e+05      -2.887415e+02 |       32
     17       5.865215e+05      -3.222335e+02 |       32
     18       5.861611e+05      -3.604579e+02 |       32
     19       5.857770e+05      -3.841205e+02 |       32
     20       5.853199e+05      -4.570915e+02 |       32
     21       5.848308e+05      -4.890437e+02 |       32
     22       5.844451e+05      -3.857513e+02 |       32
     23       5.841701e+05      -2.749470e+02 |       32
     24       5.839783e+05      -1.918100e+02 |       32
     25       5.838003e+05      -1.780317e+02 |       32
     26       5.836367e+05      -1.636251e+02 |       32
     27       5.834718e+05      -1.648364e+02 |       32
     28       5.832860e+05      -1.857983e+02 |       32
     29       5.831021e+05      -1.839514e+02 |       32
     30       5.829926e+05      -1.094354e+02 |       32
     31       5.829486e+05      -4.402975e+01 |       31
     32       5.829259e+05      -2.271475e+01 |       32
     33       5.829101e+05      -1.583617e+01 |       31
     34       5.828987e+05      -1.135902e+01 |       31
     35       5.828897e+05      -8.969100e+00 |       31
     36       5.828820e+05      -7.693405e+00 |       28
     37       5.828767e+05      -5.313560e+00 |       29
     38       5.828729e+05      -3.781363e+00 |       26
     39       5.828700e+05      -2.974540e+00 |       20
     40       5.828671e+05      -2.850610e+00 |       23
     41       5.828656e+05      -1.569162e+00 |       20
     42       5.828639e+05      -1.640205e+00 |       11
     43       5.828633e+05      -6.050553e-01 |        9
     44       5.828630e+05      -2.870713e-01 |        8
     45       5.828628e+05      -2.261378e-01 |        9
     46       5.828625e+05      -2.857450e-01 |       10
     47       5.828622e+05      -3.003524e-01 |       13
     48       5.828619e+05      -2.980749e-01 |        6
     49       5.828618e+05      -1.332035e-01 |        4
     50       5.828617e+05      -1.042777e-01 |        2
K-means terminated without convergence after 50 iterations (objv = 582861.6722013957)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.305937
INFO: iteration 2, average log likelihood -1.273768
INFO: iteration 3, average log likelihood -1.243680
WARNING: Variances had to be floored 22
INFO: iteration 4, average log likelihood -1.206398
WARNING: Variances had to be floored 12
INFO: iteration 5, average log likelihood -1.174292
WARNING: Variances had to be floored 18
INFO: iteration 6, average log likelihood -1.137783
WARNING: Variances had to be floored 1 27
INFO: iteration 7, average log likelihood -1.099909
WARNING: Variances had to be floored 11 17 22 31 32
INFO: iteration 8, average log likelihood -1.056750
WARNING: Variances had to be floored 4 6 12 21 23 24
INFO: iteration 9, average log likelihood -1.071027
INFO: iteration 10, average log likelihood -1.115119
WARNING: Variances had to be floored 1 10 22 27 30
INFO: iteration 11, average log likelihood -1.049263
WARNING: Variances had to be floored 12 17 31
INFO: iteration 12, average log likelihood -1.064005
WARNING: Variances had to be floored 4 6 23 24 32
INFO: iteration 13, average log likelihood -1.059193
WARNING: Variances had to be floored 11 18 21 22
INFO: iteration 14, average log likelihood -1.067133
WARNING: Variances had to be floored 1 12 27
INFO: iteration 15, average log likelihood -1.060457
WARNING: Variances had to be floored 10 17 30 31
INFO: iteration 16, average log likelihood -1.059202
WARNING: Variances had to be floored 4 22 23 24 32
INFO: iteration 17, average log likelihood -1.056765
WARNING: Variances had to be floored 6 12 18
INFO: iteration 18, average log likelihood -1.071988
WARNING: Variances had to be floored 1 27
INFO: iteration 19, average log likelihood -1.065539
WARNING: Variances had to be floored 17 21 22
INFO: iteration 20, average log likelihood -1.038483
WARNING: Variances had to be floored 4 11 12 23 24 30
INFO: iteration 21, average log likelihood -1.036800
WARNING: Variances had to be floored 6 10 32
INFO: iteration 22, average log likelihood -1.075169
WARNING: Variances had to be floored 1 17 22 27 31
INFO: iteration 23, average log likelihood -1.050551
INFO: iteration 24, average log likelihood -1.081352
WARNING: Variances had to be floored 12 21 23 24
INFO: iteration 25, average log likelihood -1.008430
WARNING: Variances had to be floored 1 4 6 10 11 17 22 27 30 31 32
INFO: iteration 26, average log likelihood -1.012294
INFO: iteration 27, average log likelihood -1.134115
WARNING: Variances had to be floored 12
INFO: iteration 28, average log likelihood -1.067980
WARNING: Variances had to be floored 22 23 24 28
INFO: iteration 29, average log likelihood -1.025154
WARNING: Variances had to be floored 1 4 11 17 21 27 31 32
INFO: iteration 30, average log likelihood -1.026792
WARNING: Variances had to be floored 10 12 30
INFO: iteration 31, average log likelihood -1.093832
WARNING: Variances had to be floored 6 22
INFO: iteration 32, average log likelihood -1.077864
WARNING: Variances had to be floored 1 17 23 24
INFO: iteration 33, average log likelihood -1.047016
WARNING: Variances had to be floored 4 12 27 31 32
INFO: iteration 34, average log likelihood -1.043366
WARNING: Variances had to be floored 6 11 22 30
INFO: iteration 35, average log likelihood -1.058131
WARNING: Variances had to be floored 1 17 21 28
INFO: iteration 36, average log likelihood -1.058933
WARNING: Variances had to be floored 10 23 24
INFO: iteration 37, average log likelihood -1.062934
WARNING: Variances had to be floored 4 12 22 27 31
INFO: iteration 38, average log likelihood -1.035893
WARNING: Variances had to be floored 6 11 17 32
INFO: iteration 39, average log likelihood -1.064504
WARNING: Variances had to be floored 1 21 23
INFO: iteration 40, average log likelihood -1.051187
WARNING: Variances had to be floored 22 24 30 31
INFO: iteration 41, average log likelihood -1.034391
WARNING: Variances had to be floored 4 10 12 17 27
INFO: iteration 42, average log likelihood -1.041483
WARNING: Variances had to be floored 6 23 32
INFO: iteration 43, average log likelihood -1.066490
WARNING: Variances had to be floored 1 11 21 22 24
INFO: iteration 44, average log likelihood -1.042867
WARNING: Variances had to be floored 12 17 31
INFO: iteration 45, average log likelihood -1.058914
WARNING: Variances had to be floored 4 10 27
INFO: iteration 46, average log likelihood -1.057631
WARNING: Variances had to be floored 1 6 12 22 23 24 28 30 32
INFO: iteration 47, average log likelihood -1.032392
WARNING: Variances had to be floored 11 17 31
INFO: iteration 48, average log likelihood -1.102695
WARNING: Variances had to be floored 21
INFO: iteration 49, average log likelihood -1.083817
WARNING: Variances had to be floored 4 10 22 27
INFO: iteration 50, average log likelihood -1.032694
INFO: EM with 100000 data points 50 iterations avll -1.032694
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.11755     -0.0537162    -0.130275      0.101234    -0.0196191  -0.040013     0.0342396     0.0199256    0.0739246    -0.0587846   -0.0683779   -0.0171504    0.012901    0.00650832   0.158186      0.0424233     0.0120264   -0.0686345    0.0845309    0.208967     0.0135128   -0.0275083   -0.00233512   0.0803386     0.0408918  -0.12697   
 -0.101626     0.101542      0.0920704    -0.036577    -0.133395    0.140447    -0.0919546     0.0188827   -0.113454     -0.0304114   -0.05431      0.0512082   -0.0583507  -0.124915    -0.0259073     0.0450002     0.104479     0.00323181   0.151338     0.103771     0.0155817   -0.00357755  -0.0841793   -0.0639249    -0.0324877  -0.0201811 
 -0.0724665    0.00605427    0.135042     -0.0613864   -0.0314361   0.0471711   -0.0764963     0.123221     0.131134     -0.0758333    0.112881     0.207067    -0.0624802  -0.0917558   -0.139487      0.00230493   -0.127103    -0.0753       0.00555646   0.0862662   -0.0740425    0.153074     0.0637839   -0.0329379    -0.116344   -0.0971333 
 -0.0149811   -0.00671169    0.134911     -0.072442     0.0413917   0.00836261  -0.184701     -0.0228195    0.111807      0.00925757   0.113802     0.0473262    0.0975285  -0.0442026    0.00519291    0.200459      0.0534272    0.163356     0.131958    -0.00406727  -0.0295352    0.196777    -0.015935    -0.00745159   -0.0968588   0.0663171 
  0.226112     0.0064819    -0.0997973     0.100957    -0.109535    0.0663755    0.0429566     0.153266     0.118524     -0.0971419   -0.0851198    0.182256     0.104252    0.0198869   -0.00611356   -0.0224963     0.0759263    0.0213853    0.0135824    0.160683    -0.0018177    0.13985     -0.043912    -0.0154205     0.137644    0.0361857 
 -0.0360462    0.0207069    -0.066458     -0.0269196    0.0712269   0.0318963    0.252971      0.0462308   -0.000630192  -0.0353502    0.0650952   -0.12118      0.0141275   0.0190721   -0.0581487    -0.0727621    -0.0566979    0.0510182    0.00861351   0.0791578   -0.0470272    0.00901085   0.0311475    0.153059      0.0712703   0.122483  
 -0.110184     0.0469852     0.0213825    -0.0442587    0.0449839  -0.157648    -0.190593     -0.157906     0.0157634    -0.0272209    0.101396     0.130284     0.0383099  -0.134082     0.000746672  -0.143222     -0.0242842    0.0334731    0.0701155    0.155581    -0.031266    -0.220477    -0.0775356   -0.024423      0.0163142  -0.00178749
  0.0779225    0.0324971    -0.0205587     0.0307741    0.0966183  -0.129159    -0.12385       0.0486594   -0.281426      0.153608    -0.0493534    0.028002     0.0313508   0.026104    -0.137257     -0.000559509  -0.0344346    0.0599935   -0.104219     0.0912046    0.106758     0.0413098   -0.0208816   -0.00375116    0.0968354   0.0414957 
  0.0359754   -0.111676     -0.0469096    -0.0910063   -0.102449   -0.0719728   -0.0724556    -0.00950334   0.0990449     0.0145049   -0.0425261   -0.00718898   0.102324    0.0726358   -0.0658201    -0.132761     -0.0184172    0.132198     0.134952     0.0878851   -0.0191565    0.0363413    0.138372     0.0554335     0.0503235   0.161208  
 -0.0441564    0.0820114     0.0480861    -0.113335    -0.186918   -0.142073     0.0243589    -0.0229678    0.0553155     0.0995577   -0.00334189  -0.070075     0.0856255  -0.0470475    0.119009     -0.113578      0.00735351   0.136641     0.0807066    0.00609047  -0.0124801   -0.171714     0.0200436    0.0608941     0.135784    0.11683   
  0.0840051   -0.0955446    -0.0534255     0.0387262    0.0779849  -0.0309102   -0.0764867     0.0493741   -0.050912     -0.0187031    0.00315599   0.111299    -0.048954    0.0980626   -0.105491     -0.014688      0.117678    -0.173522    -0.0739398   -0.0162662    0.162923    -0.0734077   -0.0284353    0.0921119    -0.193449    0.0792277 
  0.176605    -0.0229791     0.0985392    -0.00256903   0.0490688  -0.0617382   -0.0250652     0.067037     3.63738e-5   -0.0797511    0.214478    -0.135009     0.0275429   0.036964    -0.198007      0.133796      0.0343768    0.144337     0.0245865   -0.0377164   -0.0639821   -0.018191     0.0351934    0.00399829   -0.0241311  -0.0181722 
  0.0484284   -0.119336      0.0419492    -0.0262937   -0.0120154  -0.0732524   -0.0575921     0.00725763   0.145219     -0.0506306   -0.0597132   -0.0341446    0.129733   -0.157301    -0.114303     -0.0186365     0.109535     0.120455     0.0472436   -0.00626133  -0.190144     0.185009     0.168295     0.0309926    -0.118802    0.0953313 
 -0.118943    -0.056309     -0.120592      0.142032    -0.0686914   0.145007     0.109819      0.0472606   -0.0605752    -0.0443605   -0.0465623    0.124345    -0.0908313   0.14036     -0.0475809    -0.0596011    -0.0125401   -0.123591    -0.0640967   -0.19308     -0.0994245    0.0259547    0.0234907    0.0480711    -0.0791159   0.228142  
 -0.0666318    0.116199      0.00441199    0.00709495  -0.0854449   0.0984921    0.0292109     0.14478      0.0389234    -0.220595     0.0389417   -0.138225     0.0231981   0.108791     0.071391     -0.0302782     0.0440736    0.0802493    0.0510756   -0.0259109    0.153463     0.0295514    0.0917974   -0.229519      0.0491852   0.0598784 
 -0.0519823   -0.0207365     0.0137174     0.0852879    0.105701   -0.0670616    0.0067769    -0.0597139   -0.171347     -0.0764809    0.166419    -0.00499786  -0.0489931  -0.125041    -0.211401      0.0391013    -0.128035    -0.0642925   -0.0931635   -0.133983    -0.172024     0.0328932    0.00417084   0.140852      0.0164815  -0.130603  
 -0.0120581    0.129448      0.122689      0.171764     0.0800069  -0.121186     0.0367793     0.0124656    0.137926      0.024294    -0.0200564   -0.0323128   -0.144851   -0.138001    -0.212014     -0.0869267    -0.0110991    0.0452834   -0.0212741   -0.0833131   -0.137092     0.0900455   -0.116382     0.192697      0.0269912   0.200521  
 -0.0515145    0.123773     -0.095797      0.213942     0.129567   -0.0363939   -0.000877345  -0.0791925    0.0337271    -0.115676     0.0111711   -0.00614364   0.0112853   0.0710524    0.0105099     0.0764011     0.124449     0.0213757   -0.0119541   -0.152347    -0.00830768  -0.0901032   -0.0712638   -0.0978635     0.0332663  -0.0739982 
 -0.0163639   -0.0246019    -0.0774775    -0.0846141    0.0396476  -0.0162651    0.0228552     0.0459426   -0.0296304    -0.244011    -0.0725274    0.0452471    0.0666463  -0.0336345    0.105498      0.00514514   -0.114138     0.144589    -0.171316     0.0756453    0.0273602   -0.0218507    0.0732538   -0.0907568    -0.124183   -0.0345696 
 -0.0373742   -0.0350542     0.0300325    -0.0585127   -0.0214216  -0.0525389   -0.143832     -0.0845768   -0.00513648   -0.00477795  -0.136402    -0.313243     0.161342    0.172717    -0.0673731     0.150343      0.032465     0.150201    -0.0527875   -0.0228903   -0.0490706    0.0169168    0.0125321    0.166494     -0.194716    0.0133767 
  0.153201    -0.0275545     0.0166508     0.0443446    0.0353677   0.150601     0.0671889     0.0195012   -0.0358248    -0.028045     0.23085     -0.139908    -0.150483   -0.0775635   -0.0958618     0.0259786    -0.00431954   0.0154582   -0.07018     -0.136422     0.12426     -0.0762633    0.0749287    0.151215      0.0522922   0.089051  
 -0.0281426    0.0514731     0.0201748    -0.0738936   -0.101704   -0.128227     0.0168482     0.102997     0.0749998    -0.0844087   -0.0492158   -0.189188    -0.0872263  -0.0018926    0.0424631     0.0752389    -0.0404373    0.00600481   0.25599     -0.0675475   -0.0572456   -0.141635    -0.168567    -0.08917      -0.0895751   0.039419  
  0.052612    -0.161067      0.0933012     0.142943     0.0742873  -0.0734952   -0.0257746     0.0315085   -0.08318      -0.0692241    0.111598     0.0581441    0.0666465  -0.117865     0.168749     -0.0479589    -0.144121     0.0881393   -0.0683908    0.106128     0.160221     0.0968452   -0.0911419    0.000777056   0.0760226  -0.109749  
  0.0156739    0.0703135    -0.0316931    -0.104365    -0.146664    0.0635965   -0.137829     -0.290437     0.0693008     0.0425261    0.0293553   -0.0918078   -0.0711924   0.115692     0.0394497     0.02218      -0.0269475    0.133409     0.0619866    0.0338906    0.0481532   -0.119138     0.0454417    0.0147653    -0.073144    0.0871953 
  0.203153     0.000401636  -0.140432     -0.21057      0.0962946  -0.0795337    0.184402      0.0331579   -0.0857039    -0.0427395    0.03825      0.0733146   -0.11326     0.032632    -0.0936761     0.138693     -0.0478074   -0.0605244    0.0340357    0.0623591   -0.0488556   -0.0860283    0.137287     0.0202644    -0.0135273  -0.196008  
  0.00288231  -0.109774     -0.108496     -0.0417469   -0.0028575   0.142459    -0.204438      0.0877061    0.0551223     0.0561262    0.102255     0.0127536   -0.054038   -0.128964    -0.164492      0.0445282    -0.151499    -0.064953    -0.0431628   -0.0687898    0.119251     0.127997    -0.0586868    0.00375697   -0.0860536   0.105397  
  0.0138079   -0.121853      0.121198     -0.190711     0.051903    0.0706214    0.042691      0.0248034   -0.210318      0.123234    -0.144503    -0.119044     0.104788   -0.126405    -0.200719      0.148479     -0.00306374   0.0293137   -0.0823647    0.0662574   -0.0488547    0.0214086   -0.076804     0.0618469     0.0273038   0.0240164 
  0.142571    -0.0597042    -0.00509646    0.204771    -0.138781    0.00532681  -0.0192176     0.0103462   -0.0259763     0.0331684    0.232174    -0.0723071    0.0249815   0.0387619   -0.202155     -0.0448971     0.130954    -0.00341839   0.0359309   -0.13182      0.116412    -0.0775192    0.0519265   -0.0922003     0.109958   -0.123692  
  0.191863     0.0767369    -0.0815675    -0.114566    -0.222948    0.00931229   0.0580101     0.0393276   -0.156947      0.0384078   -0.175003     0.177056     0.0875969  -0.138172    -0.0649493     0.0588645    -0.0174657   -0.119036     0.0108995   -0.104219    -0.0929294    0.0686156   -0.00476719  -0.15529       0.118286    0.0732585 
  0.0955212   -0.0766554     0.0667921     0.170619    -0.0614594  -0.0260017   -0.179717      0.0277285    0.00846468   -0.133347    -0.0131723   -0.191579     0.156696   -0.0752271    0.076171      0.0556317    -0.0768249   -0.0937621   -0.0838113    0.0155016    0.0231759    0.0112285    0.0626684    0.0335995     0.037381    0.0533918 
  0.0821484    0.0159272     0.000132206   0.01685      0.0455119   0.0317692    0.000650082   0.00993255  -0.0518509    -0.0310351   -0.029316     0.132234    -0.117592    0.0587467    0.0323332     0.0648113     0.0761721   -0.202775     0.0896723   -0.123999     0.0111597   -0.0280207   -0.0508815   -0.122051      0.0977332   0.0718539 
  0.0894316   -0.107633     -0.068216      0.09547     -0.0101319   0.0498649    0.126881      0.0121832   -0.192277     -0.126843     0.0770009   -0.107657     0.0210276  -0.121663    -0.131626      0.203856     -0.111789     0.231144     0.0769817   -0.159986     0.196255    -0.073559    -0.021565    -0.0135768    -0.197824    0.119194  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 12 17 23 24 31 32
INFO: iteration 1, average log likelihood -1.034099
WARNING: Variances had to be floored 1 6 11 12 17 21 23 24 30 31 32
INFO: iteration 2, average log likelihood -0.988749
WARNING: Variances had to be floored 1 10 12 17 22 23 24 28 31 32
INFO: iteration 3, average log likelihood -0.983843
WARNING: Variances had to be floored 1 4 6 11 12 17 21 23 24 27 30 31 32
INFO: iteration 4, average log likelihood -0.989461
WARNING: Variances had to be floored 1 12 17 23 24 31 32
INFO: iteration 5, average log likelihood -1.016219
WARNING: Variances had to be floored 1 6 10 11 12 17 21 22 23 24 28 30 31 32
INFO: iteration 6, average log likelihood -0.973409
WARNING: Variances had to be floored 1 4 12 17 23 24 27 31 32
INFO: iteration 7, average log likelihood -1.009080
WARNING: Variances had to be floored 1 6 11 12 17 21 23 24 30 31 32
INFO: iteration 8, average log likelihood -1.001194
WARNING: Variances had to be floored 1 10 12 17 22 23 24 28 31 32
INFO: iteration 9, average log likelihood -0.989267
WARNING: Variances had to be floored 1 4 6 11 12 17 21 23 24 27 30 31 32
INFO: iteration 10, average log likelihood -0.992802
INFO: EM with 100000 data points 10 iterations avll -0.992802
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.102967    -0.0148571    0.0653238  -0.0383804    -0.0650535    0.0627598   -0.145349     0.148736    -0.0175497    0.134749     -0.0771971   -0.0476378   -0.0669665   -0.194978     0.00715464  -0.0468642   -0.0767193    -0.140357   -0.112381   -0.0331882   -0.180979      0.0967435   0.107178     0.0144482    0.00754839   -0.0300331 
  0.0821997    0.0838822    0.161244    0.0148623     0.00589742   0.0592008    0.0263535    0.149937     0.0987531    0.0143334     0.175555    -0.276028    -0.100002     0.0786781    0.124018     0.0448388    0.0433719     0.174475    0.0505439  -0.0441971   -0.033132      0.246117   -0.0368544    0.0760306   -0.00523124    0.0565596 
 -0.0162708    0.162416    -0.0580778   0.105025      0.132286    -0.079221    -0.0930924    0.123739     0.119983     0.0942749    -0.0128089   -0.0733554    0.065173     0.0407875   -0.0795687   -0.0924937   -0.044535      0.121439    0.0153126  -0.118664    -0.0438041    -0.0578093  -0.148903     0.0975547    0.0955412     0.120264  
 -0.00879836  -0.0704363   -0.0750699  -0.0637144     0.117247    -0.228973     0.115432     0.0602474   -0.0151235   -0.176836      0.0100497   -0.0766938    0.0216867   -0.0284905    0.068535    -0.0773592    0.165209     -0.0614456   0.0376785   0.191981    -0.029542     -0.145851   -0.00380946  -0.231212     0.0653747     0.170379  
 -0.104094     0.0237861    0.0193064   0.0424859     0.0193414    0.0420296   -0.0129221    0.0490605   -0.0675241    0.14114       0.0837099    0.0831933   -0.0318522   -0.175129     0.242347    -0.0939982    0.0847069    -0.0114214  -0.10255    -0.00964568  -0.127096     -0.0531928   0.00669816   0.127572    -0.0080148    -0.142028  
  0.0172226   -0.0407503    0.0789415  -0.118756     -0.012989     0.0145227   -0.199093    -0.0316921    0.220353    -0.110581     -0.113679    -0.150431    -0.0979012   -0.0287951    0.0434584    0.0783755    0.0539578     0.0684799   0.154974   -0.0321995    0.0286447     0.137217   -0.0298987   -0.268972     0.0646562    -0.0600005 
 -0.0293964    0.0557478   -0.131851   -0.0699319     0.0568855    0.17546     -0.0665098    0.112374    -0.0350204   -0.0318629     0.00740741   0.040965    -0.146124    -0.0792268   -0.0298508   -0.0647985    0.014059      0.132382   -0.136669    0.0716618    0.0347822     0.0618303  -0.0333519   -0.00367233  -0.103748     -0.0116889 
 -0.0314471    0.118582    -0.121725   -0.0630042     0.133976    -0.0714083    0.0893773   -0.0510438   -0.207472     0.0987689     0.141994    -0.127253    -0.114398     0.115143    -0.05449      0.0217722    0.0122755    -0.162941    0.0853166   0.0460511    0.102985     -0.0166325   0.0193039   -0.0071082    0.162046      0.181148  
 -0.0120536    0.154884    -0.12561    -0.0146485     0.0505761    0.0205624   -0.0200497    0.192127    -0.0256265    0.00555252   -0.022268     0.0154515    0.0044367    0.0041903    0.0398885   -0.00519448   0.000993995   0.163632    0.253158    0.0289932    0.0305811    -0.0298289   0.0492171    0.0018442    0.156829     -0.183782  
  0.0281168   -0.13171      0.0602415  -0.0841536    -0.0865335   -0.0986816    0.0116316   -0.119772     0.0837556    0.145681     -0.147579     0.0174993    0.0335112   -0.0463141    0.168365     0.0951655   -0.100417      0.013604    0.0822707   0.0149458   -0.0197997     0.0892644  -0.131867    -0.194383     0.0990265    -0.0472645 
  0.0283015   -0.172585     0.164138    0.0106        0.0587919   -0.175579     0.0730818    0.143306     0.0712244    0.0492043     0.0981904   -0.00705001   0.251221     0.0556742   -0.187435    -0.0177374    0.0789182     0.0923594   0.0990321  -0.0368473    0.137625     -0.119544   -0.0950395   -0.0562109    0.0181078     0.0656778 
 -0.125057     0.222388    -0.0417921   0.0166226    -0.143624    -0.0377084   -0.00243821   0.0639071    0.138421    -0.0350926     0.0398412   -0.198602    -0.0399238    0.170716     0.0256013    0.0578043   -0.0143792     0.198506   -0.0204206  -0.0631872    0.141784      0.0992361  -0.0303663    0.08369     -0.203728     -0.0677635 
  0.0886101    0.00715417   0.0754716   0.0943103     0.0343943   -0.0198302   -0.0716457   -0.077213     0.0219491   -0.0835888     0.0724552    0.0967504   -0.00259542   0.0756195   -0.198945     0.179221     0.0770916    -0.206883    0.0123854   0.0940563   -0.019534      0.0481319   0.0827637    0.0808714    0.0858173     0.0155991 
 -0.122551    -0.17475      0.113461    0.048657     -0.00203812   0.0253949    0.00791785  -0.0678506   -0.0992482   -0.0785965    -0.014461     0.0220676    0.221398     0.00897162   0.149087    -0.0174469    0.0230572     0.0745596  -0.102129    0.125589     0.0321735     0.0503766  -0.133761     0.0515228   -0.0358547    -0.0218711 
  0.0755927    0.0423289   -0.0623293  -0.135967      0.0132267   -0.0902066    0.0685527   -0.00103727   0.0170653    0.236651      0.0540817    0.0685594   -0.01263      0.134724    -0.252926    -0.0730052    0.0531758    -0.024462   -0.0725272   0.0803376    0.0135155    -0.0318234  -0.0115896    0.11756     -0.0852319    -0.0650499 
  0.0618824   -0.0229348   -0.0289667  -0.095062     -0.0229841   -0.0762678    0.171022    -0.113611     0.0672351    0.13486      -0.0377479   -0.0868166    0.125955     0.19512     -0.145358     0.154795     0.025612     -0.0129324  -0.128134   -0.0406204    0.00434171   -0.0107209  -0.0261993   -0.00822744   0.0324817    -0.142085  
 -0.156913    -0.0103777    0.0918988   0.0640774    -0.0598873    0.00246524  -0.0422426    0.00222677   0.00769343  -0.0772653     0.0760485   -0.114639    -0.156995    -0.0547116    0.0409987   -0.253035     0.0131078    -0.120262   -0.130543    0.010315    -0.158357      0.158992    0.140991     0.225487    -0.0283422    -0.00791423
  0.127179     0.0855768    0.110481    0.0198574     0.20632      0.141739    -0.0207295   -0.0312224    0.0365272    0.191311     -0.0642764   -0.0590859    0.121172    -0.0864095    0.00600827  -0.109873    -0.186658      0.14052    -0.01157     0.035701     0.00916493    0.0240175  -0.163944     0.0393683    0.0146598    -0.0682782 
 -0.129916     0.0167463    0.0719021   0.0083106    -0.0601414   -0.108414    -0.0827891   -0.075152    -0.0189268   -0.0946129    -0.121044    -0.0563716    0.161184     0.0574991   -0.0656216    0.0370234    0.0225867    -0.0939601   0.0106147  -0.0442631   -0.201306     -0.0393601   0.0988665    0.0934813   -0.188233     -0.0311904 
  0.00298507  -0.0652634    0.142915    0.0249165    -0.101407     0.0346544   -0.023758    -0.140062    -0.0819319   -0.21078      -0.0688493    0.0296236    0.137324    -0.106648    -0.176522     0.0336961   -0.276249      0.0143443   0.157778    0.0960451   -0.200022      0.105164    0.0686167   -0.0142009    0.144087     -0.0409677 
 -0.171025     0.0720102    0.0489342   0.0715988     0.151105    -0.0351143    0.272155    -0.0306726   -0.0464422   -0.000640974   0.0431976    0.202589     0.0586697   -0.183768    -0.0282458    0.156592    -0.014904      0.0152834  -0.115714   -0.17634     -0.0966735     0.0915137   0.00279398  -0.184529     0.146478     -0.155024  
 -0.180944    -0.245671     0.11808    -0.128248     -0.238838     0.0422799    0.122281     0.0829829   -0.0670799   -0.102056     -0.0741087    0.0577913    0.00427881  -0.106034    -0.0882533    0.194872    -0.0873122     0.024355   -0.0541242   0.0510712   -0.113004     -0.0217181   0.100372    -0.0291029    0.0653873     0.00656911
  0.0189338   -0.122031     0.0762054  -0.149178      0.035913     0.141133     0.20301      0.0157278   -0.141024    -0.20167      -0.209424    -0.15046      0.0645499   -0.0360357   -0.187348     0.062677    -0.086496     -0.16615     0.0558651   0.130209    -0.124735      0.0113697  -0.0952029    0.0110028    0.00914335    0.00550671
  0.046049    -0.118582     0.0731268  -0.0425309    -0.119779     0.230046     0.0599136   -0.126127     0.0187891    0.0580672    -0.0972457    0.0997684    0.0646038    0.0137941    0.109061     0.151011     0.0576643     0.0842417  -0.0024856   0.0391857    0.00468897    0.087977    0.10259     -0.0337724   -0.000147834  -0.0443317 
  0.0976002   -0.0716708   -0.0649474  -0.0966519     0.0660293    0.158971     0.0432245   -0.0786153   -0.085081    -0.313363      0.219604    -0.0328293   -0.0367326    0.0624326    0.16439      0.151217     0.19551      -0.0692021  -0.0613128   0.0706831    0.170922      0.0561854   0.0481519    0.0474033    0.0648843    -0.191069  
 -0.00635705  -0.0246262    0.101755    0.0336372     0.00688182  -0.267371    -0.0299455    0.170786    -0.0660349   -0.138269      0.0358501   -0.125077    -0.0431051    0.0844782    0.166254     0.0084476    0.12402       0.0821352   0.175361    0.011502    -0.166123      0.0980912   0.178694     0.248081    -0.108064     -0.0871302 
 -0.0325862    0.00304519   0.0504458   0.0647581    -0.0868934    0.00454438  -0.113492    -0.0143719    0.00939465  -0.0799819     0.0693224   -0.0457875   -0.0949363    0.0881354   -0.00374728  -0.00510588  -0.182862      0.081229    0.0945886  -0.0200021    0.187343      0.126618    0.144886     0.0144859    0.0967911    -0.0644714 
 -0.0847314    0.0253305   -0.0504313   0.068305      0.00111968   0.0996931   -0.0909511   -0.00565778   0.106015     0.205878     -0.0173839    0.0812109    0.0740403   -0.0929277    0.050898     0.106421     0.135699     -0.0388307   0.114734   -0.0366417    0.000422608  -0.212082    0.0222459   -0.0757894    0.0224131    -0.0446063 
  0.0670402    0.0107728    0.106991   -0.0923491     0.183395    -0.020753    -0.0448263   -0.0155283    0.095517     0.0496617     0.00223247   0.0873555    0.15319     -0.00944444   0.115779    -0.0502301    0.00748038   -0.0396277  -0.113586    0.0412853    0.0662114     0.139951   -0.147515     0.00858266   0.0903828    -0.0871046 
 -0.0412628    0.0232507   -0.154537   -0.000147196   0.215175    -0.0574791    0.0299447    0.0560194   -0.0788699    0.0389601     0.158073    -0.0203262   -0.0400035   -0.118994     0.172935    -0.0745564   -0.0972012    -0.0252504   0.0215166  -0.0959976    0.0416933     0.084942    0.00521254   0.0936372   -0.0749503    -0.0773082 
 -0.102047    -0.176483    -0.0919869   0.261659      0.0760574   -0.112408    -0.0648917   -0.0104475    0.00244688  -0.0999322     0.113335    -0.0917238    0.0838      -0.0677458    0.00639151   0.12882     -0.115472      0.104506    0.0665829  -0.149723     0.00392001   -0.0926639   0.0881798   -0.094446     0.0866076     0.0774368 
  0.025495     0.122779     0.0936419  -0.105         0.00614324   0.113255    -0.0823844   -0.197299    -0.00114875   0.0913484     0.048176     0.152587     0.210253    -0.145568    -0.0288197    0.0316842    0.0216133    -0.0134375   0.067804   -0.0490109   -0.013099     -0.0433896   0.063871    -0.0863373    0.000614846  -0.173853  kind full, method split
0: avll = -1.4300343559975852
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.430054
INFO: iteration 2, average log likelihood -1.429959
INFO: iteration 3, average log likelihood -1.429880
INFO: iteration 4, average log likelihood -1.429789
INFO: iteration 5, average log likelihood -1.429681
INFO: iteration 6, average log likelihood -1.429561
INFO: iteration 7, average log likelihood -1.429433
INFO: iteration 8, average log likelihood -1.429296
INFO: iteration 9, average log likelihood -1.429124
INFO: iteration 10, average log likelihood -1.428864
INFO: iteration 11, average log likelihood -1.428431
INFO: iteration 12, average log likelihood -1.427748
INFO: iteration 13, average log likelihood -1.426839
INFO: iteration 14, average log likelihood -1.425925
INFO: iteration 15, average log likelihood -1.425268
INFO: iteration 16, average log likelihood -1.424915
INFO: iteration 17, average log likelihood -1.424754
INFO: iteration 18, average log likelihood -1.424686
INFO: iteration 19, average log likelihood -1.424658
INFO: iteration 20, average log likelihood -1.424646
INFO: iteration 21, average log likelihood -1.424640
INFO: iteration 22, average log likelihood -1.424638
INFO: iteration 23, average log likelihood -1.424636
INFO: iteration 24, average log likelihood -1.424636
INFO: iteration 25, average log likelihood -1.424635
INFO: iteration 26, average log likelihood -1.424635
INFO: iteration 27, average log likelihood -1.424634
INFO: iteration 28, average log likelihood -1.424634
INFO: iteration 29, average log likelihood -1.424634
INFO: iteration 30, average log likelihood -1.424633
INFO: iteration 31, average log likelihood -1.424633
INFO: iteration 32, average log likelihood -1.424633
INFO: iteration 33, average log likelihood -1.424633
INFO: iteration 34, average log likelihood -1.424633
INFO: iteration 35, average log likelihood -1.424633
INFO: iteration 36, average log likelihood -1.424632
INFO: iteration 37, average log likelihood -1.424632
INFO: iteration 38, average log likelihood -1.424632
INFO: iteration 39, average log likelihood -1.424632
INFO: iteration 40, average log likelihood -1.424632
INFO: iteration 41, average log likelihood -1.424632
INFO: iteration 42, average log likelihood -1.424632
INFO: iteration 43, average log likelihood -1.424632
INFO: iteration 44, average log likelihood -1.424632
INFO: iteration 45, average log likelihood -1.424632
INFO: iteration 46, average log likelihood -1.424632
INFO: iteration 47, average log likelihood -1.424632
INFO: iteration 48, average log likelihood -1.424632
INFO: iteration 49, average log likelihood -1.424632
INFO: iteration 50, average log likelihood -1.424632
INFO: EM with 100000 data points 50 iterations avll -1.424632
952.4 data points per parameter
1: avll = [-1.43005,-1.42996,-1.42988,-1.42979,-1.42968,-1.42956,-1.42943,-1.4293,-1.42912,-1.42886,-1.42843,-1.42775,-1.42684,-1.42592,-1.42527,-1.42491,-1.42475,-1.42469,-1.42466,-1.42465,-1.42464,-1.42464,-1.42464,-1.42464,-1.42464,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.424651
INFO: iteration 2, average log likelihood -1.424553
INFO: iteration 3, average log likelihood -1.424471
INFO: iteration 4, average log likelihood -1.424376
INFO: iteration 5, average log likelihood -1.424266
INFO: iteration 6, average log likelihood -1.424151
INFO: iteration 7, average log likelihood -1.424044
INFO: iteration 8, average log likelihood -1.423956
INFO: iteration 9, average log likelihood -1.423892
INFO: iteration 10, average log likelihood -1.423846
INFO: iteration 11, average log likelihood -1.423815
INFO: iteration 12, average log likelihood -1.423792
INFO: iteration 13, average log likelihood -1.423775
INFO: iteration 14, average log likelihood -1.423760
INFO: iteration 15, average log likelihood -1.423748
INFO: iteration 16, average log likelihood -1.423736
INFO: iteration 17, average log likelihood -1.423725
INFO: iteration 18, average log likelihood -1.423713
INFO: iteration 19, average log likelihood -1.423701
INFO: iteration 20, average log likelihood -1.423687
INFO: iteration 21, average log likelihood -1.423674
INFO: iteration 22, average log likelihood -1.423659
INFO: iteration 23, average log likelihood -1.423644
INFO: iteration 24, average log likelihood -1.423629
INFO: iteration 25, average log likelihood -1.423614
INFO: iteration 26, average log likelihood -1.423599
INFO: iteration 27, average log likelihood -1.423586
INFO: iteration 28, average log likelihood -1.423573
INFO: iteration 29, average log likelihood -1.423562
INFO: iteration 30, average log likelihood -1.423553
INFO: iteration 31, average log likelihood -1.423544
INFO: iteration 32, average log likelihood -1.423537
INFO: iteration 33, average log likelihood -1.423532
INFO: iteration 34, average log likelihood -1.423527
INFO: iteration 35, average log likelihood -1.423523
INFO: iteration 36, average log likelihood -1.423520
INFO: iteration 37, average log likelihood -1.423518
INFO: iteration 38, average log likelihood -1.423516
INFO: iteration 39, average log likelihood -1.423514
INFO: iteration 40, average log likelihood -1.423513
INFO: iteration 41, average log likelihood -1.423511
INFO: iteration 42, average log likelihood -1.423510
INFO: iteration 43, average log likelihood -1.423510
INFO: iteration 44, average log likelihood -1.423509
INFO: iteration 45, average log likelihood -1.423508
INFO: iteration 46, average log likelihood -1.423508
INFO: iteration 47, average log likelihood -1.423507
INFO: iteration 48, average log likelihood -1.423507
INFO: iteration 49, average log likelihood -1.423507
INFO: iteration 50, average log likelihood -1.423506
INFO: EM with 100000 data points 50 iterations avll -1.423506
473.9 data points per parameter
2: avll = [-1.42465,-1.42455,-1.42447,-1.42438,-1.42427,-1.42415,-1.42404,-1.42396,-1.42389,-1.42385,-1.42381,-1.42379,-1.42377,-1.42376,-1.42375,-1.42374,-1.42372,-1.42371,-1.4237,-1.42369,-1.42367,-1.42366,-1.42364,-1.42363,-1.42361,-1.4236,-1.42359,-1.42357,-1.42356,-1.42355,-1.42354,-1.42354,-1.42353,-1.42353,-1.42352,-1.42352,-1.42352,-1.42352,-1.42351,-1.42351,-1.42351,-1.42351,-1.42351,-1.42351,-1.42351,-1.42351,-1.42351,-1.42351,-1.42351,-1.42351]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.423518
INFO: iteration 2, average log likelihood -1.423458
INFO: iteration 3, average log likelihood -1.423407
INFO: iteration 4, average log likelihood -1.423350
INFO: iteration 5, average log likelihood -1.423285
INFO: iteration 6, average log likelihood -1.423209
INFO: iteration 7, average log likelihood -1.423125
INFO: iteration 8, average log likelihood -1.423036
INFO: iteration 9, average log likelihood -1.422947
INFO: iteration 10, average log likelihood -1.422862
INFO: iteration 11, average log likelihood -1.422786
INFO: iteration 12, average log likelihood -1.422719
INFO: iteration 13, average log likelihood -1.422661
INFO: iteration 14, average log likelihood -1.422614
INFO: iteration 15, average log likelihood -1.422574
INFO: iteration 16, average log likelihood -1.422542
INFO: iteration 17, average log likelihood -1.422515
INFO: iteration 18, average log likelihood -1.422493
INFO: iteration 19, average log likelihood -1.422475
INFO: iteration 20, average log likelihood -1.422458
INFO: iteration 21, average log likelihood -1.422444
INFO: iteration 22, average log likelihood -1.422431
INFO: iteration 23, average log likelihood -1.422418
INFO: iteration 24, average log likelihood -1.422407
INFO: iteration 25, average log likelihood -1.422395
INFO: iteration 26, average log likelihood -1.422384
INFO: iteration 27, average log likelihood -1.422373
INFO: iteration 28, average log likelihood -1.422363
INFO: iteration 29, average log likelihood -1.422352
INFO: iteration 30, average log likelihood -1.422341
INFO: iteration 31, average log likelihood -1.422330
INFO: iteration 32, average log likelihood -1.422319
INFO: iteration 33, average log likelihood -1.422309
INFO: iteration 34, average log likelihood -1.422298
INFO: iteration 35, average log likelihood -1.422288
INFO: iteration 36, average log likelihood -1.422278
INFO: iteration 37, average log likelihood -1.422268
INFO: iteration 38, average log likelihood -1.422258
INFO: iteration 39, average log likelihood -1.422248
INFO: iteration 40, average log likelihood -1.422239
INFO: iteration 41, average log likelihood -1.422230
INFO: iteration 42, average log likelihood -1.422221
INFO: iteration 43, average log likelihood -1.422212
INFO: iteration 44, average log likelihood -1.422204
INFO: iteration 45, average log likelihood -1.422196
INFO: iteration 46, average log likelihood -1.422189
INFO: iteration 47, average log likelihood -1.422182
INFO: iteration 48, average log likelihood -1.422175
INFO: iteration 49, average log likelihood -1.422168
INFO: iteration 50, average log likelihood -1.422162
INFO: EM with 100000 data points 50 iterations avll -1.422162
236.4 data points per parameter
3: avll = [-1.42352,-1.42346,-1.42341,-1.42335,-1.42328,-1.42321,-1.42312,-1.42304,-1.42295,-1.42286,-1.42279,-1.42272,-1.42266,-1.42261,-1.42257,-1.42254,-1.42252,-1.42249,-1.42247,-1.42246,-1.42244,-1.42243,-1.42242,-1.42241,-1.4224,-1.42238,-1.42237,-1.42236,-1.42235,-1.42234,-1.42233,-1.42232,-1.42231,-1.4223,-1.42229,-1.42228,-1.42227,-1.42226,-1.42225,-1.42224,-1.42223,-1.42222,-1.42221,-1.4222,-1.4222,-1.42219,-1.42218,-1.42217,-1.42217,-1.42216]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.422165
INFO: iteration 2, average log likelihood -1.422112
INFO: iteration 3, average log likelihood -1.422065
INFO: iteration 4, average log likelihood -1.422013
INFO: iteration 5, average log likelihood -1.421951
INFO: iteration 6, average log likelihood -1.421876
INFO: iteration 7, average log likelihood -1.421789
INFO: iteration 8, average log likelihood -1.421691
INFO: iteration 9, average log likelihood -1.421586
INFO: iteration 10, average log likelihood -1.421478
INFO: iteration 11, average log likelihood -1.421371
INFO: iteration 12, average log likelihood -1.421268
INFO: iteration 13, average log likelihood -1.421171
INFO: iteration 14, average log likelihood -1.421083
INFO: iteration 15, average log likelihood -1.421004
INFO: iteration 16, average log likelihood -1.420934
INFO: iteration 17, average log likelihood -1.420872
INFO: iteration 18, average log likelihood -1.420817
INFO: iteration 19, average log likelihood -1.420769
INFO: iteration 20, average log likelihood -1.420725
INFO: iteration 21, average log likelihood -1.420686
INFO: iteration 22, average log likelihood -1.420650
INFO: iteration 23, average log likelihood -1.420616
INFO: iteration 24, average log likelihood -1.420584
INFO: iteration 25, average log likelihood -1.420554
INFO: iteration 26, average log likelihood -1.420526
INFO: iteration 27, average log likelihood -1.420499
INFO: iteration 28, average log likelihood -1.420473
INFO: iteration 29, average log likelihood -1.420447
INFO: iteration 30, average log likelihood -1.420423
INFO: iteration 31, average log likelihood -1.420400
INFO: iteration 32, average log likelihood -1.420377
INFO: iteration 33, average log likelihood -1.420355
INFO: iteration 34, average log likelihood -1.420334
INFO: iteration 35, average log likelihood -1.420314
INFO: iteration 36, average log likelihood -1.420295
INFO: iteration 37, average log likelihood -1.420276
INFO: iteration 38, average log likelihood -1.420258
INFO: iteration 39, average log likelihood -1.420241
INFO: iteration 40, average log likelihood -1.420224
INFO: iteration 41, average log likelihood -1.420208
INFO: iteration 42, average log likelihood -1.420192
INFO: iteration 43, average log likelihood -1.420178
INFO: iteration 44, average log likelihood -1.420163
INFO: iteration 45, average log likelihood -1.420150
INFO: iteration 46, average log likelihood -1.420137
INFO: iteration 47, average log likelihood -1.420124
INFO: iteration 48, average log likelihood -1.420112
INFO: iteration 49, average log likelihood -1.420101
INFO: iteration 50, average log likelihood -1.420090
INFO: EM with 100000 data points 50 iterations avll -1.420090
118.1 data points per parameter
4: avll = [-1.42216,-1.42211,-1.42206,-1.42201,-1.42195,-1.42188,-1.42179,-1.42169,-1.42159,-1.42148,-1.42137,-1.42127,-1.42117,-1.42108,-1.421,-1.42093,-1.42087,-1.42082,-1.42077,-1.42073,-1.42069,-1.42065,-1.42062,-1.42058,-1.42055,-1.42053,-1.4205,-1.42047,-1.42045,-1.42042,-1.4204,-1.42038,-1.42036,-1.42033,-1.42031,-1.42029,-1.42028,-1.42026,-1.42024,-1.42022,-1.42021,-1.42019,-1.42018,-1.42016,-1.42015,-1.42014,-1.42012,-1.42011,-1.4201,-1.42009]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.420088
INFO: iteration 2, average log likelihood -1.420021
INFO: iteration 3, average log likelihood -1.419958
INFO: iteration 4, average log likelihood -1.419886
INFO: iteration 5, average log likelihood -1.419800
INFO: iteration 6, average log likelihood -1.419694
INFO: iteration 7, average log likelihood -1.419570
INFO: iteration 8, average log likelihood -1.419428
INFO: iteration 9, average log likelihood -1.419274
INFO: iteration 10, average log likelihood -1.419115
INFO: iteration 11, average log likelihood -1.418960
INFO: iteration 12, average log likelihood -1.418814
INFO: iteration 13, average log likelihood -1.418683
INFO: iteration 14, average log likelihood -1.418568
INFO: iteration 15, average log likelihood -1.418469
INFO: iteration 16, average log likelihood -1.418386
INFO: iteration 17, average log likelihood -1.418316
INFO: iteration 18, average log likelihood -1.418257
INFO: iteration 19, average log likelihood -1.418206
INFO: iteration 20, average log likelihood -1.418163
INFO: iteration 21, average log likelihood -1.418125
INFO: iteration 22, average log likelihood -1.418091
INFO: iteration 23, average log likelihood -1.418061
INFO: iteration 24, average log likelihood -1.418034
INFO: iteration 25, average log likelihood -1.418009
INFO: iteration 26, average log likelihood -1.417987
INFO: iteration 27, average log likelihood -1.417966
INFO: iteration 28, average log likelihood -1.417947
INFO: iteration 29, average log likelihood -1.417929
INFO: iteration 30, average log likelihood -1.417912
INFO: iteration 31, average log likelihood -1.417896
INFO: iteration 32, average log likelihood -1.417881
INFO: iteration 33, average log likelihood -1.417866
INFO: iteration 34, average log likelihood -1.417852
INFO: iteration 35, average log likelihood -1.417839
INFO: iteration 36, average log likelihood -1.417826
INFO: iteration 37, average log likelihood -1.417813
INFO: iteration 38, average log likelihood -1.417801
INFO: iteration 39, average log likelihood -1.417789
INFO: iteration 40, average log likelihood -1.417778
INFO: iteration 41, average log likelihood -1.417766
INFO: iteration 42, average log likelihood -1.417755
INFO: iteration 43, average log likelihood -1.417745
INFO: iteration 44, average log likelihood -1.417734
INFO: iteration 45, average log likelihood -1.417724
INFO: iteration 46, average log likelihood -1.417714
INFO: iteration 47, average log likelihood -1.417705
INFO: iteration 48, average log likelihood -1.417695
INFO: iteration 49, average log likelihood -1.417687
INFO: iteration 50, average log likelihood -1.417678
INFO: EM with 100000 data points 50 iterations avll -1.417678
59.0 data points per parameter
5: avll = [-1.42009,-1.42002,-1.41996,-1.41989,-1.4198,-1.41969,-1.41957,-1.41943,-1.41927,-1.41912,-1.41896,-1.41881,-1.41868,-1.41857,-1.41847,-1.41839,-1.41832,-1.41826,-1.41821,-1.41816,-1.41812,-1.41809,-1.41806,-1.41803,-1.41801,-1.41799,-1.41797,-1.41795,-1.41793,-1.41791,-1.4179,-1.41788,-1.41787,-1.41785,-1.41784,-1.41783,-1.41781,-1.4178,-1.41779,-1.41778,-1.41777,-1.41776,-1.41774,-1.41773,-1.41772,-1.41771,-1.4177,-1.4177,-1.41769,-1.41768]
[-1.43003,-1.43005,-1.42996,-1.42988,-1.42979,-1.42968,-1.42956,-1.42943,-1.4293,-1.42912,-1.42886,-1.42843,-1.42775,-1.42684,-1.42592,-1.42527,-1.42491,-1.42475,-1.42469,-1.42466,-1.42465,-1.42464,-1.42464,-1.42464,-1.42464,-1.42464,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42463,-1.42465,-1.42455,-1.42447,-1.42438,-1.42427,-1.42415,-1.42404,-1.42396,-1.42389,-1.42385,-1.42381,-1.42379,-1.42377,-1.42376,-1.42375,-1.42374,-1.42372,-1.42371,-1.4237,-1.42369,-1.42367,-1.42366,-1.42364,-1.42363,-1.42361,-1.4236,-1.42359,-1.42357,-1.42356,-1.42355,-1.42354,-1.42354,-1.42353,-1.42353,-1.42352,-1.42352,-1.42352,-1.42352,-1.42351,-1.42351,-1.42351,-1.42351,-1.42351,-1.42351,-1.42351,-1.42351,-1.42351,-1.42351,-1.42351,-1.42351,-1.42352,-1.42346,-1.42341,-1.42335,-1.42328,-1.42321,-1.42312,-1.42304,-1.42295,-1.42286,-1.42279,-1.42272,-1.42266,-1.42261,-1.42257,-1.42254,-1.42252,-1.42249,-1.42247,-1.42246,-1.42244,-1.42243,-1.42242,-1.42241,-1.4224,-1.42238,-1.42237,-1.42236,-1.42235,-1.42234,-1.42233,-1.42232,-1.42231,-1.4223,-1.42229,-1.42228,-1.42227,-1.42226,-1.42225,-1.42224,-1.42223,-1.42222,-1.42221,-1.4222,-1.4222,-1.42219,-1.42218,-1.42217,-1.42217,-1.42216,-1.42216,-1.42211,-1.42206,-1.42201,-1.42195,-1.42188,-1.42179,-1.42169,-1.42159,-1.42148,-1.42137,-1.42127,-1.42117,-1.42108,-1.421,-1.42093,-1.42087,-1.42082,-1.42077,-1.42073,-1.42069,-1.42065,-1.42062,-1.42058,-1.42055,-1.42053,-1.4205,-1.42047,-1.42045,-1.42042,-1.4204,-1.42038,-1.42036,-1.42033,-1.42031,-1.42029,-1.42028,-1.42026,-1.42024,-1.42022,-1.42021,-1.42019,-1.42018,-1.42016,-1.42015,-1.42014,-1.42012,-1.42011,-1.4201,-1.42009,-1.42009,-1.42002,-1.41996,-1.41989,-1.4198,-1.41969,-1.41957,-1.41943,-1.41927,-1.41912,-1.41896,-1.41881,-1.41868,-1.41857,-1.41847,-1.41839,-1.41832,-1.41826,-1.41821,-1.41816,-1.41812,-1.41809,-1.41806,-1.41803,-1.41801,-1.41799,-1.41797,-1.41795,-1.41793,-1.41791,-1.4179,-1.41788,-1.41787,-1.41785,-1.41784,-1.41783,-1.41781,-1.4178,-1.41779,-1.41778,-1.41777,-1.41776,-1.41774,-1.41773,-1.41772,-1.41771,-1.4177,-1.4177,-1.41769,-1.41768]
32×26 Array{Float64,2}:
  0.257573    0.143796    0.0394635   -0.167427     0.184645    -0.0174863  -0.0597252  -0.134308     0.205712    -0.251902   -0.0811726  -0.168941    -0.812464   -0.648451   -0.397604     0.352395    0.402213    -0.75245    -0.294172      0.101659     0.150509      0.0930655   0.410165    0.0238767  -0.0938927   -0.0632361
 -0.240753    0.0667817  -0.436806    -0.18986      0.440234    -0.238797    0.344224   -0.112826    -0.380229     0.172677   -0.180284    0.171938    -0.28064    -0.616807   -0.503792     0.100558    0.0864039    0.2561     -0.778515      0.481594     0.568573     -0.404187    0.103341   -0.259569    0.663144    -0.132207 
 -0.293907    0.0398175  -0.0409461   -0.556241     0.0118015   -0.230341   -0.173917    0.331973    -0.167753     0.271591    0.165811   -0.225964    -0.287458    0.386782   -0.135883    -0.0164253  -0.0926804    0.234961   -0.642595      0.323338     0.0732831    -0.276138    0.291956    0.0296914  -0.782203     0.240894 
 -0.425763   -0.625448    0.524        0.214545    -0.0423424   -0.10014    -0.020298    0.212249    -0.0591562    0.319393    0.45151     0.0411607   -0.380418    0.286648   -0.488189    -0.179808    0.525779     0.0199853  -0.0180638     0.64904      0.209156      0.149423   -0.047599   -0.202704   -0.135694     0.171832 
 -0.0289071  -0.625352   -0.155004     0.0473343    0.205197    -0.200329   -0.820213   -0.176672     0.282378    -0.623658    0.162284   -0.247049    -0.211218   -0.270408    0.163617    -0.0647726   0.763129     0.363766   -0.0387509     0.00737487   0.127172     -0.144317    0.350894    0.257189   -0.218424    -0.213296 
 -0.16107     0.379855    0.546739     0.030019    -0.0493566   -0.0151221  -0.565986   -0.00449699   0.112062    -0.569659   -0.147388   -0.43213     -0.593151    0.271534    0.0547637    0.026769    0.402306    -0.048033   -0.0279546    -0.0522893    0.067638     -0.287669   -0.589912    0.0253158   0.153599     0.154251 
  0.583357    0.0217805  -0.0920389   -0.0860686   -0.589951     0.168396   -1.00151    -0.216604    -0.00571238   0.141802    0.339501   -0.255842    -0.0413091  -0.246218   -0.203473     0.68264    -0.230868     0.361861    0.000243431  -0.280701     0.570758      0.186955   -0.0637226   0.650883    0.632538    -0.0790032
 -0.20966     0.215091   -0.0291916   -0.0304106    0.158674    -0.34466    -0.542357   -0.527403     0.41806      0.395       0.236253    0.267424     0.447084   -0.076831   -0.0325189    0.394272    0.145683     0.303471   -0.375772     -0.571312    -0.217467      0.147791    0.0734286   0.452392    0.148279     0.119008 
  0.573771   -0.0309287   0.00633371  -0.272636     0.129594     0.025581    0.564201    0.602393    -0.0354945    0.287016   -0.243269    0.345306     0.142942   -0.151997   -0.060168     0.0199333   0.053        0.253947    0.0790464     0.138959    -0.325386      0.241279    0.23095    -0.779984    0.0611591    0.31239  
 -0.0356739   0.287984   -0.552489     0.108011     0.0191813    0.278863    0.282991    0.296511    -0.050159     0.408272   -0.3471      0.351619     0.29488    -0.476701    0.32872      0.336869   -0.0098696    0.662017    0.462298      0.0948455   -0.586963     -0.0353388   0.114743    0.513083    0.406533     0.221377 
  0.25209     0.103805   -0.125065    -0.254851     0.450233    -0.221338    0.625001    0.184894     0.471792     0.239552    0.0665301   0.00236648   0.084281   -1.18503     1.07993     -0.362064   -0.319759    -0.183357    0.170349     -0.799962    -0.409155      0.311629    0.296572   -0.178693   -0.518383    -0.424662 
 -0.102339   -0.24641    -0.202234    -0.126008     0.40427      0.403387   -0.165172   -0.100854    -0.132817     0.49436     0.043372    0.301117     0.575472    0.167241    0.804193    -0.0201152  -0.053955     0.190902    0.170298      0.177896    -0.183799     -0.293069    0.186101   -0.442277   -0.0974639    0.158386 
 -0.127024    0.227385    0.415948    -0.486067     0.065378     0.263032    0.149244    0.125071    -0.256985    -0.570637   -0.290089   -0.577689     0.425597    0.250166    0.404575    -0.0998633  -0.481979     0.0807957  -0.137946     -0.141938     0.35292       0.23577    -0.259949   -0.141242   -0.159859    -0.131304 
 -0.570379    0.272093    0.540306    -0.207008    -0.134782    -0.0737331   0.431426    0.0189042   -0.0606537    0.461586   -0.179592    0.0163353    0.387901    0.294078   -0.0443493    0.027534   -0.164639     0.262034   -0.772893     -0.293133     0.13795       0.253088   -0.333581    0.0116914   0.335505     0.0277103
 -0.052441   -0.0264906  -0.119416    -0.0792958    0.0603831    0.117941    0.170188   -0.0600685   -0.025597    -0.138208    0.0328674   0.0908903    0.105582   -0.252464    0.0497017   -0.0891449   0.0951045   -0.129111    0.0819296     0.0504085   -0.01035      -0.102124    0.0190212  -0.0828736   0.0713655   -0.115906 
  0.0376359   0.0632828   0.116795     0.176965    -0.105496    -0.0651323  -0.122092    0.0499567   -0.035741     0.178401    0.0454836  -0.130767    -0.0779619   0.276185   -0.00747381   0.1336     -0.182402     0.0176757   0.00148078   -0.0875668   -0.0749179     0.176889    0.01064     0.116428   -0.00687846  -0.0204044
 -0.537427   -0.483515   -0.274967     0.522371    -0.00951644   0.687585   -0.72278     0.378753     0.138493     0.153189   -0.565434   -0.337527    -0.421152    0.044078   -0.0624532    0.0531446   0.119071     0.498648   -0.442977      0.00051219  -0.339776     -0.462156   -0.0369527   0.11153    -0.758911    -0.198659 
  0.0949566   0.719145   -0.558413     0.499345    -0.286764     0.368304    0.155036    0.278218    -0.105441     0.483032   -0.0928999  -0.784557    -0.448268   -0.36085     0.312686    -0.102031   -0.84163      0.308792   -0.368499     -0.459412    -0.586291     -0.219799   -0.443009   -0.289611   -1.45328e-5  -0.382435 
  0.0541825   0.254846   -0.197589     0.015322     0.222142     0.0957502   0.201383    0.827337     0.1283      -0.318365   -0.241631   -0.496299    -0.0979784   0.0161217  -0.502516    -0.269602   -0.593483     0.0109745   0.23627       0.412646     0.278274      0.0837004  -0.106441    0.147422    0.0427003    0.354119 
  0.0128169   0.274429   -0.463702     0.807232    -0.289598    -0.657457    0.0859413   0.281158     0.0365086    0.29311     0.0460005  -0.610901     0.117309   -0.12618    -0.0505538   -0.37668    -0.810407    -0.24497     0.0521315    -0.0255553    0.566609      0.0357074   0.129798    0.116544    0.435594     0.158455 
  0.242114   -0.139966    0.724647     0.0963479    0.497913    -0.177305   -0.481723   -0.0307443   -0.0346688   -0.63194     0.153435   -0.748058    -0.282038    0.332185    0.328931    -0.324531   -0.0703135   -0.182658   -0.132506      0.517537     0.349929     -0.255449   -0.0547925  -0.670232   -0.161724     0.391299 
 -0.345743   -0.355285    0.363909    -0.10316     -0.194907    -0.310008   -0.370644   -0.131681    -0.0308687   -0.484947    0.169349   -0.569076    -0.153181    0.299599   -0.391339    -0.0759328   0.152838    -0.0188471  -0.256677      0.0242579    0.365731      0.353267   -0.199334    0.539007   -0.0620958   -0.352287 
  0.315075    0.094649    0.411722     0.232793    -0.257075     0.236082   -0.175642    0.0767032    0.195992    -0.150448    0.0421963  -0.103493     0.0500949   0.560922    0.626273    -0.143405    0.0421839   -0.339592    0.794909     -0.393955    -0.822507      0.553442   -0.188451    0.35246    -0.289926     0.0816569
 -0.189681   -0.222199    0.636989     0.225313    -0.16156     -0.66089    -0.158767   -0.400418     0.0457231   -0.213518    0.743373   -0.202333     0.307972    0.341772    0.584926    -0.435689   -0.167217    -0.445681    0.0861565    -0.34716      0.268433      0.356506   -0.0691437  -0.0591739  -0.00528997   0.160081 
 -0.0228089  -0.150342   -0.114071     0.0696512   -0.289671     0.737276    0.0816481  -0.68794     -0.377779     0.0524178   0.0870369  -0.089518     0.108841    0.206471   -0.156675     0.0577041  -0.00973386  -0.205652    0.164459      0.127321    -0.0903236    -0.535756    0.430864    0.189927   -0.290077    -1.09371  
 -0.0920084  -0.244115   -0.0202996   -0.0319952    0.788449    -0.068949    0.579443   -0.24638     -0.510007    -0.537929    0.174278    0.521289     0.0816516   0.085778   -0.113693    -0.192027    0.174025    -0.38976     0.655435      0.337921    -0.000408621  -0.190636   -0.0273539  -0.316704   -0.0936421   -0.441869 
  0.279453   -0.0828595   0.0443146   -0.0944928   -0.232326    -0.0509621   0.41181    -0.0123461    0.430982    -0.833      -0.166092    0.338373     0.620761   -0.404168   -0.1125       0.196391    0.278243    -0.163584    0.626994     -0.32122      0.0492404     0.324641   -0.277339    0.156109    0.607141    -0.27264  
  0.27859    -0.493088   -0.256262     0.00916423  -0.511547    -0.296969    0.304061   -0.531111     0.117818     0.374108   -0.22509     0.584102     0.0328331   0.183383   -0.614224     0.388719    0.0116015   -0.342039    0.24363      -0.14902     -0.30242       0.477086    0.0373486   0.0891753   0.319306    -0.465477 
 -0.0263807   0.409279   -0.252152    -0.358794     0.0505136    0.100554   -0.656735    0.083347     0.438809     0.121456   -0.457532    0.21458      0.12177    -0.325596    0.0853628    0.332798    0.295961     0.421713   -0.25997      -0.154824    -0.15248      -0.0419604  -0.0850115   0.383844    0.108599     0.356737 
  0.0835873   0.165238   -0.0673927    0.471288     0.0102834    0.168616   -0.447321   -0.416275     0.10156      0.494303    1.34725     0.567837    -0.341538   -0.488886    0.055377     0.243434    0.637269    -0.0232127   0.190552     -0.375891    -0.211943     -0.543153    0.284697   -0.247152   -0.196447     0.0353494
  0.103229   -0.273247   -0.260593    -0.0550282   -3.0998e-5    0.19178     0.77251     0.508878     0.0727607    0.0398019  -0.100623    0.118358     0.238486   -0.0732453  -0.478797    -0.140109   -0.423037     0.030837    0.323947      0.414024    -0.129953      0.0106042   0.254012   -0.411942   -0.0416152    0.117179 
  0.103881    0.444272    0.294006    -0.300767     0.0778385   -0.196144    0.652412   -0.518857    -0.403133     0.420947    0.161025    0.870034     0.304438    0.0604574   0.57752      0.193272    0.0337959   -0.29889     0.199938     -0.170164    -0.16088       0.341822   -0.231125   -0.141658    0.487195    -0.105412 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.417670
INFO: iteration 2, average log likelihood -1.417662
INFO: iteration 3, average log likelihood -1.417654
INFO: iteration 4, average log likelihood -1.417647
INFO: iteration 5, average log likelihood -1.417640
INFO: iteration 6, average log likelihood -1.417633
INFO: iteration 7, average log likelihood -1.417626
INFO: iteration 8, average log likelihood -1.417620
INFO: iteration 9, average log likelihood -1.417614
INFO: iteration 10, average log likelihood -1.417608
INFO: EM with 100000 data points 10 iterations avll -1.417608
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.169590e+05
      1       7.142755e+05      -2.026835e+05 |       32
      2       7.008565e+05      -1.341909e+04 |       32
      3       6.954955e+05      -5.360920e+03 |       32
      4       6.927458e+05      -2.749779e+03 |       32
      5       6.910639e+05      -1.681812e+03 |       32
      6       6.898150e+05      -1.248951e+03 |       32
      7       6.888329e+05      -9.821120e+02 |       32
      8       6.880711e+05      -7.617962e+02 |       32
      9       6.874597e+05      -6.114187e+02 |       32
     10       6.869450e+05      -5.146530e+02 |       32
     11       6.865139e+05      -4.310949e+02 |       32
     12       6.861473e+05      -3.666643e+02 |       32
     13       6.858412e+05      -3.060421e+02 |       32
     14       6.855540e+05      -2.872461e+02 |       32
     15       6.852903e+05      -2.636655e+02 |       32
     16       6.850659e+05      -2.244277e+02 |       32
     17       6.848646e+05      -2.012785e+02 |       32
     18       6.846747e+05      -1.899320e+02 |       32
     19       6.845014e+05      -1.732317e+02 |       32
     20       6.843464e+05      -1.550499e+02 |       32
     21       6.841917e+05      -1.547073e+02 |       32
     22       6.840562e+05      -1.355193e+02 |       32
     23       6.839241e+05      -1.320240e+02 |       32
     24       6.837801e+05      -1.440280e+02 |       32
     25       6.836313e+05      -1.488505e+02 |       32
     26       6.834990e+05      -1.322521e+02 |       32
     27       6.833768e+05      -1.222125e+02 |       32
     28       6.832611e+05      -1.156796e+02 |       32
     29       6.831699e+05      -9.124568e+01 |       32
     30       6.830931e+05      -7.681549e+01 |       32
     31       6.830174e+05      -7.561236e+01 |       32
     32       6.829469e+05      -7.051065e+01 |       32
     33       6.828809e+05      -6.603937e+01 |       32
     34       6.828179e+05      -6.299853e+01 |       32
     35       6.827545e+05      -6.340826e+01 |       32
     36       6.826952e+05      -5.928982e+01 |       32
     37       6.826334e+05      -6.181952e+01 |       32
     38       6.825732e+05      -6.017316e+01 |       32
     39       6.825141e+05      -5.907255e+01 |       32
     40       6.824695e+05      -4.464837e+01 |       32
     41       6.824322e+05      -3.726871e+01 |       32
     42       6.823964e+05      -3.578031e+01 |       32
     43       6.823639e+05      -3.257727e+01 |       32
     44       6.823364e+05      -2.747028e+01 |       32
     45       6.823078e+05      -2.861411e+01 |       32
     46       6.822791e+05      -2.870921e+01 |       32
     47       6.822491e+05      -2.993052e+01 |       32
     48       6.822256e+05      -2.355852e+01 |       32
     49       6.822011e+05      -2.442349e+01 |       32
     50       6.821802e+05      -2.097669e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 682180.170149822)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.429646
INFO: iteration 2, average log likelihood -1.424573
INFO: iteration 3, average log likelihood -1.423177
INFO: iteration 4, average log likelihood -1.422077
INFO: iteration 5, average log likelihood -1.420888
INFO: iteration 6, average log likelihood -1.419837
INFO: iteration 7, average log likelihood -1.419183
INFO: iteration 8, average log likelihood -1.418858
INFO: iteration 9, average log likelihood -1.418689
INFO: iteration 10, average log likelihood -1.418582
INFO: iteration 11, average log likelihood -1.418504
INFO: iteration 12, average log likelihood -1.418440
INFO: iteration 13, average log likelihood -1.418387
INFO: iteration 14, average log likelihood -1.418340
INFO: iteration 15, average log likelihood -1.418298
INFO: iteration 16, average log likelihood -1.418260
INFO: iteration 17, average log likelihood -1.418226
INFO: iteration 18, average log likelihood -1.418195
INFO: iteration 19, average log likelihood -1.418165
INFO: iteration 20, average log likelihood -1.418138
INFO: iteration 21, average log likelihood -1.418113
INFO: iteration 22, average log likelihood -1.418090
INFO: iteration 23, average log likelihood -1.418068
INFO: iteration 24, average log likelihood -1.418047
INFO: iteration 25, average log likelihood -1.418027
INFO: iteration 26, average log likelihood -1.418009
INFO: iteration 27, average log likelihood -1.417991
INFO: iteration 28, average log likelihood -1.417974
INFO: iteration 29, average log likelihood -1.417959
INFO: iteration 30, average log likelihood -1.417943
INFO: iteration 31, average log likelihood -1.417929
INFO: iteration 32, average log likelihood -1.417915
INFO: iteration 33, average log likelihood -1.417901
INFO: iteration 34, average log likelihood -1.417888
INFO: iteration 35, average log likelihood -1.417876
INFO: iteration 36, average log likelihood -1.417864
INFO: iteration 37, average log likelihood -1.417852
INFO: iteration 38, average log likelihood -1.417840
INFO: iteration 39, average log likelihood -1.417829
INFO: iteration 40, average log likelihood -1.417817
INFO: iteration 41, average log likelihood -1.417806
INFO: iteration 42, average log likelihood -1.417795
INFO: iteration 43, average log likelihood -1.417785
INFO: iteration 44, average log likelihood -1.417774
INFO: iteration 45, average log likelihood -1.417763
INFO: iteration 46, average log likelihood -1.417753
INFO: iteration 47, average log likelihood -1.417742
INFO: iteration 48, average log likelihood -1.417732
INFO: iteration 49, average log likelihood -1.417721
INFO: iteration 50, average log likelihood -1.417711
INFO: EM with 100000 data points 50 iterations avll -1.417711
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0623871     0.275216   -0.937187    -0.0028477   0.428444     0.0988169   0.209315     0.408946   -0.0454626   0.483753   -0.398215    0.423277    0.339006    -0.882615    0.364019    0.0830274   0.150559     0.743817     0.318342     0.0795579  -0.358243    -0.207877    0.0674896   0.0685811    0.520548     0.639824 
 -0.178405      0.345575   -0.332776    -0.359599   -0.368469     0.189452   -0.57067     -0.165226    0.46577    -0.246627   -0.461662   -0.024765    0.228988    -0.256509   -0.116471    0.57363     0.121477     0.406688    -0.278398    -0.346082   -0.22871     -0.162668   -0.0857686   1.05391      0.112387    -0.206564 
 -0.49901       0.0242785  -0.148592    -0.482225   -0.133       -0.269554   -0.130267     0.219087   -0.0374784   0.409641    0.029088    0.061298   -0.300073     0.0413998  -0.447947    0.0556823   0.122111     0.475146    -0.922892     0.325642    0.0148638   -0.245968    0.222891    0.0124677   -0.339419     0.289096 
 -0.263703     -0.208187   -0.275505     0.495583    0.106418     0.519421   -0.744207     0.409647    0.297264   -0.0394239  -0.294946   -0.779633   -0.446303     0.130428   -0.113424   -0.273718   -0.0979386    0.305742    -0.288823     0.213725   -0.40393     -0.275964   -0.0868752   0.00925343  -0.633614     0.0595935
  0.376975     -0.400865   -0.507713    -0.337969   -0.0643439    0.395741    0.454913     0.0813212  -0.096478    0.228089   -0.10757     0.358387    0.648094     0.0267264   0.108507    0.258563   -0.421724     0.0552559    0.278671     0.436235   -0.0518873    0.0724545   0.385432   -0.357953     0.0418246   -0.0697474
  0.130292      0.367949    0.118986    -0.0287883  -0.00750412  -0.136913   -0.261183    -0.300584    0.138462    0.451768    0.319967    0.22505     0.00854126  -0.247009    0.293095    0.165403    0.0515126   -0.0879211   -0.300484    -0.305084   -0.0203093    0.106339    0.0690479  -0.0675769    0.135791     0.286602 
 -0.230461     -0.537344    0.854153    -0.171364   -0.302994    -0.345847   -0.310423     0.0621975  -0.0943243  -0.357965    0.556173   -0.402825   -0.297811     0.670616   -0.491881   -0.254653    0.22085     -0.171661    -0.361297     0.0546037   0.548001     0.259185   -0.0031627  -0.0852166   -0.626615    -0.615914 
 -0.401554      0.0921282   0.48877     -0.40354     0.122976     0.186936    0.217903    -0.130967   -0.221467    0.0185414  -0.155711   -0.266357    0.801165     0.622498    0.601186   -0.280664   -0.432871     0.187234    -0.306438    -0.133955    0.0455991    0.351176   -0.267215   -0.0478859   -0.128913     0.0379177
  0.223236      0.21878    -0.196327    -0.055834    0.466265    -0.0787048   0.63871      0.245991    0.601743    0.481485    0.110096   -0.0890597  -0.175576    -1.24001     0.822984   -0.184935   -0.485107    -0.200163     0.00505173  -0.842025   -0.511141     0.220462    0.399215   -0.112728    -0.65955     -0.502835 
  0.127378      0.01816    -0.0891194   -0.0759571   0.603771     0.0725877   0.257108    -0.321257   -0.262451   -0.243885    0.0323626   0.0223117  -0.418513    -0.446306   -0.188074   -0.0954189   0.398579    -0.407263    -0.207265     0.439286    0.145337    -0.378371    0.460799   -0.667402     0.020934    -0.180512 
 -0.165781     -0.0127093  -0.037395     0.366306   -0.51571      0.407028    0.265025    -0.186811   -0.754298    0.854109    0.18465    -0.0294593  -0.28083      0.148337    0.319633    0.0264502  -0.532977     0.156009    -0.116578    -0.182233   -0.311493    -0.424135    0.142396   -0.286252     0.197923    -0.487429 
  0.773323     -0.152449    0.0965415    0.014381   -0.613472    -0.170383   -0.846824    -0.168346    0.069568   -0.0826432   0.709282   -0.19551    -0.361209    -0.21164    -0.198085    0.292224   -0.0816059    0.155071     0.343922    -0.435256    0.574041     0.427921   -0.0396083   0.682692     0.617273     0.0410438
 -0.00736846   -0.218192    0.653866     0.244593    0.212035    -0.346306   -0.482103    -0.361422   -0.0758868  -0.601805    0.524146   -0.738272   -0.158631     0.301212    0.768066   -0.533392   -0.0704868   -0.287388     0.129951     0.280939    0.304769    -0.0623166  -0.317458   -0.339245     0.0986952    0.442566 
 -0.0446094    -0.329473    0.187573     0.113654    0.08402     -0.175708   -0.0463367    0.219248    0.0466972   0.140017    0.2314      0.0717642  -0.227019     0.0613494  -0.243864   -0.0667047   0.316088     0.0307308    0.0136214    0.24002    -0.0334799    0.144185    0.117812   -0.117985    -0.089773     0.242611 
  0.647641     -0.156982    0.549701     0.115401   -0.381819    -0.0955289   0.558718    -0.251497    0.239058   -0.33086    -0.155011    0.132293    0.433261    -0.213245    0.236058    0.0921186   0.22292     -0.248002     0.339008    -0.646329   -0.459641     0.891464   -0.222613   -0.00818435   0.289923    -0.564918 
 -0.0107786    -0.248462    0.120539    -0.0222612   0.517194     0.270028    0.238246    -0.132983   -0.112696   -0.109532    0.188808    0.572699    0.235517     0.214385   -0.0165515   0.10564     0.235255    -0.167146     0.844447     0.358659   -0.353362    -0.101189    0.0884205  -0.282849    -0.0920154   -0.196303 
  0.602469     -0.347076   -0.370517    -0.0217206  -0.322307    -0.298938    0.0831548   -0.318996    0.265236    0.259554   -0.484832    0.515356   -0.221903    -0.0378605  -0.789402    0.570156    0.231281    -0.216584     0.272629    -0.0203896  -0.33947      0.428551    0.110958    0.0945988    0.417771    -0.399907 
 -0.128151      0.163696    0.141596    -0.658084    0.293383    -0.48813     1.19765      0.384676   -0.353537    0.395306   -0.38053     0.478851    0.0921708   -0.500168   -0.411236    0.280793    0.0108247   -0.197336    -0.745699    -0.197083    0.393923    -0.117648   -0.33172    -0.488036     0.731203    -0.0618859
 -0.552667     -0.0434971   0.356448     0.337427   -0.068435    -0.157313   -0.277363    -0.462903   -0.101892    0.274202    0.164811    0.192665   -0.250542     0.452018   -0.366007    0.228143    0.225416    -0.129316    -0.0940416   -0.195755   -0.0522687    0.0106391  -0.494246    0.443934     0.285956    -0.172117 
  0.104925      0.164759    0.464687     0.123803   -0.519163    -0.297388    0.00679784   0.16049     0.160281   -0.365455   -0.0748765   0.188217   -0.420135     0.930275    0.319667   -0.12829     0.0290909   -1.05405      0.499698     0.0275618  -0.561754     0.165401    0.199196    0.113302    -0.783613     0.277307 
 -0.0896358    -0.207163   -0.105892     0.0817921   0.269501     0.150825   -0.872913    -0.130611   -0.0130417   0.0884196   0.153955   -0.115821   -0.167212    -0.164806    0.657099    0.133552    0.631073     0.499905    -0.133602    -0.252446   -0.126873    -0.448253    0.214603   -0.0238892   -0.368283    -0.0180127
  0.000830399   0.498763    0.828917    -0.289084    0.0392954   -0.177014   -0.601384     0.357767    0.322729   -0.627168   -0.532365   -0.392731   -0.108087     0.496885    0.0407696   0.350888    0.327801     0.208849     0.00528565  -0.0807763   0.489724    -0.0232892  -0.419711   -0.485185     0.238768     0.354277 
 -0.0436936     0.165606    0.0476891   -0.0701227   0.00478164   0.0706706   0.0745458    0.0957924  -0.174742   -0.0577701  -0.096722   -0.285533   -0.0208367    0.0644035   0.0527147  -0.0375106  -0.28449      0.00832908  -0.137884     0.0682757   0.132903     0.0103037  -0.0363501  -0.044381    -0.00448714  -0.0729595
 -0.0442007    -0.089935    0.00905528  -0.230578    0.218046    -0.200721    0.269892    -0.0294057   0.0538375  -0.519476    0.215418    0.487641    0.392932    -0.349044    0.27137    -0.334186    0.362897    -0.242309     0.578461    -0.158832    0.0156218    0.0485845  -0.264542   -0.0699127    0.197789    -0.0356206
 -0.0073019     0.523315   -0.415062     0.686866   -0.291227    -0.490135    0.0940011    0.566564    0.26754    -0.0178111  -0.038333   -0.649172    0.0452586   -0.307502   -0.209187   -0.352509   -0.827425    -0.309357     0.034697    -0.220364    0.511489    -0.126424   -0.0618391   0.0255683    0.263097     0.0987668
 -0.549764     -0.0947553  -1.01749     -0.0622091  -0.445928    -0.0528683   1.04529     -0.744569   -0.147653    0.246305    0.327516    0.437281    0.330898    -0.172449   -0.426378   -0.510452   -0.00834371  -0.273111     0.187546    -0.141942    0.207702     0.0433265  -0.0357624   0.293826     0.308716    -0.829551 
 -0.427448     -0.289419   -0.321759     0.0210946   0.528404    -0.226353   -0.086876    -0.0366682  -0.429755   -0.46312    -0.146301   -0.398651    0.157361     0.0374578  -0.588988    0.0860188  -0.188095     0.144641    -0.0809663    0.782519    0.957402    -0.147042   -0.0144419   0.396035     0.28656     -0.103412 
 -0.0427089     0.271786    0.116786    -0.0154949   0.0704406   -0.381998   -0.142012    -0.127577    0.381345    0.514613    0.297084    0.212989    0.486743     0.150834    0.107832    0.207427   -0.288417     0.191648    -0.237442    -0.336185   -0.0119134    0.326775    0.176663    0.128323     0.242676     0.425857 
  0.364675      0.0851974   0.0130326   -0.0434219  -0.048172     0.224929    0.679146     1.02023    -0.045229   -0.146853   -0.176644   -0.325883   -0.170204     0.0346841  -0.438985   -0.449456   -0.556815     0.291884     0.247943     0.505583   -0.0350567    0.193839   -0.0687545  -0.352622    -0.109648     0.243363 
  0.177286      0.282125    0.00707319   0.246279   -0.0914644    0.30587     0.151289     0.234741    0.0154399   0.0750923  -0.260053    0.0329902   0.29412      0.0698175   0.507203    0.116336   -0.232812     0.239343     0.431148    -0.407825   -0.615604     0.181999   -0.185203    0.237319     0.0278566   -0.103358 
 -0.078263     -0.180612   -0.185401     0.0636774  -0.195557     0.355621   -0.25155     -0.486022    0.123706   -0.248833    0.0737524  -0.133317    0.114714    -0.0389635  -0.106726    0.194644    0.151268    -0.220433     0.0911695   -0.137878    0.00282592  -0.179977    0.194045    0.330567    -0.164942    -0.755459 
 -0.0646953     0.288568    0.331162    -0.187285   -0.0688332    0.306939   -0.0736755    0.23998     0.223498   -0.306677   -0.255644   -0.519128   -0.609712    -0.614747   -0.214925    0.335512    0.2627      -0.18539     -0.272944     0.207882   -0.029007     0.270791   -0.177659    0.599997     0.168428     0.266786 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.417700
INFO: iteration 2, average log likelihood -1.417690
INFO: iteration 3, average log likelihood -1.417680
INFO: iteration 4, average log likelihood -1.417669
INFO: iteration 5, average log likelihood -1.417659
INFO: iteration 6, average log likelihood -1.417649
INFO: iteration 7, average log likelihood -1.417639
INFO: iteration 8, average log likelihood -1.417629
INFO: iteration 9, average log likelihood -1.417619
INFO: iteration 10, average log likelihood -1.417609
INFO: EM with 100000 data points 10 iterations avll -1.417609
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
