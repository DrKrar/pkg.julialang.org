>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.7.0
INFO: Installing JLD v0.6.6
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.1
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.3.0
INFO: Installing ScikitLearnBase v0.2.0
INFO: Installing StaticArrays v0.1.0
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1241
Commit 80770e9 (2016-11-14 21:35 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-101-generic #148-Ubuntu SMP Thu Oct 20 22:08:32 UTC 2016 x86_64 x86_64
Memory: 2.939281463623047 GB (662.23828125 MB free)
Uptime: 28608.0 sec
Load Avg:  0.94091796875  0.96630859375  1.00244140625
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3503 MHz    1742212 s       8462 s     199925 s     588029 s         54 s
#2  3503 MHz     696150 s       2246 s     105415 s    1969767 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.4
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.7.0
 - JLD                           0.6.6
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.1
 - Rmath                         0.1.4
 - SHA                           0.3.0
 - ScikitLearnBase               0.2.0
 - StaticArrays                  0.1.0
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-1.2139231630694484e6,[4812.59,95187.4],
[4389.77 -3100.49 103.366; -4720.81 3105.89 -447.992],

Array{Float64,2}[
[6209.47 -429.707 -381.829; -429.707 5243.85 889.742; -381.829 889.742 3884.25],

[94020.5 368.876 396.818; 368.876 94760.9 -431.754; 396.818 -431.754 97781.2]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.401714e+03
      1       9.074075e+02      -4.943067e+02 |        5
      2       8.394107e+02      -6.799688e+01 |        3
      3       8.341353e+02      -5.275357e+00 |        2
      4       8.177323e+02      -1.640299e+01 |        0
      5       8.177323e+02       0.000000e+00 |        0
K-means converged with 5 iterations (objv = 817.7323078128939)
INFO: K-means with 272 data points using 5 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.045914
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.771621
INFO: iteration 2, lowerbound -3.650619
INFO: iteration 3, lowerbound -3.523668
INFO: iteration 4, lowerbound -3.384649
INFO: iteration 5, lowerbound -3.240993
INFO: iteration 6, lowerbound -3.097317
INFO: dropping number of Gaussions to 7
INFO: iteration 7, lowerbound -2.951791
INFO: dropping number of Gaussions to 5
INFO: iteration 8, lowerbound -2.798511
INFO: iteration 9, lowerbound -2.649189
INFO: iteration 10, lowerbound -2.534469
INFO: dropping number of Gaussions to 4
INFO: iteration 11, lowerbound -2.451032
INFO: iteration 12, lowerbound -2.396966
INFO: dropping number of Gaussions to 3
INFO: iteration 13, lowerbound -2.354534
INFO: iteration 14, lowerbound -2.324194
INFO: iteration 15, lowerbound -2.309490
INFO: iteration 16, lowerbound -2.308599
INFO: dropping number of Gaussions to 2
INFO: iteration 17, lowerbound -2.302915
INFO: iteration 18, lowerbound -2.299259
INFO: iteration 19, lowerbound -2.299256
INFO: iteration 20, lowerbound -2.299254
INFO: iteration 21, lowerbound -2.299254
INFO: iteration 22, lowerbound -2.299253
INFO: iteration 23, lowerbound -2.299253
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Wed 16 Nov 2016 01:27:12 PM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Wed 16 Nov 2016 01:27:14 PM UTC: K-means with 272 data points using 5 iterations
11.3 data points per parameter
,Wed 16 Nov 2016 01:27:15 PM UTC: EM with 272 data points 0 iterations avll -2.045914
5.8 data points per parameter
,Wed 16 Nov 2016 01:27:16 PM UTC: GMM converted to Variational GMM
,Wed 16 Nov 2016 01:27:17 PM UTC: iteration 1, lowerbound -3.771621
,Wed 16 Nov 2016 01:27:17 PM UTC: iteration 2, lowerbound -3.650619
,Wed 16 Nov 2016 01:27:17 PM UTC: iteration 3, lowerbound -3.523668
,Wed 16 Nov 2016 01:27:17 PM UTC: iteration 4, lowerbound -3.384649
,Wed 16 Nov 2016 01:27:18 PM UTC: iteration 5, lowerbound -3.240993
,Wed 16 Nov 2016 01:27:18 PM UTC: iteration 6, lowerbound -3.097317
,Wed 16 Nov 2016 01:27:18 PM UTC: dropping number of Gaussions to 7
,Wed 16 Nov 2016 01:27:18 PM UTC: iteration 7, lowerbound -2.951791
,Wed 16 Nov 2016 01:27:18 PM UTC: dropping number of Gaussions to 5
,Wed 16 Nov 2016 01:27:18 PM UTC: iteration 8, lowerbound -2.798511
,Wed 16 Nov 2016 01:27:18 PM UTC: iteration 9, lowerbound -2.649189
,Wed 16 Nov 2016 01:27:18 PM UTC: iteration 10, lowerbound -2.534469
,Wed 16 Nov 2016 01:27:18 PM UTC: dropping number of Gaussions to 4
,Wed 16 Nov 2016 01:27:18 PM UTC: iteration 11, lowerbound -2.451032
,Wed 16 Nov 2016 01:27:18 PM UTC: iteration 12, lowerbound -2.396966
,Wed 16 Nov 2016 01:27:18 PM UTC: dropping number of Gaussions to 3
,Wed 16 Nov 2016 01:27:18 PM UTC: iteration 13, lowerbound -2.354534
,Wed 16 Nov 2016 01:27:18 PM UTC: iteration 14, lowerbound -2.324194
,Wed 16 Nov 2016 01:27:18 PM UTC: iteration 15, lowerbound -2.309490
,Wed 16 Nov 2016 01:27:19 PM UTC: iteration 16, lowerbound -2.308599
,Wed 16 Nov 2016 01:27:19 PM UTC: dropping number of Gaussions to 2
,Wed 16 Nov 2016 01:27:19 PM UTC: iteration 17, lowerbound -2.302915
,Wed 16 Nov 2016 01:27:19 PM UTC: iteration 18, lowerbound -2.299259
,Wed 16 Nov 2016 01:27:19 PM UTC: iteration 19, lowerbound -2.299256
,Wed 16 Nov 2016 01:27:19 PM UTC: iteration 20, lowerbound -2.299254
,Wed 16 Nov 2016 01:27:19 PM UTC: iteration 21, lowerbound -2.299254
,Wed 16 Nov 2016 01:27:19 PM UTC: iteration 22, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:19 PM UTC: iteration 23, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:19 PM UTC: iteration 24, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:19 PM UTC: iteration 25, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:19 PM UTC: iteration 26, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:19 PM UTC: iteration 27, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:19 PM UTC: iteration 28, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:19 PM UTC: iteration 29, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:19 PM UTC: iteration 30, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:20 PM UTC: iteration 31, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:20 PM UTC: iteration 32, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:20 PM UTC: iteration 33, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:20 PM UTC: iteration 34, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:20 PM UTC: iteration 35, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:20 PM UTC: iteration 36, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:20 PM UTC: iteration 37, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:20 PM UTC: iteration 38, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:20 PM UTC: iteration 39, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:20 PM UTC: iteration 40, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:20 PM UTC: iteration 41, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:20 PM UTC: iteration 42, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:20 PM UTC: iteration 43, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:21 PM UTC: iteration 44, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:21 PM UTC: iteration 45, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:21 PM UTC: iteration 46, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:21 PM UTC: iteration 47, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:21 PM UTC: iteration 48, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:21 PM UTC: iteration 49, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:21 PM UTC: iteration 50, lowerbound -2.299253
,Wed 16 Nov 2016 01:27:21 PM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.9549,178.045]
β = [95.9549,178.045]
m = [2.00023 53.852; 4.2503 79.2869]
ν = [97.9549,180.045]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.375876 -0.00895312; 0.0 0.0127487],

[0.184042 -0.00764405; 0.0 0.00858171]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 99999.99999999996
avll from stats: -0.9864313347162431
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9864313347162407
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9864313347162408
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.00000000001
avll from stats: -0.98949267955118
avll from llpg:  -0.98949267955118
avll direct:     -0.98949267955118
sum posterior: 100000.0
32×26 Array{Float64,2}:
  0.00417918  -0.0188778   0.0584723   -0.00621318   0.0576203    0.0349999    0.0873478    0.128755   -0.071804    0.0606795    -0.044258     0.152055    -0.0210715   -0.0605027  -0.143681   -0.0839982    0.0782587  -0.0191108   -0.157154     -0.0955593    0.0395477    0.0649992    -0.012534     0.100487    -0.0267234    -0.0719521 
  0.231817     0.0343034   0.205112    -0.103555     0.0933715    0.0420904   -0.00835617   0.0905335   0.126069   -0.0380902    -0.0834101   -0.125567     0.0821655    0.0925269  -0.125589   -0.0438764   -0.0899778   0.101734    -0.223776     -0.0668886    0.0543706    0.0388184     0.0138826   -0.0023222   -0.120746     -0.0254199 
 -0.0687727   -0.033986   -0.0483505    0.0924918   -0.037158    -0.0143435   -0.0774825    0.067983    0.0334002   0.0199272    -0.223618    -0.0648271    0.0480918   -0.0424109   0.0412264   0.0945398   -0.103932   -0.0148947   -0.0213698     0.118502    -0.0723351   -0.132235     -0.0902589   -0.0149054   -0.0557542    -0.0843271 
  0.0653545    0.051837    0.0223307    0.0211805   -0.149532    -0.0300392   -0.0965218    0.0480916   0.0686441   0.125638     -0.00213266  -0.019015    -0.24045      0.0493925   0.0658384   0.018413     0.101969   -0.27363      0.113895      0.0281467   -0.140087     0.119327     -0.123455    -0.194848     0.046715      0.0181065 
 -0.0721554   -0.0318044  -0.0048174   -0.0086431   -0.0632221   -0.01875     -0.0678766   -0.0347317   0.0365508   0.00164382   -0.00723325  -0.0757401    0.00344139  -0.160702   -0.111913    0.0550309   -0.0781148  -0.13457      0.00362652   -0.00227393  -0.0360948   -0.0411262    -0.129218     0.014279    -0.0255124    -0.0289835 
 -0.0317206   -0.0475521  -0.00455707  -0.100044    -0.217742     0.0479267    0.109419    -0.0930314  -0.247495   -0.0199449     0.0290005   -0.0890819   -0.0676798    0.07435    -0.12291     0.120604     0.219272    0.0438039    0.122344     -0.157861    -0.0795487   -0.0658562     0.0391249    0.0662378   -0.0312928    -0.0440928 
 -0.113076     0.158688    0.0460594    0.165489    -0.127973    -0.0316735    0.0796695    0.0164141   0.106499    0.0879116    -0.0410979   -0.0155323   -0.0277177   -0.0771432  -0.0426319  -0.0282349    0.107511    0.0403856   -0.103239     -0.3266      -0.0678882    0.0697921    -0.0176746    0.00836076   0.0966395     0.185743  
 -0.0233597    0.0550257  -0.0896795    0.041342    -0.0453732    0.059296     0.0543526    0.0485611   0.119218   -0.0916754    -0.0724429    0.176154    -0.102868    -0.0474185  -0.0873372  -0.12351     -0.0446865  -0.0797816   -0.0177655    -0.00184164  -0.13917     -0.0908471    -0.00322858   0.133892     0.0600623     0.144439  
  0.0962727   -0.0775154   0.0625303   -0.0617013   -0.186186    -0.0285188    0.0557002   -0.0237933  -0.0583601   0.192389      0.0434549   -0.0712744    0.00852344  -0.0348237   0.117691    0.00785454   0.054566   -0.0116747   -0.24163      -0.0280004    0.0726733   -0.16125      -0.074062     0.0474932    0.0359895    -0.0929207 
  0.0347916   -0.0774813  -0.0226154    0.0882182   -0.0896451    0.116076    -0.160338     0.0737979   0.0741653   0.0957467     0.00653447   0.15461      0.00517233  -0.0926114  -0.0880669   0.0303315   -0.134213   -0.0512791   -0.0433114     0.098905     0.116317    -0.0743569    -0.046982     0.0584032    0.161561      0.0218375 
 -0.0405296   -0.0664079   0.0442821   -0.0992905   -0.0493593    0.0414402   -0.0686684   -0.0866453   0.0681218  -0.206423      0.0368741    0.0532646    0.0270832   -0.0196933   0.113243    0.0033828   -0.0147014  -0.188817     0.128117      0.0602657   -0.00768311  -0.0512827    -0.00647055  -0.121694    -0.119214      0.0441521 
 -0.0802584   -0.0853504   0.114617    -0.0403052    0.0283466    0.0460954   -0.0208051    0.0659486  -0.025809    0.0515868     0.0254924   -0.114831     0.0639892    0.0552185   0.0501967   0.188074     0.0712966  -0.0747615   -0.160494     -0.0644025    0.0664917    0.0922416    -0.0627241    0.0586553   -0.122449      0.0777983 
  0.103546    -0.0486307   0.227524     0.0241309    0.110766     0.160505    -0.0619962    0.060405   -0.0726468  -0.0211776     0.0209413    0.0604942    0.00453768  -0.0214822   0.0401012   0.00684548  -0.110749   -0.0454043   -0.000715529   0.0279387   -0.0666119   -0.016207      0.12732     -0.167135    -0.021684     -0.0430118 
 -0.0224505   -0.0904843   0.106284    -0.259007     0.0339064    0.00707197  -0.0268923    0.0415114   0.0251343  -0.0334107    -0.0401925    0.0612063   -0.00394405  -0.0591369  -0.0291283  -0.0455325    0.162268    0.306527    -0.00538428   -0.0510235    0.00464466   0.124053     -0.0314456    0.154671    -0.0850038    -0.0940319 
 -0.0494593    0.0652924  -0.0245126   -0.0282607   -0.0518478   -0.188594    -0.00530095   0.114656    0.0829949   0.0985711     0.0889753    0.0315141   -0.017942     0.0490338   0.0246205  -0.0797869   -0.145659    0.0304203   -0.0602063    -0.0641744   -0.128087    -0.102256     -0.0578542    0.130646     0.0655521     0.193817  
 -0.0894624    0.0424651   0.0630303   -0.0427751    0.11878      0.0752216   -0.0456359    0.0828613   0.133721    0.241654      0.151895    -0.235536    -0.0964054    0.0415426  -0.0702114   0.069887     0.0315101  -0.221841     0.00766396   -0.0526391   -0.0573619    0.164309     -0.198848     0.0565701    0.171556      0.0074468 
  0.141278    -0.0483428  -0.0995478   -0.0833009    0.0109857   -0.218338     0.222088    -0.168024    0.027236    0.0910795     0.0965271   -0.180488     0.185668     0.0525464  -0.0583858   0.116945    -0.0177538  -0.196813     0.0314061    -0.0517986   -0.155151     0.00143815   -0.156096    -0.120113    -0.183496     -0.0552354 
 -0.0129467   -0.0952262  -0.102372    -0.0334686    0.0611534   -0.0663144   -0.0389177    0.0327084  -0.149276   -0.205897      0.212749     0.134864     0.0660919   -0.0026927   0.0371868  -0.0487705   -0.0367994  -0.0187399    0.0146152    -0.119303    -0.0701068    0.089356      0.0627622    0.15278      0.0723531     0.0737729 
  0.0453684    0.0264819   0.0649297    0.0254243    0.156362    -0.0364527    0.0346543   -0.0209249  -0.067866   -0.120217      0.114484    -0.154154    -0.044036     0.0670928   0.112463   -0.0912972    0.0158884  -0.123422    -0.167924     -0.00529411  -0.00782015   0.0755654     0.033029    -0.0515148   -0.0323419    -0.0771911 
 -0.159724    -0.156127   -0.136062    -0.0106272   -0.0283339    0.0614488   -0.0516112   -0.036157    0.0998456   0.0202213    -0.00963876  -0.0412662    0.221624    -0.0333334  -0.0447181   0.0679871   -0.104647    0.0968439    0.036166     -0.0933229    0.0958053    0.124244      0.132592    -0.0299462    0.00192447    0.149773  
 -0.0935464   -0.0656973  -0.0850094    0.0041389    0.0800453    0.00404565  -0.0700091   -0.0622055  -0.0390118   0.0075562    -0.0457967    0.0203506    0.0935572    0.0367681  -0.0794287   0.062874    -0.0053725  -0.0641417    0.122322     -0.108285     0.139859     0.0695191     0.0137876   -0.0506695   -0.0806424    -0.123692  
  0.0704288    0.0688651  -0.136036    -0.063552    -0.0823008    0.171211     0.130672    -0.0339148  -0.174594   -0.0366497     0.0462024    0.107508    -0.0564211   -0.0317531   0.140438   -0.00603227  -0.0153032   0.132189     0.205406      0.0458393    0.0867307    0.0334134     0.0744472    0.0501602    0.122109      0.0804307 
 -0.0393704   -0.239187    0.175633    -0.00539699  -0.00773608  -0.127315     0.12168     -0.0541501   0.0497488   0.0634515    -0.0455178   -0.0174718    0.048853    -0.0855758   0.150257   -0.109612     0.0393193   0.0753168   -0.103077      0.0363853   -0.0580213   -0.0231758    -0.0101292    0.0348233    0.0445764     0.132281  
 -0.0992146   -0.0775375   0.0826853   -0.109137    -0.0390197   -0.0924424    0.00466519   0.183401    0.100713    0.126611      0.00799297   0.00222308   0.110026    -0.0500963  -0.0621865  -0.117638    -0.0883327   0.00740243  -0.142611     -0.0196283   -0.0421911    0.0174288     0.0866736    0.0692172    0.0304657    -0.0892027 
 -0.130185    -0.039504   -0.00393638   0.162531     0.145462    -0.0435723   -0.0697742   -0.030442    0.0809733  -0.116088      0.00114107   0.159381     0.00827574  -0.0860319  -0.0470666  -0.115954     0.0173703  -0.00122304  -0.201912     -0.0232166    0.0218453    0.142743      0.0762701   -0.0690357   -0.0702129    -0.0761395 
  0.106966    -0.11139     0.134733    -0.0889871    0.0235941    0.0292824   -0.088296     0.216466   -0.219484   -0.0193339     0.0570763    0.0529903    0.0482639    0.0458376  -0.0232393   0.0726141    0.0537813  -0.0320745   -0.0392947     0.12584      0.0387684    0.0344858     0.221823    -0.0248491   -0.0763769     0.0873865 
  0.09629      0.0292426  -0.0768547   -0.170543     0.0797297   -0.0254333    0.0190464    0.114146    0.0692953   0.000392911   0.0189001   -0.00748934   0.0346708   -0.0275704  -0.0217768  -0.0565837    0.0291127  -0.0874329    0.0667286     0.0096862    0.00113787   0.00166989    0.163759     0.0369051    0.00106802    0.0499941 
 -0.0597963   -0.0652913   0.210541    -0.0345303    0.104689    -0.0110108   -0.128752     0.123508   -0.177647   -0.0465861    -0.0678845   -0.0606341    0.146916     0.0963012  -0.203085   -0.00717358   0.0393228   0.15141      0.0653675    -0.253858    -0.136597     0.0584098     0.10625     -0.119955    -0.0125909    -0.149416  
  0.141105    -0.143037   -0.0243239    0.0155898    0.0605508    0.13221      0.0808948    0.10856     0.263938    0.037143     -0.0597719   -0.0924781   -0.18598      0.0384029  -0.0662156   0.156899     0.114993    0.117784     0.145077      0.0405611   -0.0289689   -0.000147188   0.0685082   -0.124605    -0.0983996    -0.00765278
 -0.0438008    0.0411262   0.0913488    0.136776    -0.0627016   -0.138909    -0.130941    -0.127558   -0.063524   -0.0492594    -0.097842     0.211486    -0.0455608   -0.08608     0.0941721  -0.0406921    0.0997796   0.0775821    0.0856619     0.122187     0.0589706   -0.0325482     0.155487    -0.104698    -0.0459894    -0.238615  
  0.140763    -0.102945   -0.157838     0.0229133    0.045518     0.0413337   -0.086        0.01852     0.138819    0.0899371    -0.0714843   -0.188155     0.0302875    0.128303   -0.113271   -0.156809    -0.0587043  -0.0238421   -0.0837492     0.0894642    0.0297862   -0.0170157     0.231852     0.100089    -0.0879556     0.0229999 
 -0.0663372    0.103       0.0035781   -0.0110927    0.149426    -0.0711032   -0.0779357   -0.298005    0.0838581  -0.137301      0.144864     0.102305    -0.0685119    0.0230283  -0.0310644   0.0528316   -0.0031329  -0.093356    -0.00409318    0.123854    -0.00643533  -0.133464      0.0227103    0.106136    -0.000622116   0.0574658 kind diag, method split
0: avll = -1.4309856441622653
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.431054
INFO: iteration 2, average log likelihood -1.430980
INFO: iteration 3, average log likelihood -1.430275
INFO: iteration 4, average log likelihood -1.422959
INFO: iteration 5, average log likelihood -1.404985
INFO: iteration 6, average log likelihood -1.398080
INFO: iteration 7, average log likelihood -1.397006
INFO: iteration 8, average log likelihood -1.396622
INFO: iteration 9, average log likelihood -1.396431
INFO: iteration 10, average log likelihood -1.396320
INFO: iteration 11, average log likelihood -1.396246
INFO: iteration 12, average log likelihood -1.396192
INFO: iteration 13, average log likelihood -1.396150
INFO: iteration 14, average log likelihood -1.396116
INFO: iteration 15, average log likelihood -1.396087
INFO: iteration 16, average log likelihood -1.396061
INFO: iteration 17, average log likelihood -1.396037
INFO: iteration 18, average log likelihood -1.396015
INFO: iteration 19, average log likelihood -1.395995
INFO: iteration 20, average log likelihood -1.395976
INFO: iteration 21, average log likelihood -1.395959
INFO: iteration 22, average log likelihood -1.395943
INFO: iteration 23, average log likelihood -1.395929
INFO: iteration 24, average log likelihood -1.395916
INFO: iteration 25, average log likelihood -1.395903
INFO: iteration 26, average log likelihood -1.395892
INFO: iteration 27, average log likelihood -1.395881
INFO: iteration 28, average log likelihood -1.395871
INFO: iteration 29, average log likelihood -1.395861
INFO: iteration 30, average log likelihood -1.395852
INFO: iteration 31, average log likelihood -1.395843
INFO: iteration 32, average log likelihood -1.395835
INFO: iteration 33, average log likelihood -1.395827
INFO: iteration 34, average log likelihood -1.395820
INFO: iteration 35, average log likelihood -1.395812
INFO: iteration 36, average log likelihood -1.395805
INFO: iteration 37, average log likelihood -1.395797
INFO: iteration 38, average log likelihood -1.395790
INFO: iteration 39, average log likelihood -1.395783
INFO: iteration 40, average log likelihood -1.395775
INFO: iteration 41, average log likelihood -1.395768
INFO: iteration 42, average log likelihood -1.395760
INFO: iteration 43, average log likelihood -1.395753
INFO: iteration 44, average log likelihood -1.395745
INFO: iteration 45, average log likelihood -1.395737
INFO: iteration 46, average log likelihood -1.395730
INFO: iteration 47, average log likelihood -1.395723
INFO: iteration 48, average log likelihood -1.395716
INFO: iteration 49, average log likelihood -1.395710
INFO: iteration 50, average log likelihood -1.395704
INFO: EM with 100000 data points 50 iterations avll -1.395704
952.4 data points per parameter
1: avll = [-1.43105,-1.43098,-1.43028,-1.42296,-1.40499,-1.39808,-1.39701,-1.39662,-1.39643,-1.39632,-1.39625,-1.39619,-1.39615,-1.39612,-1.39609,-1.39606,-1.39604,-1.39602,-1.39599,-1.39598,-1.39596,-1.39594,-1.39593,-1.39592,-1.3959,-1.39589,-1.39588,-1.39587,-1.39586,-1.39585,-1.39584,-1.39584,-1.39583,-1.39582,-1.39581,-1.3958,-1.3958,-1.39579,-1.39578,-1.39578,-1.39577,-1.39576,-1.39575,-1.39575,-1.39574,-1.39573,-1.39572,-1.39572,-1.39571,-1.3957]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.395799
INFO: iteration 2, average log likelihood -1.395694
INFO: iteration 3, average log likelihood -1.395241
INFO: iteration 4, average log likelihood -1.390753
INFO: iteration 5, average log likelihood -1.376271
INFO: iteration 6, average log likelihood -1.365983
INFO: iteration 7, average log likelihood -1.362738
INFO: iteration 8, average log likelihood -1.360857
INFO: iteration 9, average log likelihood -1.359203
INFO: iteration 10, average log likelihood -1.357666
INFO: iteration 11, average log likelihood -1.356341
INFO: iteration 12, average log likelihood -1.355274
INFO: iteration 13, average log likelihood -1.354472
INFO: iteration 14, average log likelihood -1.353874
INFO: iteration 15, average log likelihood -1.353406
INFO: iteration 16, average log likelihood -1.353028
INFO: iteration 17, average log likelihood -1.352707
INFO: iteration 18, average log likelihood -1.352424
INFO: iteration 19, average log likelihood -1.352174
INFO: iteration 20, average log likelihood -1.351960
INFO: iteration 21, average log likelihood -1.351778
INFO: iteration 22, average log likelihood -1.351625
INFO: iteration 23, average log likelihood -1.351500
INFO: iteration 24, average log likelihood -1.351400
INFO: iteration 25, average log likelihood -1.351317
INFO: iteration 26, average log likelihood -1.351248
INFO: iteration 27, average log likelihood -1.351189
INFO: iteration 28, average log likelihood -1.351137
INFO: iteration 29, average log likelihood -1.351090
INFO: iteration 30, average log likelihood -1.351047
INFO: iteration 31, average log likelihood -1.351007
INFO: iteration 32, average log likelihood -1.350969
INFO: iteration 33, average log likelihood -1.350933
INFO: iteration 34, average log likelihood -1.350899
INFO: iteration 35, average log likelihood -1.350867
INFO: iteration 36, average log likelihood -1.350836
INFO: iteration 37, average log likelihood -1.350806
INFO: iteration 38, average log likelihood -1.350778
INFO: iteration 39, average log likelihood -1.350750
INFO: iteration 40, average log likelihood -1.350723
INFO: iteration 41, average log likelihood -1.350696
INFO: iteration 42, average log likelihood -1.350669
INFO: iteration 43, average log likelihood -1.350643
INFO: iteration 44, average log likelihood -1.350616
INFO: iteration 45, average log likelihood -1.350589
INFO: iteration 46, average log likelihood -1.350561
INFO: iteration 47, average log likelihood -1.350534
INFO: iteration 48, average log likelihood -1.350506
INFO: iteration 49, average log likelihood -1.350478
INFO: iteration 50, average log likelihood -1.350449
INFO: EM with 100000 data points 50 iterations avll -1.350449
473.9 data points per parameter
2: avll = [-1.3958,-1.39569,-1.39524,-1.39075,-1.37627,-1.36598,-1.36274,-1.36086,-1.3592,-1.35767,-1.35634,-1.35527,-1.35447,-1.35387,-1.35341,-1.35303,-1.35271,-1.35242,-1.35217,-1.35196,-1.35178,-1.35162,-1.3515,-1.3514,-1.35132,-1.35125,-1.35119,-1.35114,-1.35109,-1.35105,-1.35101,-1.35097,-1.35093,-1.3509,-1.35087,-1.35084,-1.35081,-1.35078,-1.35075,-1.35072,-1.3507,-1.35067,-1.35064,-1.35062,-1.35059,-1.35056,-1.35053,-1.35051,-1.35048,-1.35045]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.350558
INFO: iteration 2, average log likelihood -1.350387
INFO: iteration 3, average log likelihood -1.349822
INFO: iteration 4, average log likelihood -1.344423
INFO: iteration 5, average log likelihood -1.326065
INFO: iteration 6, average log likelihood -1.311544
INFO: iteration 7, average log likelihood -1.305768
INFO: iteration 8, average log likelihood -1.302430
INFO: iteration 9, average log likelihood -1.299987
INFO: iteration 10, average log likelihood -1.297685
INFO: iteration 11, average log likelihood -1.294378
INFO: iteration 12, average log likelihood -1.291141
INFO: iteration 13, average log likelihood -1.289079
INFO: iteration 14, average log likelihood -1.287726
INFO: iteration 15, average log likelihood -1.287029
INFO: iteration 16, average log likelihood -1.286718
INFO: iteration 17, average log likelihood -1.286565
INFO: iteration 18, average log likelihood -1.286478
INFO: iteration 19, average log likelihood -1.286422
INFO: iteration 20, average log likelihood -1.286384
INFO: iteration 21, average log likelihood -1.286355
INFO: iteration 22, average log likelihood -1.286332
INFO: iteration 23, average log likelihood -1.286311
INFO: iteration 24, average log likelihood -1.286293
INFO: iteration 25, average log likelihood -1.286275
INFO: iteration 26, average log likelihood -1.286256
INFO: iteration 27, average log likelihood -1.286237
INFO: iteration 28, average log likelihood -1.286215
INFO: iteration 29, average log likelihood -1.286190
INFO: iteration 30, average log likelihood -1.286159
INFO: iteration 31, average log likelihood -1.286121
INFO: iteration 32, average log likelihood -1.286074
INFO: iteration 33, average log likelihood -1.286019
INFO: iteration 34, average log likelihood -1.285959
INFO: iteration 35, average log likelihood -1.285895
INFO: iteration 36, average log likelihood -1.285827
INFO: iteration 37, average log likelihood -1.285756
INFO: iteration 38, average log likelihood -1.285681
INFO: iteration 39, average log likelihood -1.285601
INFO: iteration 40, average log likelihood -1.285518
INFO: iteration 41, average log likelihood -1.285430
INFO: iteration 42, average log likelihood -1.285340
INFO: iteration 43, average log likelihood -1.285255
INFO: iteration 44, average log likelihood -1.285186
INFO: iteration 45, average log likelihood -1.285136
INFO: iteration 46, average log likelihood -1.285104
INFO: iteration 47, average log likelihood -1.285085
INFO: iteration 48, average log likelihood -1.285073
INFO: iteration 49, average log likelihood -1.285066
INFO: iteration 50, average log likelihood -1.285061
INFO: EM with 100000 data points 50 iterations avll -1.285061
236.4 data points per parameter
3: avll = [-1.35056,-1.35039,-1.34982,-1.34442,-1.32607,-1.31154,-1.30577,-1.30243,-1.29999,-1.29768,-1.29438,-1.29114,-1.28908,-1.28773,-1.28703,-1.28672,-1.28657,-1.28648,-1.28642,-1.28638,-1.28636,-1.28633,-1.28631,-1.28629,-1.28627,-1.28626,-1.28624,-1.28621,-1.28619,-1.28616,-1.28612,-1.28607,-1.28602,-1.28596,-1.2859,-1.28583,-1.28576,-1.28568,-1.2856,-1.28552,-1.28543,-1.28534,-1.28526,-1.28519,-1.28514,-1.2851,-1.28508,-1.28507,-1.28507,-1.28506]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.285280
INFO: iteration 2, average log likelihood -1.284984
INFO: iteration 3, average log likelihood -1.283012
INFO: iteration 4, average log likelihood -1.266643
INFO: iteration 5, average log likelihood -1.238452
INFO: iteration 6, average log likelihood -1.210160
WARNING: Variances had to be floored 2 3
INFO: iteration 7, average log likelihood -1.185311
WARNING: Variances had to be floored 12
INFO: iteration 8, average log likelihood -1.201274
WARNING: Variances had to be floored 3 6
INFO: iteration 9, average log likelihood -1.194250
INFO: iteration 10, average log likelihood -1.199462
WARNING: Variances had to be floored 2 3
INFO: iteration 11, average log likelihood -1.178563
INFO: iteration 12, average log likelihood -1.195148
WARNING: Variances had to be floored 3 12
INFO: iteration 13, average log likelihood -1.178903
WARNING: Variances had to be floored 6
INFO: iteration 14, average log likelihood -1.190858
WARNING: Variances had to be floored 2 3
INFO: iteration 15, average log likelihood -1.184106
INFO: iteration 16, average log likelihood -1.192990
WARNING: Variances had to be floored 3
INFO: iteration 17, average log likelihood -1.172580
WARNING: Variances had to be floored 1 12
INFO: iteration 18, average log likelihood -1.174303
WARNING: Variances had to be floored 3 6
INFO: iteration 19, average log likelihood -1.185092
WARNING: Variances had to be floored 13
INFO: iteration 20, average log likelihood -1.187861
WARNING: Variances had to be floored 2 3
INFO: iteration 21, average log likelihood -1.186800
INFO: iteration 22, average log likelihood -1.192963
WARNING: Variances had to be floored 3 12
INFO: iteration 23, average log likelihood -1.172490
WARNING: Variances had to be floored 1
INFO: iteration 24, average log likelihood -1.184732
WARNING: Variances had to be floored 3 6
INFO: iteration 25, average log likelihood -1.184448
WARNING: Variances had to be floored 2
INFO: iteration 26, average log likelihood -1.190318
WARNING: Variances had to be floored 3
INFO: iteration 27, average log likelihood -1.186702
INFO: iteration 28, average log likelihood -1.180214
WARNING: Variances had to be floored 1 3 12
INFO: iteration 29, average log likelihood -1.166669
INFO: iteration 30, average log likelihood -1.190183
WARNING: Variances had to be floored 3 13
INFO: iteration 31, average log likelihood -1.171177
INFO: iteration 32, average log likelihood -1.193316
WARNING: Variances had to be floored 2 3
INFO: iteration 33, average log likelihood -1.172389
WARNING: Variances had to be floored 6
INFO: iteration 34, average log likelihood -1.186306
WARNING: Variances had to be floored 1 3 12
INFO: iteration 35, average log likelihood -1.177426
INFO: iteration 36, average log likelihood -1.197680
WARNING: Variances had to be floored 3
INFO: iteration 37, average log likelihood -1.180394
INFO: iteration 38, average log likelihood -1.181901
WARNING: Variances had to be floored 2 3
INFO: iteration 39, average log likelihood -1.166462
INFO: iteration 40, average log likelihood -1.179310
WARNING: Variances had to be floored 1 3 12 13
INFO: iteration 41, average log likelihood -1.159968
INFO: iteration 42, average log likelihood -1.205346
WARNING: Variances had to be floored 3 6
INFO: iteration 43, average log likelihood -1.181792
INFO: iteration 44, average log likelihood -1.190783
WARNING: Variances had to be floored 2 3
INFO: iteration 45, average log likelihood -1.171457
INFO: iteration 46, average log likelihood -1.186229
WARNING: Variances had to be floored 1 3 12
INFO: iteration 47, average log likelihood -1.169545
INFO: iteration 48, average log likelihood -1.194320
WARNING: Variances had to be floored 3
INFO: iteration 49, average log likelihood -1.176620
WARNING: Variances had to be floored 2
INFO: iteration 50, average log likelihood -1.175821
INFO: EM with 100000 data points 50 iterations avll -1.175821
118.1 data points per parameter
4: avll = [-1.28528,-1.28498,-1.28301,-1.26664,-1.23845,-1.21016,-1.18531,-1.20127,-1.19425,-1.19946,-1.17856,-1.19515,-1.1789,-1.19086,-1.18411,-1.19299,-1.17258,-1.1743,-1.18509,-1.18786,-1.1868,-1.19296,-1.17249,-1.18473,-1.18445,-1.19032,-1.1867,-1.18021,-1.16667,-1.19018,-1.17118,-1.19332,-1.17239,-1.18631,-1.17743,-1.19768,-1.18039,-1.1819,-1.16646,-1.17931,-1.15997,-1.20535,-1.18179,-1.19078,-1.17146,-1.18623,-1.16954,-1.19432,-1.17662,-1.17582]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 5 6 25 26
INFO: iteration 1, average log likelihood -1.173049
WARNING: Variances had to be floored 5 6 11 12 25 26
INFO: iteration 2, average log likelihood -1.161808
WARNING: Variances had to be floored 1 2 5 6 23 24 25 26
INFO: iteration 3, average log likelihood -1.155874
WARNING: Variances had to be floored 4 5 6 11 12 22 25 26
INFO: iteration 4, average log likelihood -1.140029
WARNING: Variances had to be floored 1 2 5 6 11 12 22 23 24 25 26
INFO: iteration 5, average log likelihood -1.129157
WARNING: Variances had to be floored 5 6 10 11 12 25 26 28
INFO: iteration 6, average log likelihood -1.115472
WARNING: Variances had to be floored 1 2 4 5 6 11 12 22 23 24 25 26
INFO: iteration 7, average log likelihood -1.109213
WARNING: Variances had to be floored 5 6 10 11 12 25 26
INFO: iteration 8, average log likelihood -1.116167
WARNING: Variances had to be floored 1 2 5 6 11 12 22 23 24 25 26 28
INFO: iteration 9, average log likelihood -1.101822
WARNING: Variances had to be floored 4 5 6 10 11 12 25 26
INFO: iteration 10, average log likelihood -1.103295
WARNING: Variances had to be floored 1 2 5 6 11 12 22 23 24 25 26
INFO: iteration 11, average log likelihood -1.111049
WARNING: Variances had to be floored 5 6 10 11 12 22 25 26 28
INFO: iteration 12, average log likelihood -1.102055
WARNING: Variances had to be floored 1 2 4 5 6 11 12 23 24 25 26
INFO: iteration 13, average log likelihood -1.103136
WARNING: Variances had to be floored 5 6 10 11 12 22 25 26
INFO: iteration 14, average log likelihood -1.104511
WARNING: Variances had to be floored 1 2 4 5 6 11 12 23 24 25 26 28
INFO: iteration 15, average log likelihood -1.091234
WARNING: Variances had to be floored 5 6 10 11 12 14 22 25 26
INFO: iteration 16, average log likelihood -1.100107
WARNING: Variances had to be floored 1 2 4 5 6 11 12 23 24 25 26
INFO: iteration 17, average log likelihood -1.104488
WARNING: Variances had to be floored 5 6 10 11 12 22 25 26 28
INFO: iteration 18, average log likelihood -1.104772
WARNING: Variances had to be floored 1 2 4 5 6 11 12 23 24 25 26
INFO: iteration 19, average log likelihood -1.095127
WARNING: Variances had to be floored 5 6 10 11 12 14 25 26
INFO: iteration 20, average log likelihood -1.091298
WARNING: Variances had to be floored 1 2 4 5 6 11 12 23 24 25 26 28
INFO: iteration 21, average log likelihood -1.091931
WARNING: Variances had to be floored 5 6 10 11 12 21 25 26
INFO: iteration 22, average log likelihood -1.098848
WARNING: Variances had to be floored 1 2 4 5 6 11 12 23 24 25 26
INFO: iteration 23, average log likelihood -1.099924
WARNING: Variances had to be floored 5 6 10 11 12 25 26 28
INFO: iteration 24, average log likelihood -1.100484
WARNING: Variances had to be floored 1 2 4 5 6 11 12 14 23 24 25 26
INFO: iteration 25, average log likelihood -1.093948
WARNING: Variances had to be floored 5 6 10 11 12 25 26
INFO: iteration 26, average log likelihood -1.114116
WARNING: Variances had to be floored 1 2 5 6 11 12 23 24 25 26
INFO: iteration 27, average log likelihood -1.101327
WARNING: Variances had to be floored 4 5 6 10 11 12 25 26 28
INFO: iteration 28, average log likelihood -1.085509
WARNING: Variances had to be floored 1 2 5 6 11 12 14 23 24 25 26
INFO: iteration 29, average log likelihood -1.098493
WARNING: Variances had to be floored 4 5 6 10 11 12 25 26
INFO: iteration 30, average log likelihood -1.096350
WARNING: Variances had to be floored 1 2 5 6 11 12 18 23 24 25 26 28
INFO: iteration 31, average log likelihood -1.101025
WARNING: Variances had to be floored 4 5 6 10 11 12 25 26
INFO: iteration 32, average log likelihood -1.098455
WARNING: Variances had to be floored 1 2 5 6 11 12 14 23 24 25 26
INFO: iteration 33, average log likelihood -1.095459
WARNING: Variances had to be floored 4 5 6 10 11 12 25 26 28
INFO: iteration 34, average log likelihood -1.095117
WARNING: Variances had to be floored 1 2 5 6 11 12 18 23 24 25 26
INFO: iteration 35, average log likelihood -1.106572
WARNING: Variances had to be floored 4 5 6 10 11 12 25 26
INFO: iteration 36, average log likelihood -1.094362
WARNING: Variances had to be floored 1 2 5 6 11 12 14 23 24 25 26 28
INFO: iteration 37, average log likelihood -1.093417
WARNING: Variances had to be floored 4 5 6 10 11 12 18 25 26
INFO: iteration 38, average log likelihood -1.099620
WARNING: Variances had to be floored 1 2 5 6 11 12 23 24 25 26
INFO: iteration 39, average log likelihood -1.112261
WARNING: Variances had to be floored 4 5 6 10 11 12 25 26
INFO: iteration 40, average log likelihood -1.088897
WARNING: Variances had to be floored 1 2 5 6 11 12 14 18 23 24 25 26 28
INFO: iteration 41, average log likelihood -1.089426
WARNING: Variances had to be floored 4 5 6 10 11 12 25 26
INFO: iteration 42, average log likelihood -1.107647
WARNING: Variances had to be floored 1 2 5 6 11 12 23 24 25 26
INFO: iteration 43, average log likelihood -1.108066
WARNING: Variances had to be floored 4 5 6 10 11 12 18 25 26
INFO: iteration 44, average log likelihood -1.086034
WARNING: Variances had to be floored 1 2 5 6 11 12 14 23 24 25 26 28
INFO: iteration 45, average log likelihood -1.097605
WARNING: Variances had to be floored 4 5 6 10 11 12 25 26
INFO: iteration 46, average log likelihood -1.102776
WARNING: Variances had to be floored 1 2 5 6 11 12 18 23 24 25 26
INFO: iteration 47, average log likelihood -1.103731
WARNING: Variances had to be floored 4 5 6 10 11 12 25 26
INFO: iteration 48, average log likelihood -1.093632
WARNING: Variances had to be floored 1 2 5 6 11 12 14 23 24 25 26 28
INFO: iteration 49, average log likelihood -1.092305
WARNING: Variances had to be floored 4 5 6 10 11 12 18 25 26
INFO: iteration 50, average log likelihood -1.099308
INFO: EM with 100000 data points 50 iterations avll -1.099308
59.0 data points per parameter
5: avll = [-1.17305,-1.16181,-1.15587,-1.14003,-1.12916,-1.11547,-1.10921,-1.11617,-1.10182,-1.1033,-1.11105,-1.10205,-1.10314,-1.10451,-1.09123,-1.10011,-1.10449,-1.10477,-1.09513,-1.0913,-1.09193,-1.09885,-1.09992,-1.10048,-1.09395,-1.11412,-1.10133,-1.08551,-1.09849,-1.09635,-1.10102,-1.09845,-1.09546,-1.09512,-1.10657,-1.09436,-1.09342,-1.09962,-1.11226,-1.0889,-1.08943,-1.10765,-1.10807,-1.08603,-1.0976,-1.10278,-1.10373,-1.09363,-1.0923,-1.09931]
[-1.43099,-1.43105,-1.43098,-1.43028,-1.42296,-1.40499,-1.39808,-1.39701,-1.39662,-1.39643,-1.39632,-1.39625,-1.39619,-1.39615,-1.39612,-1.39609,-1.39606,-1.39604,-1.39602,-1.39599,-1.39598,-1.39596,-1.39594,-1.39593,-1.39592,-1.3959,-1.39589,-1.39588,-1.39587,-1.39586,-1.39585,-1.39584,-1.39584,-1.39583,-1.39582,-1.39581,-1.3958,-1.3958,-1.39579,-1.39578,-1.39578,-1.39577,-1.39576,-1.39575,-1.39575,-1.39574,-1.39573,-1.39572,-1.39572,-1.39571,-1.3957,-1.3958,-1.39569,-1.39524,-1.39075,-1.37627,-1.36598,-1.36274,-1.36086,-1.3592,-1.35767,-1.35634,-1.35527,-1.35447,-1.35387,-1.35341,-1.35303,-1.35271,-1.35242,-1.35217,-1.35196,-1.35178,-1.35162,-1.3515,-1.3514,-1.35132,-1.35125,-1.35119,-1.35114,-1.35109,-1.35105,-1.35101,-1.35097,-1.35093,-1.3509,-1.35087,-1.35084,-1.35081,-1.35078,-1.35075,-1.35072,-1.3507,-1.35067,-1.35064,-1.35062,-1.35059,-1.35056,-1.35053,-1.35051,-1.35048,-1.35045,-1.35056,-1.35039,-1.34982,-1.34442,-1.32607,-1.31154,-1.30577,-1.30243,-1.29999,-1.29768,-1.29438,-1.29114,-1.28908,-1.28773,-1.28703,-1.28672,-1.28657,-1.28648,-1.28642,-1.28638,-1.28636,-1.28633,-1.28631,-1.28629,-1.28627,-1.28626,-1.28624,-1.28621,-1.28619,-1.28616,-1.28612,-1.28607,-1.28602,-1.28596,-1.2859,-1.28583,-1.28576,-1.28568,-1.2856,-1.28552,-1.28543,-1.28534,-1.28526,-1.28519,-1.28514,-1.2851,-1.28508,-1.28507,-1.28507,-1.28506,-1.28528,-1.28498,-1.28301,-1.26664,-1.23845,-1.21016,-1.18531,-1.20127,-1.19425,-1.19946,-1.17856,-1.19515,-1.1789,-1.19086,-1.18411,-1.19299,-1.17258,-1.1743,-1.18509,-1.18786,-1.1868,-1.19296,-1.17249,-1.18473,-1.18445,-1.19032,-1.1867,-1.18021,-1.16667,-1.19018,-1.17118,-1.19332,-1.17239,-1.18631,-1.17743,-1.19768,-1.18039,-1.1819,-1.16646,-1.17931,-1.15997,-1.20535,-1.18179,-1.19078,-1.17146,-1.18623,-1.16954,-1.19432,-1.17662,-1.17582,-1.17305,-1.16181,-1.15587,-1.14003,-1.12916,-1.11547,-1.10921,-1.11617,-1.10182,-1.1033,-1.11105,-1.10205,-1.10314,-1.10451,-1.09123,-1.10011,-1.10449,-1.10477,-1.09513,-1.0913,-1.09193,-1.09885,-1.09992,-1.10048,-1.09395,-1.11412,-1.10133,-1.08551,-1.09849,-1.09635,-1.10102,-1.09845,-1.09546,-1.09512,-1.10657,-1.09436,-1.09342,-1.09962,-1.11226,-1.0889,-1.08943,-1.10765,-1.10807,-1.08603,-1.0976,-1.10278,-1.10373,-1.09363,-1.0923,-1.09931]
32×26 Array{Float64,2}:
 -0.226301     0.198184    -0.0977811   -0.0402619     0.0369281     0.0634836  -0.0248404   -0.0419335    -0.0249798   -0.0675062   -0.512105   -0.121382    -0.0315106     0.0465018    0.0570651    0.22274       0.0778678   -0.00320976  -0.0504672    -0.0653045     0.171536     0.077955    -0.0781598    0.0615157   -0.0342315    0.044113  
 -0.008103    -0.249703     0.220093    -0.0400859     0.0434188     0.0260971  -0.0183504    0.0814017    -0.026066     0.162753     0.411193   -0.110142     0.107784      0.0277968    0.054254     0.166743      0.058848    -0.0901275   -0.255136     -0.0665489    -0.0236561    0.106199    -0.0599008    0.0587992   -0.172814     0.102026  
  0.136487    -0.0493283   -0.0858836   -0.0699107     0.0147522    -0.271304    0.244814    -0.16494       0.0281544    0.0866667    0.10013    -0.183485     0.163227      0.0181502   -0.0607919    0.116089     -0.0125525   -0.178583     0.0359936    -0.0652664    -0.15254      0.0102951   -0.171817    -0.11593     -0.180999    -0.0660049 
  0.127378    -0.146056    -0.0288764    0.0161148     0.0386959     0.129222    0.0815034    0.0698433     0.263375    -0.00567686  -0.053125   -0.0901355   -0.175915      0.0279265   -0.0627427    0.154948      0.111007     0.0868507    0.131116      0.0464751    -0.0277534   -0.00428324   0.0931424   -0.130115    -0.0975872   -0.00875431
 -0.0671311   -0.0469812    0.234748    -0.0307064     0.276228     -0.141748   -0.128456     0.0481311    -0.176118     0.132526    -0.179869   -0.10187      0.0975823     0.0937526   -0.192672    -0.0210531     0.0463099    0.236199     0.157105     -0.252474     -0.151066    -0.525339     0.245903    -0.126491     0.147907    -0.00315357
 -0.063846    -0.114873     0.200222    -0.0035772    -0.121264      0.138294   -0.128035     0.26347      -0.173373    -0.309408     0.0379838  -0.0352381    0.181019      0.0935025   -0.208824    -0.00467306    0.0517996    0.046703     0.0247075    -0.25241      -0.121902     0.573922     0.062226    -0.102809    -0.197403    -0.27736   
  0.122487     0.00973722   0.196784    -0.463473      0.0544227     0.0526329   0.0121168    0.032695      0.110585    -0.0940164   -0.0808057  -0.163164     0.034849      0.00446969  -0.12337     -0.0340591     0.006085     0.0903014   -0.303011     -0.0650801     0.152145    -0.0962131   -0.0341383   -0.0198714   -0.110897    -0.0398629 
  0.271196     0.0401939    0.21071      0.235128      0.123374      0.10885    -0.0219514    0.163425      0.261433     0.0248565   -0.0852486  -0.106746     0.124809      0.136845    -0.127634    -0.0593568    -0.135984     0.113922    -0.228658     -0.0494099    -0.00432804   0.163407     0.0533755    0.00215372  -0.121491     0.0486023 
  0.100759     0.0127501    0.0717591   -0.0426251     0.0857661     0.0780412  -0.0467977    0.0954904    -0.00142592  -0.027758     0.0092767   0.00505557   0.0309765    -0.0307659   -0.00794805  -0.0188693    -0.0447668   -0.0577217    0.0342879    -0.0070759    -0.0302652   -0.029938     0.140122    -0.0687811   -0.015607    -0.0123677 
 -0.128744    -0.0408875    0.0207439    0.143602      0.137115     -0.0309833  -0.0773668   -0.0215574     0.0705909   -0.09921      0.0163745   0.185923     0.0167352    -0.0742344   -0.075169    -0.113435      0.0195764    0.00210349  -0.199648      0.00775278    0.0245709    0.151172     0.151249    -0.0414704   -0.0556985   -0.0576091 
  0.0542839    0.13419      0.0167552    0.14611      -0.0389707    -0.106969   -0.0474755   -0.144566     -0.00718049  -0.482735    -0.0689085   0.233818     0.13067      -0.0267095    0.11324     -0.158949      0.0629486    0.0779724    0.0852924     0.120077      0.0776388   -0.294863     0.121726    -0.116628    -0.070266    -0.119238  
 -0.131385     0.0213722    0.168852     0.108343     -0.0916956    -0.149271   -0.20509     -0.0929587    -0.133093     0.368282    -0.154404    0.184495    -0.205198     -0.11831      0.0705454    0.0114993     0.158683     0.0670134    0.0860194     0.125678      0.103401     0.260444     0.159669    -0.0952033   -0.0245902   -0.349196  
  0.00682409   0.00493716  -0.00412173  -0.157812     -0.189301      0.0499117   0.103989    -0.0443       -0.264287     0.00637689   0.0319412  -0.0905889   -0.125042      0.0821683   -0.124267     0.113869      0.222514     0.0514259    0.138638     -0.162343     -0.0966871   -0.0488907    0.0384771    0.0677094   -0.0308157   -0.0583953 
 -0.0447517   -0.0759246    0.057066    -0.118615     -0.0792071    -0.0526006   0.0390802    0.0979193     0.0465279    0.116294    -0.0236549  -0.0118564    0.0698647    -0.0451025   -0.011226    -0.0843771    -0.0262666    0.010873    -0.0990284    -0.0413429    -0.0441985   -0.00322043   0.0484748    0.0498869    0.0135618   -0.0124111 
 -0.0463361    0.0365959   -0.087717     0.0434856    -0.0427463     0.0245213   0.0458589    0.0512094     0.129672    -0.124318    -0.0700808   0.175178    -0.0957907    -0.0518077   -0.0737686   -0.119975     -0.0195406   -0.0896619   -0.0117203    -0.00452558   -0.136673    -0.0876525    0.00260012   0.132735     0.0771969    0.175338  
 -0.0871114    0.0487115    0.0576806    0.0078811     0.120046      0.0694723  -0.0502181    0.064209      0.150715     0.25061      0.160857   -0.197501    -0.0470534     0.0427042   -0.0797304    0.0637804     0.0169904   -0.220698     0.0103709    -0.0515639    -0.0528007    0.1641      -0.17122      0.0499935    0.172625     0.00708772
  0.0118112   -0.109064    -0.105107    -0.0669524     0.0458203    -0.0776145  -0.0311028    0.0493443    -0.14973     -0.20679      0.219828    0.127701     0.0460819    -0.0427664   -0.020469    -0.0405574    -0.0192338   -0.00463113  -0.0386702    -0.175297     -0.0654762    0.087492     0.0710031    0.115741     0.0629581    0.0750182 
  0.0985292   -0.111258     0.133283    -0.0765076     0.0148128     0.0214439  -0.0870044    0.219108     -0.219272    -0.00534875   0.0550676   0.0720002    0.0565547     0.0500019   -0.0421246    0.0661175     0.0728191   -0.0337262   -0.0411928     0.0842281     0.0390902    0.0361068    0.181789    -0.0107171   -0.0774941    0.0822155 
  0.031776    -0.038157     0.0251557   -0.12119      -0.0851719     0.0442437   0.0481661    0.0294655    -0.0741707    0.0577161    0.0258961   0.0324332   -0.00655405   -0.0506773    0.0773699   -0.0186939     0.0610842    0.134948    -0.0313903    -0.0150493     0.0576589    0.00119637  -0.0158438    0.0819567    0.012934    -0.043655  
 -0.0620189   -0.00271459  -0.0167362    0.053999     -0.0352417     0.0327049  -0.0257275    0.000464685   0.0462174    0.0592822   -0.0249643   0.0440552    0.0177776    -0.036931    -0.05903      0.018639     -0.0144532   -0.00463955   0.00554668   -0.08404       0.0571729    0.0358376   -0.00944305   0.0233514    0.0549265    0.00908781
 -0.0113826    0.0578966    0.00120624  -0.0177475    -0.107052     -0.101753   -0.0439542    0.0786077     0.07461      0.104956     0.04493     0.0191876   -0.123449      0.0549399    0.0401808   -0.0568083    -0.0300089   -0.121916     0.0341659    -0.00703543   -0.132756     0.0107979   -0.0921829    0.00773228   0.0650329    0.111273  
 -0.10545     -0.198797     0.0259693   -0.0194753    -0.0263089    -0.0382638   0.00540889  -0.0410878     0.0777083    0.0585737   -0.0349599  -0.0373901    0.130647     -0.0576681    0.00374194  -0.00537778   -0.0530488    0.0795205    0.00225059   -0.0405298     0.0185852    0.0571118    0.0468383    0.0169149    0.022443     0.151953  
 -0.0804014    0.0999822    0.0733152    0.0154358     0.139183     -0.117498   -0.137622    -0.323521      0.0850274   -0.115456     0.134366    0.101008    -0.0866563     0.0380647   -0.134492     0.0323968    -0.0337112   -0.868218     0.000404611   0.054096     -0.00784956  -0.124534     0.00530803   0.0361619   -0.00565814   0.187022  
 -0.039654     0.109024    -0.0965926    0.000985145   0.133183     -0.0523695   0.019825    -0.191918      0.0887889   -0.118017     0.148378    0.100311    -0.0611608     0.012763     0.0664848    0.161827     -0.029342     0.773101    -0.00968147    0.215708     -0.00705543  -0.122948     0.0407553    0.161096    -0.0182606   -0.215027  
  0.185778    -0.150832    -0.302703     0.0479234     0.0415406     0.0033376  -0.0987245    0.0331722    -0.335444     0.0398493    0.048459   -0.176519     0.0306204     0.129031    -0.373283    -0.290983     -0.0563551   -0.0233085   -0.0921938     0.0520924     0.0293094    0.165844     0.232087     0.0981772   -0.376069    -0.00844559
  0.0771037   -0.0609823   -0.161337     0.00386222    0.00471378    0.0625444  -0.0964224    0.124123      0.418789     0.126809    -0.212075   -0.206875     0.0319851     0.128962     0.305667    -0.0111655    -0.0614278   -0.030953    -0.0756066     0.11742       0.0284669   -0.171972     0.231988     0.0975761    0.289117     0.088246  
 -0.0101802   -0.0328491   -0.0333006    0.0339631     0.000908942   0.032147   -0.0127685    0.117734      0.0123738    0.0544595   -0.126841    0.0938622    0.0294511    -0.068885    -0.0883364   -0.0264165    -0.03115     -0.0202193   -0.0865279     0.000869066  -0.0100702   -0.0359666   -0.0580325    0.0581191   -0.0422956   -0.0726181 
  0.0335118    0.0349408    0.0577516    0.0443537     0.0918338    -0.0960782   0.020897    -0.0132493    -0.0393805   -0.0792644    0.0711791  -0.143785     0.0300912     0.0407995    0.148608    -0.0784378    -0.00543899  -0.0905403   -0.159692      0.000907449  -0.0182283    0.0536601    0.0372896   -0.0523039   -0.0340198   -0.0735002 
 -0.0595186   -0.0658463    0.103626    -0.0887659    -0.0673074     0.027074   -0.0863371   -0.125071      0.0908116   -0.203189     0.0453318   0.0528001    0.0170762    -0.026247     0.100958     0.000664989  -0.0121565   -0.168643     0.118259     -0.00496921   -0.0159218   -0.0627974   -0.036278    -1.46709     -0.0871799    0.0400183 
 -0.0530766   -0.0691235   -0.0207703   -0.0969526    -0.0393977     0.0607458  -0.0575916   -0.0615917     0.0608115   -0.206686    -0.012954    0.0533615    0.0428408    -0.00670853   0.122252     0.0034012     0.0108789   -0.20267      0.129367      0.194096      0.0425517   -0.0430751    0.0148308    1.19692     -0.15634      0.0438632 
 -1.12965     -0.0334469   -0.00586165  -0.00830247   -0.114337     -0.0173273  -0.0546321   -0.0352151     0.159717     0.0178114   -0.0541306  -0.125156     0.000393492  -0.134573    -0.151253     0.0323707    -0.0875578   -0.140079    -0.0076866     0.107698     -0.00673024  -0.0396845   -0.0891741    0.0389496   -0.00324227  -0.0227263 
  1.00284     -0.0344193    0.00661729  -0.0114962    -0.00212114   -0.0151866  -0.0568718   -0.050457      0.0394594    0.031308     0.0604559  -0.0287343   -0.028266     -0.182266    -0.0601962    0.0495654    -0.0637073   -0.0911575   -0.00268314   -0.145269     -0.0371865   -0.03977     -0.172075     0.0123212   -0.0135297    0.0104534 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 2 5 6 11 12 23 24 25 26
INFO: iteration 1, average log likelihood -1.112182
WARNING: Variances had to be floored 1 2 4 5 6 10 11 12 23 24 25 26
INFO: iteration 2, average log likelihood -1.082968
WARNING: Variances had to be floored 1 2 5 6 11 12 14 18 23 24 25 26 28
INFO: iteration 3, average log likelihood -1.089572
WARNING: Variances had to be floored 1 2 4 5 6 10 11 12 23 24 25 26
INFO: iteration 4, average log likelihood -1.093958
WARNING: Variances had to be floored 1 2 5 6 11 12 23 24 25 26
INFO: iteration 5, average log likelihood -1.099034
WARNING: Variances had to be floored 1 2 4 5 6 10 11 12 18 23 24 25 26 28
INFO: iteration 6, average log likelihood -1.072371
WARNING: Variances had to be floored 1 2 5 6 11 12 14 23 24 25 26
INFO: iteration 7, average log likelihood -1.100177
WARNING: Variances had to be floored 1 2 4 5 6 10 11 12 23 24 25 26 28
INFO: iteration 8, average log likelihood -1.086609
WARNING: Variances had to be floored 1 2 5 6 11 12 18 23 24 25 26
INFO: iteration 9, average log likelihood -1.096745
WARNING: Variances had to be floored 1 2 4 5 6 10 11 12 23 24 25 26 28
INFO: iteration 10, average log likelihood -1.081539
INFO: EM with 100000 data points 10 iterations avll -1.081539
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.131030e+05
      1       7.089205e+05      -2.041826e+05 |       32
      2       6.772894e+05      -3.163102e+04 |       32
      3       6.598594e+05      -1.743005e+04 |       32
      4       6.497921e+05      -1.006730e+04 |       32
      5       6.432315e+05      -6.560607e+03 |       32
      6       6.385487e+05      -4.682825e+03 |       32
      7       6.338308e+05      -4.717896e+03 |       32
      8       6.293014e+05      -4.529321e+03 |       32
      9       6.262135e+05      -3.087941e+03 |       32
     10       6.244259e+05      -1.787636e+03 |       32
     11       6.232552e+05      -1.170708e+03 |       32
     12       6.224668e+05      -7.883872e+02 |       32
     13       6.217648e+05      -7.019782e+02 |       32
     14       6.210725e+05      -6.922652e+02 |       32
     15       6.204383e+05      -6.342076e+02 |       32
     16       6.199063e+05      -5.320478e+02 |       32
     17       6.195024e+05      -4.039209e+02 |       32
     18       6.191711e+05      -3.312637e+02 |       32
     19       6.188805e+05      -2.906381e+02 |       32
     20       6.186215e+05      -2.589068e+02 |       32
     21       6.184506e+05      -1.709718e+02 |       32
     22       6.183447e+05      -1.058449e+02 |       32
     23       6.182874e+05      -5.734603e+01 |       32
     24       6.182458e+05      -4.157630e+01 |       31
     25       6.182183e+05      -2.746483e+01 |       32
     26       6.182011e+05      -1.726317e+01 |       30
     27       6.181887e+05      -1.241229e+01 |       31
     28       6.181790e+05      -9.637192e+00 |       28
     29       6.181721e+05      -6.932928e+00 |       30
     30       6.181685e+05      -3.639462e+00 |       23
     31       6.181654e+05      -3.087482e+00 |       29
     32       6.181631e+05      -2.277520e+00 |       22
     33       6.181613e+05      -1.798210e+00 |       22
     34       6.181602e+05      -1.125580e+00 |       18
     35       6.181594e+05      -7.236237e-01 |       10
     36       6.181591e+05      -3.260266e-01 |        3
     37       6.181591e+05      -4.553230e-02 |        5
     38       6.181590e+05      -1.230236e-01 |        6
     39       6.181588e+05      -1.926873e-01 |        5
     40       6.181587e+05      -7.538786e-02 |        4
     41       6.181586e+05      -4.135498e-02 |        4
     42       6.181586e+05      -4.496883e-02 |        6
     43       6.181584e+05      -1.718894e-01 |        5
     44       6.181581e+05      -3.186751e-01 |        7
     45       6.181579e+05      -2.535771e-01 |        4
     46       6.181577e+05      -1.896653e-01 |        5
     47       6.181575e+05      -1.431516e-01 |        5
     48       6.181574e+05      -7.090607e-02 |        6
     49       6.181574e+05      -7.319583e-02 |        2
     50       6.181574e+05      -2.077525e-02 |        0
K-means terminated without convergence after 50 iterations (objv = 618157.3545083154)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.345102
INFO: iteration 2, average log likelihood -1.315388
INFO: iteration 3, average log likelihood -1.281170
INFO: iteration 4, average log likelihood -1.239032
WARNING: Variances had to be floored 6
INFO: iteration 5, average log likelihood -1.180162
WARNING: Variances had to be floored 8 18 29 32
INFO: iteration 6, average log likelihood -1.124925
WARNING: Variances had to be floored 2 21 25
INFO: iteration 7, average log likelihood -1.119274
WARNING: Variances had to be floored 5 6 9 10 12 17 27
INFO: iteration 8, average log likelihood -1.085686
WARNING: Variances had to be floored 11 19
INFO: iteration 9, average log likelihood -1.133054
WARNING: Variances had to be floored 8 18 20 29
INFO: iteration 10, average log likelihood -1.085847
WARNING: Variances had to be floored 2 6 23 32
INFO: iteration 11, average log likelihood -1.093820
WARNING: Variances had to be floored 5 9 19 25
INFO: iteration 12, average log likelihood -1.096202
WARNING: Variances had to be floored 1 10 17 18 27
INFO: iteration 13, average log likelihood -1.080169
WARNING: Variances had to be floored 8 12 20 29
INFO: iteration 14, average log likelihood -1.079255
WARNING: Variances had to be floored 2 6 11 23 32
INFO: iteration 15, average log likelihood -1.085559
WARNING: Variances had to be floored 5 9 19 25
INFO: iteration 16, average log likelihood -1.104849
WARNING: Variances had to be floored 8 10 18 27 29
INFO: iteration 17, average log likelihood -1.088776
WARNING: Variances had to be floored 1 17 20
INFO: iteration 18, average log likelihood -1.103539
WARNING: Variances had to be floored 2 6 12 23 32
INFO: iteration 19, average log likelihood -1.071178
WARNING: Variances had to be floored 5 8 9 11 19 25 29
INFO: iteration 20, average log likelihood -1.090247
WARNING: Variances had to be floored 10 18 27
INFO: iteration 21, average log likelihood -1.131288
WARNING: Variances had to be floored 17
INFO: iteration 22, average log likelihood -1.100107
WARNING: Variances had to be floored 1 2 6 20 23 32
INFO: iteration 23, average log likelihood -1.034263
WARNING: Variances had to be floored 5 8 9 12 19 25 29
INFO: iteration 24, average log likelihood -1.072510
WARNING: Variances had to be floored 10 11 18 27
INFO: iteration 25, average log likelihood -1.137743
WARNING: Variances had to be floored 17
INFO: iteration 26, average log likelihood -1.121591
WARNING: Variances had to be floored 2 6 23
INFO: iteration 27, average log likelihood -1.049428
WARNING: Variances had to be floored 1 5 8 9 19 20 25 29 32
INFO: iteration 28, average log likelihood -1.045120
WARNING: Variances had to be floored 10 11 18 27
INFO: iteration 29, average log likelihood -1.133166
WARNING: Variances had to be floored 12 17 23
INFO: iteration 30, average log likelihood -1.117816
WARNING: Variances had to be floored 2 6
INFO: iteration 31, average log likelihood -1.084563
WARNING: Variances had to be floored 5 8 9 25 29 32
INFO: iteration 32, average log likelihood -1.055891
WARNING: Variances had to be floored 1 10 12 17 18 19 20 23 27
INFO: iteration 33, average log likelihood -1.086707
WARNING: Variances had to be floored 6 11
INFO: iteration 34, average log likelihood -1.149811
WARNING: Variances had to be floored 2
INFO: iteration 35, average log likelihood -1.116848
WARNING: Variances had to be floored 5 8 9 25 29 32
INFO: iteration 36, average log likelihood -1.057690
WARNING: Variances had to be floored 6 17 18 19 20 23
INFO: iteration 37, average log likelihood -1.085522
WARNING: Variances had to be floored 1 12 27
INFO: iteration 38, average log likelihood -1.122969
WARNING: Variances had to be floored 2 10 11
INFO: iteration 39, average log likelihood -1.083526
WARNING: Variances had to be floored 5 8 9 23 25 29 32
INFO: iteration 40, average log likelihood -1.055248
WARNING: Variances had to be floored 6 18 19
INFO: iteration 41, average log likelihood -1.118527
WARNING: Variances had to be floored 17 20 27
INFO: iteration 42, average log likelihood -1.107109
WARNING: Variances had to be floored 2 12 29
INFO: iteration 43, average log likelihood -1.061549
WARNING: Variances had to be floored 1 5 8 9 10 18 23 25 32
INFO: iteration 44, average log likelihood -1.055354
WARNING: Variances had to be floored 6
INFO: iteration 45, average log likelihood -1.145749
WARNING: Variances had to be floored 11 19 20 27
INFO: iteration 46, average log likelihood -1.100697
WARNING: Variances had to be floored 2 9 12 17 29
INFO: iteration 47, average log likelihood -1.064819
WARNING: Variances had to be floored 5 6 8 18 23 25 32
INFO: iteration 48, average log likelihood -1.079435
WARNING: Variances had to be floored 1
INFO: iteration 49, average log likelihood -1.150254
WARNING: Variances had to be floored 10
INFO: iteration 50, average log likelihood -1.084146
INFO: EM with 100000 data points 50 iterations avll -1.084146
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0627064   0.0875568    0.0856793    0.126381    -0.0762996  -0.111541     -0.105596    -0.118661     -0.100295    -0.0731749   -0.101501      0.187587   -0.0477368   -0.0825557   0.0844029   -0.0721558    0.153837      0.0722075    0.08712      0.0943727    0.104003    -0.0435755    0.147708    -0.0993963   -0.0420192    -0.252039   
 -0.098587   -0.0648085    0.0850812   -0.0396447    0.0395761   0.0394829    -0.0206339    0.0277247    -0.0213217    0.0681301    0.0269603    -0.113182    0.0428707    0.0402629   0.0494583    0.191154     0.0677197    -0.0518652   -0.160996    -0.0661202    0.054946     0.0926389   -0.0659379    0.0589282   -0.117182      0.072106   
  0.0681544   0.0688578   -0.135669    -0.0664409   -0.0877046   0.17565       0.129        0.000722139  -0.214482    -0.0301481    0.023021      0.0924877  -0.0575026   -0.0177525   0.141481    -0.00469077  -0.0098803     0.128621     0.205842     0.0453722    0.0894254    0.0199808    0.070168     0.044196     0.114713      0.075424   
  0.0878877  -0.0686643    0.0643982   -0.0555831   -0.187386   -0.0155472     0.0516949   -0.0244012    -0.079705     0.19567      0.0878794    -0.082332    0.00669823  -0.0135619   0.116316     0.0103509    0.0569271     0.0185696   -0.254505    -0.0201136    0.0797524   -0.136462    -0.0910548    0.0623337    0.054632     -0.0924158  
 -0.0641705   0.0911381    0.00698561   0.0218647    0.165708   -0.0792558    -0.0437462   -0.339024      0.109203    -0.150324     0.143613      0.0854089  -0.0797381    0.0358361  -0.0296761    0.119277    -0.0129586    -0.219529    -0.0102074    0.180802    -0.00548404  -0.110955     0.0173829    0.0671717   -0.02999      -0.00797034 
  0.121496   -0.101388    -0.224333     0.0320265    0.0188262   0.0264998    -0.0979186    0.0737201     0.0261418    0.0767242   -0.0895453    -0.175727    0.032564     0.130142   -0.0378606   -0.154964    -0.0559146    -0.0235246   -0.076923     0.0834707    0.0277368   -0.00147971   0.232833     0.0904388   -0.051976      0.0349919  
  1.10711    -0.0334619    0.00264871  -0.0134025   -0.0387501  -0.0180238    -0.0551344   -0.0467085     0.0820611    0.0397836    0.0140802    -0.0451609  -0.019601    -0.182195   -0.0851064    0.0553275   -0.0678943    -0.105265    -0.00427225  -0.100347    -0.0305452   -0.0408331   -0.158155     0.0213354   -0.0234343    -0.000243652
  0.153727   -0.16694     -0.028663     0.0162278    0.0306274   0.129265      0.0828136    0.122198      0.251244     0.00503999  -0.0760153    -0.0881144  -0.178961     0.0195007  -0.0601532    0.147814     0.121275      0.175758     0.127095     0.04779     -0.0249421   -0.00743372   0.120668    -0.138363    -0.0964602    -0.00788505 
 -0.0919603  -0.0645849   -0.0769056   -0.0232749    0.0866635   0.0102438    -0.0683854   -0.0625729     0.00125558   0.0134437   -0.0506074     0.0221952   0.0903183    0.048835   -0.079278     0.0796381    0.00397156   -0.0424089    0.128199    -0.0986263    0.148912     0.0569979    0.0152344   -0.0309328   -0.0803111    -0.147267   
  0.104774   -0.0123637    0.227017     0.0329547    0.110768    0.158672     -0.0922272    0.062946     -0.0720558   -0.0276619   -0.000745782   0.0613026   0.0262704   -0.0413677   0.00656085   0.01578     -0.107446     -0.0407787    9.83726e-5  -0.00794165  -0.0625897   -0.00883927   0.122857    -0.178377    -0.0482841    -0.0762132  
 -0.138458   -0.106236    -0.130304    -0.0321687   -0.0111942   0.0265262    -0.10206     -0.0500946     0.0980797   -0.00439575   0.0110556    -0.01385     0.164231    -0.0228315  -0.0789446    0.108671    -0.0883753     0.0778366    0.0267917   -0.0577416    0.0821508    0.0594564    0.0820568    0.00852611   0.00100124    0.138039   
  0.0343341   0.0645929    0.0198485    0.0239977   -0.152285   -0.0262869    -0.0965449    0.0378769     0.0696483    0.112888     0.00139438   -0.01499    -0.237782     0.0543637   0.0674796   -0.0313621    0.0519547    -0.272309     0.119302     0.0562439   -0.14473      0.107209    -0.106681    -0.151268     0.0529081     0.017609   
 -0.0365621  -0.0701693    0.108855    -0.254003     0.0324746   0.0144638    -0.0180103    0.0462058     0.025107    -0.0525812   -0.0430493     0.0983297  -0.0054067   -0.0863998  -0.0325812   -0.0447738    0.172027      0.255405     0.0271755   -0.0516374    0.0185804    0.135938    -0.0314703    0.161739    -0.159134     -0.0872742  
  0.0158675  -0.101606    -0.116568    -0.0482402    0.0536368  -0.0695593    -0.0320802    0.0326762    -0.147915    -0.222393     0.230973      0.13016     0.0456819   -0.0468316  -0.0282342   -0.0371526   -0.0110922    -0.00922269  -0.0201101   -0.183925    -0.0662671    0.0841804    0.066142     0.12448      0.0635412     0.0812512  
 -1.61322    -0.0369926   -0.002777    -0.00449917  -0.100037   -0.0152199    -0.0591512   -0.0349704     0.139413     0.0225996   -0.0256557    -0.110627   -0.00875367  -0.136362   -0.133098     0.0249138   -0.0853025    -0.135195    -0.00932094   0.0947559   -0.011777    -0.0392974   -0.0992291    0.0285171    0.00533177   -0.00834891 
  0.129639   -0.0487931   -0.0876599   -0.0636284    0.0168671  -0.258107      0.240856    -0.168331      0.0360183    0.0835219    0.0978581    -0.18275     0.15274      0.0182544  -0.0621244    0.117573    -0.0099517    -0.177995     0.0369666   -0.0724672   -0.150925     0.00875934  -0.160397    -0.118987    -0.177803     -0.0636041  
  0.0282068  -0.0708395   -0.0231676    0.0893177   -0.0862945   0.101532     -0.163019     0.0697589     0.110138     0.0915198    0.0226575     0.135524    0.0255957   -0.107404   -0.086174     0.0463114   -0.155123     -0.0507826   -0.0453535    0.118025     0.109374    -0.0597748   -0.0338502    0.0509642    0.149473      0.025942   
 -0.0695169  -0.0763932    0.216263    -0.00201028   0.0755907   0.000947191  -0.126385     0.145004     -0.171549    -0.0983728   -0.0652927    -0.058383    0.138213     0.0961218  -0.194373    -0.00704948   0.0532123     0.144775     0.0893762   -0.239504    -0.141984     0.0391288    0.15488     -0.119079    -0.0233386    -0.14543    
  0.0319143  -0.0258874    0.0546647   -0.0020452    0.0546952   0.0366719     0.0912049    0.122684     -0.060295     0.0618077   -0.00750982    0.153266   -0.0287779   -0.0645329  -0.133881    -0.0836567    0.0693264    -0.0111426   -0.146599    -0.10903      0.0406636    0.0818714   -0.0159571    0.106348    -0.0278941    -0.0643795  
  0.0992612  -0.110515     0.128223    -0.077046     0.0165133   0.0175295    -0.0851613    0.218158     -0.217995    -0.0129829    0.0558819     0.0725792   0.0540707    0.0453719  -0.0406982    0.0641044    0.073549     -0.0333453   -0.0412731    0.0815288    0.037688     0.0360741    0.181398    -0.0128357   -0.0776281     0.081976   
 -0.0551308  -0.216254     0.166006    -0.006454    -0.014103   -0.121198      0.0980273   -0.0572513     0.0560552    0.0812623   -0.0330729    -0.0264002   0.0485212   -0.0809871   0.103642    -0.105572    -0.00488632    0.0663266   -0.02285      0.00700427  -0.0617468    0.00216673   0.0103599    0.0518669    0.0425383     0.142352   
 -0.0428454   0.0416565   -0.0915811    0.0491326   -0.0431454   0.0265183     0.0466252    0.0499346     0.125467    -0.13085     -0.0689393     0.174327   -0.100651    -0.0519213  -0.0745497   -0.123124    -0.0182153    -0.0925659   -0.0112202   -0.00235625  -0.13731     -0.0905789    0.00019472   0.131031     0.073794      0.180227   
 -0.0513143   0.0565888   -0.0280239   -0.0323502   -0.0903721  -0.185319      0.0257632    0.115369      0.0909899    0.0924285    0.0877251     0.0257939  -0.0161185    0.0722154   0.0366194   -0.0771572   -0.150376      0.0273959   -0.0316406   -0.0671335   -0.15629     -0.103639    -0.168441     0.129796     0.0807191     0.262199   
 -0.109032    0.151886     0.0521806    0.191112    -0.12822    -0.0277704     0.0726706    0.00994289    0.0506991    0.0905405   -0.0328736    -0.0145965  -0.0684709   -0.0679886  -0.0490508   -0.0306092    0.0942878     0.0418305   -0.103165    -0.317066    -0.0825654    0.0721078   -0.0293179    0.0235421    0.103398      0.149198   
 -0.12171    -0.0299949    0.0211799    0.146508     0.161457   -0.0443866    -0.136776    -0.0175128     0.0696801   -0.0942304    0.0332758     0.222177    0.00525615  -0.0728738  -0.0691662   -0.112072     0.0183278     0.0138231   -0.206024    -0.00991776   0.0248857    0.166097     0.238392    -0.0192969   -0.0551906    -0.0691334  
  0.108943    0.0129076    0.115806    -0.127071    -0.0343562   0.0590136     0.0396329    0.0346032    -0.00918893  -0.00556784  -0.0345798    -0.105063   -0.00655292   0.0787559  -0.123421     0.0239136    0.0579495     0.0777979   -0.0801476   -0.099602    -0.00461932   0.00790291   0.0245564    0.0303595   -0.0808129    -0.0251897  
 -0.0831884  -0.0938079    0.0840393   -0.152851    -0.0389553  -0.103659      0.0142007    0.147156      0.0922745    0.115122    -0.0108964     0.0104613   0.0849322   -0.0529242  -0.014518    -0.116874    -0.0809546     0.0279859   -0.118792    -0.0322719   -0.0419782    0.017592     0.0517149    0.0316731    0.0143539    -0.0818241  
 -0.0661655  -0.00881408  -0.101835     0.0841938   -0.0362638   0.00215283   -0.0840407    0.0532891     0.0444605    0.0163376   -0.242758     -0.0334706   0.0772198   -0.0435571   0.0188902    0.0256216   -0.100818     -0.0413632   -0.0321515    0.121722    -0.0636014   -0.125097    -0.0955747   -0.0122496   -0.0566456    -0.0906763  
  0.0758829   0.0242066   -0.0854862   -0.10719      0.0720087  -0.000981816  -0.00554379   0.109485      0.0710511   -0.0434189    0.0225199    -0.0360375   0.0348829   -0.0233608  -0.0292998   -0.0614311    0.0161237    -0.0764107    0.0444838    0.00270518   0.00199735  -0.0235506    0.154722     0.0272408   -0.000452549   0.0446849  
 -0.0860591   0.0478713    0.0581608    0.00706945   0.120268    0.0696689    -0.050846     0.0630638     0.150029     0.2509       0.161522     -0.198459   -0.0487019    0.0428149  -0.0771386    0.0643608    0.0171857    -0.220353     0.0112867   -0.0513618   -0.0517318    0.164219    -0.17258      0.0510944    0.172673      0.00718592 
 -0.054475   -0.0660817    0.0428972   -0.0932364   -0.0519211   0.0432205    -0.0771307   -0.0941        0.077463    -0.205071     0.0171524     0.0533794   0.029067    -0.0167053   0.111434     0.00279347  -0.000100502  -0.185679     0.123886     0.0915662    0.0116083   -0.053708    -0.0124337   -0.17287     -0.121816      0.041274   
  0.0662724   0.0551895    0.0770171    0.100418     0.147478   -0.112701      0.0557051   -0.0406538    -0.0817216   -0.126971     0.111966     -0.13986     0.00680968   0.0643979   0.136749    -0.0997151    0.0279134    -0.112692    -0.189615     0.0073116   -0.0108374    0.0743631    0.054263    -0.0490148   -0.0340138    -0.0792018  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 2 6 8 9 12 17 19 20 23 25 27 29 32
INFO: iteration 1, average log likelihood -1.015712
WARNING: Variances had to be floored 2 5 6 8 9 11 12 17 18 19 20 23 25 27 29 32
INFO: iteration 2, average log likelihood -0.988194
WARNING: Variances had to be floored 1 2 5 6 8 9 10 11 12 17 18 19 20 23 25 27 29 32
INFO: iteration 3, average log likelihood -0.992492
WARNING: Variances had to be floored 2 6 8 9 12 17 19 20 23 25 27 29 32
INFO: iteration 4, average log likelihood -1.015149
WARNING: Variances had to be floored 2 5 6 8 9 11 12 17 18 19 20 23 25 27 29 32
INFO: iteration 5, average log likelihood -0.987932
WARNING: Variances had to be floored 1 2 5 6 8 9 10 11 12 17 18 19 20 23 25 27 29 32
INFO: iteration 6, average log likelihood -0.992460
WARNING: Variances had to be floored 2 6 8 9 12 17 19 20 23 25 27 29 32
INFO: iteration 7, average log likelihood -1.015149
WARNING: Variances had to be floored 2 5 6 8 9 11 12 17 18 19 20 23 25 27 29 32
INFO: iteration 8, average log likelihood -0.987931
WARNING: Variances had to be floored 1 2 5 6 8 9 10 11 12 17 18 19 20 23 25 27 29 32
INFO: iteration 9, average log likelihood -0.992460
WARNING: Variances had to be floored 2 6 8 9 12 17 19 20 23 25 27 29 32
INFO: iteration 10, average log likelihood -1.015149
INFO: EM with 100000 data points 10 iterations avll -1.015149
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0996071   -0.0442567     0.0307831    0.0869442    -0.0307025   -0.0917753   0.0441679   -0.121362     0.0436221    0.143533     0.171696     -0.0864547   -0.0028686   -0.0135909     0.00432921   -0.0464295   -0.00325235   -0.0458649   -0.052172    -0.077026     0.0821415   0.121897     0.143828     0.148718     0.0231569   -0.0423396 
  0.0668908    0.00486087    0.126452     0.0779335    -0.00444942  -0.197001   -0.136669     0.00689521  -0.00892127  -0.241587     0.135342     -0.0345944    0.0189203    0.0875727    -0.101001      0.0981178   -0.102959      0.153649     0.0502528    0.0680989    0.0050898  -0.0749478   -0.0401308   -0.0403329   -0.0520638    0.00275264
 -0.0239885   -0.142868     -0.0289585   -0.0393937    -0.00147187  -0.029614    0.120697     0.0394471    0.0892515   -0.0523658    0.124239     -0.018548     0.165347    -0.056827     -0.0714728     0.00379745   0.102506      0.0305323    0.0956408    0.0507156    0.0894711   0.153644     0.0727421   -0.0097786   -0.0756545   -0.186121  
  0.00944786   0.0222772     0.0765261   -0.234593     -0.0793426    0.0893984   0.184277     0.0164847   -0.0961618   -0.163976     0.0784689    -0.0941306    0.0669093    0.041141      0.0192517    -0.011851     0.128372     -0.0671455   -0.00912393   0.0578083    0.0356691  -0.0649108   -0.00960534  -0.206051    -0.019257    -0.227512  
  0.00498491  -0.00725338    0.141079     0.060927      0.0197647    0.142764    0.0309843   -0.185712     0.0520768    0.0012447    0.0497648    -0.00709031  -0.0251613    0.116071     -0.277543     -0.0349395   -0.0667896    -0.0214099    0.0202932   -0.00148845  -0.0487152  -0.0520004   -0.0257001    0.149227    -0.0740737    0.103218  
 -0.135176     0.0971094     0.0315386   -0.0987726    -0.147254     0.14737    -0.00889582   0.115784     0.0561161    0.00731958   0.0927087    -0.0939027    0.10614      0.117204     -0.163321     -0.0421243    0.00145353   -0.184245    -0.0583682    0.00333437  -0.0785651   0.0117719   -0.0765523    0.106106    -0.0594977    0.165622  
  0.0259886   -0.062305      0.31638      0.0193993    -0.172839    -0.14113    -0.103758    -0.170375    -0.101883    -0.0127901   -0.0399663     0.0857228   -0.169299    -0.188238      0.167371      0.00931959  -0.0773587     0.0757153   -0.163673    -0.137656     0.0719124  -0.0870581   -0.0598647    0.157311     0.0460572   -0.0987523 
  0.170839    -0.0163944     0.0992623    0.0339102     0.239853    -0.0261376  -0.134351     0.0770925   -0.0835254    0.0557374    0.137911     -0.135961    -0.00970308   0.0118951     0.00731346    0.112534     0.0968146    -0.0646374   -0.0881189   -0.0362963   -0.0424669   0.246101    -0.0153724    0.0030623    0.0945473   -0.0443284 
 -0.100691    -0.129649      0.00666109   0.12879       0.00972482   0.0483666   0.00569747  -0.021491    -0.0875102   -0.0730533    0.0666181    -0.00282889   0.0905447    0.0395076    -0.232334      0.119217    -0.139862     -0.00186406  -0.072868     0.0159242    0.0478243  -0.0616374   -0.0257487    0.0938847    0.0769454    0.0339044 
  0.0155416    0.0194606     0.147248    -0.0816337     0.0430159   -0.0820718   0.00724026   0.0644747    0.0507017    0.0342595    0.0481979     0.105822    -0.108167    -0.167396      0.0784066     0.0466713    0.272098     -0.0805605   -0.0866398   -0.0280449    0.0281918  -0.0573432   -0.0522488    0.270673    -0.184634     0.0599122 
  0.159984    -0.155517     -0.158876    -0.0508283     0.181963     0.0428469   0.0819144    0.0085647    0.0737733   -0.0809825   -0.0851386    -0.080248     0.184031     0.00708851    0.0195861    -0.0223571    0.121093      0.104906     0.0851938    0.00599002   0.152615   -0.0881887    0.06954      0.0936825    0.0964022    0.0559317 
 -0.0322872   -0.130016     -0.153617    -0.0142075     0.115363    -0.0479889  -0.00461137   0.1211       0.09686     -0.0813146   -0.0848474     0.0763029   -0.0528338    0.0217197    -0.0432003     0.0958281    0.0374972     0.165305     0.11453     -0.0325194    0.0071746   0.00622405  -0.00355958  -0.131417     0.141828     0.0946341 
  0.0747686   -0.0217926     0.180043    -0.0747654     0.0273683   -0.0360171   0.0254995   -0.0330054    0.0927381    0.249549    -0.117878     -0.0273203    0.141765     0.16535       0.0350213     0.0703645    0.116047     -0.112927    -0.0391732   -0.16713     -0.0218359   0.0559616   -0.0579302   -0.0726078   -0.0106095    0.141627  
 -0.074082    -0.205692      0.0441093   -0.0303896    -0.105526     0.103537    0.0447802   -0.216313     0.110309     0.0307075    0.00943407   -0.150667     0.0459865    0.181468      0.0047386    -0.165513    -0.0583306     0.0931296   -0.0299382   -0.083855     0.0572574   0.0496321   -0.174935    -0.04531      0.0922485   -0.0134071 
 -0.0991381   -0.0197122     0.0642126   -0.0558936     0.222026     0.0615447  -0.120416    -0.028794    -0.0252071   -0.0857383    0.0392971    -0.0111309   -0.156387    -0.0191308    -0.0191139    -0.108594     0.0625428     0.0885951   -0.0562821   -0.0673012   -0.0298333  -0.19212     -0.0346374   -0.026168    -0.0459383    0.0334389 
 -0.029828     0.045477      0.044763    -0.109037     -0.0401714    0.0393714   0.0194979   -0.180134    -0.0445224   -0.013924    -0.12038       0.104209     0.112991    -0.0487869     0.0684081     0.121693     0.292191      0.303494    -0.0715522   -0.247257     0.116135   -0.133106    -0.0286043    0.11643     -0.0357157   -0.105629  
 -0.0957548    0.106347      0.138897     0.00834682    0.0368051    0.049759    0.0473865   -0.00561864   0.0822701    0.187605     0.0589212    -0.11436     -0.0190561   -0.00711754   -0.114885      0.0338222   -0.153936      0.0188528   -0.0644743    0.0944951    0.154685    0.016594    -0.131873     0.143564    -0.0283046   -0.226235  
  0.0306084    0.00864265   -0.0646249    0.0764764     0.0918106    0.0517223  -0.00423152   0.0840007   -0.0988384   -0.0865749   -0.1979        0.0321371    0.107636    -0.10169       0.0294754     0.110591     0.0340559    -0.196594    -0.110216    -0.268888     0.129101    0.0551753    0.0862232   -0.0560365   -0.0436575    0.0541934 
  0.0230045    0.0973442     0.0103942    0.00330855    0.0081164    0.0816861  -0.131558    -0.0446824   -0.0244807   -0.085396    -0.085059      0.179145     0.114251     0.0617505     0.0300886     0.0331777   -0.0234124     0.00836455  -0.0773483    0.051565    -0.0362635   0.0699717   -0.0207196   -0.172711     0.00208263  -0.065566  
  0.0406894   -0.0323898     0.0921344    0.0510861    -0.203134    -0.124569   -0.106742     0.0620633    0.0525429    0.0922087    0.0355357     0.10272      0.0641644   -0.114231      0.0153936     0.0108892    0.0895524    -0.0883448    0.0570384    0.0626468   -0.0751172  -0.0829368    0.0840132   -0.0705595   -0.054583     0.0224714 
 -0.0537156   -0.00474468   -0.177991    -0.0758703    -0.114737     0.079339   -0.00978977  -0.0430045    0.126227    -0.0347475   -0.0477787    -0.10078      0.0160242   -0.0193897    -0.0212528    -0.0332678    0.00842648    0.0508535    0.0596744    0.0532468    0.073856    0.0100315   -0.09686      0.0458913    0.0275785   -0.124355  
 -0.0629516   -0.0599493    -0.0108341   -0.0646511     0.149959    -0.0836001   0.0438588    0.0228853    0.056561    -0.0218274   -0.144907     -0.0347452   -0.0658234   -0.0491467     0.0269114     0.065381    -0.109999      0.163262    -0.133714    -0.0136338   -0.0104118  -0.0801443   -0.0961466   -0.0407721    0.00517079   0.0346948 
  0.137643    -0.0822789    -0.0452806    0.270823      0.0399985   -0.239666    0.0261949    0.0509072    0.178737     0.068982    -0.108918      0.119845    -0.0399546   -0.131741      0.0840807    -0.221157     0.251204     -0.0364291   -0.240684    -0.12059     -0.0533032  -0.0943238   -0.0200656   -0.0846325    0.0941548    0.0307869 
 -0.118044     0.000116502   0.178487     0.083013      0.00738345   0.0443294  -0.0666666   -0.0704201   -0.0737579   -0.096688     0.0970333    -0.142503     0.0969216   -0.0370112    -0.137863     -0.0126908   -0.000810356  -0.0779176   -0.0958637    0.0445695   -0.120798   -0.091002    -0.0629085   -0.0631887    0.0336161    0.0181937 
 -0.0198105   -0.204116     -0.136271    -0.00119315   -0.0986253   -0.119164   -0.126062    -0.0585431    0.00854381  -0.0628182   -0.000527423  -0.130197    -0.148295     0.0735471     0.0458676     0.0612758   -0.161989     -0.0191934   -0.0133576    0.0799728    0.102724   -0.198954     0.0764083    0.0264333   -0.145617    -0.0315465 
 -0.052682    -0.15661       0.110926    -0.0583178    -0.0778246    0.101877   -0.0465449   -0.108703    -0.0935387   -0.0886887    0.0832127     0.0399532   -0.0843928   -0.0303893    -0.0374752    -0.077298    -0.0384276     0.164757     0.18107     -0.0237121   -0.0839103  -0.147099    -0.00354501   0.0796019    0.0373083   -0.187137  
 -0.157379     0.260913      0.21487      0.062936      0.0338537   -0.117186   -0.0434842    0.00970093  -0.079408     0.0258361   -0.0637977     0.0553095    0.00752968   0.0806631    -0.0507299    -0.196006    -0.0187733    -0.124161     0.25277     -0.132688    -0.132555   -0.229057    -0.022912    -0.0478211    0.132372    -0.100033  
  0.0827882    0.0702866    -0.173994    -0.126745      0.0979328   -0.0562796   0.0607017   -0.0550844    0.0471207   -0.0813939   -0.0211974     0.137553     0.00945531  -0.0172194     0.0194426    -0.106125    -0.0737887    -0.0164147    0.0161774    0.112925    -0.163242    0.176892     0.101364    -0.0199195    0.050673     0.0788965 
  0.0586901    0.0824088     0.0163326   -0.000801926  -0.0491645    0.0338715  -0.206927    -0.0117975    0.0193466   -0.0300605    0.0760571    -0.0269382    0.0059581   -0.148181      0.0530695     0.045654     0.108031      0.0186696   -0.0656401   -0.109647    -0.0680068   0.119478     0.172007     0.00846598  -0.100702    -0.0278904 
 -0.0390291   -0.0619328     0.142786     0.138884     -0.0324366    0.0400048  -0.0790512   -0.0584465    0.0800634   -0.0903455    0.0290468     0.188395     0.0196872   -0.000704503  -0.252618     -0.0620649    0.071783     -0.114307    -0.0362287    0.1073      -0.234112    0.128484    -0.112945     0.0539781    0.0464577   -0.139661  
 -0.103649     0.00285812   -0.0805498   -0.096284      0.0332041    0.124582   -0.233231    -0.00866835   0.080426     0.126421     0.0158466     0.149696     0.153907     0.0304862    -0.000406381  -0.0677732   -0.0639816     0.0394357   -0.115468    -0.0520494   -0.109906    0.00188319  -0.124662    -0.0206955   -0.162106     0.0286403 
  0.0183325    0.139976     -0.0494023    0.0421682     0.0556458    0.0858385  -0.0930222    0.153858     0.065465    -0.17545      0.185519     -0.00766439   0.120063     0.143682      0.201791     -0.0224717    0.0298124     0.0369061   -0.193783    -0.219154    -0.0888261  -0.0178022    0.00885456   0.175582    -0.00899865   0.0620206 kind full, method split
0: avll = -1.4156703631075578
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.415688
INFO: iteration 2, average log likelihood -1.415615
INFO: iteration 3, average log likelihood -1.415552
INFO: iteration 4, average log likelihood -1.415476
INFO: iteration 5, average log likelihood -1.415384
INFO: iteration 6, average log likelihood -1.415275
INFO: iteration 7, average log likelihood -1.415155
INFO: iteration 8, average log likelihood -1.415023
INFO: iteration 9, average log likelihood -1.414865
INFO: iteration 10, average log likelihood -1.414640
INFO: iteration 11, average log likelihood -1.414271
INFO: iteration 12, average log likelihood -1.413672
INFO: iteration 13, average log likelihood -1.412828
INFO: iteration 14, average log likelihood -1.411909
INFO: iteration 15, average log likelihood -1.411189
INFO: iteration 16, average log likelihood -1.410772
INFO: iteration 17, average log likelihood -1.410574
INFO: iteration 18, average log likelihood -1.410488
INFO: iteration 19, average log likelihood -1.410452
INFO: iteration 20, average log likelihood -1.410436
INFO: iteration 21, average log likelihood -1.410430
INFO: iteration 22, average log likelihood -1.410426
INFO: iteration 23, average log likelihood -1.410425
INFO: iteration 24, average log likelihood -1.410424
INFO: iteration 25, average log likelihood -1.410424
INFO: iteration 26, average log likelihood -1.410423
INFO: iteration 27, average log likelihood -1.410423
INFO: iteration 28, average log likelihood -1.410423
INFO: iteration 29, average log likelihood -1.410422
INFO: iteration 30, average log likelihood -1.410422
INFO: iteration 31, average log likelihood -1.410422
INFO: iteration 32, average log likelihood -1.410422
INFO: iteration 33, average log likelihood -1.410422
INFO: iteration 34, average log likelihood -1.410422
INFO: iteration 35, average log likelihood -1.410422
INFO: iteration 36, average log likelihood -1.410421
INFO: iteration 37, average log likelihood -1.410421
INFO: iteration 38, average log likelihood -1.410421
INFO: iteration 39, average log likelihood -1.410421
INFO: iteration 40, average log likelihood -1.410421
INFO: iteration 41, average log likelihood -1.410421
INFO: iteration 42, average log likelihood -1.410421
INFO: iteration 43, average log likelihood -1.410421
INFO: iteration 44, average log likelihood -1.410421
INFO: iteration 45, average log likelihood -1.410421
INFO: iteration 46, average log likelihood -1.410421
INFO: iteration 47, average log likelihood -1.410421
INFO: iteration 48, average log likelihood -1.410421
INFO: iteration 49, average log likelihood -1.410421
INFO: iteration 50, average log likelihood -1.410421
INFO: EM with 100000 data points 50 iterations avll -1.410421
952.4 data points per parameter
1: avll = [-1.41569,-1.41561,-1.41555,-1.41548,-1.41538,-1.41528,-1.41515,-1.41502,-1.41487,-1.41464,-1.41427,-1.41367,-1.41283,-1.41191,-1.41119,-1.41077,-1.41057,-1.41049,-1.41045,-1.41044,-1.41043,-1.41043,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.410435
INFO: iteration 2, average log likelihood -1.410365
INFO: iteration 3, average log likelihood -1.410300
INFO: iteration 4, average log likelihood -1.410221
INFO: iteration 5, average log likelihood -1.410121
INFO: iteration 6, average log likelihood -1.410001
INFO: iteration 7, average log likelihood -1.409872
INFO: iteration 8, average log likelihood -1.409747
INFO: iteration 9, average log likelihood -1.409639
INFO: iteration 10, average log likelihood -1.409550
INFO: iteration 11, average log likelihood -1.409479
INFO: iteration 12, average log likelihood -1.409423
INFO: iteration 13, average log likelihood -1.409380
INFO: iteration 14, average log likelihood -1.409345
INFO: iteration 15, average log likelihood -1.409318
INFO: iteration 16, average log likelihood -1.409296
INFO: iteration 17, average log likelihood -1.409278
INFO: iteration 18, average log likelihood -1.409262
INFO: iteration 19, average log likelihood -1.409249
INFO: iteration 20, average log likelihood -1.409237
INFO: iteration 21, average log likelihood -1.409226
INFO: iteration 22, average log likelihood -1.409216
INFO: iteration 23, average log likelihood -1.409207
INFO: iteration 24, average log likelihood -1.409199
INFO: iteration 25, average log likelihood -1.409191
INFO: iteration 26, average log likelihood -1.409185
INFO: iteration 27, average log likelihood -1.409178
INFO: iteration 28, average log likelihood -1.409173
INFO: iteration 29, average log likelihood -1.409168
INFO: iteration 30, average log likelihood -1.409163
INFO: iteration 31, average log likelihood -1.409159
INFO: iteration 32, average log likelihood -1.409156
INFO: iteration 33, average log likelihood -1.409153
INFO: iteration 34, average log likelihood -1.409150
INFO: iteration 35, average log likelihood -1.409148
INFO: iteration 36, average log likelihood -1.409145
INFO: iteration 37, average log likelihood -1.409144
INFO: iteration 38, average log likelihood -1.409142
INFO: iteration 39, average log likelihood -1.409140
INFO: iteration 40, average log likelihood -1.409139
INFO: iteration 41, average log likelihood -1.409138
INFO: iteration 42, average log likelihood -1.409137
INFO: iteration 43, average log likelihood -1.409136
INFO: iteration 44, average log likelihood -1.409136
INFO: iteration 45, average log likelihood -1.409135
INFO: iteration 46, average log likelihood -1.409134
INFO: iteration 47, average log likelihood -1.409134
INFO: iteration 48, average log likelihood -1.409133
INFO: iteration 49, average log likelihood -1.409133
INFO: iteration 50, average log likelihood -1.409133
INFO: EM with 100000 data points 50 iterations avll -1.409133
473.9 data points per parameter
2: avll = [-1.41043,-1.41036,-1.4103,-1.41022,-1.41012,-1.41,-1.40987,-1.40975,-1.40964,-1.40955,-1.40948,-1.40942,-1.40938,-1.40935,-1.40932,-1.4093,-1.40928,-1.40926,-1.40925,-1.40924,-1.40923,-1.40922,-1.40921,-1.4092,-1.40919,-1.40918,-1.40918,-1.40917,-1.40917,-1.40916,-1.40916,-1.40916,-1.40915,-1.40915,-1.40915,-1.40915,-1.40914,-1.40914,-1.40914,-1.40914,-1.40914,-1.40914,-1.40914,-1.40914,-1.40914,-1.40913,-1.40913,-1.40913,-1.40913,-1.40913]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.409143
INFO: iteration 2, average log likelihood -1.409078
INFO: iteration 3, average log likelihood -1.409018
INFO: iteration 4, average log likelihood -1.408946
INFO: iteration 5, average log likelihood -1.408858
INFO: iteration 6, average log likelihood -1.408751
INFO: iteration 7, average log likelihood -1.408631
INFO: iteration 8, average log likelihood -1.408509
INFO: iteration 9, average log likelihood -1.408394
INFO: iteration 10, average log likelihood -1.408294
INFO: iteration 11, average log likelihood -1.408210
INFO: iteration 12, average log likelihood -1.408144
INFO: iteration 13, average log likelihood -1.408092
INFO: iteration 14, average log likelihood -1.408052
INFO: iteration 15, average log likelihood -1.408021
INFO: iteration 16, average log likelihood -1.407996
INFO: iteration 17, average log likelihood -1.407976
INFO: iteration 18, average log likelihood -1.407960
INFO: iteration 19, average log likelihood -1.407945
INFO: iteration 20, average log likelihood -1.407932
INFO: iteration 21, average log likelihood -1.407921
INFO: iteration 22, average log likelihood -1.407909
INFO: iteration 23, average log likelihood -1.407899
INFO: iteration 24, average log likelihood -1.407889
INFO: iteration 25, average log likelihood -1.407879
INFO: iteration 26, average log likelihood -1.407869
INFO: iteration 27, average log likelihood -1.407859
INFO: iteration 28, average log likelihood -1.407849
INFO: iteration 29, average log likelihood -1.407840
INFO: iteration 30, average log likelihood -1.407830
INFO: iteration 31, average log likelihood -1.407820
INFO: iteration 32, average log likelihood -1.407811
INFO: iteration 33, average log likelihood -1.407801
INFO: iteration 34, average log likelihood -1.407792
INFO: iteration 35, average log likelihood -1.407783
INFO: iteration 36, average log likelihood -1.407774
INFO: iteration 37, average log likelihood -1.407765
INFO: iteration 38, average log likelihood -1.407757
INFO: iteration 39, average log likelihood -1.407749
INFO: iteration 40, average log likelihood -1.407741
INFO: iteration 41, average log likelihood -1.407734
INFO: iteration 42, average log likelihood -1.407727
INFO: iteration 43, average log likelihood -1.407720
INFO: iteration 44, average log likelihood -1.407714
INFO: iteration 45, average log likelihood -1.407708
INFO: iteration 46, average log likelihood -1.407703
INFO: iteration 47, average log likelihood -1.407697
INFO: iteration 48, average log likelihood -1.407692
INFO: iteration 49, average log likelihood -1.407688
INFO: iteration 50, average log likelihood -1.407683
INFO: EM with 100000 data points 50 iterations avll -1.407683
236.4 data points per parameter
3: avll = [-1.40914,-1.40908,-1.40902,-1.40895,-1.40886,-1.40875,-1.40863,-1.40851,-1.40839,-1.40829,-1.40821,-1.40814,-1.40809,-1.40805,-1.40802,-1.408,-1.40798,-1.40796,-1.40795,-1.40793,-1.40792,-1.40791,-1.4079,-1.40789,-1.40788,-1.40787,-1.40786,-1.40785,-1.40784,-1.40783,-1.40782,-1.40781,-1.4078,-1.40779,-1.40778,-1.40777,-1.40777,-1.40776,-1.40775,-1.40774,-1.40773,-1.40773,-1.40772,-1.40771,-1.40771,-1.4077,-1.4077,-1.40769,-1.40769,-1.40768]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.407687
INFO: iteration 2, average log likelihood -1.407625
INFO: iteration 3, average log likelihood -1.407563
INFO: iteration 4, average log likelihood -1.407486
INFO: iteration 5, average log likelihood -1.407389
INFO: iteration 6, average log likelihood -1.407265
INFO: iteration 7, average log likelihood -1.407116
INFO: iteration 8, average log likelihood -1.406951
INFO: iteration 9, average log likelihood -1.406781
INFO: iteration 10, average log likelihood -1.406619
INFO: iteration 11, average log likelihood -1.406475
INFO: iteration 12, average log likelihood -1.406350
INFO: iteration 13, average log likelihood -1.406247
INFO: iteration 14, average log likelihood -1.406162
INFO: iteration 15, average log likelihood -1.406092
INFO: iteration 16, average log likelihood -1.406035
INFO: iteration 17, average log likelihood -1.405987
INFO: iteration 18, average log likelihood -1.405945
INFO: iteration 19, average log likelihood -1.405908
INFO: iteration 20, average log likelihood -1.405875
INFO: iteration 21, average log likelihood -1.405845
INFO: iteration 22, average log likelihood -1.405817
INFO: iteration 23, average log likelihood -1.405792
INFO: iteration 24, average log likelihood -1.405767
INFO: iteration 25, average log likelihood -1.405744
INFO: iteration 26, average log likelihood -1.405722
INFO: iteration 27, average log likelihood -1.405702
INFO: iteration 28, average log likelihood -1.405682
INFO: iteration 29, average log likelihood -1.405663
INFO: iteration 30, average log likelihood -1.405645
INFO: iteration 31, average log likelihood -1.405628
INFO: iteration 32, average log likelihood -1.405611
INFO: iteration 33, average log likelihood -1.405595
INFO: iteration 34, average log likelihood -1.405580
INFO: iteration 35, average log likelihood -1.405566
INFO: iteration 36, average log likelihood -1.405552
INFO: iteration 37, average log likelihood -1.405539
INFO: iteration 38, average log likelihood -1.405526
INFO: iteration 39, average log likelihood -1.405514
INFO: iteration 40, average log likelihood -1.405503
INFO: iteration 41, average log likelihood -1.405492
INFO: iteration 42, average log likelihood -1.405481
INFO: iteration 43, average log likelihood -1.405471
INFO: iteration 44, average log likelihood -1.405461
INFO: iteration 45, average log likelihood -1.405452
INFO: iteration 46, average log likelihood -1.405443
INFO: iteration 47, average log likelihood -1.405434
INFO: iteration 48, average log likelihood -1.405425
INFO: iteration 49, average log likelihood -1.405417
INFO: iteration 50, average log likelihood -1.405410
INFO: EM with 100000 data points 50 iterations avll -1.405410
118.1 data points per parameter
4: avll = [-1.40769,-1.40762,-1.40756,-1.40749,-1.40739,-1.40726,-1.40712,-1.40695,-1.40678,-1.40662,-1.40647,-1.40635,-1.40625,-1.40616,-1.40609,-1.40604,-1.40599,-1.40594,-1.40591,-1.40588,-1.40585,-1.40582,-1.40579,-1.40577,-1.40574,-1.40572,-1.4057,-1.40568,-1.40566,-1.40565,-1.40563,-1.40561,-1.4056,-1.40558,-1.40557,-1.40555,-1.40554,-1.40553,-1.40551,-1.4055,-1.40549,-1.40548,-1.40547,-1.40546,-1.40545,-1.40544,-1.40543,-1.40543,-1.40542,-1.40541]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.405410
INFO: iteration 2, average log likelihood -1.405346
INFO: iteration 3, average log likelihood -1.405286
INFO: iteration 4, average log likelihood -1.405215
INFO: iteration 5, average log likelihood -1.405126
INFO: iteration 6, average log likelihood -1.405014
INFO: iteration 7, average log likelihood -1.404879
INFO: iteration 8, average log likelihood -1.404726
INFO: iteration 9, average log likelihood -1.404563
INFO: iteration 10, average log likelihood -1.404399
INFO: iteration 11, average log likelihood -1.404243
INFO: iteration 12, average log likelihood -1.404099
INFO: iteration 13, average log likelihood -1.403969
INFO: iteration 14, average log likelihood -1.403853
INFO: iteration 15, average log likelihood -1.403749
INFO: iteration 16, average log likelihood -1.403656
INFO: iteration 17, average log likelihood -1.403573
INFO: iteration 18, average log likelihood -1.403498
INFO: iteration 19, average log likelihood -1.403429
INFO: iteration 20, average log likelihood -1.403366
INFO: iteration 21, average log likelihood -1.403309
INFO: iteration 22, average log likelihood -1.403255
INFO: iteration 23, average log likelihood -1.403205
INFO: iteration 24, average log likelihood -1.403159
INFO: iteration 25, average log likelihood -1.403115
INFO: iteration 26, average log likelihood -1.403074
INFO: iteration 27, average log likelihood -1.403035
INFO: iteration 28, average log likelihood -1.402999
INFO: iteration 29, average log likelihood -1.402965
INFO: iteration 30, average log likelihood -1.402933
INFO: iteration 31, average log likelihood -1.402903
INFO: iteration 32, average log likelihood -1.402875
INFO: iteration 33, average log likelihood -1.402849
INFO: iteration 34, average log likelihood -1.402824
INFO: iteration 35, average log likelihood -1.402801
INFO: iteration 36, average log likelihood -1.402778
INFO: iteration 37, average log likelihood -1.402757
INFO: iteration 38, average log likelihood -1.402737
INFO: iteration 39, average log likelihood -1.402718
INFO: iteration 40, average log likelihood -1.402700
INFO: iteration 41, average log likelihood -1.402682
INFO: iteration 42, average log likelihood -1.402665
INFO: iteration 43, average log likelihood -1.402649
INFO: iteration 44, average log likelihood -1.402633
INFO: iteration 45, average log likelihood -1.402617
INFO: iteration 46, average log likelihood -1.402603
INFO: iteration 47, average log likelihood -1.402588
INFO: iteration 48, average log likelihood -1.402575
INFO: iteration 49, average log likelihood -1.402561
INFO: iteration 50, average log likelihood -1.402548
INFO: EM with 100000 data points 50 iterations avll -1.402548
59.0 data points per parameter
5: avll = [-1.40541,-1.40535,-1.40529,-1.40522,-1.40513,-1.40501,-1.40488,-1.40473,-1.40456,-1.4044,-1.40424,-1.4041,-1.40397,-1.40385,-1.40375,-1.40366,-1.40357,-1.4035,-1.40343,-1.40337,-1.40331,-1.40326,-1.40321,-1.40316,-1.40312,-1.40307,-1.40304,-1.403,-1.40296,-1.40293,-1.4029,-1.40288,-1.40285,-1.40282,-1.4028,-1.40278,-1.40276,-1.40274,-1.40272,-1.4027,-1.40268,-1.40266,-1.40265,-1.40263,-1.40262,-1.4026,-1.40259,-1.40257,-1.40256,-1.40255]
[-1.41567,-1.41569,-1.41561,-1.41555,-1.41548,-1.41538,-1.41528,-1.41515,-1.41502,-1.41487,-1.41464,-1.41427,-1.41367,-1.41283,-1.41191,-1.41119,-1.41077,-1.41057,-1.41049,-1.41045,-1.41044,-1.41043,-1.41043,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41042,-1.41043,-1.41036,-1.4103,-1.41022,-1.41012,-1.41,-1.40987,-1.40975,-1.40964,-1.40955,-1.40948,-1.40942,-1.40938,-1.40935,-1.40932,-1.4093,-1.40928,-1.40926,-1.40925,-1.40924,-1.40923,-1.40922,-1.40921,-1.4092,-1.40919,-1.40918,-1.40918,-1.40917,-1.40917,-1.40916,-1.40916,-1.40916,-1.40915,-1.40915,-1.40915,-1.40915,-1.40914,-1.40914,-1.40914,-1.40914,-1.40914,-1.40914,-1.40914,-1.40914,-1.40914,-1.40913,-1.40913,-1.40913,-1.40913,-1.40913,-1.40914,-1.40908,-1.40902,-1.40895,-1.40886,-1.40875,-1.40863,-1.40851,-1.40839,-1.40829,-1.40821,-1.40814,-1.40809,-1.40805,-1.40802,-1.408,-1.40798,-1.40796,-1.40795,-1.40793,-1.40792,-1.40791,-1.4079,-1.40789,-1.40788,-1.40787,-1.40786,-1.40785,-1.40784,-1.40783,-1.40782,-1.40781,-1.4078,-1.40779,-1.40778,-1.40777,-1.40777,-1.40776,-1.40775,-1.40774,-1.40773,-1.40773,-1.40772,-1.40771,-1.40771,-1.4077,-1.4077,-1.40769,-1.40769,-1.40768,-1.40769,-1.40762,-1.40756,-1.40749,-1.40739,-1.40726,-1.40712,-1.40695,-1.40678,-1.40662,-1.40647,-1.40635,-1.40625,-1.40616,-1.40609,-1.40604,-1.40599,-1.40594,-1.40591,-1.40588,-1.40585,-1.40582,-1.40579,-1.40577,-1.40574,-1.40572,-1.4057,-1.40568,-1.40566,-1.40565,-1.40563,-1.40561,-1.4056,-1.40558,-1.40557,-1.40555,-1.40554,-1.40553,-1.40551,-1.4055,-1.40549,-1.40548,-1.40547,-1.40546,-1.40545,-1.40544,-1.40543,-1.40543,-1.40542,-1.40541,-1.40541,-1.40535,-1.40529,-1.40522,-1.40513,-1.40501,-1.40488,-1.40473,-1.40456,-1.4044,-1.40424,-1.4041,-1.40397,-1.40385,-1.40375,-1.40366,-1.40357,-1.4035,-1.40343,-1.40337,-1.40331,-1.40326,-1.40321,-1.40316,-1.40312,-1.40307,-1.40304,-1.403,-1.40296,-1.40293,-1.4029,-1.40288,-1.40285,-1.40282,-1.4028,-1.40278,-1.40276,-1.40274,-1.40272,-1.4027,-1.40268,-1.40266,-1.40265,-1.40263,-1.40262,-1.4026,-1.40259,-1.40257,-1.40256,-1.40255]
32×26 Array{Float64,2}:
  0.256544   -0.202549   -0.13261     0.157214    -0.0618269    -0.119402    0.051588    0.124739      0.0773294    0.0935552  -0.37777     0.125146     0.2129      -0.096419   -0.786731    -0.631448    0.225433    -0.559508     0.108985     0.33176      0.00564418   0.321097    -0.278785    0.155461     0.197837    -0.334566  
  0.333183    0.0733343  -0.0181685   0.0681155   -0.268756      0.0362137  -0.381744    0.0715395    -0.00313917  -0.0615987  -0.169335   -0.0178846   -0.055718     0.0517592  -0.649138    -0.0108104   0.569086    -0.219516     0.0974597   -0.405031     0.0984197    0.115513    -0.346076    0.334939     0.0436927    0.679747  
  0.43273     0.492936    0.030783   -0.369508     0.163215     -0.248378    0.152862   -0.552278      0.0640054   -0.139486   -0.692906    0.295031    -0.165616    -0.0228888   0.0262778   -0.254306    0.299234     0.567312     0.276132     0.0806449   -0.0333482    0.225488     0.174896   -0.472615     0.0417281   -0.207836  
  0.599793    0.498679   -0.0888403  -0.459169    -0.0922703     0.37157     0.546424   -0.0388951    -0.272156     0.270417   -0.226371    0.29773     -0.247998     0.383395   -0.106933    -0.291651    0.341414     0.00765136   0.250952    -0.229074    -0.214468    -0.250225    -0.498979    0.452757     0.626834    -0.0157362 
 -0.536262   -0.68365     0.398904    0.322123    -0.470466     -0.0106824  -0.322583    0.385003     -0.365698     0.263309    0.359821   -0.182999     0.0996358    0.456305    0.0660114    0.11773     0.426553    -0.0996477   -0.317493    -0.207588    -0.0857996   -0.52501     -0.0978297   0.293101     0.257525     0.162566  
 -0.642192   -0.297893    0.577236   -0.229363    -0.305171      0.407589   -0.57738    -0.011039     -0.0688672    0.186331   -0.147395    0.569319    -0.267541     0.247506   -0.238556    -0.128748   -0.400522     0.272137     0.0318496    0.380236    -0.260322    -0.0914913   -0.244185   -0.218043    -0.0410743   -0.204099  
 -0.0783262   0.251471    0.251359    0.252056     0.448825     -0.294355   -0.210393    0.402809      0.0269316    0.246198    0.516651   -0.026305    -0.0974599    0.347734    0.181008     0.0172517  -0.574026    -0.794312     0.319038    -0.0677171    0.339239    -0.0964653   -0.431892   -0.192385     0.0166828   -0.326651  
  0.213323    0.615939    0.350035    0.2641      -0.410387      0.610657   -0.175203    0.229287     -0.148447    -0.249161    0.464638    0.418486    -0.159955     0.62109    -0.107743    -0.307138   -0.433409    -0.426192     0.231735     0.319412     0.23779      0.212399    -0.264713   -0.00135023  -0.663756     0.556123  
 -0.156528   -0.322518    0.359372    0.277564     0.091142      0.153273   -0.371127   -0.0938645    -0.481478    -0.313568   -0.579877   -0.37292     -0.0358916   -0.388657   -0.0607696    0.0267403   0.0179566    0.118119    -0.0957164    0.132082     0.538709     0.771053     0.213347   -0.119828    -0.678278    -0.304988  
 -0.0490142  -0.352845    0.320601    0.219298    -0.0819992    -0.559193   -0.408704    0.141736      0.305716    -0.7059      0.253148   -0.10804      0.320642    -0.289431    0.269675     0.394462    0.375253     0.313053    -0.739008    -0.401148     0.115347     0.4867       0.31061    -0.458792    -0.694521     0.339402  
 -0.358426    0.132615   -0.20809     0.382801    -0.269033     -0.236013    0.827817    0.235029      0.268295    -0.0808327   0.336229   -0.253615     0.171811     0.303916    0.608972    -0.161123   -0.284191     0.0419138   -0.477377     0.128269    -0.328552    -0.0229673    0.227531   -0.673606     0.221148    -0.490421  
 -0.410676   -0.5484     -0.260554    0.032449     0.501387     -0.458867    0.309763   -0.201083      0.103695     0.217127   -0.464331   -0.0220989    0.396389    -0.612491    0.350186     0.169584   -0.0339408    0.0916869   -0.163676     0.22477     -0.382234     0.178035    -0.031214   -0.219512     0.144776    -0.412833  
 -0.15779    -0.0033226   0.0374542   0.00216614  -0.000880645   0.145501   -0.0415789   0.125255     -0.319193     0.0212562  -0.0576055   0.028319    -0.0309576    0.0936099   0.0593799    0.03271     0.0179928    0.183187     0.110908    -0.00118982  -0.179658     0.0958891   -0.0522316  -0.0321457    0.0418885   -0.134047  
  0.0536428  -0.128619   -0.030133    0.0856643    0.000993001  -0.0792845   0.110478   -0.134797      0.348412    -0.130209    0.0187     -0.0788362    0.144492    -0.0972895  -0.00788119  -0.0675169   0.00916168  -0.190072    -0.223185    -0.0859082    0.0785192   -0.0651914   -0.0394323   0.0288969   -0.10916      0.0531801 
  0.260863    0.136232    0.0156101  -0.0190633    0.294521     -0.0685541   0.0114075  -0.233609     -0.0472704   -0.117152    0.602438   -0.255615    -0.188627     0.40563     0.136354     0.247022   -0.0662521    0.130091    -0.0499601   -0.123835     0.344112    -0.224081     0.60707     0.121384    -5.74574e-5  -0.086099  
 -0.102086    0.234603    0.0195424  -0.247749    -0.0468107    -0.313129   -0.12808    -0.000302628   0.232708     0.432345   -0.0286877  -0.178244     0.186849    -0.22959     0.238399     0.355662    0.138507     0.566119    -0.0450329    0.236284    -0.275492    -0.537981     0.137815    0.12438      0.0496482    0.510596  
 -0.16276    -0.0418441  -0.494541   -0.319975    -0.110926     -0.54891    -0.345105    0.248352      0.0267122    0.649779   -0.0950154  -0.0258982    1.38948      0.90845    -0.0263479   -0.165863    0.255811    -0.351691     0.354499     0.127723    -0.160431    -0.289491     0.0484519  -0.284075    -0.667382     0.366347  
 -0.0630666   0.124992   -0.0921506   0.321721    -0.563808     -0.345922    0.194338    0.246227      0.0610432   -0.368634   -0.675538   -0.267406     0.876554    -0.530476   -0.484205    -0.205735    0.683063     0.279918     0.167162     0.223267    -0.171814     0.277696    -0.281951   -0.185205    -0.352031     0.285415  
 -0.665595   -0.222557    0.0301005  -0.096673     0.0802422    -0.343548    0.147399   -0.650077      0.234784    -0.58099     0.362113   -0.411898     0.437662    -0.303048   -0.301987    -0.176405    0.462908    -0.441084    -0.338988    -0.282283     0.272542    -0.575345    -0.306026   -0.0763121   -0.290997     0.198984  
  0.8269      0.581065    0.19119     0.0547603    0.332489     -0.291712   -0.317797    0.187083      0.89172     -0.207855   -0.25662    -0.262717     0.0600456   -0.687567   -0.30313      0.357229   -0.0969004   -0.135455    -0.0661236   -0.717069    -0.184849     0.0727337   -0.352085    0.197758    -0.252404     0.272796  
  0.24877     0.0993412   0.362097    0.349015    -0.125038      0.514352    0.551074   -0.0320757     0.341131    -0.502662   -0.190463    0.381817    -0.675653    -0.514518    0.271513    -0.170867   -0.177949    -0.652967    -0.496473    -0.468007     0.13272      0.332085    -0.25093    -0.055515    -0.0358663   -0.0628196 
 -0.185989   -0.246844   -0.0246579  -0.171965     0.0741598     0.581373    0.0413104  -0.571833     -0.260187    -0.312118   -0.0941479   0.0537745   -0.555341    -0.666331    0.0256326    0.0891571   0.0049267    0.547422    -0.686912    -0.533878    -0.307516     0.187809    -0.217315    0.424715     0.250406     0.124096  
  0.352354   -0.129348    0.221973   -0.360797    -0.0645184     0.469634   -0.175488   -0.865301      0.819508    -0.215878   -0.482081    0.86093      0.129906    -0.995411    0.0551209   -0.526712   -0.852673    -0.241299     0.00436833   0.675219     0.406751     0.105348    -0.399718    0.18003     -0.0209884    0.227177  
  0.366173    0.023385    0.262265   -0.424231     0.266567      0.084098   -1.00446    -0.333195     -0.0838271    0.0619491  -0.191456    0.542807    -0.248228    -0.374846   -0.881406     0.0706485   0.264874    -0.207998     0.652743    -0.0302223    0.145535     0.205447    -0.272806    0.855312    -0.18247      0.428405  
 -0.535885   -0.308225    0.006746   -0.0312603   -0.124916      0.500544    0.238087   -0.338917     -0.584819    -0.130162   -0.063748    0.270935    -0.0045184    0.404806   -0.0681746   -0.740216   -0.237856    -0.0898685    0.197471     0.499033     0.267578     0.0344245    0.0626191  -0.0830541    0.0360984   -0.57202   
  0.100647    0.077298    0.175128    0.206536    -0.161972     -0.153167   -0.0504715   0.163166      0.0990521    0.0484919   0.0606576  -0.00633258  -0.00845874   0.140378    0.093783    -0.0990881  -0.0144667   -0.263241    -0.198327    -0.148986    -0.174967     0.00849838  -0.167901   -0.16398     -0.0233478   -0.0115263 
  0.434336   -0.268208   -0.939107   -0.494675    -0.0583508     0.349324    0.293295   -0.171838     -0.111479     0.395286    0.371924   -0.360132     0.0775745    0.120704    0.00559449   0.115183   -0.235268    -0.23441     -0.228577     0.543738     0.24936     -0.0564855    0.530502    0.0989978    0.916254    -0.00178923
  0.0683214  -0.309815   -0.436985   -0.151016    -0.13805       0.426017    0.0441525  -0.00381012    0.277429     0.293132    0.731118    0.231195    -0.168104     0.268389   -0.28948      0.211354   -0.582331    -0.618728     0.294974    -0.495221    -0.185691    -0.401838    -0.086852    0.731989     0.227759    -0.427335  
 -0.2585      0.0156716   0.0259941  -0.324529     0.392729     -0.206181   -0.111641   -0.247848      0.196752     0.133543    0.400655   -0.15667     -0.316777    -0.0521878   0.639294     0.704988   -0.42797      0.48578      0.0431049   -0.109286     0.0848063   -0.317477     0.680496   -0.217391    -0.336131    -0.0134178 
  0.25943     0.115255    0.238749    0.488055     0.0920733     0.512399   -0.341787    0.913394     -0.176626     0.277356    0.0394099   0.16515     -0.233971     0.353232    0.620169     0.0812614  -0.580926     0.595648     0.369527    -0.0267912   -0.381543     0.21621      0.350495    0.123787     0.205774    -0.338525  
  0.321297    0.177133   -0.666156   -0.258817     0.715728     -0.942157    0.277908   -0.0550712    -0.00664805   0.0932668   0.137597   -0.286301    -0.00812538  -0.484447    0.0252035    0.284567    1.01106      0.163996    -0.167896    -0.514314     0.0557266   -0.440093    -0.11046     0.308798     0.723603    -0.11993   
 -0.092667   -0.0205379   0.0146797   0.0472135    0.0637615     0.43404    -0.123799    0.19499      -0.472829    -0.296616    0.0396528  -0.12491      0.131336     0.504294   -0.579034     0.215882    1.05071      1.13175      0.0582378   -0.172086     0.0343445   -0.184144     0.622226    0.179172     0.241334    -0.231295  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.402536
INFO: iteration 2, average log likelihood -1.402524
INFO: iteration 3, average log likelihood -1.402512
INFO: iteration 4, average log likelihood -1.402501
INFO: iteration 5, average log likelihood -1.402490
INFO: iteration 6, average log likelihood -1.402480
INFO: iteration 7, average log likelihood -1.402470
INFO: iteration 8, average log likelihood -1.402461
INFO: iteration 9, average log likelihood -1.402451
INFO: iteration 10, average log likelihood -1.402443
INFO: EM with 100000 data points 10 iterations avll -1.402443
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.121433e+05
      1       6.935607e+05      -2.185826e+05 |       32
      2       6.801989e+05      -1.336177e+04 |       32
      3       6.748292e+05      -5.369720e+03 |       32
      4       6.720380e+05      -2.791178e+03 |       32
      5       6.702336e+05      -1.804423e+03 |       32
      6       6.689401e+05      -1.293448e+03 |       32
      7       6.679339e+05      -1.006265e+03 |       32
      8       6.671783e+05      -7.555601e+02 |       32
      9       6.665454e+05      -6.328913e+02 |       32
     10       6.659890e+05      -5.564296e+02 |       32
     11       6.655335e+05      -4.554792e+02 |       32
     12       6.651357e+05      -3.977877e+02 |       32
     13       6.648021e+05      -3.336382e+02 |       32
     14       6.645224e+05      -2.796509e+02 |       32
     15       6.642807e+05      -2.417350e+02 |       32
     16       6.640561e+05      -2.246233e+02 |       32
     17       6.638383e+05      -2.178013e+02 |       32
     18       6.636433e+05      -1.949309e+02 |       32
     19       6.634589e+05      -1.843947e+02 |       32
     20       6.632816e+05      -1.773892e+02 |       32
     21       6.631104e+05      -1.711731e+02 |       32
     22       6.629553e+05      -1.550536e+02 |       32
     23       6.628151e+05      -1.402733e+02 |       32
     24       6.626882e+05      -1.268486e+02 |       32
     25       6.625819e+05      -1.063059e+02 |       32
     26       6.624859e+05      -9.604763e+01 |       32
     27       6.623970e+05      -8.882506e+01 |       32
     28       6.623117e+05      -8.534149e+01 |       32
     29       6.622233e+05      -8.841785e+01 |       32
     30       6.621471e+05      -7.616927e+01 |       32
     31       6.620751e+05      -7.201872e+01 |       32
     32       6.620130e+05      -6.211218e+01 |       32
     33       6.619622e+05      -5.074979e+01 |       32
     34       6.619002e+05      -6.198447e+01 |       32
     35       6.618392e+05      -6.103789e+01 |       32
     36       6.617800e+05      -5.916654e+01 |       32
     37       6.617160e+05      -6.403419e+01 |       32
     38       6.616573e+05      -5.873811e+01 |       32
     39       6.616032e+05      -5.406799e+01 |       32
     40       6.615547e+05      -4.849197e+01 |       32
     41       6.615081e+05      -4.657906e+01 |       32
     42       6.614600e+05      -4.816104e+01 |       32
     43       6.614128e+05      -4.718384e+01 |       32
     44       6.613695e+05      -4.327525e+01 |       32
     45       6.613190e+05      -5.051153e+01 |       32
     46       6.612641e+05      -5.488553e+01 |       32
     47       6.612103e+05      -5.384889e+01 |       32
     48       6.611527e+05      -5.756786e+01 |       32
     49       6.610996e+05      -5.304796e+01 |       32
     50       6.610462e+05      -5.346311e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 661046.1751042729)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.414148
INFO: iteration 2, average log likelihood -1.409104
INFO: iteration 3, average log likelihood -1.407743
INFO: iteration 4, average log likelihood -1.406718
INFO: iteration 5, average log likelihood -1.405672
INFO: iteration 6, average log likelihood -1.404769
INFO: iteration 7, average log likelihood -1.404186
INFO: iteration 8, average log likelihood -1.403871
INFO: iteration 9, average log likelihood -1.403697
INFO: iteration 10, average log likelihood -1.403585
INFO: iteration 11, average log likelihood -1.403502
INFO: iteration 12, average log likelihood -1.403435
INFO: iteration 13, average log likelihood -1.403377
INFO: iteration 14, average log likelihood -1.403327
INFO: iteration 15, average log likelihood -1.403282
INFO: iteration 16, average log likelihood -1.403240
INFO: iteration 17, average log likelihood -1.403202
INFO: iteration 18, average log likelihood -1.403166
INFO: iteration 19, average log likelihood -1.403133
INFO: iteration 20, average log likelihood -1.403102
INFO: iteration 21, average log likelihood -1.403072
INFO: iteration 22, average log likelihood -1.403043
INFO: iteration 23, average log likelihood -1.403016
INFO: iteration 24, average log likelihood -1.402990
INFO: iteration 25, average log likelihood -1.402964
INFO: iteration 26, average log likelihood -1.402940
INFO: iteration 27, average log likelihood -1.402917
INFO: iteration 28, average log likelihood -1.402894
INFO: iteration 29, average log likelihood -1.402871
INFO: iteration 30, average log likelihood -1.402850
INFO: iteration 31, average log likelihood -1.402828
INFO: iteration 32, average log likelihood -1.402808
INFO: iteration 33, average log likelihood -1.402788
INFO: iteration 34, average log likelihood -1.402768
INFO: iteration 35, average log likelihood -1.402749
INFO: iteration 36, average log likelihood -1.402730
INFO: iteration 37, average log likelihood -1.402712
INFO: iteration 38, average log likelihood -1.402694
INFO: iteration 39, average log likelihood -1.402676
INFO: iteration 40, average log likelihood -1.402659
INFO: iteration 41, average log likelihood -1.402643
INFO: iteration 42, average log likelihood -1.402626
INFO: iteration 43, average log likelihood -1.402611
INFO: iteration 44, average log likelihood -1.402596
INFO: iteration 45, average log likelihood -1.402581
INFO: iteration 46, average log likelihood -1.402567
INFO: iteration 47, average log likelihood -1.402553
INFO: iteration 48, average log likelihood -1.402540
INFO: iteration 49, average log likelihood -1.402527
INFO: iteration 50, average log likelihood -1.402515
INFO: EM with 100000 data points 50 iterations avll -1.402515
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0430561  -0.455991    -0.218372   -0.20742     -0.0190907    1.03317      0.0497975   -0.388464    -0.169397    -0.0686681    0.378235     0.381938   -0.359479   -0.25168    -0.149592     0.0771579  -0.577375   -0.451987   -0.422401   -0.477286     0.138131    0.0894368  -0.144321    0.746331     0.113569   -0.388765  
 -0.16333    -0.611196     0.368255    0.511864    -0.0535602   -0.422758    -0.604681     0.208222     0.569156    -0.541823     0.289306    -0.284658    0.515023   -0.332511   -0.0874283    0.381837    0.397365   -0.0905289  -0.582733   -0.601372     0.059362   -0.025333   -0.0528405   0.0237508   -0.403354    0.394382  
 -0.0476596  -0.0902359   -0.196375    0.0824209   -0.0856193    0.654146    -0.00585468   0.166539    -0.451482    -0.698922    -0.175379    -0.313405    0.294633   -0.0812073  -0.469951     0.409866    0.871346    1.22869    -0.152203   -0.0844448    0.155177    0.135349    0.618139    0.0827002   -0.0310087  -0.121782  
  0.299031    0.217521    -0.0708661  -0.198398     0.598371    -0.178465     0.0508884   -0.610089    -0.233976    -0.785306    -0.278672     0.03136    -0.272653   -0.397233   -0.131112    -0.508934   -0.412522    0.139607    0.240079    0.0904842    0.435533    0.620943    0.0406957  -0.0762646   -0.16389    -0.660699  
  0.0313579  -0.123901    -0.816221   -0.24552      0.76373     -1.05201      0.190234     0.0443327    0.0992903    0.152563    -0.194908    -0.194765    0.222727   -0.961817    0.197654     0.457959    0.678882    0.203468   -0.314966   -0.340854    -0.129126   -0.275629   -0.132342    0.195071     0.516101   -0.246264  
 -0.394482   -0.397008    -0.103889    0.2131       0.201969    -0.301994     0.10793     -0.252646     0.0536459   -0.00673091   0.118872    -0.307642    0.143756    0.126071    0.163023     0.137343   -0.0692909   0.226459   -0.163685    0.336809     0.0661358   0.193942    0.635717   -0.568045    -0.0503264  -0.65292   
 -0.684258   -0.376345    -0.122265    0.653906    -0.377087     0.00840651   0.218834     0.619753    -0.5039       0.00932888  -0.159103    -0.128453    0.532966   -0.188535   -0.033255    -0.34993     0.243397   -0.419398    0.140167    0.282579    -0.277785    0.0939406  -0.45988    -0.0625802    0.0926935   0.0203075 
  0.0600318  -0.452203    -0.0467218  -0.175001    -0.0476603   -0.0182629   -0.058168    -0.655038    -0.208978    -0.0107765   -0.200289     0.154241   -0.214249   -0.21508    -0.203121    -0.341016    0.776978    0.498513   -0.231696   -0.144588    -0.427821   -0.0780353   0.114458    0.744481     0.429894    0.327485  
  0.705469    0.479504    -0.133622   -0.14825     -0.655018     0.567215     0.297124     0.155482    -0.0717484   -0.0985475    0.385144    -0.0896435  -0.0295187   0.797478   -0.138981    -0.571696   -0.102138   -0.238177    0.321128    0.180377     0.287777    0.0389124  -0.0425799   0.137631     0.230391    0.269476  
 -0.231512    0.148322     0.253276    0.122355     0.590888    -0.571911    -0.380087     0.430367     0.19101      0.608999     0.525389     0.317958   -0.0177516   0.286174   -0.00423074  -0.087926   -0.656755   -1.07147     0.505542    0.00554766   0.0878439  -0.174857   -0.489919    0.00413307   0.0127369  -0.423647  
 -0.24216    -0.0303571    0.261845    0.00933027  -0.250468     0.0623025   -0.423266     0.264167    -0.146377     0.346178     0.380638    -0.0974028  -0.0361598   0.428126   -0.130035     0.200981    0.212845    0.169781    0.0360016  -0.0306672   -0.0720874  -0.619237    0.127841    0.29054      0.107781    0.309123  
  0.39872     0.439635     0.164814   -0.561513     0.523647    -0.407849    -0.451469    -0.428043     0.430943     0.268599     0.290067    -0.0511988  -0.360734   -0.21704     0.35205      0.650967   -0.0946207   0.446278    0.108196   -0.25902      0.0940176  -0.333257    0.452299    0.127005    -0.267974    0.440214  
 -0.0299571   0.493831     0.623083    0.312517    -0.232261     0.506205    -0.342464     0.16013     -0.0800145   -0.316335     0.376278     0.650359   -0.281784    0.478464   -0.0592749   -0.189048   -0.525433   -0.479787    0.190191    0.375788     0.253539    0.358124   -0.180385   -0.103927    -1.04986     0.475828  
  0.826406    0.811408     0.285246    0.147102    -0.0929258    0.151226    -0.134196     0.204497     0.903558    -0.42528     -0.551229     0.100499   -0.269503   -0.72343    -0.299743     0.0718934  -0.301052   -0.185093   -0.02944    -0.551903    -0.0164934   0.0345482  -0.469387    0.281034    -0.212728    0.261941  
  0.0742851   0.106342    -0.361066   -0.12879     -0.00988953  -0.0854507    0.665885    -0.0599906    0.306683     0.207034    -0.00617437   0.0767499   0.206182   -0.0795923   0.514146     0.0525077  -0.162839    0.0378972  -0.282353    0.0908992   -0.385835   -0.122086    0.146049   -0.115767     0.275059   -0.073034  
  0.478076    0.434729    -0.024833   -0.276736     0.204205     0.34076      0.457622    -0.0743613   -0.138906     0.35728     -0.202816     0.499493   -0.271096    0.594102   -0.0916626   -0.149828    0.571978    0.187285    0.354303   -0.252659    -0.18698    -0.436775   -0.219677    0.17055      0.751479   -0.281274  
 -0.252179   -0.177641     0.214821    0.352251    -0.197723    -0.108368     0.285895    -0.0169484    0.24735     -0.227125    -0.0696585    0.0257243   0.0335119  -0.0801248   0.436423    -0.131202   -0.101874   -0.135414   -0.431685   -0.0918908   -0.153365    0.209431    0.0321745  -0.498031    -0.198857   -0.163957  
 -0.550205   -0.223285    -0.411943   -0.542995     0.0526157   -0.226703     0.346144    -0.583102     0.152799    -0.202963     0.716189    -0.419998    0.144613   -0.0416109  -0.138495     0.135132   -0.194951   -0.253064   -0.144179    0.137955     0.345598   -0.86435     0.266269    0.0387751    0.28346     0.0802021 
 -0.576932   -0.0963974    0.47285    -0.16509      0.181958     0.294135    -0.40285     -0.00642992  -0.275901     0.0347966   -0.10378      0.228315   -0.237936   -0.259567    0.338861     0.362169   -0.3524      0.750139   -0.011423   -0.155903    -0.404359   -0.0411013  -0.132789   -0.0803613   -0.141402   -0.0860461 
 -0.108404   -0.0728245   -0.617128   -0.326556    -0.108217    -0.503931    -0.319932     0.231125    -0.00314149   0.693566    -0.00230085  -0.0205727   1.36852     0.968213    0.0170397   -0.169278    0.269065   -0.353788    0.296159    0.0705425   -0.182165   -0.346448    0.104408   -0.301195    -0.682068    0.440714  
 -0.0450619  -0.0733143    0.0381357   0.0124626    0.0953796   -0.00167073  -0.0335901   -0.0761853    0.0184773   -0.076811     0.105661    -0.0753812   0.0511582   0.0297617   0.0204623    0.0224265  -0.0717259  -0.0208298  -0.0480051  -0.0321343    0.076285   -0.110694   -0.0101186   0.046346    -0.0644557  -0.00925731
 -0.54272    -0.52309      0.599309    0.0124968   -0.507209     0.573744    -0.409985    -0.133404    -0.0545911    0.0483758    0.015775     0.498323   -0.230931    0.586829   -0.50431     -0.482474   -0.349416   -0.20977     0.0341484   0.275955    -0.0213526  -0.202881   -0.405508   -0.186972     0.215895   -0.430464  
  0.167792    0.122877     0.187914   -0.107877    -0.0550499   -0.0548788   -0.320542    -0.183393    -0.174025    -0.204991    -0.466866     0.26004     0.236874   -0.248506   -0.990443    -0.32986     0.64311    -0.266888    0.373609   -0.011187     0.0636159   0.248963   -0.586844    0.346685    -0.174492    0.329908  
 -0.244015   -0.094413    -0.126121   -0.0986612   -0.0458691    0.517128    -0.0695831    0.190099    -0.51936      0.405772    -0.0390185    0.202616   -0.0204963   0.335514    0.293645    -0.285906   -0.536554    0.543899    0.315029    0.839521    -0.093041    0.0474194   0.519406   -0.0301481    0.191206   -0.377084  
  0.0832789   0.0781925   -0.0196986   0.518937     0.139273     0.0855476    0.215652     0.602223     0.156117    -0.0809266    0.595001    -0.367114   -0.191562    0.402105    0.929157     0.272009   -0.841948    0.069707    0.074171   -0.226403     0.177031   -0.0336571   0.356819   -0.105098     0.0239905  -0.391691  
  0.0808554  -0.518092     0.334872   -0.330148     0.0616822    0.02705      0.00660263  -0.676545     0.606394     0.316059    -0.570758     0.579706    0.315585   -1.04026     0.289158    -0.212724   -0.597337   -0.383235   -0.0635761   0.915907     0.107853    0.110858   -0.558411   -0.08891     -0.0260752   0.253367  
  0.108569    0.508724     0.134533    0.511947    -0.274262    -0.641319     0.24896      0.659889    -0.847446     0.132606     0.360579    -1.08307    -0.266382    0.920242    0.26637      0.205819    0.585296   -0.019861   -0.390616   -0.772323    -0.329736   -0.351068    0.213363   -0.0901487    0.186649   -0.561655  
 -0.169288    0.163632     0.382111   -0.16798     -0.215025    -0.562166    -0.210629     0.0171353   -0.0351702   -0.00448506  -0.720838    -0.242514    0.251892   -0.198105    0.176967     0.0148259   0.673488    0.758511    0.0235473   0.339718     0.0823601   0.225773    0.13874    -0.656529    -0.269777    0.265097  
  0.118811    0.10806      0.064116    0.137276     0.219532     0.00555041   0.171918    -0.275248     0.0561608   -0.500652     0.300102    -0.180689   -0.377849   -0.09138     0.143775     0.110126    0.430439   -0.211419   -0.394053   -0.721539     0.356066    0.0235677  -0.301972   -0.222035    -0.0233681   0.388395  
  0.758892   -0.0209718   -0.480311   -0.123769     0.0952489    0.0434472   -0.501225     0.366294    -0.0519849    0.640806     0.0928117   -0.269556   -0.0293329   0.103681   -0.566056     0.272262    0.0228255  -0.216256    0.357776    0.0308858   -0.0112579   0.282783    0.128655    0.810351     0.338796    0.176638  
  0.256981    0.0429509    0.0707319   0.0676777   -0.090772     0.0876656   -0.171019     0.208905    -0.0620923    0.0828284   -0.442547     0.231453   -0.159036   -0.0615362  -0.331981    -0.169668    0.0889688  -0.155276    0.041631   -0.0190898   -0.0509705   0.495657   -0.15458     0.0623236    0.021076   -0.104758  
  0.0224231   0.00188747  -0.156016   -0.00781876  -0.0900348   -0.368759     0.210634    -0.325954     0.353069    -0.339276    -0.141077    -0.331798    0.68752    -0.252866   -0.645347    -0.268719    0.504313   -0.352387   -0.103943   -0.0627923    0.0283155  -0.0935986  -0.340958   -0.0201359   -0.273776    0.00835582INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.402503
INFO: iteration 2, average log likelihood -1.402492
INFO: iteration 3, average log likelihood -1.402481
INFO: iteration 4, average log likelihood -1.402470
INFO: iteration 5, average log likelihood -1.402460
INFO: iteration 6, average log likelihood -1.402450
INFO: iteration 7, average log likelihood -1.402440
INFO: iteration 8, average log likelihood -1.402431
INFO: iteration 9, average log likelihood -1.402421
INFO: iteration 10, average log likelihood -1.402412
INFO: EM with 100000 data points 10 iterations avll -1.402412
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
