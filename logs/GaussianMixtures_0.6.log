>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.6.6
INFO: Installing JLD v0.6.4
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.0
INFO: Installing Rmath v0.1.3
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.2.0
INFO: Installing StaticArrays v0.0.8
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated
INFO: METADATA is out-of-date â€” you may not have the latest version of GaussianMixtures
INFO: Use `Pkg.update()` to get the latest versions of your packages

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.787
Commit c71f205 (2016-09-26 16:28 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-98-generic #145-Ubuntu SMP Sat Oct 8 20:13:07 UTC 2016 x86_64 x86_64
Memory: 2.939289093017578 GB (686.9453125 MB free)
Uptime: 22941.0 sec
Load Avg:  0.98974609375  1.03076171875  1.0439453125
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3500 MHz    1487126 s       7652 s     129052 s     409196 s         63 s
#2  3500 MHz     604052 s        258 s      80743 s    1513005 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.2
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.6.6
 - JLD                           0.6.4
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.0
 - Rmath                         0.1.3
 - SHA                           0.2.1
 - ScikitLearnBase               0.2.0
 - StaticArrays                  0.0.8
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:345
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:378
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:346
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1739
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-2.2416962810311336e6,[75849.6,24150.4],
[-14746.7 11959.6 -6910.63; 14486.2 -11867.4 6929.26],

Array{Float64,2}[
[69091.6 2827.97 7688.65; 2827.97 84226.4 13788.6; 7688.65 13788.6 77061.4],

[31073.0 -3293.72 -8077.6; -3293.72 16920.2 -13505.3; -8077.6 -13505.3 22501.6]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.590019e+03
      1       1.129667e+03      -4.603521e+02 |        5
      2       1.039212e+03      -9.045467e+01 |        2
      3       9.839697e+02      -5.524254e+01 |        0
      4       9.839697e+02       0.000000e+00 |        0
K-means converged with 4 iterations (objv = 983.9697222173463)
INFO: K-means with 272 data points using 4 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.070682
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.800764
INFO: iteration 2, lowerbound -3.668473
INFO: iteration 3, lowerbound -3.516793
INFO: iteration 4, lowerbound -3.330522
INFO: iteration 5, lowerbound -3.125964
INFO: iteration 6, lowerbound -2.933076
INFO: dropping number of Gaussions to 7
INFO: iteration 7, lowerbound -2.768792
INFO: dropping number of Gaussions to 6
INFO: iteration 8, lowerbound -2.634262
INFO: dropping number of Gaussions to 5
INFO: iteration 9, lowerbound -2.522394
INFO: iteration 10, lowerbound -2.438556
INFO: iteration 11, lowerbound -2.389769
INFO: dropping number of Gaussions to 3
INFO: iteration 12, lowerbound -2.347796
INFO: iteration 13, lowerbound -2.317029
INFO: iteration 14, lowerbound -2.307605
INFO: dropping number of Gaussions to 2
INFO: iteration 15, lowerbound -2.302974
INFO: iteration 16, lowerbound -2.299264
INFO: iteration 17, lowerbound -2.299258
INFO: iteration 18, lowerbound -2.299255
INFO: iteration 19, lowerbound -2.299254
INFO: iteration 20, lowerbound -2.299253
INFO: iteration 21, lowerbound -2.299253
INFO: iteration 22, lowerbound -2.299253
INFO: iteration 23, lowerbound -2.299253
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Fri 14 Oct 2016 10:52:14 AM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Fri 14 Oct 2016 10:52:16 AM UTC: K-means with 272 data points using 4 iterations
11.3 data points per parameter
,Fri 14 Oct 2016 10:52:17 AM UTC: EM with 272 data points 0 iterations avll -2.070682
5.8 data points per parameter
,Fri 14 Oct 2016 10:52:18 AM UTC: GMM converted to Variational GMM
,Fri 14 Oct 2016 10:52:20 AM UTC: iteration 1, lowerbound -3.800764
,Fri 14 Oct 2016 10:52:20 AM UTC: iteration 2, lowerbound -3.668473
,Fri 14 Oct 2016 10:52:20 AM UTC: iteration 3, lowerbound -3.516793
,Fri 14 Oct 2016 10:52:20 AM UTC: iteration 4, lowerbound -3.330522
,Fri 14 Oct 2016 10:52:20 AM UTC: iteration 5, lowerbound -3.125964
,Fri 14 Oct 2016 10:52:21 AM UTC: iteration 6, lowerbound -2.933076
,Fri 14 Oct 2016 10:52:21 AM UTC: dropping number of Gaussions to 7
,Fri 14 Oct 2016 10:52:21 AM UTC: iteration 7, lowerbound -2.768792
,Fri 14 Oct 2016 10:52:21 AM UTC: dropping number of Gaussions to 6
,Fri 14 Oct 2016 10:52:21 AM UTC: iteration 8, lowerbound -2.634262
,Fri 14 Oct 2016 10:52:21 AM UTC: dropping number of Gaussions to 5
,Fri 14 Oct 2016 10:52:21 AM UTC: iteration 9, lowerbound -2.522394
,Fri 14 Oct 2016 10:52:21 AM UTC: iteration 10, lowerbound -2.438556
,Fri 14 Oct 2016 10:52:21 AM UTC: iteration 11, lowerbound -2.389769
,Fri 14 Oct 2016 10:52:21 AM UTC: dropping number of Gaussions to 3
,Fri 14 Oct 2016 10:52:21 AM UTC: iteration 12, lowerbound -2.347796
,Fri 14 Oct 2016 10:52:21 AM UTC: iteration 13, lowerbound -2.317029
,Fri 14 Oct 2016 10:52:21 AM UTC: iteration 14, lowerbound -2.307605
,Fri 14 Oct 2016 10:52:21 AM UTC: dropping number of Gaussions to 2
,Fri 14 Oct 2016 10:52:21 AM UTC: iteration 15, lowerbound -2.302974
,Fri 14 Oct 2016 10:52:21 AM UTC: iteration 16, lowerbound -2.299264
,Fri 14 Oct 2016 10:52:22 AM UTC: iteration 17, lowerbound -2.299258
,Fri 14 Oct 2016 10:52:22 AM UTC: iteration 18, lowerbound -2.299255
,Fri 14 Oct 2016 10:52:22 AM UTC: iteration 19, lowerbound -2.299254
,Fri 14 Oct 2016 10:52:22 AM UTC: iteration 20, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:22 AM UTC: iteration 21, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:22 AM UTC: iteration 22, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:22 AM UTC: iteration 23, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:22 AM UTC: iteration 24, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:22 AM UTC: iteration 25, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:22 AM UTC: iteration 26, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:22 AM UTC: iteration 27, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:22 AM UTC: iteration 28, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:22 AM UTC: iteration 29, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:22 AM UTC: iteration 30, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:23 AM UTC: iteration 31, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:23 AM UTC: iteration 32, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:23 AM UTC: iteration 33, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:23 AM UTC: iteration 34, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:23 AM UTC: iteration 35, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:23 AM UTC: iteration 36, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:23 AM UTC: iteration 37, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:23 AM UTC: iteration 38, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:23 AM UTC: iteration 39, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:23 AM UTC: iteration 40, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:23 AM UTC: iteration 41, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:23 AM UTC: iteration 42, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:23 AM UTC: iteration 43, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:23 AM UTC: iteration 44, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:23 AM UTC: iteration 45, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:23 AM UTC: iteration 46, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:24 AM UTC: iteration 47, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:24 AM UTC: iteration 48, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:24 AM UTC: iteration 49, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:24 AM UTC: iteration 50, lowerbound -2.299253
,Fri 14 Oct 2016 10:52:24 AM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
Î± = [178.045,95.9549]
Î² = [178.045,95.9549]
m = [4.2503 79.2869; 2.00023 53.852]
Î½ = [180.045,97.9549]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.184042 -0.00764405; 0.0 0.00858171],

[0.375876 -0.00895312; 0.0 0.0127487]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.00000000003
avll from stats: -1.0034337116452088
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -1.0034337116452121
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -1.0034337116452123
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 99999.99999999999
avll from stats: -0.9866799175988185
avll from llpg:  -0.9866799175988185
avll direct:     -0.9866799175988183
sum posterior: 100000.0
32Ã—26 Array{Float64,2}:
 -0.00825643   0.0419137    0.156884     -0.0106061   -0.183413     -0.124989      0.0742847  -0.0380672    0.00525148  -0.0186397    0.176035    0.0480952   -0.0861231     0.140423      0.0751862    -0.0295933   -0.154577     0.0305522   -0.027905     0.100568    -0.0113659    -0.124506     0.102189    -0.00861684   0.227548    -0.117782  
  0.141034    -0.150958    -0.0318693    -0.13858      0.089988     -0.000812992  -0.0204315  -0.0223298   -0.106144     0.0500413    0.0546223  -0.0051914   -0.181643      0.0244339     0.172194     -0.270621     0.0302671    0.154245     0.0536075    0.0232555   -0.0355442    -0.157244    -0.0573196    0.0886014    0.0624646    0.0036307 
 -0.0951532    0.0824556    0.0863671    -0.0335259    0.0158634     0.186585      0.02832     0.0368321   -0.119145    -0.0782677    0.12417    -0.0473558   -0.0559435    -0.0632828     0.0420196    -0.0210426    0.0986787    0.00173569  -0.116925    -0.124225     0.0641924     0.0917226   -0.0939208    0.0572478   -0.0494934   -0.0893576 
  0.239965     0.062454    -0.118605      0.0035539   -0.0606145    -0.00768657   -0.0527725  -0.0761384    0.0503136    0.0439579   -0.030797    0.0221176   -0.17645      -0.100771      0.0708914    -0.179819     0.0972861    0.171011    -0.0668829   -0.00729967   0.00708172    0.083647     0.0120043   -0.0314974    0.0467134   -0.0387251 
  0.138695    -0.0851617    0.0484307     0.0198785    0.00869306    0.0451683     0.0294023  -0.0221354    0.0761759   -0.0175169    0.151061    0.0272352   -0.0289735     0.0945403     0.152922     -0.0964607   -0.0513603    0.126105     0.0220114    0.163166     0.0455411     0.190663    -0.0175982    0.125165     0.117958    -0.00335078
 -0.0604722    0.0904554    0.0869297    -0.23769      0.0620074     0.147786     -0.10059    -0.0605171    0.0136441    0.042206     0.07211    -0.0534244    0.109138     -0.0665484     0.216009      0.0837131    0.0249648    0.0697867    0.168434     0.0882241   -0.000409597  -0.0979005    0.0263351   -0.111192    -0.0994867   -0.0659948 
 -0.103476     0.0200905   -0.000894595   0.0808206    0.178021      0.11386       0.139879    0.0984271   -0.0743051   -0.0814124   -0.0375509   0.0624387    0.00443856    0.0551463     0.151718      0.0500307    0.0408054   -0.0893135    0.0101123   -0.0980847   -0.17803       0.116408    -0.0961168   -0.0325864    0.0622861   -0.0171848 
  0.124421     0.0325281    0.0158614     0.105642    -0.057832      0.196609      0.0248408   0.21301      0.0551002   -0.0899485    0.15573     0.039091     0.0040783    -0.0459827     0.0822003    -0.169179    -0.149873    -0.00729989  -0.0443313    0.123356    -0.0802325     0.049645    -0.160809    -0.0397459    0.125885     0.089519  
 -0.114362     0.0251916   -0.00468437    0.0945647   -0.0318125     0.0890043     0.107433   -0.161143    -0.127457     0.0936816   -0.0866898  -0.0364206    0.0606789     0.0707168    -0.076837     -0.0399007   -0.0523064    0.00126448   0.0160158    0.24869      0.00431056    0.0549704    0.00872989   0.128463    -0.0828845    0.0368938 
 -0.0967997    0.117325    -0.0266155     0.0656193    0.102661      0.00332722    0.105423    0.00408342  -0.199073     0.0279295    0.0184257   0.0152639   -0.057731      0.0770078    -0.00654176   -0.0387828   -0.106458    -0.06916     -0.137037     0.0781366    0.128333     -0.034694    -0.116694    -0.0627235   -0.0815074    0.0306621 
 -0.0847479   -0.172182     0.209641     -0.0209408    0.216527     -0.00414119   -0.0553839   0.0204678    0.0251013   -0.0580897    0.0749298   0.101231     0.0449752    -0.0374058    -0.217188     -0.12885      0.107461    -0.0189843    0.134513     0.0302393    0.0310423    -0.0332903    0.066577    -0.138052     0.0317583    0.0361301 
 -0.108196     0.188125    -0.0521891    -0.0396442   -0.122556     -0.0596279    -0.0541131   0.0852       0.00352204   0.0266288    0.0384634   0.117567    -0.148742      0.000684301  -0.0190183    -0.00844923  -0.049754    -0.0902754   -0.0732228   -0.175531     0.0297301     0.0707462    0.10016     -0.00837456  -0.0464046   -0.0431999 
  0.0837405   -0.0514769    0.0548917     0.125126    -0.000109802   0.00974782    0.0650207   0.0708867    0.0605852    0.0788228   -0.0783052   0.0794297    0.124897      0.136968      0.0737693     0.179067     0.101063     0.151503    -0.154229    -0.0345862   -0.0310498    -0.00740345  -0.0225667    0.123954    -0.101162    -0.0463204 
 -0.132257     0.0508724   -0.00801753    0.0262474    0.0589062    -0.0501582    -0.0012963   0.17624     -0.0451397   -0.261947     0.0233068   0.0407591    0.178279     -0.0824882    -0.0957457    -0.0556709    0.0799815    0.146028    -0.036281     0.0062015    0.133168      0.107135    -0.0255612   -0.191846    -0.00409391   0.0479787 
 -0.164341    -0.058906     0.052164     -0.0200069    0.139546     -0.173721     -0.102268   -0.110274     0.0879319    0.146614    -0.0750803   0.148219     0.0666926    -0.080895     -0.0566541     0.0272333   -0.152706     0.115184    -0.0232642    0.0910122   -0.088144     -0.125472     0.0780428   -0.0901434   -0.167964    -0.0886278 
 -0.119804     0.209847    -0.00547816   -0.0712676    0.103109      0.0224664     0.0550323   0.175926    -0.00818337   0.00379571  -0.0766925  -0.25545      0.00679427    0.0981748    -0.00876934    0.14073     -0.0446346    0.109103     0.0439173    0.0347658    0.0524905     0.031216    -0.0477182   -0.0858342    0.131024    -0.089846  
 -0.0458857    0.0448319    0.0408087     0.154705    -0.0118084     0.0727521     0.0847589   0.0236718   -0.0915351   -0.118411    -0.108801    0.18566      0.166234     -0.00499202   -0.157413      0.0699726    0.0234965    0.0855726    0.210425    -0.0535614    0.0614579     0.00191808   0.0856919    0.0720964    0.0780611    0.0421407 
  0.0600631   -0.00439795  -0.0355359     0.136216     0.123667      0.119762     -0.0964215  -0.099819    -0.0186899    0.125033    -0.0571693  -0.244268     0.0484471     0.056163      0.0390377     0.0729542    0.117668     0.00350043   0.196802    -0.142755     0.039735     -0.102163    -0.0236188   -0.126169     0.0903033    0.174511  
  0.0542549   -0.0180907    0.102875      0.227198    -0.0265318    -0.0988701    -0.0286537  -0.127267    -0.0374824    0.00429704  -0.116965   -0.0308007   -0.168785      0.0330515     0.0404067     0.01847      0.0118988    0.15636      0.0769143    0.00395473  -0.0157384    -0.0113922    0.0915647    0.194719     0.0628156   -0.123399  
  0.0229583   -0.0083479   -0.0595169    -0.0977982   -0.00342051   -0.264841     -0.0201711   0.0180081   -0.0700518   -0.00659651   0.143866   -0.0278867    0.204929     -0.0255831     0.0855821    -0.279575    -0.125255     0.0756397    0.043462    -0.00758904  -0.0903499    -0.0568973   -0.0926453    0.0601138    0.0685213   -0.0956807 
 -0.0607933   -0.107328    -0.0353924     0.0143214   -0.0719939    -0.00464427   -0.0808413   0.043586    -0.0407128    0.0351177    0.075627   -0.0146384    0.240359     -0.00494929   -0.0380981    -0.00569391   0.00807898   0.0523008    0.0827725   -0.22104      0.0357499     0.0562481    0.0866076    0.14875     -0.053217    -0.0174895 
 -0.115303    -0.0935687    0.118164      0.0957801    0.0366287    -0.0116368     0.0671897  -0.0971175   -0.0102015    0.0604118    0.0512129   0.0785845    0.046653     -0.239307     -0.0816045     0.121287    -0.0273836   -0.0617153   -0.0853891    0.0117997   -0.228796     -0.00640099  -0.0356813   -0.00998521   0.150334    -0.0716149 
 -0.0770915    0.0582339   -0.0158241     0.0726781    0.0037637     0.062857      0.117832    0.0797083    0.0740894    0.058421    -0.0238607   0.053143    -0.139331      0.194787     -0.0783742     0.1227       0.195621     0.0366182   -0.0227333   -0.0638553    0.0601508    -0.0311818   -0.0389107    0.0349286   -0.0765483    0.129157  
  0.0169032   -0.0603153   -0.0286468    -0.00344311  -0.0417153    -0.120202      0.0329718  -0.0118562   -0.026622     0.126598     0.0801089  -0.0740609    0.0340059     0.113039      0.159638     -0.00891697  -0.0956504   -0.105417     0.0650475    0.207018    -0.158836     -0.12817      0.101741    -0.152749    -0.0672206   -0.0101058 
  0.0303382   -0.0501174    0.0426903     0.00858407   0.0139634    -0.154382     -0.128985    0.00525671  -0.0892219    0.162197    -0.0103335  -0.0274332    0.0698921    -0.167492      0.0828846    -0.0116057    0.0742349    0.0723068   -0.169497     0.11094     -0.115747     -0.12545     -0.119438     0.0147355   -0.0713776   -0.0828872 
 -0.0291096   -0.00157653   0.257279      0.0150363   -0.0064988    -0.0588034     0.0146299  -0.112832     0.0280397   -0.0128417   -0.0594989  -0.0883719   -0.0619239    -0.125841     -0.0846419    -0.012879    -0.0346228    0.00503639   0.031394    -0.0354505   -0.0534922     0.018586     0.161985    -0.0916948    0.233401     0.0368269 
 -0.0512865   -0.0923171   -0.21221      -0.137454     0.0120642     0.00537959   -0.0812286   0.11793      0.00327921   0.121816    -0.0536157  -0.00378403  -0.0191221    -0.0199021     0.133843      0.0771065   -0.113292     0.0141125    0.0602098   -0.0831804    0.00350798   -0.0565002   -0.0985831    0.373806     0.287166     0.0422156 
 -0.134275    -0.0526758    0.0750354    -0.0550006   -0.0890239    -0.0644299    -0.104672   -0.0930582   -0.0455725   -0.141197    -0.07404    -0.158088    -0.0650468     0.31774      -0.0161654     0.0287724   -0.0182424    0.0107337   -0.0501032   -0.00823094  -0.0242873     0.138993    -0.0264133   -0.0637572   -0.0402264    0.0282728 
 -0.0538545   -0.0273917   -0.0585815    -0.009051    -0.0782107    -0.117448     -0.0701536   0.0130787   -0.0200155    0.100848     0.0376313  -0.00713898  -0.000189184   0.100196     -0.20373       0.0504742    0.0252437   -0.181524     0.0727572   -0.0241492   -0.0246622     0.127168    -0.0157406   -0.0321808   -0.154523    -0.222666  
  0.0537843   -0.121783    -0.0882811     0.119509    -0.045592     -0.144453     -0.0019434   0.133691     0.144275     0.0752921    0.105192    0.163594     0.116543     -0.172709      0.0110953     0.0185691   -0.128189    -0.199215     0.0337289   -0.126663     0.116623      0.0478629   -0.139099     0.0854756    0.0484953   -0.111999  
 -0.023556     0.0733142    0.0276787     0.0365037    0.0742515     0.0871124     0.120842   -0.0189898   -0.120032    -0.128118     0.0490587   0.0815175   -0.13206       0.000209846   0.161816      0.0253344   -0.00678716  -0.198658     0.153566     0.00544097   0.0582937     0.00965505   0.0925156   -0.073701     0.0624247   -0.0944283 
 -0.0422782    0.119436    -0.0127666     0.0249799    0.209346      0.154585     -0.0519509   0.0335207   -0.0936384   -0.012288     0.126833    0.00581322  -0.0806871     0.0987313     0.000927683   0.0569425   -0.09618     -0.0636648    0.00247113  -0.0316396    0.00938955   -0.088186     0.10557     -0.260318    -0.121963    -0.0630826 kind diag, method split
0: avll = -1.3946853837896154
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.394768
INFO: iteration 2, average log likelihood -1.394685
INFO: iteration 3, average log likelihood -1.393961
INFO: iteration 4, average log likelihood -1.385597
INFO: iteration 5, average log likelihood -1.366704
INFO: iteration 6, average log likelihood -1.360885
INFO: iteration 7, average log likelihood -1.359976
INFO: iteration 8, average log likelihood -1.359616
INFO: iteration 9, average log likelihood -1.359421
INFO: iteration 10, average log likelihood -1.359299
INFO: iteration 11, average log likelihood -1.359218
INFO: iteration 12, average log likelihood -1.359164
INFO: iteration 13, average log likelihood -1.359128
INFO: iteration 14, average log likelihood -1.359103
INFO: iteration 15, average log likelihood -1.359085
INFO: iteration 16, average log likelihood -1.359073
INFO: iteration 17, average log likelihood -1.359064
INFO: iteration 18, average log likelihood -1.359058
INFO: iteration 19, average log likelihood -1.359053
INFO: iteration 20, average log likelihood -1.359049
INFO: iteration 21, average log likelihood -1.359046
INFO: iteration 22, average log likelihood -1.359044
INFO: iteration 23, average log likelihood -1.359043
INFO: iteration 24, average log likelihood -1.359041
INFO: iteration 25, average log likelihood -1.359041
INFO: iteration 26, average log likelihood -1.359040
INFO: iteration 27, average log likelihood -1.359039
INFO: iteration 28, average log likelihood -1.359039
INFO: iteration 29, average log likelihood -1.359038
INFO: iteration 30, average log likelihood -1.359038
INFO: iteration 31, average log likelihood -1.359038
INFO: iteration 32, average log likelihood -1.359038
INFO: iteration 33, average log likelihood -1.359037
INFO: iteration 34, average log likelihood -1.359037
INFO: iteration 35, average log likelihood -1.359037
INFO: iteration 36, average log likelihood -1.359037
INFO: iteration 37, average log likelihood -1.359037
INFO: iteration 38, average log likelihood -1.359037
INFO: iteration 39, average log likelihood -1.359037
INFO: iteration 40, average log likelihood -1.359037
INFO: iteration 41, average log likelihood -1.359037
INFO: iteration 42, average log likelihood -1.359037
INFO: iteration 43, average log likelihood -1.359037
INFO: iteration 44, average log likelihood -1.359037
INFO: iteration 45, average log likelihood -1.359037
INFO: iteration 46, average log likelihood -1.359037
INFO: iteration 47, average log likelihood -1.359037
INFO: iteration 48, average log likelihood -1.359037
INFO: iteration 49, average log likelihood -1.359037
INFO: iteration 50, average log likelihood -1.359037
INFO: EM with 100000 data points 50 iterations avll -1.359037
952.4 data points per parameter
1: avll = [-1.39477,-1.39469,-1.39396,-1.3856,-1.3667,-1.36088,-1.35998,-1.35962,-1.35942,-1.3593,-1.35922,-1.35916,-1.35913,-1.3591,-1.35909,-1.35907,-1.35906,-1.35906,-1.35905,-1.35905,-1.35905,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.359156
INFO: iteration 2, average log likelihood -1.359042
INFO: iteration 3, average log likelihood -1.358506
INFO: iteration 4, average log likelihood -1.353213
INFO: iteration 5, average log likelihood -1.336845
INFO: iteration 6, average log likelihood -1.325077
INFO: iteration 7, average log likelihood -1.320682
INFO: iteration 8, average log likelihood -1.317862
INFO: iteration 9, average log likelihood -1.315036
INFO: iteration 10, average log likelihood -1.312594
INFO: iteration 11, average log likelihood -1.311208
INFO: iteration 12, average log likelihood -1.310528
INFO: iteration 13, average log likelihood -1.310197
INFO: iteration 14, average log likelihood -1.310016
INFO: iteration 15, average log likelihood -1.309897
INFO: iteration 16, average log likelihood -1.309807
INFO: iteration 17, average log likelihood -1.309728
INFO: iteration 18, average log likelihood -1.309657
INFO: iteration 19, average log likelihood -1.309589
INFO: iteration 20, average log likelihood -1.309516
INFO: iteration 21, average log likelihood -1.309430
INFO: iteration 22, average log likelihood -1.309321
INFO: iteration 23, average log likelihood -1.309177
INFO: iteration 24, average log likelihood -1.309002
INFO: iteration 25, average log likelihood -1.308802
INFO: iteration 26, average log likelihood -1.308572
INFO: iteration 27, average log likelihood -1.308337
INFO: iteration 28, average log likelihood -1.308109
INFO: iteration 29, average log likelihood -1.307893
INFO: iteration 30, average log likelihood -1.307716
INFO: iteration 31, average log likelihood -1.307593
INFO: iteration 32, average log likelihood -1.307506
INFO: iteration 33, average log likelihood -1.307439
INFO: iteration 34, average log likelihood -1.307384
INFO: iteration 35, average log likelihood -1.307338
INFO: iteration 36, average log likelihood -1.307298
INFO: iteration 37, average log likelihood -1.307265
INFO: iteration 38, average log likelihood -1.307237
INFO: iteration 39, average log likelihood -1.307213
INFO: iteration 40, average log likelihood -1.307193
INFO: iteration 41, average log likelihood -1.307177
INFO: iteration 42, average log likelihood -1.307164
INFO: iteration 43, average log likelihood -1.307153
INFO: iteration 44, average log likelihood -1.307143
INFO: iteration 45, average log likelihood -1.307136
INFO: iteration 46, average log likelihood -1.307130
INFO: iteration 47, average log likelihood -1.307125
INFO: iteration 48, average log likelihood -1.307121
INFO: iteration 49, average log likelihood -1.307118
INFO: iteration 50, average log likelihood -1.307115
INFO: EM with 100000 data points 50 iterations avll -1.307115
473.9 data points per parameter
2: avll = [-1.35916,-1.35904,-1.35851,-1.35321,-1.33684,-1.32508,-1.32068,-1.31786,-1.31504,-1.31259,-1.31121,-1.31053,-1.3102,-1.31002,-1.3099,-1.30981,-1.30973,-1.30966,-1.30959,-1.30952,-1.30943,-1.30932,-1.30918,-1.309,-1.3088,-1.30857,-1.30834,-1.30811,-1.30789,-1.30772,-1.30759,-1.30751,-1.30744,-1.30738,-1.30734,-1.3073,-1.30726,-1.30724,-1.30721,-1.30719,-1.30718,-1.30716,-1.30715,-1.30714,-1.30714,-1.30713,-1.30713,-1.30712,-1.30712,-1.30712]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.307296
INFO: iteration 2, average log likelihood -1.307071
INFO: iteration 3, average log likelihood -1.305667
INFO: iteration 4, average log likelihood -1.297366
INFO: iteration 5, average log likelihood -1.284208
INFO: iteration 6, average log likelihood -1.273855
INFO: iteration 7, average log likelihood -1.267765
INFO: iteration 8, average log likelihood -1.264498
INFO: iteration 9, average log likelihood -1.262539
INFO: iteration 10, average log likelihood -1.261249
INFO: iteration 11, average log likelihood -1.260378
INFO: iteration 12, average log likelihood -1.259725
INFO: iteration 13, average log likelihood -1.259142
INFO: iteration 14, average log likelihood -1.258574
INFO: iteration 15, average log likelihood -1.258020
INFO: iteration 16, average log likelihood -1.257510
INFO: iteration 17, average log likelihood -1.257078
INFO: iteration 18, average log likelihood -1.256725
INFO: iteration 19, average log likelihood -1.256435
INFO: iteration 20, average log likelihood -1.256202
INFO: iteration 21, average log likelihood -1.256006
INFO: iteration 22, average log likelihood -1.255824
INFO: iteration 23, average log likelihood -1.255637
INFO: iteration 24, average log likelihood -1.255451
INFO: iteration 25, average log likelihood -1.255285
INFO: iteration 26, average log likelihood -1.255155
INFO: iteration 27, average log likelihood -1.255059
INFO: iteration 28, average log likelihood -1.254985
INFO: iteration 29, average log likelihood -1.254915
INFO: iteration 30, average log likelihood -1.254829
INFO: iteration 31, average log likelihood -1.254699
INFO: iteration 32, average log likelihood -1.254451
INFO: iteration 33, average log likelihood -1.253915
WARNING: Variances had to be floored 5
INFO: iteration 34, average log likelihood -1.252849
INFO: iteration 35, average log likelihood -1.264387
INFO: iteration 36, average log likelihood -1.258556
INFO: iteration 37, average log likelihood -1.256994
INFO: iteration 38, average log likelihood -1.256162
INFO: iteration 39, average log likelihood -1.255655
INFO: iteration 40, average log likelihood -1.255351
INFO: iteration 41, average log likelihood -1.255193
INFO: iteration 42, average log likelihood -1.255113
INFO: iteration 43, average log likelihood -1.255069
INFO: iteration 44, average log likelihood -1.255042
INFO: iteration 45, average log likelihood -1.255024
INFO: iteration 46, average log likelihood -1.255010
INFO: iteration 47, average log likelihood -1.254999
INFO: iteration 48, average log likelihood -1.254990
INFO: iteration 49, average log likelihood -1.254981
INFO: iteration 50, average log likelihood -1.254972
INFO: EM with 100000 data points 50 iterations avll -1.254972
236.4 data points per parameter
3: avll = [-1.3073,-1.30707,-1.30567,-1.29737,-1.28421,-1.27386,-1.26777,-1.2645,-1.26254,-1.26125,-1.26038,-1.25972,-1.25914,-1.25857,-1.25802,-1.25751,-1.25708,-1.25672,-1.25643,-1.2562,-1.25601,-1.25582,-1.25564,-1.25545,-1.25529,-1.25515,-1.25506,-1.25498,-1.25491,-1.25483,-1.2547,-1.25445,-1.25391,-1.25285,-1.26439,-1.25856,-1.25699,-1.25616,-1.25565,-1.25535,-1.25519,-1.25511,-1.25507,-1.25504,-1.25502,-1.25501,-1.255,-1.25499,-1.25498,-1.25497]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.255215
INFO: iteration 2, average log likelihood -1.254930
INFO: iteration 3, average log likelihood -1.253994
INFO: iteration 4, average log likelihood -1.245476
INFO: iteration 5, average log likelihood -1.221840
WARNING: Variances had to be floored 9
INFO: iteration 6, average log likelihood -1.193145
WARNING: Variances had to be floored 12
INFO: iteration 7, average log likelihood -1.182942
WARNING: Variances had to be floored 9
INFO: iteration 8, average log likelihood -1.176417
WARNING: Variances had to be floored 10
INFO: iteration 9, average log likelihood -1.174143
WARNING: Variances had to be floored 9
INFO: iteration 10, average log likelihood -1.175613
WARNING: Variances had to be floored 12
INFO: iteration 11, average log likelihood -1.173292
WARNING: Variances had to be floored 9
INFO: iteration 12, average log likelihood -1.171583
WARNING: Variances had to be floored 10
INFO: iteration 13, average log likelihood -1.171412
WARNING: Variances had to be floored 9
INFO: iteration 14, average log likelihood -1.173705
WARNING: Variances had to be floored 12
INFO: iteration 15, average log likelihood -1.171707
WARNING: Variances had to be floored 9
INFO: iteration 16, average log likelihood -1.170084
WARNING: Variances had to be floored 10
INFO: iteration 17, average log likelihood -1.169978
WARNING: Variances had to be floored 9
INFO: iteration 18, average log likelihood -1.171868
WARNING: Variances had to be floored 12
INFO: iteration 19, average log likelihood -1.168733
WARNING: Variances had to be floored 4 9
INFO: iteration 20, average log likelihood -1.165493
WARNING: Variances had to be floored 10
INFO: iteration 21, average log likelihood -1.175639
WARNING: Variances had to be floored 9
INFO: iteration 22, average log likelihood -1.173921
WARNING: Variances had to be floored 12
INFO: iteration 23, average log likelihood -1.170874
WARNING: Variances had to be floored 9
INFO: iteration 24, average log likelihood -1.168608
WARNING: Variances had to be floored 10
INFO: iteration 25, average log likelihood -1.167380
WARNING: Variances had to be floored 9
INFO: iteration 26, average log likelihood -1.168037
WARNING: Variances had to be floored 4 12
INFO: iteration 27, average log likelihood -1.164871
WARNING: Variances had to be floored 9
INFO: iteration 28, average log likelihood -1.175093
WARNING: Variances had to be floored 10
INFO: iteration 29, average log likelihood -1.170999
WARNING: Variances had to be floored 9
INFO: iteration 30, average log likelihood -1.171960
WARNING: Variances had to be floored 12
INFO: iteration 31, average log likelihood -1.168637
WARNING: Variances had to be floored 9
INFO: iteration 32, average log likelihood -1.165768
WARNING: Variances had to be floored 4 10
INFO: iteration 33, average log likelihood -1.164379
WARNING: Variances had to be floored 9
INFO: iteration 34, average log likelihood -1.177910
WARNING: Variances had to be floored 12
INFO: iteration 35, average log likelihood -1.171750
WARNING: Variances had to be floored 9
INFO: iteration 36, average log likelihood -1.169127
WARNING: Variances had to be floored 10
INFO: iteration 37, average log likelihood -1.167990
WARNING: Variances had to be floored 9
INFO: iteration 38, average log likelihood -1.168758
WARNING: Variances had to be floored 4 12
INFO: iteration 39, average log likelihood -1.165168
WARNING: Variances had to be floored 9
INFO: iteration 40, average log likelihood -1.175003
WARNING: Variances had to be floored 10
INFO: iteration 41, average log likelihood -1.170930
WARNING: Variances had to be floored 9
INFO: iteration 42, average log likelihood -1.171916
WARNING: Variances had to be floored 12
INFO: iteration 43, average log likelihood -1.168610
WARNING: Variances had to be floored 9
INFO: iteration 44, average log likelihood -1.165732
WARNING: Variances had to be floored 4 10
INFO: iteration 45, average log likelihood -1.164304
WARNING: Variances had to be floored 9
INFO: iteration 46, average log likelihood -1.177779
WARNING: Variances had to be floored 12
INFO: iteration 47, average log likelihood -1.171619
WARNING: Variances had to be floored 9
INFO: iteration 48, average log likelihood -1.168968
WARNING: Variances had to be floored 10
INFO: iteration 49, average log likelihood -1.167852
WARNING: Variances had to be floored 9
INFO: iteration 50, average log likelihood -1.168612
INFO: EM with 100000 data points 50 iterations avll -1.168612
118.1 data points per parameter
4: avll = [-1.25522,-1.25493,-1.25399,-1.24548,-1.22184,-1.19314,-1.18294,-1.17642,-1.17414,-1.17561,-1.17329,-1.17158,-1.17141,-1.1737,-1.17171,-1.17008,-1.16998,-1.17187,-1.16873,-1.16549,-1.17564,-1.17392,-1.17087,-1.16861,-1.16738,-1.16804,-1.16487,-1.17509,-1.171,-1.17196,-1.16864,-1.16577,-1.16438,-1.17791,-1.17175,-1.16913,-1.16799,-1.16876,-1.16517,-1.175,-1.17093,-1.17192,-1.16861,-1.16573,-1.1643,-1.17778,-1.17162,-1.16897,-1.16785,-1.16861]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 7 8 23 24
INFO: iteration 1, average log likelihood -1.165148
WARNING: Variances had to be floored 7 8 17 18 23 24
INFO: iteration 2, average log likelihood -1.157781
WARNING: Variances had to be floored 7 8 19 20 23 24
INFO: iteration 3, average log likelihood -1.160818
WARNING: Variances had to be floored 7 8 17 18 23 24
INFO: iteration 4, average log likelihood -1.149691
WARNING: Variances had to be floored 7 8 12 23 24
INFO: iteration 5, average log likelihood -1.126146
WARNING: Variances had to be floored 2 7 8 17 18 19 20 23 24 31
INFO: iteration 6, average log likelihood -1.098278
WARNING: Variances had to be floored 7 8 12 23 24
INFO: iteration 7, average log likelihood -1.099656
WARNING: Variances had to be floored 7 8 16 17 18 23 24 31
INFO: iteration 8, average log likelihood -1.084169
WARNING: Variances had to be floored 2 7 8 12 19 20 23 24
INFO: iteration 9, average log likelihood -1.099209
WARNING: Variances had to be floored 7 8 17 18 23 24 31
INFO: iteration 10, average log likelihood -1.091884
WARNING: Variances had to be floored 1 7 8 12 23 24
INFO: iteration 11, average log likelihood -1.090646
WARNING: Variances had to be floored 2 7 8 17 18 19 20 23 24 31
INFO: iteration 12, average log likelihood -1.082327
WARNING: Variances had to be floored 7 8 12 16 23 24
INFO: iteration 13, average log likelihood -1.091266
WARNING: Variances had to be floored 7 8 17 18 23 24 31
INFO: iteration 14, average log likelihood -1.092739
WARNING: Variances had to be floored 2 7 8 19 20 23 24
INFO: iteration 15, average log likelihood -1.092827
WARNING: Variances had to be floored 7 8 12 17 18 23 24 31
INFO: iteration 16, average log likelihood -1.083492
WARNING: Variances had to be floored 7 8 23 24
INFO: iteration 17, average log likelihood -1.090285
WARNING: Variances had to be floored 2 7 8 12 16 17 18 19 20 23 24 31
INFO: iteration 18, average log likelihood -1.069944
WARNING: Variances had to be floored 7 8 23 24
INFO: iteration 19, average log likelihood -1.099162
WARNING: Variances had to be floored 7 8 12 17 18 23 24 31
INFO: iteration 20, average log likelihood -1.075037
WARNING: Variances had to be floored 1 7 8 19 20 23 24
INFO: iteration 21, average log likelihood -1.080320
WARNING: Variances had to be floored 7 8 12 17 18 23 24
INFO: iteration 22, average log likelihood -1.077963
WARNING: Variances had to be floored 2 7 8 16 23 24 31
INFO: iteration 23, average log likelihood -1.068280
WARNING: Variances had to be floored 7 8 12 17 18 19 20 23 24
INFO: iteration 24, average log likelihood -1.076560
WARNING: Variances had to be floored 7 8 23 24 31
INFO: iteration 25, average log likelihood -1.080275
WARNING: Variances had to be floored 1 7 8 12 17 18 23 24
INFO: iteration 26, average log likelihood -1.068108
WARNING: Variances had to be floored 7 8 19 20 23 24
INFO: iteration 27, average log likelihood -1.083178
WARNING: Variances had to be floored 2 7 8 12 16 17 18 23 24 31
INFO: iteration 28, average log likelihood -1.060483
WARNING: Variances had to be floored 7 8 23 24
INFO: iteration 29, average log likelihood -1.087102
WARNING: Variances had to be floored 7 8 12 17 18 19 20 23 24 31
INFO: iteration 30, average log likelihood -1.067973
WARNING: Variances had to be floored 1 7 8 23 24
INFO: iteration 31, average log likelihood -1.079896
WARNING: Variances had to be floored 7 8 12 17 18 23 24
INFO: iteration 32, average log likelihood -1.074689
WARNING: Variances had to be floored 2 7 8 16 19 20 23 24 31
INFO: iteration 33, average log likelihood -1.067112
WARNING: Variances had to be floored 7 8 12 17 18 23 24
INFO: iteration 34, average log likelihood -1.079350
WARNING: Variances had to be floored 7 8 23 24 31
INFO: iteration 35, average log likelihood -1.078413
WARNING: Variances had to be floored 1 7 8 12 17 18 19 20 23 24
INFO: iteration 36, average log likelihood -1.067171
WARNING: Variances had to be floored 7 8 23 24
INFO: iteration 37, average log likelihood -1.085967
WARNING: Variances had to be floored 2 7 8 12 16 17 18 23 24 31
INFO: iteration 38, average log likelihood -1.058544
WARNING: Variances had to be floored 7 8 19 20 23 24
INFO: iteration 39, average log likelihood -1.086134
WARNING: Variances had to be floored 7 8 12 17 18 23 24 31
INFO: iteration 40, average log likelihood -1.070803
WARNING: Variances had to be floored 1 7 8 23 24
INFO: iteration 41, average log likelihood -1.078084
WARNING: Variances had to be floored 7 8 12 17 18 19 20 23 24
INFO: iteration 42, average log likelihood -1.073768
WARNING: Variances had to be floored 2 7 8 16 23 24 31
INFO: iteration 43, average log likelihood -1.069929
WARNING: Variances had to be floored 7 8 12 17 18 23 24
INFO: iteration 44, average log likelihood -1.077406
WARNING: Variances had to be floored 7 8 19 20 23 24 31
INFO: iteration 45, average log likelihood -1.077387
WARNING: Variances had to be floored 1 7 8 12 17 18 23 24
INFO: iteration 46, average log likelihood -1.069983
WARNING: Variances had to be floored 7 8 23 24
INFO: iteration 47, average log likelihood -1.084168
WARNING: Variances had to be floored 2 7 8 12 16 17 18 19 20 23 24 31
INFO: iteration 48, average log likelihood -1.057596
WARNING: Variances had to be floored 7 8 23 24
INFO: iteration 49, average log likelihood -1.088953
WARNING: Variances had to be floored 7 8 12 17 18 23 24 31
INFO: iteration 50, average log likelihood -1.068897
INFO: EM with 100000 data points 50 iterations avll -1.068897
59.0 data points per parameter
5: avll = [-1.16515,-1.15778,-1.16082,-1.14969,-1.12615,-1.09828,-1.09966,-1.08417,-1.09921,-1.09188,-1.09065,-1.08233,-1.09127,-1.09274,-1.09283,-1.08349,-1.09029,-1.06994,-1.09916,-1.07504,-1.08032,-1.07796,-1.06828,-1.07656,-1.08028,-1.06811,-1.08318,-1.06048,-1.0871,-1.06797,-1.0799,-1.07469,-1.06711,-1.07935,-1.07841,-1.06717,-1.08597,-1.05854,-1.08613,-1.0708,-1.07808,-1.07377,-1.06993,-1.07741,-1.07739,-1.06998,-1.08417,-1.0576,-1.08895,-1.0689]
[-1.39469,-1.39477,-1.39469,-1.39396,-1.3856,-1.3667,-1.36088,-1.35998,-1.35962,-1.35942,-1.3593,-1.35922,-1.35916,-1.35913,-1.3591,-1.35909,-1.35907,-1.35906,-1.35906,-1.35905,-1.35905,-1.35905,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35904,-1.35916,-1.35904,-1.35851,-1.35321,-1.33684,-1.32508,-1.32068,-1.31786,-1.31504,-1.31259,-1.31121,-1.31053,-1.3102,-1.31002,-1.3099,-1.30981,-1.30973,-1.30966,-1.30959,-1.30952,-1.30943,-1.30932,-1.30918,-1.309,-1.3088,-1.30857,-1.30834,-1.30811,-1.30789,-1.30772,-1.30759,-1.30751,-1.30744,-1.30738,-1.30734,-1.3073,-1.30726,-1.30724,-1.30721,-1.30719,-1.30718,-1.30716,-1.30715,-1.30714,-1.30714,-1.30713,-1.30713,-1.30712,-1.30712,-1.30712,-1.3073,-1.30707,-1.30567,-1.29737,-1.28421,-1.27386,-1.26777,-1.2645,-1.26254,-1.26125,-1.26038,-1.25972,-1.25914,-1.25857,-1.25802,-1.25751,-1.25708,-1.25672,-1.25643,-1.2562,-1.25601,-1.25582,-1.25564,-1.25545,-1.25529,-1.25515,-1.25506,-1.25498,-1.25491,-1.25483,-1.2547,-1.25445,-1.25391,-1.25285,-1.26439,-1.25856,-1.25699,-1.25616,-1.25565,-1.25535,-1.25519,-1.25511,-1.25507,-1.25504,-1.25502,-1.25501,-1.255,-1.25499,-1.25498,-1.25497,-1.25522,-1.25493,-1.25399,-1.24548,-1.22184,-1.19314,-1.18294,-1.17642,-1.17414,-1.17561,-1.17329,-1.17158,-1.17141,-1.1737,-1.17171,-1.17008,-1.16998,-1.17187,-1.16873,-1.16549,-1.17564,-1.17392,-1.17087,-1.16861,-1.16738,-1.16804,-1.16487,-1.17509,-1.171,-1.17196,-1.16864,-1.16577,-1.16438,-1.17791,-1.17175,-1.16913,-1.16799,-1.16876,-1.16517,-1.175,-1.17093,-1.17192,-1.16861,-1.16573,-1.1643,-1.17778,-1.17162,-1.16897,-1.16785,-1.16861,-1.16515,-1.15778,-1.16082,-1.14969,-1.12615,-1.09828,-1.09966,-1.08417,-1.09921,-1.09188,-1.09065,-1.08233,-1.09127,-1.09274,-1.09283,-1.08349,-1.09029,-1.06994,-1.09916,-1.07504,-1.08032,-1.07796,-1.06828,-1.07656,-1.08028,-1.06811,-1.08318,-1.06048,-1.0871,-1.06797,-1.0799,-1.07469,-1.06711,-1.07935,-1.07841,-1.06717,-1.08597,-1.05854,-1.08613,-1.0708,-1.07808,-1.07377,-1.06993,-1.07741,-1.07739,-1.06998,-1.08417,-1.0576,-1.08895,-1.0689]
32Ã—26 Array{Float64,2}:
 -0.0286943    0.0452753    0.0434666     0.130755     -0.000673119   0.0600317     0.0696029   -0.00401396  -0.0781752    -0.120339    -0.10414      0.183821      0.159691    -0.00559467   -0.147347     0.10512      0.0130117   0.0873459    0.204803    -0.159538     0.0614951    0.013587      0.114685      0.0720442    0.0703864    0.0506598 
  0.12048     -0.0546348    0.0573021    -0.00193803   -0.0121564     0.0610465     0.0359442   -0.0348259    0.0528678     0.0018027    0.0922909    0.0238562    -0.0148868    0.0898764     0.135705    -0.0821082   -0.0136949   0.107191     0.0186594    0.177435     0.0430136    0.189247     -0.0156282     0.134167     0.0835994    0.00643002
  0.229764     0.0750034   -0.0554769     0.0113456    -0.0464789     0.170269      0.0311232    0.233983    -0.0345259    -0.0881664    0.104505     0.00697587    0.00129014  -0.0700171     0.0898805   -0.165447    -0.120285   -0.00061815   0.0265336    0.135808    -0.0613254    0.01647      -0.535475     -0.0412164    0.123454     0.114582  
  0.154873    -0.00509931   0.0632203     0.160638     -0.0593882     0.220373      0.00318452   0.202456     0.162923     -0.0943266    0.177431     0.0688122    -0.0123772    0.0324374     0.0550459   -0.173197    -0.161771   -0.0163452   -0.0912994    0.154391    -0.0706989    0.0533799     0.178331     -0.0411764    0.110378     0.085648  
 -0.00637643   0.0486466    0.147193     -0.0146039    -0.165004     -0.122009      0.0596311   -0.0756089   -0.000298456  -0.0227493    0.167167     0.0410657    -0.0735273    0.137511      0.0743822   -0.0220884   -0.136674    0.010827    -0.0274311    0.0958027   -0.00230672  -0.124311      0.182828     -0.0150984    0.221673    -0.118636  
 -0.126975     0.062381    -0.0246571     0.0330983     0.0496492    -0.0323523    -0.00344796   0.152053    -0.0542512    -0.269999     0.0243603    0.0411085     0.182096    -0.0830404    -0.108502    -0.0305137    0.0552552   0.139801    -0.0285541    0.00763086   0.127696     0.103742     -0.0264547    -0.183446    -0.0347274    0.0477536 
 -0.430265    -0.0319023    0.103061      0.157512      0.0497075    -0.0988079    -0.028441    -0.195056    -0.032681      0.129773    -0.212761    -0.0266878    -0.191353     0.0527554     0.0756788    0.0268561   -0.210635    0.0873854    0.125015     0.171149     0.0729455    0.00320708    0.199249      0.276867     0.0307334   -0.137021  
  0.572454    -0.00352613   0.103885      0.265381     -0.116877     -0.0976124    -0.0166409   -0.0492984   -0.0336665    -0.175351     0.00745232  -0.0290985    -0.155186     0.0127119    -0.00406837   0.0492173    0.325439    0.201542     0.048698    -0.248301    -0.0903413   -0.0268233    -0.0865543     0.134937     0.0739008   -0.0946924 
 -0.0612524   -0.101819    -0.211509     -0.0710026    -0.00141008   -0.0078454    -0.0610774    0.128183     0.00371266    0.146861    -0.0519106   -0.0118821    -0.0118395   -0.0396089     0.125471     0.0868026   -0.111581    0.018147     0.0687136   -0.059355     0.0062807   -0.0674501    -0.0807425     0.377792     0.26659      0.0478232 
  0.0523204   -0.0338998   -0.00709968    0.0871024     0.0200801     0.0121227    -0.0300359   -0.0151061   -0.00277649    0.101749    -0.0167615   -0.0658765     0.063762     0.0882975     0.0890865    0.0853479    0.0436625   0.0051416    0.0402241    0.0360174   -0.0491249   -0.060183      0.0154031    -0.0386717   -0.0493013    0.033701  
 -0.0733109   -0.106276     0.0109835    -0.00102178   -0.0721555     0.0154835    -0.0477715    0.0448467   -0.0603183     0.0477548    0.0753772   -0.0121158     0.24246     -0.021497     -0.0280414   -0.008309     0.0142585   0.0559834    0.0338033   -0.222787     0.0349825    0.0581827     0.101647      0.149414    -0.0636374   -0.0173648 
 -0.111758    -0.104074     0.0388853     0.112294      0.0358695    -0.0124871     0.0773583   -0.0459082    0.0144001     0.0734513    0.0500263    0.0421962     0.0561235   -0.238506     -0.114933     0.151837    -0.0196632  -0.0508383   -0.0943929    0.00196474  -0.238379    -0.031286     -0.0278696     0.0237754    0.117632    -0.0773007 
 -0.0317111    0.021496     0.248286      0.0135251    -0.0162093    -0.056669     -0.00535427  -0.183988     0.0290949    -0.050189    -0.0544697   -0.115888     -0.271706    -0.121558     -0.103002     0.0140619   -0.0196117   0.0368086    0.0611688   -0.0316984   -0.0875691    0.0302854     0.216674     -0.108534     0.210617     0.0497941 
 -0.0399149   -0.021845     0.272862      0.00920839   -0.0641148     0.0234666     0.0515164   -0.00246382   0.0289817     0.0592319    0.0289335    0.0205605     0.31785     -0.12869      -0.0727934   -0.0566046   -0.0532197  -0.147154    -0.0111707    0.0341652    0.230863     0.0385663     0.133198     -0.111236     0.315807     0.021561  
 -0.125796    -0.0596451    0.0920121    -0.0810904    -0.0893435    -0.102561     -0.144941    -0.0456084   -0.0280373    -0.166439    -0.0819169   -0.179307      0.00883078   0.338799     -0.0299901    0.012423    -0.0104745   0.0157474   -0.0584632   -0.0422112   -0.0176798    0.12377      -0.0374119    -0.097145     0.0211853    0.0279305 
 -0.182715     0.033275    -0.000326887   0.0772851    -0.0489578     0.0762459     0.099791    -0.161382    -0.131953      0.0704757   -0.0869497   -0.0358301     0.0482944    0.101559     -0.072973    -0.0340982   -0.0523367  -0.00372187  -0.00114766   0.229534     0.0115864    0.041987      0.00456383    0.112077    -0.0752912    0.0337365 
  0.0244553   -0.0159839   -0.058519     -0.0967195     0.10223      -0.219629     -0.0190564   -0.0177853   -0.0794905    -0.00496831   0.118958    -0.0277521     0.205629     0.0641548     0.0797912   -0.281291     0.0182823   0.0751132   -1.45312     -0.00791467  -0.0895289   -0.0328071    -0.0806063    -0.0540968    0.0931149   -0.108791  
  0.0225992   -0.00131127  -0.0620941    -0.101973     -0.061516     -0.284521     -0.0236231    0.0254304   -0.0616175    -0.00945853   0.152071    -0.0276152     0.215206    -0.250588      0.0833088   -0.277053    -0.217919    0.0597374    1.38019     -0.00755222  -0.0900189   -0.0340531    -0.0939856     0.141415     0.0655147   -0.13113   
 -0.0543988   -0.0259154   -0.0410421    -0.0056528    -0.0781       -0.139552     -0.00363642   0.00215818  -0.0177234     0.099361     0.0346263    0.0647683    -0.00573112   0.0766306    -0.238773     0.0506512    0.0260073  -0.164305     0.0684858   -0.0027524   -0.00650763   0.144672     -0.0103951    -0.0305324   -0.144131    -0.220815  
 -0.0616855   -0.049676    -0.065827      0.000634656  -0.0133847    -0.0810563    -0.242288     0.0577018    0.0123957     0.106545     0.0751611   -0.314135     -0.0670093    0.144379     -0.127553     0.0927887    0.0283901  -0.133122     0.0639335   -0.122367    -0.131973    -0.0163224    -0.000127629  -0.0543795   -0.250054    -0.211311  
 -0.10358      0.180708    -0.666293     -0.041972     -0.125821     -0.283068     -0.0466461    0.0842383   -0.0464329     0.0360671    0.0919107    0.109028     -0.0659146    0.00884443   -0.0407548    0.00929221   0.0318728  -0.0328575   -0.0446961   -0.170819    -0.0722331   -0.000576627   0.099911      0.00833585  -0.038329     0.0368339 
 -0.0997974    0.193579     0.508606     -0.049062     -0.125677      0.0935348    -0.0606075    0.0828352    0.0499825     0.0240054   -0.0181686    0.131566     -0.182673     0.0439041    -0.00931339  -0.0379854   -0.158791   -0.126802    -0.144107    -0.179948     0.167224     0.113578      0.0926555    -0.0177619   -0.0343903   -0.133015  
 -0.103387     0.00626169  -0.00959838    0.109281      0.202479     -0.271921      0.135532    -0.274573     0.147445     -0.0842723   -0.0294829    0.0329406    -0.0624536    0.12386       0.191641     0.0481847    0.0367181  -0.0768912    0.00837371  -0.121469    -0.144897     0.162736     -0.153227      0.127996     0.0378598    0.0624842 
 -0.103107     0.0185051    0.0241504     0.0676478     0.160583      0.609555      0.130702     0.434156    -0.291632     -0.0847484   -0.0517476    0.0594411     0.0499903    0.000850683   0.0971675    0.0495245    0.0539639  -0.0898451    0.0124816   -0.0981003   -0.214339     0.0789948    -0.0636561    -0.230816     0.0946047   -0.0205021 
  0.0139929   -0.0412524    0.01197      -0.0737048     0.000715276   0.0944055     0.0120967   -0.00231765  -0.123909     -0.00742669   0.0919186   -0.0306912    -0.110933    -0.0281915     0.134678    -0.136234     0.0591964   0.0781589   -0.0344829   -0.0496409    0.00779131  -0.0398953    -0.0679362     0.0669274   -0.0109444   -0.0355123 
  0.0343764   -0.0274512    0.0131521     0.0334886     0.0347907    -0.0583965     0.00412795   0.044128    -0.0316402     0.0097567    0.051087     0.0657224     0.00443655  -0.0852359     0.081309     0.0164719   -0.0251647  -0.101714     0.0155635   -0.00154662   0.0269473   -0.030357     -0.0551431    -0.00310976   0.00271735  -0.0971524 
 -0.101433    -0.00444543   0.015458     -0.103602      0.100257      0.0150126    -0.0866053   -0.0569272    0.0287124     0.0759213    0.0071958    0.0053957     0.0716241   -0.0392072     0.054967     0.0554504   -0.0934537   0.0493678    0.056688     0.0847337   -0.0333633   -0.109917      0.0375977    -0.127558    -0.127669    -0.0763212 
 -0.080043    -0.15424      0.198253     -0.0205161     0.223374      0.000567356  -0.0628592    0.0196672    0.0128985    -0.0350853    0.0761753    0.0851302     0.0366023   -0.0452871    -0.204067    -0.1078       0.0765347  -0.0199948    0.123974    -0.00361785   0.0323287   -0.0335786     0.0631484    -0.150513     0.0229417    0.0509426 
  0.21547      0.0619877   -0.144575      0.0132516    -0.0680852    -0.013268     -0.0461779   -0.0763726    0.0492106     0.0423521   -0.0316539    0.000631696  -0.176703    -0.108669      0.0659826   -0.180643     0.0654198   0.170151    -0.06866     -0.0304814   -0.021981     0.093662     -0.00614648   -0.0821253    0.0600928   -0.0360811 
 -0.0467283    0.0620559   -0.0256171     0.0502331    -0.00557446    0.0595069     0.129575     0.0764558    0.0698472     0.070086    -0.031853     0.0438002    -0.141392     0.137431     -0.0764175    0.0978626    0.256034    0.0348271   -0.00622035  -0.0627491    0.040621    -0.0387208     0.0390421     0.0532587   -0.0760407    0.130334  
 -0.113939     0.217235    -0.0066164    -0.0554537     0.0764345     0.0130378     0.0580811    0.177242     0.0111769     0.00442181  -0.0670099   -0.246625     -0.00942611   0.104554     -0.0271178    0.163902    -0.0404223   0.105578     0.0451676    0.0500181    0.0123107    0.025024     -0.0344787    -0.0907539    0.119402    -0.094329  
 -0.096873     0.164426    -0.0384387     0.0483983     0.157578      0.00788033    0.0784556   -0.0233671   -0.203685      0.0319826    0.0225406    0.0176888    -0.062815     0.0775763     0.00756009  -0.0122007   -0.087137   -0.0704306   -0.0869729    0.137449     0.0696224   -0.0373237    -0.0991896    -0.0579304   -0.103905     0.0450286 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 7 8 19 20 23 24
INFO: iteration 1, average log likelihood -1.077074
WARNING: Variances had to be floored 1 2 7 8 12 17 18 19 20 23 24 31
INFO: iteration 2, average log likelihood -1.057684
WARNING: Variances had to be floored 1 7 8 16 19 20 23 24
INFO: iteration 3, average log likelihood -1.071533
WARNING: Variances had to be floored 1 2 7 8 12 17 18 19 23 24 31
INFO: iteration 4, average log likelihood -1.060994
WARNING: Variances had to be floored 1 7 8 19 20 23 24
INFO: iteration 5, average log likelihood -1.073262
WARNING: Variances had to be floored 1 2 7 8 12 16 17 18 19 20 23 24 31
INFO: iteration 6, average log likelihood -1.055329
WARNING: Variances had to be floored 1 7 8 19 20 23 24
INFO: iteration 7, average log likelihood -1.076944
WARNING: Variances had to be floored 1 2 7 8 12 17 18 19 23 24 31
INFO: iteration 8, average log likelihood -1.057526
WARNING: Variances had to be floored 1 7 8 16 19 20 23 24
INFO: iteration 9, average log likelihood -1.071371
WARNING: Variances had to be floored 1 2 7 8 12 17 18 19 23 24 31
INFO: iteration 10, average log likelihood -1.060978
INFO: EM with 100000 data points 10 iterations avll -1.060978
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.347797e+05
      1       6.531826e+05      -1.815971e+05 |       32
      2       6.241801e+05      -2.900246e+04 |       32
      3       6.089916e+05      -1.518851e+04 |       32
      4       5.990901e+05      -9.901530e+03 |       32
      5       5.933350e+05      -5.755062e+03 |       32
      6       5.900071e+05      -3.327929e+03 |       32
      7       5.880778e+05      -1.929282e+03 |       32
      8       5.870175e+05      -1.060320e+03 |       32
      9       5.862230e+05      -7.944767e+02 |       32
     10       5.854173e+05      -8.057392e+02 |       32
     11       5.844448e+05      -9.724541e+02 |       32
     12       5.836361e+05      -8.086970e+02 |       32
     13       5.831970e+05      -4.391672e+02 |       32
     14       5.829508e+05      -2.461319e+02 |       32
     15       5.827916e+05      -1.592016e+02 |       32
     16       5.826954e+05      -9.622598e+01 |       32
     17       5.826315e+05      -6.389492e+01 |       32
     18       5.825836e+05      -4.791511e+01 |       32
     19       5.825428e+05      -4.079904e+01 |       32
     20       5.825002e+05      -4.258112e+01 |       32
     21       5.824512e+05      -4.904375e+01 |       31
     22       5.823974e+05      -5.373043e+01 |       32
     23       5.823138e+05      -8.368639e+01 |       32
     24       5.821714e+05      -1.423806e+02 |       32
     25       5.819724e+05      -1.990069e+02 |       32
     26       5.816987e+05      -2.736898e+02 |       32
     27       5.813854e+05      -3.133042e+02 |       32
     28       5.810357e+05      -3.497131e+02 |       32
     29       5.806292e+05      -4.064390e+02 |       32
     30       5.803120e+05      -3.172653e+02 |       32
     31       5.801515e+05      -1.604934e+02 |       32
     32       5.800643e+05      -8.712815e+01 |       32
     33       5.800122e+05      -5.214948e+01 |       32
     34       5.799789e+05      -3.333014e+01 |       30
     35       5.799557e+05      -2.320822e+01 |       32
     36       5.799401e+05      -1.552891e+01 |       31
     37       5.799284e+05      -1.175382e+01 |       29
     38       5.799182e+05      -1.012293e+01 |       28
     39       5.799098e+05      -8.494152e+00 |       31
     40       5.799034e+05      -6.366867e+00 |       26
     41       5.798982e+05      -5.228671e+00 |       29
     42       5.798933e+05      -4.852425e+00 |       24
     43       5.798893e+05      -4.049127e+00 |       25
     44       5.798834e+05      -5.902005e+00 |       27
     45       5.798783e+05      -5.098072e+00 |       28
     46       5.798747e+05      -3.564413e+00 |       28
     47       5.798712e+05      -3.530959e+00 |       23
     48       5.798685e+05      -2.650266e+00 |       18
     49       5.798657e+05      -2.780825e+00 |       25
     50       5.798624e+05      -3.304022e+00 |       27
K-means terminated without convergence after 50 iterations (objv = 579862.4255238043)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.302323
INFO: iteration 2, average log likelihood -1.276954
INFO: iteration 3, average log likelihood -1.253596
INFO: iteration 4, average log likelihood -1.218706
WARNING: Variances had to be floored 23
INFO: iteration 5, average log likelihood -1.176323
WARNING: Variances had to be floored 2 6
INFO: iteration 6, average log likelihood -1.141346
WARNING: Variances had to be floored 8 13
INFO: iteration 7, average log likelihood -1.111984
WARNING: Variances had to be floored 23 25
INFO: iteration 8, average log likelihood -1.078957
WARNING: Variances had to be floored 7 9 16 18 22 28
INFO: iteration 9, average log likelihood -1.053962
WARNING: Variances had to be floored 2
INFO: iteration 10, average log likelihood -1.093648
WARNING: Variances had to be floored 13 23 26
INFO: iteration 11, average log likelihood -1.068040
WARNING: Variances had to be floored 16 24 25
INFO: iteration 12, average log likelihood -1.056909
WARNING: Variances had to be floored 2 6 8 18 28
INFO: iteration 13, average log likelihood -1.052703
WARNING: Variances had to be floored 7 23
INFO: iteration 14, average log likelihood -1.086051
WARNING: Variances had to be floored 13 26
INFO: iteration 15, average log likelihood -1.065748
WARNING: Variances had to be floored 2 9 16 22
INFO: iteration 16, average log likelihood -1.043081
WARNING: Variances had to be floored 23 24 25 28
INFO: iteration 17, average log likelihood -1.057701
WARNING: Variances had to be floored 7 8
INFO: iteration 18, average log likelihood -1.072238
WARNING: Variances had to be floored 6 13 16 18 26
INFO: iteration 19, average log likelihood -1.043066
WARNING: Variances had to be floored 2 23
INFO: iteration 20, average log likelihood -1.062539
WARNING: Variances had to be floored 9 22 24 25 28
INFO: iteration 21, average log likelihood -1.046090
WARNING: Variances had to be floored 7
INFO: iteration 22, average log likelihood -1.064863
WARNING: Variances had to be floored 2 6 8 13 16 18 23 26
INFO: iteration 23, average log likelihood -1.031632
INFO: iteration 24, average log likelihood -1.103208
WARNING: Variances had to be floored 9 25
INFO: iteration 25, average log likelihood -1.047185
WARNING: Variances had to be floored 2 7 18 22 28
INFO: iteration 26, average log likelihood -1.017558
WARNING: Variances had to be floored 6 13 16 23 24 26
INFO: iteration 27, average log likelihood -1.061368
WARNING: Variances had to be floored 8
INFO: iteration 28, average log likelihood -1.101237
WARNING: Variances had to be floored 9 25
INFO: iteration 29, average log likelihood -1.063978
WARNING: Variances had to be floored 2 7
INFO: iteration 30, average log likelihood -1.031508
WARNING: Variances had to be floored 13 16 22 23 24 26 28
INFO: iteration 31, average log likelihood -1.020736
WARNING: Variances had to be floored 6 18
INFO: iteration 32, average log likelihood -1.085036
WARNING: Variances had to be floored 2 8 9 25
INFO: iteration 33, average log likelihood -1.056676
WARNING: Variances had to be floored 7
INFO: iteration 34, average log likelihood -1.068804
WARNING: Variances had to be floored 13 16 23 26 28
INFO: iteration 35, average log likelihood -1.030064
WARNING: Variances had to be floored 2 6 22
INFO: iteration 36, average log likelihood -1.062213
WARNING: Variances had to be floored 7 8 9 18 24 25
INFO: iteration 37, average log likelihood -1.049958
INFO: iteration 38, average log likelihood -1.086598
WARNING: Variances had to be floored 13 16 23 26 28
INFO: iteration 39, average log likelihood -1.041444
WARNING: Variances had to be floored 2
INFO: iteration 40, average log likelihood -1.075529
WARNING: Variances had to be floored 22 25
INFO: iteration 41, average log likelihood -1.041358
WARNING: Variances had to be floored 6 7 8 9 18 23 24 26 28
INFO: iteration 42, average log likelihood -1.016147
WARNING: Variances had to be floored 2 13 16
INFO: iteration 43, average log likelihood -1.104085
INFO: iteration 44, average log likelihood -1.100128
WARNING: Variances had to be floored 23
INFO: iteration 45, average log likelihood -1.049798
WARNING: Variances had to be floored 2 13 18 22 24 25 26 28
INFO: iteration 46, average log likelihood -1.009703
WARNING: Variances had to be floored 6 7 8 16
INFO: iteration 47, average log likelihood -1.093305
WARNING: Variances had to be floored 23
INFO: iteration 48, average log likelihood -1.098055
WARNING: Variances had to be floored 9
INFO: iteration 49, average log likelihood -1.067130
WARNING: Variances had to be floored 2 13 25 28
INFO: iteration 50, average log likelihood -1.032006
INFO: EM with 100000 data points 50 iterations avll -1.032006
59.0 data points per parameter
32Ã—26 Array{Float64,2}:
 -0.0727904   -0.10602      0.010916    -0.00302214  -0.0721002    0.0173858   -0.047138     0.0450154   -0.0597555    0.0487081    0.0755086   -0.0116469     0.242949     -0.0220915   -0.0280585   -0.00835051   0.0137842    0.0561564    0.0349673   -0.223064      0.0355188     0.0586905   0.101334     0.149488    -0.0634924  -0.0174032 
 -0.125521     0.211508    -0.00546565  -0.0578671    0.0724018    0.00857637   0.063017     0.18808      0.0187768    0.00415924  -0.0765663   -0.260991     -0.000388973   0.10171     -0.0219127    0.174349    -0.0370675    0.108666     0.0494265    0.0466084     0.0153993     0.0306519  -0.0384179   -0.0830022    0.131022   -0.0952853 
 -0.00576677   0.0515859    0.150414    -0.0161989   -0.167366    -0.123269     0.0614155   -0.0741934    0.00329325  -0.025434     0.166625     0.0432361    -0.0739312     0.139185     0.073387    -0.022255    -0.138877     0.0117706   -0.0271713    0.0945781    -0.000433338  -0.124866    0.188567    -0.0132633    0.229947   -0.1196    
 -0.0616848   -0.099163    -0.211719    -0.075829     0.00240891  -0.0103066   -0.0620222    0.131522     0.00428596   0.150403    -0.0525977   -0.0103774    -0.0123632    -0.0394203    0.123711     0.0884209   -0.112305     0.0119786    0.0715963   -0.0654577     0.0065301    -0.0655386  -0.079153     0.379851     0.274165    0.0490134 
 -0.125008    -0.0475334    0.0800557   -0.0575363   -0.0860462   -0.0741426   -0.110866    -0.0581965   -0.0383587   -0.150049    -0.075873    -0.151858      0.00778559    0.303906    -0.0240481    0.0157245   -0.0142791    0.0155672   -0.0494629   -0.00940784   -0.0105339     0.118763   -0.0304574   -0.0638088    0.0194233   0.027702  
 -0.0550893   -0.0347807   -0.0502194   -0.00561005  -0.0799348   -0.134609    -0.0484763    0.0190651   -0.0135683    0.100152     0.0491014   -0.00101664   -0.0246611     0.0982076   -0.205347     0.0559936    0.0234401   -0.166802     0.0735726   -0.0199204    -0.0252623     0.133587   -0.0119186   -0.0315905   -0.156887   -0.221184  
  0.0151576   -0.0344578   -0.0154297    0.0137125   -0.0508409   -0.107529    -0.0180573   -0.00110274  -0.0320201    0.0998153    0.097262    -0.0555634     0.0366218     0.106064     0.151031    -0.00892145  -0.0795268   -0.140958     0.0602181    0.212338     -0.163647     -0.131155    0.0955951   -0.147229    -0.0617705   0.00326018
 -0.112386    -0.10144      0.0253337    0.108043     0.0349327   -0.0175828    0.0857455   -0.0370454    0.0252896    0.0746051    0.0523132    0.0487631     0.0545565    -0.257795    -0.121813     0.157452    -0.0244871   -0.0733327   -0.0834993   -0.000336792  -0.236569     -0.0263135  -0.0341595    0.0271266    0.121629   -0.0817835 
  0.0589694   -0.107524    -0.0978311    0.0781848   -0.0354739   -0.209161    -0.00689674   0.143415     0.172812     0.0263191    0.115312     0.176288      0.10449      -0.215288     0.0050004    0.0408684   -0.121429    -0.189729     0.035956    -0.199569      0.121961      0.0503618  -0.12512      0.0787842    0.0263404  -0.115641  
 -0.0444916    0.0624208   -0.0246669    0.0499388   -0.00493799   0.0593076    0.126631     0.0805941    0.0687586    0.0688094   -0.0318247    0.0485713    -0.143488      0.131921    -0.0762162    0.096858     0.264909     0.03244     -0.00879995  -0.063332      0.0405005    -0.0374272   0.0385596    0.0522659   -0.076934    0.130495  
 -0.0987246    0.156375    -0.0372517    0.0514337    0.155667     0.00265802   0.0806103   -0.0231054   -0.206779     0.0313118    0.0201625    0.0169001    -0.0574148     0.0774386    0.00588937  -0.0130038   -0.0830064   -0.0686685   -0.0944332    0.13266       0.0727863    -0.03505    -0.106013    -0.0563616   -0.103231    0.0440232 
  0.13335     -0.174172    -0.0347175   -0.140593     0.0206421   -0.00157643  -0.014546    -0.0247363   -0.11826      0.0498715    0.061114    -0.0120373    -0.166083     -0.00618205   0.219118    -0.26307      0.0226876    0.165325     0.0337956    0.0308083    -0.0410652    -0.162375   -0.0533284    0.0936616    0.0381035  -0.0207318 
  0.119908    -0.0605691    0.0632588   -0.0234185   -0.00888945   0.0583185    0.0244081   -0.0216112    0.0773005   -0.012229     0.117946     0.0268835    -0.0298195     0.0880007    0.156084    -0.0975753   -0.00452274   0.122676     0.0199878    0.172826      0.0534627     0.201714   -0.0179046    0.151393     0.114876    0.00753218
 -0.0717096    0.00689395   0.0220241   -0.258142     0.036157     0.14704     -0.0962903   -0.0528615    0.0190232    0.0525723    0.0449092   -0.13807       0.105458     -0.0610679    0.223637     0.080888     0.0150494    0.0727516    0.171293     0.107661      0.0250757    -0.101504   -0.0232983   -0.0858404   -0.107984   -0.0407044 
 -0.127549     0.0605651   -0.0253921    0.0331077    0.0499484   -0.0363224   -0.00268046   0.154519    -0.0547141   -0.272494     0.0250356    0.0409457     0.181142     -0.0834478   -0.112294    -0.0331593    0.0581282    0.141597    -0.0288716    0.00726347    0.128305      0.105022   -0.0265414   -0.181581    -0.0348721   0.0478415 
  0.07488     -0.0172038    0.103178     0.209406    -0.0334399   -0.0983956   -0.0212496   -0.121716    -0.0329355   -0.0265055   -0.100901    -0.0281579    -0.174782      0.0335689    0.0354236    0.0379946    0.0595663    0.142403     0.0862316   -0.0395601    -0.00606948   -0.0122961   0.0522209    0.204937     0.0521465  -0.116715  
 -0.10235      0.187043    -0.0821701   -0.0457809   -0.126252    -0.0950328   -0.052963     0.0836859    0.00124829   0.0305332    0.0382232    0.120886     -0.125262      0.0242852   -0.0258503   -0.014379    -0.0626856   -0.0786781   -0.0907633   -0.175807      0.0460962     0.0643769   0.0952393   -0.0059445   -0.0381927  -0.0497227 
 -0.121061     0.0260618    0.0722692   -0.0251262    0.00380289   0.0696567   -0.00749575  -0.0309805   -0.0334237   -0.00156837   0.0591304    0.0299297     0.0145943    -0.0599344    0.010825    -0.00752979   0.00360331   0.0471579   -0.0917338   -0.0494788    -0.00502513   -0.0018541  -0.0482043    0.00735777  -0.0987274  -0.0867187 
  0.19587      0.0344781    0.00562574   0.0877212   -0.0523798    0.194928     0.0156711    0.219696     0.0658266   -0.0915818    0.141821     0.0390804    -0.00411801   -0.0210155    0.0749939   -0.170664    -0.140537    -0.00789305  -0.0299207    0.147904     -0.0672376     0.0405603  -0.176701    -0.040769     0.119504    0.102418  
  0.0827203   -0.0549989    0.0351974    0.122425     0.00195642   0.0131914    0.0578082    0.0675478    0.0631395    0.0881032   -0.0838734    0.07891       0.121772      0.133096     0.072148     0.175897     0.0996888    0.156821    -0.156399     0.0283338    -0.0273667     0.0485049  -0.0485772    0.144393    -0.0936362  -0.0532995 
  0.0535944   -0.0326255   -0.017842     0.137219     0.122082     0.101444    -0.129041    -0.0963366   -0.0344339    0.133725    -0.0583831   -0.242426      0.046768      0.031949     0.0454652    0.0903759    0.125878     0.00065293   0.19725     -0.127482      0.0204352    -0.0983294  -0.0216904   -0.122835     0.0348861   0.156962  
 -0.157238    -0.0102157    0.0401034   -0.0169722    0.163058    -0.311122    -0.0814427   -0.0977057    0.10475      0.183363    -0.146383     0.192408      0.0703011    -0.105506    -0.0636804    0.0278224   -0.261776     0.0872439    0.0113802    0.114099     -0.10854      -0.122925    0.103781    -0.083287    -0.142844   -0.0843241 
 -0.103991     0.0294362   -0.00409481   0.0824053   -0.0389942    0.0882401    0.111155    -0.164721    -0.112919     0.113296    -0.0879108   -0.0120867     0.0626869     0.0811339   -0.0697697   -0.0405377   -0.0634863   -0.00215568   0.00885045   0.247224      0.000772259   0.0358229   0.00624427   0.121156    -0.0849276   0.0313187 
 -0.0112723    0.0316228    0.0413103    0.0436022    0.0606696    0.0428489    0.0954422    0.0299178   -0.0849669   -0.136648     0.0682352    0.0823085    -0.0704505    -0.0059611    0.118304     0.0257213   -0.0332696   -0.196439     0.146461    -0.00546511    0.114552      0.0139788   0.067056    -0.0277627    0.0504385  -0.104479  
 -0.0323012    0.0456923    0.0456276    0.133899    -0.00667781   0.056798     0.0724886    0.008188    -0.0761613   -0.121517    -0.106039     0.186248      0.161926     -0.00702952  -0.146748     0.10661      0.0107217    0.0920426    0.20728     -0.160062      0.0626383     0.0187555   0.108931     0.072537     0.0738868   0.052557  
 -0.103437     0.00718341   0.00391911   0.085367     0.174399     0.160912     0.130251     0.0700876   -0.0736016   -0.0845057   -0.0344129    0.0418612    -0.0125168     0.0574657    0.140399     0.0485598    0.0469199   -0.0911783    0.00540204  -0.11176      -0.172276      0.118784   -0.11084     -0.0418325    0.0622752   0.0181558 
  0.0563959   -0.0641846    0.0560275    0.0115896    0.0192233   -0.127929    -0.124185     0.00496644  -0.0887131    0.167524    -0.00889146  -0.0319028     0.051552     -0.172102     0.081162    -0.0163503    0.0760532    0.075117    -0.16203      0.112145     -0.116868     -0.126605   -0.162258     0.00645952  -0.0658299  -0.0872524 
  0.0230727   -0.00913708  -0.0619224   -0.0989927    0.0174355   -0.253926    -0.0208631    0.00398138  -0.0695995   -0.00579415   0.136686    -0.0273793     0.208674     -0.104477     0.0786972   -0.276859    -0.105172     0.0639414    0.053333    -0.0068663    -0.088875     -0.0334649  -0.0865604    0.0477881    0.0793811  -0.122788  
  0.21664      0.0633905   -0.14696      0.0128047   -0.0692118   -0.0145078   -0.0452637   -0.0765137    0.0501156    0.0422871   -0.0328515   -0.000763736  -0.176588     -0.10983      0.0659832   -0.181206     0.06934      0.170221    -0.0688305   -0.0299405    -0.02204       0.0945979  -0.00728071  -0.0810616    0.0612174  -0.0371508 
 -0.0341809    0.00738547   0.258429     0.0119582   -0.0316819   -0.0294077    0.014642    -0.122594     0.0291205   -0.0138062   -0.0265007   -0.0684354    -0.0639379    -0.124718    -0.0965248   -0.0109666   -0.0313239   -0.0211162    0.0358052   -0.00981027    0.0264133     0.0334846   0.186925    -0.110804     0.248162    0.0395316 
 -0.0883061   -0.178353     0.218601    -0.0299924    0.237538    -0.00409821  -0.0616456    0.0224415    0.0197233   -0.0489862    0.0708792    0.0996        0.0551368    -0.0680172   -0.215636    -0.127395     0.0858974   -0.0117391    0.137183     0.00587245    0.0379354    -0.0305749   0.0654287   -0.144072     0.0306463   0.0496273 
 -0.010131     0.0849253   -0.0463537    0.026697     0.168726     0.160073    -0.04618      0.0271927   -0.0827469   -0.0131529    0.131321     0.0149818    -0.0817118     0.118912    -0.04249      0.0613034   -0.105914    -0.0675964   -0.00383256  -0.0447766     0.00243131   -0.0885266   0.0907402   -0.260076    -0.113741   -0.0623047 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 7 16 23 26
INFO: iteration 1, average log likelihood -1.054986
WARNING: Variances had to be floored 6 7 8 16 22 23 24 26
INFO: iteration 2, average log likelihood -1.017390
WARNING: Variances had to be floored 2 6 7 9 16 18 22 23 24 26 28
INFO: iteration 3, average log likelihood -0.991315
WARNING: Variances had to be floored 7 8 13 16 23 25 26
INFO: iteration 4, average log likelihood -1.031237
WARNING: Variances had to be floored 6 7 16 22 23 24 26
INFO: iteration 5, average log likelihood -1.022047
WARNING: Variances had to be floored 2 7 8 9 16 18 23 24 26 28
INFO: iteration 6, average log likelihood -0.995186
WARNING: Variances had to be floored 6 7 16 22 23 26
INFO: iteration 7, average log likelihood -1.031538
WARNING: Variances had to be floored 7 8 9 13 16 18 22 23 24 25 26
INFO: iteration 8, average log likelihood -1.002246
WARNING: Variances had to be floored 2 6 7 16 23 24 26 28
INFO: iteration 9, average log likelihood -1.015595
WARNING: Variances had to be floored 7 8 16 22 23 26
INFO: iteration 10, average log likelihood -1.031471
INFO: EM with 100000 data points 10 iterations avll -1.031471
59.0 data points per parameter
32Ã—26 Array{Float64,2}:
  0.0170407   -0.059268     0.00397392  -0.0520201    0.0141669   -0.0812395    0.00242709   0.0762428   -0.112629      0.099969   -0.108714   -0.0324929    0.094934     0.0251343   -0.0092827    0.02752     -0.0110001    0.0768003    0.0602624  -0.0166752   -0.103368     0.265746    -0.106366     0.135004    -0.0784295     0.186398  
 -0.122901     0.0154598    0.0769838   -0.145727     0.207225     0.0218661   -0.0998894    0.135756    -0.261393      0.136235   -0.0456885   0.148807    -0.053856    -0.0378826    0.00217618  -0.0342361    0.023238     0.196986    -0.160657   -0.0946175    0.0250844    0.104479     0.151901    -0.128971    -0.123585     -0.0979033 
 -0.101601     0.139561    -0.0207603    0.0639145   -0.0122724    0.0299463   -0.0541613   -0.0575892   -0.0434802     0.115528   -0.142143    0.169212    -0.0707095    0.00302187  -0.0983066    0.277117     0.116849    -0.0541859    0.0469516   0.158707    -0.060705    -0.0765287    0.116098     0.0965964    0.328376      0.0659662 
 -0.0418134    0.00535331  -0.184322    -0.0039584   -0.284486    -0.0928145   -0.0631657   -0.0448761    0.000552764   0.0563791  -0.0155856   0.188333    -0.0600534   -0.0286315    0.0210228    0.0842347    0.146203    -0.0301618    0.103224    0.172876    -0.149325     0.125665     0.0361148   -0.0400997   -0.172982      0.104418  
  0.0279729    0.124538     0.176174    -0.109648    -0.0908411   -0.048872    -0.0348777   -0.0371032    0.0362416     0.14759    -0.165073    0.0723779   -0.0397496    0.171874     0.144928    -0.112044    -0.0200112   -0.0188152    0.179566   -0.00565987  -0.171579    -0.0447684   -0.0361566   -0.10581      0.0746981     0.21388   
  0.0545134   -0.0108441   -0.0127874   -0.103568     0.0129824    0.0953845    0.0565386    0.0649127   -0.107424     -0.118481   -0.018115    0.0469636    0.00696194  -0.042975    -0.0504545    0.184431     0.0246034   -0.0109683    0.199817   -0.0406462   -0.0282968    0.128409    -0.0979669   -0.00774675  -0.000235404  -0.190833  
 -0.00960394   0.107757     0.0390507   -0.113289     0.0513942    0.0704729    0.155768    -0.102339     0.121776     -0.050112   -0.0653467  -0.191665    -0.207919    -0.0968946    0.169449    -0.0134883   -0.00349217   0.0528141    0.0942133  -0.209151    -0.217706    -0.231365     0.172991     0.0116567    0.0204549    -0.194782  
 -0.230681     0.0387868    0.00204113  -0.0702512    0.00707487   0.0108456    0.258992     0.118101    -0.0924478     0.0383155  -0.017222    0.0971235    0.0667567   -0.172794     0.0640308    0.0456312    0.0877823   -0.115362    -0.0505503  -0.0636193   -0.0713418   -0.0677193    0.0321767   -0.0666367    0.0625982     0.21546   
  0.148086     0.119332     0.0405681   -0.110376     0.122809     0.0126256    0.126882    -0.232311     0.0983719     0.245387    0.0271216  -0.211566     0.00421694  -0.0610047    0.111137    -0.0179284   -0.211313     0.267388    -0.0804411   0.154954     0.010596    -0.260613    -0.171426    -0.0664504    0.0767747     0.121238  
 -0.105175     0.097196    -0.129493     0.175016    -0.0385596    0.156272     0.118958    -0.045317     0.1217       -0.071542   -0.137825   -0.104628     0.1216      -0.0447519    0.205054     0.035491    -0.0561037    0.0402047    0.016833   -0.0925816    0.00127994   0.262552     0.0710087   -0.120804     0.401671     -0.193154  
  0.0509618    0.0915435   -0.076359    -0.0779131   -0.0128788   -0.150391    -0.1471       0.190257     0.0407538     0.0291249  -0.363673    0.00516054  -0.126928    -0.0163912   -0.0435751    0.00949861  -0.137612    -0.0987031    0.0388521  -0.00223518  -0.0732158   -0.0965006   -0.170601     0.0799739    0.110934     -0.0673333 
  0.00590411   0.125141    -0.0308981   -0.0662178    0.0451453    0.0475859   -0.0617526    0.21848      0.109906     -0.0393012   0.0851036   0.14984      0.0459064   -0.154254    -0.142422    -0.0541584    0.00426426  -0.0889509    0.10055     0.00666997   0.0973114    0.0920976   -0.0595103   -0.115675     0.101497      0.0146918 
 -0.0377762   -0.0340395   -0.104587    -0.152265     0.0972546   -0.0451099    0.0708039    0.299835     0.00127608    0.0225292   0.101674    0.113865     0.0923385   -0.105011     0.110506     0.0728737   -0.0371695   -0.0950202   -0.0541437   0.0304281    0.0474372   -0.0334787    0.132738    -0.00948736   0.0407533     0.0548499 
 -0.0487202   -0.110014    -0.0951284   -0.0773373   -0.159163    -0.0186108    0.0231203   -0.104637    -0.0420772     0.0374283  -0.175561    0.0970062   -0.138804    -0.0969548    0.121088    -0.0255576   -0.17227     -0.00392365   0.0154881  -0.195607    -0.00168301   0.104304     0.0629813    0.13036     -0.0254807    -0.0749102 
 -0.00907779   0.0403639    0.150946    -0.0602542   -0.0642964    0.204256     0.0591036    0.170145    -0.0321734    -0.163139    0.1308     -0.0537304   -0.0731649    0.113209     0.161829     0.00579367   0.130478     0.0319719   -0.0101688   0.121651    -0.0585106    0.0430139   -0.0588244    0.0793219    0.08364       0.00199488
  0.0213924   -0.00861691   0.0848688   -0.032172     0.0465052   -0.0509424   -0.114304    -0.249939     0.058124      0.112346   -0.103369   -0.0100567   -0.125751     0.170699     0.0681775    0.162628     0.286412    -0.114907     0.0274123  -0.108893     0.0836378    0.0524278   -0.0489717    0.00347462   0.019922     -0.155731  
 -0.0736938   -0.137164    -0.219822     0.031263    -0.0767935    0.026011    -0.070523     0.181802    -0.02264       0.0854381  -0.0305006   0.0725495   -0.1337      -0.0888412    0.152169     0.00150423  -0.0446081   -0.0410477   -0.0348183   0.0947782   -0.00688083   0.0650093   -0.165171     0.101727    -0.0179979     0.110367  
 -0.100211    -0.122326     0.00272325   0.0350121   -0.0551718    0.00291598  -0.0395459   -0.0243484    0.0768116    -0.106704   -0.0212717  -0.156023     0.0736807    0.0724748    0.0780868   -0.14892     -0.174068    -0.0378174   -0.0747584  -0.0182901   -0.0151039   -0.103701    -0.107986     0.0208846    0.101074     -0.26844   
  0.179493     0.0506435   -0.0142684    0.116071     0.225853     0.073285    -0.0455148    0.0641057   -0.0144957    -0.0488654  -0.0274696   0.0696755   -0.0485496   -0.0437021   -0.0817749    0.00863477  -0.112574    -0.127711    -0.110699   -0.0797406   -0.0386986   -0.0777127   -0.0733098   -0.207849    -0.0738961    -0.0791966 
  0.00719501  -0.0648624   -0.0203118    0.0378815    0.00438691  -0.0952845    0.00539686   0.0368685    0.0159852    -0.146482   -0.0672858  -0.0421752   -0.0485825   -0.123878     0.0746798   -0.0757874    0.0728315   -0.0205082   -0.102482   -0.110962     0.105036    -0.146017     0.00887221   0.0287152   -0.0560637     0.0867362 
  0.190895    -0.108732    -0.0778103    0.177985    -0.0115454   -0.0437507   -0.0160884   -0.0415739    0.059455     -0.0731487  -0.105672   -0.118264     0.027131     0.0337494   -0.197707     0.115491    -0.0902433   -0.117819    -0.0490586  -0.132895     0.00747678  -0.126248    -0.0784511   -0.0536      -0.00569589    0.0378037 
 -0.135532    -0.0365385    0.167251    -0.00571677  -0.118044     0.0407809   -0.0548564   -0.0333169    0.0191008     0.090325   -0.137671    0.23412      0.0149013   -0.0571698   -0.166155    -0.0953836   -0.153347     0.0211709    0.10976     0.0575708   -0.149912     0.0423491    0.0573179    0.0813982    0.0586888     0.179659  
  0.0716056   -0.00768703   0.108195     0.0319392    0.0207458    0.0623695    0.100699    -0.0268662    0.0740725    -0.0487744  -0.0170076  -0.0723896    0.0894       0.00896644   0.248786     0.0455546    0.0617667    0.0123409    0.080175    0.00202226   0.0880095   -0.0605607   -0.0134295   -0.0215205   -0.0407428    -0.0378999 
 -0.0898444    0.0176367    0.19594     -0.129645    -0.11968      0.0379552   -0.0352727    0.103043    -0.114522     -0.228517    0.0822981   0.0175321    0.0917768    0.025731     0.114837    -0.0270933    0.0875178    0.0717347    0.0427787  -0.024156     0.171254     0.0179943    0.145448    -0.0896965   -0.0243257    -0.124522  
  0.0214619    0.0846247   -0.0962736    0.10755     -0.0326619   -0.0308028   -0.0560686    0.126045     0.0176848     0.0641209  -0.103154   -0.13306     -0.106344     0.0953024   -0.118609     0.248859    -0.113566    -0.0561365    0.0393611   0.0530527    0.0671368    0.0643028    0.125162     0.0194142   -0.0470377    -0.0509171 
 -0.0900242    0.0829362   -0.0495603   -0.104038     0.0077461   -0.124514    -0.117299     0.07458     -0.0612454    -0.0261009   0.102037    0.0334878   -0.166676     0.0333223    0.0433989    0.120494    -0.00555255   0.0108005    0.0305789  -0.0597577   -0.108833    -0.161323    -0.0737719   -0.108898    -0.040157      0.0402643 
 -0.275547     0.0556548   -0.109237     0.101144     0.0243706   -0.0639447   -0.151279    -0.0663481    0.0621767    -0.0373141  -0.0345413   0.0907134   -0.0831279   -0.022703    -0.00747579   0.0788872   -0.0595866    0.0580107   -0.111106   -0.012447     0.174724    -0.0118614    0.00288855   0.102456     0.124423     -0.197789  
 -0.121383     0.00166695   0.0987022   -0.0223968    0.178645     0.0248482    0.0326605   -0.0134767   -0.099393      0.0392713  -0.157383   -0.0416909    0.116204    -0.0655956   -0.0460298   -0.0498263    0.0657288    0.174163     0.0163734  -0.0320003   -0.0526223   -0.070828    -0.0995736    0.0578918    0.0451754    -0.164767  
  0.0302314   -0.15708      0.240561     0.0254288    0.0939247   -0.0424662    0.0324902   -0.0868184   -0.0564097     0.0971809  -0.0377131  -0.0563203    0.00388076  -0.0345927    0.209664     0.14068     -0.0755181   -0.0210433   -0.0271816  -0.013431     0.177923    -0.266904     0.0247808   -0.0622354   -0.0799491     0.0887494 
 -0.00103809   0.0151622    0.142635    -0.122143    -0.153439     0.114169    -0.199365     0.00877028   0.0209756    -0.0310873   0.053326   -0.159926    -0.190853    -0.0677496   -0.180652     0.0948986   -0.192231    -0.0451793   -0.0375622   0.0589527    0.0584684   -0.00756764   0.0435313    0.0331397   -0.0500842     0.089737  
 -0.116595    -0.0538284    0.0948232    0.0108693   -0.0211009   -0.0824319    0.0209936    0.00437887  -0.045174      0.0633897   0.0926642   0.0744805    0.107112    -0.0655687   -0.131201     0.0246572   -0.0395695    0.0102904    0.112461    0.102668    -0.177192     0.124345    -0.0930947   -0.0870255    0.0226205    -0.0631892 
  0.200699    -0.132602     0.145592    -0.014308    -0.045492     0.158992    -0.0202427    0.0120571    0.180737      0.165658   -0.0465616   0.00041318  -0.0288182    0.0539272    0.111681     0.095044     0.0730684    0.119736    -0.198656    0.0371444    0.105887    -0.065823     0.0710546    0.0373229    0.10461      -0.0726122 kind full, method split
0: avll = -1.41577672123395
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.415794
INFO: iteration 2, average log likelihood -1.415710
INFO: iteration 3, average log likelihood -1.415630
INFO: iteration 4, average log likelihood -1.415524
INFO: iteration 5, average log likelihood -1.415383
INFO: iteration 6, average log likelihood -1.415207
INFO: iteration 7, average log likelihood -1.414995
INFO: iteration 8, average log likelihood -1.414733
INFO: iteration 9, average log likelihood -1.414374
INFO: iteration 10, average log likelihood -1.413844
INFO: iteration 11, average log likelihood -1.413098
INFO: iteration 12, average log likelihood -1.412229
INFO: iteration 13, average log likelihood -1.411463
INFO: iteration 14, average log likelihood -1.410954
INFO: iteration 15, average log likelihood -1.410681
INFO: iteration 16, average log likelihood -1.410549
INFO: iteration 17, average log likelihood -1.410489
INFO: iteration 18, average log likelihood -1.410461
INFO: iteration 19, average log likelihood -1.410449
INFO: iteration 20, average log likelihood -1.410442
INFO: iteration 21, average log likelihood -1.410439
INFO: iteration 22, average log likelihood -1.410437
INFO: iteration 23, average log likelihood -1.410436
INFO: iteration 24, average log likelihood -1.410435
INFO: iteration 25, average log likelihood -1.410435
INFO: iteration 26, average log likelihood -1.410434
INFO: iteration 27, average log likelihood -1.410434
INFO: iteration 28, average log likelihood -1.410433
INFO: iteration 29, average log likelihood -1.410433
INFO: iteration 30, average log likelihood -1.410433
INFO: iteration 31, average log likelihood -1.410432
INFO: iteration 32, average log likelihood -1.410432
INFO: iteration 33, average log likelihood -1.410432
INFO: iteration 34, average log likelihood -1.410432
INFO: iteration 35, average log likelihood -1.410432
INFO: iteration 36, average log likelihood -1.410431
INFO: iteration 37, average log likelihood -1.410431
INFO: iteration 38, average log likelihood -1.410431
INFO: iteration 39, average log likelihood -1.410431
INFO: iteration 40, average log likelihood -1.410431
INFO: iteration 41, average log likelihood -1.410431
INFO: iteration 42, average log likelihood -1.410431
INFO: iteration 43, average log likelihood -1.410431
INFO: iteration 44, average log likelihood -1.410431
INFO: iteration 45, average log likelihood -1.410431
INFO: iteration 46, average log likelihood -1.410430
INFO: iteration 47, average log likelihood -1.410430
INFO: iteration 48, average log likelihood -1.410430
INFO: iteration 49, average log likelihood -1.410430
INFO: iteration 50, average log likelihood -1.410430
INFO: EM with 100000 data points 50 iterations avll -1.410430
952.4 data points per parameter
1: avll = [-1.41579,-1.41571,-1.41563,-1.41552,-1.41538,-1.41521,-1.415,-1.41473,-1.41437,-1.41384,-1.4131,-1.41223,-1.41146,-1.41095,-1.41068,-1.41055,-1.41049,-1.41046,-1.41045,-1.41044,-1.41044,-1.41044,-1.41044,-1.41044,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.410444
INFO: iteration 2, average log likelihood -1.410364
INFO: iteration 3, average log likelihood -1.410282
INFO: iteration 4, average log likelihood -1.410172
INFO: iteration 5, average log likelihood -1.410027
INFO: iteration 6, average log likelihood -1.409855
INFO: iteration 7, average log likelihood -1.409676
INFO: iteration 8, average log likelihood -1.409507
INFO: iteration 9, average log likelihood -1.409356
INFO: iteration 10, average log likelihood -1.409226
INFO: iteration 11, average log likelihood -1.409120
INFO: iteration 12, average log likelihood -1.409041
INFO: iteration 13, average log likelihood -1.408987
INFO: iteration 14, average log likelihood -1.408951
INFO: iteration 15, average log likelihood -1.408928
INFO: iteration 16, average log likelihood -1.408913
INFO: iteration 17, average log likelihood -1.408902
INFO: iteration 18, average log likelihood -1.408894
INFO: iteration 19, average log likelihood -1.408889
INFO: iteration 20, average log likelihood -1.408884
INFO: iteration 21, average log likelihood -1.408881
INFO: iteration 22, average log likelihood -1.408878
INFO: iteration 23, average log likelihood -1.408876
INFO: iteration 24, average log likelihood -1.408874
INFO: iteration 25, average log likelihood -1.408873
INFO: iteration 26, average log likelihood -1.408872
INFO: iteration 27, average log likelihood -1.408871
INFO: iteration 28, average log likelihood -1.408870
INFO: iteration 29, average log likelihood -1.408869
INFO: iteration 30, average log likelihood -1.408869
INFO: iteration 31, average log likelihood -1.408868
INFO: iteration 32, average log likelihood -1.408868
INFO: iteration 33, average log likelihood -1.408867
INFO: iteration 34, average log likelihood -1.408867
INFO: iteration 35, average log likelihood -1.408867
INFO: iteration 36, average log likelihood -1.408866
INFO: iteration 37, average log likelihood -1.408866
INFO: iteration 38, average log likelihood -1.408866
INFO: iteration 39, average log likelihood -1.408866
INFO: iteration 40, average log likelihood -1.408865
INFO: iteration 41, average log likelihood -1.408865
INFO: iteration 42, average log likelihood -1.408865
INFO: iteration 43, average log likelihood -1.408865
INFO: iteration 44, average log likelihood -1.408865
INFO: iteration 45, average log likelihood -1.408864
INFO: iteration 46, average log likelihood -1.408864
INFO: iteration 47, average log likelihood -1.408864
INFO: iteration 48, average log likelihood -1.408864
INFO: iteration 49, average log likelihood -1.408864
INFO: iteration 50, average log likelihood -1.408864
INFO: EM with 100000 data points 50 iterations avll -1.408864
473.9 data points per parameter
2: avll = [-1.41044,-1.41036,-1.41028,-1.41017,-1.41003,-1.40986,-1.40968,-1.40951,-1.40936,-1.40923,-1.40912,-1.40904,-1.40899,-1.40895,-1.40893,-1.40891,-1.4089,-1.40889,-1.40889,-1.40888,-1.40888,-1.40888,-1.40888,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40886,-1.40886,-1.40886,-1.40886,-1.40886,-1.40886,-1.40886,-1.40886,-1.40886]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.408873
INFO: iteration 2, average log likelihood -1.408818
INFO: iteration 3, average log likelihood -1.408766
INFO: iteration 4, average log likelihood -1.408701
INFO: iteration 5, average log likelihood -1.408615
INFO: iteration 6, average log likelihood -1.408507
INFO: iteration 7, average log likelihood -1.408380
INFO: iteration 8, average log likelihood -1.408245
INFO: iteration 9, average log likelihood -1.408115
INFO: iteration 10, average log likelihood -1.407997
INFO: iteration 11, average log likelihood -1.407892
INFO: iteration 12, average log likelihood -1.407803
INFO: iteration 13, average log likelihood -1.407728
INFO: iteration 14, average log likelihood -1.407667
INFO: iteration 15, average log likelihood -1.407618
INFO: iteration 16, average log likelihood -1.407577
INFO: iteration 17, average log likelihood -1.407544
INFO: iteration 18, average log likelihood -1.407515
INFO: iteration 19, average log likelihood -1.407490
INFO: iteration 20, average log likelihood -1.407467
INFO: iteration 21, average log likelihood -1.407446
INFO: iteration 22, average log likelihood -1.407426
INFO: iteration 23, average log likelihood -1.407408
INFO: iteration 24, average log likelihood -1.407391
INFO: iteration 25, average log likelihood -1.407375
INFO: iteration 26, average log likelihood -1.407359
INFO: iteration 27, average log likelihood -1.407345
INFO: iteration 28, average log likelihood -1.407332
INFO: iteration 29, average log likelihood -1.407319
INFO: iteration 30, average log likelihood -1.407307
INFO: iteration 31, average log likelihood -1.407296
INFO: iteration 32, average log likelihood -1.407286
INFO: iteration 33, average log likelihood -1.407276
INFO: iteration 34, average log likelihood -1.407266
INFO: iteration 35, average log likelihood -1.407257
INFO: iteration 36, average log likelihood -1.407249
INFO: iteration 37, average log likelihood -1.407241
INFO: iteration 38, average log likelihood -1.407233
INFO: iteration 39, average log likelihood -1.407225
INFO: iteration 40, average log likelihood -1.407218
INFO: iteration 41, average log likelihood -1.407210
INFO: iteration 42, average log likelihood -1.407203
INFO: iteration 43, average log likelihood -1.407196
INFO: iteration 44, average log likelihood -1.407189
INFO: iteration 45, average log likelihood -1.407182
INFO: iteration 46, average log likelihood -1.407175
INFO: iteration 47, average log likelihood -1.407167
INFO: iteration 48, average log likelihood -1.407160
INFO: iteration 49, average log likelihood -1.407153
INFO: iteration 50, average log likelihood -1.407146
INFO: EM with 100000 data points 50 iterations avll -1.407146
236.4 data points per parameter
3: avll = [-1.40887,-1.40882,-1.40877,-1.4087,-1.40862,-1.40851,-1.40838,-1.40825,-1.40812,-1.408,-1.40789,-1.4078,-1.40773,-1.40767,-1.40762,-1.40758,-1.40754,-1.40751,-1.40749,-1.40747,-1.40745,-1.40743,-1.40741,-1.40739,-1.40737,-1.40736,-1.40735,-1.40733,-1.40732,-1.40731,-1.4073,-1.40729,-1.40728,-1.40727,-1.40726,-1.40725,-1.40724,-1.40723,-1.40723,-1.40722,-1.40721,-1.4072,-1.4072,-1.40719,-1.40718,-1.40717,-1.40717,-1.40716,-1.40715,-1.40715]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.407149
INFO: iteration 2, average log likelihood -1.407087
INFO: iteration 3, average log likelihood -1.407031
INFO: iteration 4, average log likelihood -1.406969
INFO: iteration 5, average log likelihood -1.406892
INFO: iteration 6, average log likelihood -1.406799
INFO: iteration 7, average log likelihood -1.406688
INFO: iteration 8, average log likelihood -1.406563
INFO: iteration 9, average log likelihood -1.406430
INFO: iteration 10, average log likelihood -1.406297
INFO: iteration 11, average log likelihood -1.406170
INFO: iteration 12, average log likelihood -1.406052
INFO: iteration 13, average log likelihood -1.405946
INFO: iteration 14, average log likelihood -1.405851
INFO: iteration 15, average log likelihood -1.405769
INFO: iteration 16, average log likelihood -1.405697
INFO: iteration 17, average log likelihood -1.405635
INFO: iteration 18, average log likelihood -1.405583
INFO: iteration 19, average log likelihood -1.405538
INFO: iteration 20, average log likelihood -1.405499
INFO: iteration 21, average log likelihood -1.405465
INFO: iteration 22, average log likelihood -1.405435
INFO: iteration 23, average log likelihood -1.405409
INFO: iteration 24, average log likelihood -1.405384
INFO: iteration 25, average log likelihood -1.405362
INFO: iteration 26, average log likelihood -1.405341
INFO: iteration 27, average log likelihood -1.405322
INFO: iteration 28, average log likelihood -1.405303
INFO: iteration 29, average log likelihood -1.405286
INFO: iteration 30, average log likelihood -1.405269
INFO: iteration 31, average log likelihood -1.405253
INFO: iteration 32, average log likelihood -1.405237
INFO: iteration 33, average log likelihood -1.405222
INFO: iteration 34, average log likelihood -1.405208
INFO: iteration 35, average log likelihood -1.405194
INFO: iteration 36, average log likelihood -1.405180
INFO: iteration 37, average log likelihood -1.405167
INFO: iteration 38, average log likelihood -1.405154
INFO: iteration 39, average log likelihood -1.405141
INFO: iteration 40, average log likelihood -1.405129
INFO: iteration 41, average log likelihood -1.405117
INFO: iteration 42, average log likelihood -1.405105
INFO: iteration 43, average log likelihood -1.405094
INFO: iteration 44, average log likelihood -1.405082
INFO: iteration 45, average log likelihood -1.405071
INFO: iteration 46, average log likelihood -1.405060
INFO: iteration 47, average log likelihood -1.405050
INFO: iteration 48, average log likelihood -1.405039
INFO: iteration 49, average log likelihood -1.405029
INFO: iteration 50, average log likelihood -1.405019
INFO: EM with 100000 data points 50 iterations avll -1.405019
118.1 data points per parameter
4: avll = [-1.40715,-1.40709,-1.40703,-1.40697,-1.40689,-1.4068,-1.40669,-1.40656,-1.40643,-1.4063,-1.40617,-1.40605,-1.40595,-1.40585,-1.40577,-1.4057,-1.40564,-1.40558,-1.40554,-1.4055,-1.40547,-1.40544,-1.40541,-1.40538,-1.40536,-1.40534,-1.40532,-1.4053,-1.40529,-1.40527,-1.40525,-1.40524,-1.40522,-1.40521,-1.40519,-1.40518,-1.40517,-1.40515,-1.40514,-1.40513,-1.40512,-1.40511,-1.40509,-1.40508,-1.40507,-1.40506,-1.40505,-1.40504,-1.40503,-1.40502]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.405017
INFO: iteration 2, average log likelihood -1.404950
INFO: iteration 3, average log likelihood -1.404884
INFO: iteration 4, average log likelihood -1.404806
INFO: iteration 5, average log likelihood -1.404708
INFO: iteration 6, average log likelihood -1.404585
INFO: iteration 7, average log likelihood -1.404439
INFO: iteration 8, average log likelihood -1.404276
INFO: iteration 9, average log likelihood -1.404105
INFO: iteration 10, average log likelihood -1.403937
INFO: iteration 11, average log likelihood -1.403778
INFO: iteration 12, average log likelihood -1.403632
INFO: iteration 13, average log likelihood -1.403501
INFO: iteration 14, average log likelihood -1.403384
INFO: iteration 15, average log likelihood -1.403282
INFO: iteration 16, average log likelihood -1.403193
INFO: iteration 17, average log likelihood -1.403116
INFO: iteration 18, average log likelihood -1.403048
INFO: iteration 19, average log likelihood -1.402988
INFO: iteration 20, average log likelihood -1.402935
INFO: iteration 21, average log likelihood -1.402888
INFO: iteration 22, average log likelihood -1.402845
INFO: iteration 23, average log likelihood -1.402806
INFO: iteration 24, average log likelihood -1.402769
INFO: iteration 25, average log likelihood -1.402736
INFO: iteration 26, average log likelihood -1.402704
INFO: iteration 27, average log likelihood -1.402675
INFO: iteration 28, average log likelihood -1.402647
INFO: iteration 29, average log likelihood -1.402620
INFO: iteration 30, average log likelihood -1.402596
INFO: iteration 31, average log likelihood -1.402572
INFO: iteration 32, average log likelihood -1.402550
INFO: iteration 33, average log likelihood -1.402529
INFO: iteration 34, average log likelihood -1.402509
INFO: iteration 35, average log likelihood -1.402490
INFO: iteration 36, average log likelihood -1.402472
INFO: iteration 37, average log likelihood -1.402456
INFO: iteration 38, average log likelihood -1.402440
INFO: iteration 39, average log likelihood -1.402425
INFO: iteration 40, average log likelihood -1.402410
INFO: iteration 41, average log likelihood -1.402396
INFO: iteration 42, average log likelihood -1.402383
INFO: iteration 43, average log likelihood -1.402371
INFO: iteration 44, average log likelihood -1.402359
INFO: iteration 45, average log likelihood -1.402348
INFO: iteration 46, average log likelihood -1.402337
INFO: iteration 47, average log likelihood -1.402326
INFO: iteration 48, average log likelihood -1.402316
INFO: iteration 49, average log likelihood -1.402307
INFO: iteration 50, average log likelihood -1.402298
INFO: EM with 100000 data points 50 iterations avll -1.402298
59.0 data points per parameter
5: avll = [-1.40502,-1.40495,-1.40488,-1.40481,-1.40471,-1.40459,-1.40444,-1.40428,-1.40411,-1.40394,-1.40378,-1.40363,-1.4035,-1.40338,-1.40328,-1.40319,-1.40312,-1.40305,-1.40299,-1.40294,-1.40289,-1.40284,-1.40281,-1.40277,-1.40274,-1.4027,-1.40267,-1.40265,-1.40262,-1.4026,-1.40257,-1.40255,-1.40253,-1.40251,-1.40249,-1.40247,-1.40246,-1.40244,-1.40242,-1.40241,-1.4024,-1.40238,-1.40237,-1.40236,-1.40235,-1.40234,-1.40233,-1.40232,-1.40231,-1.4023]
[-1.41578,-1.41579,-1.41571,-1.41563,-1.41552,-1.41538,-1.41521,-1.415,-1.41473,-1.41437,-1.41384,-1.4131,-1.41223,-1.41146,-1.41095,-1.41068,-1.41055,-1.41049,-1.41046,-1.41045,-1.41044,-1.41044,-1.41044,-1.41044,-1.41044,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41043,-1.41044,-1.41036,-1.41028,-1.41017,-1.41003,-1.40986,-1.40968,-1.40951,-1.40936,-1.40923,-1.40912,-1.40904,-1.40899,-1.40895,-1.40893,-1.40891,-1.4089,-1.40889,-1.40889,-1.40888,-1.40888,-1.40888,-1.40888,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40887,-1.40886,-1.40886,-1.40886,-1.40886,-1.40886,-1.40886,-1.40886,-1.40886,-1.40886,-1.40887,-1.40882,-1.40877,-1.4087,-1.40862,-1.40851,-1.40838,-1.40825,-1.40812,-1.408,-1.40789,-1.4078,-1.40773,-1.40767,-1.40762,-1.40758,-1.40754,-1.40751,-1.40749,-1.40747,-1.40745,-1.40743,-1.40741,-1.40739,-1.40737,-1.40736,-1.40735,-1.40733,-1.40732,-1.40731,-1.4073,-1.40729,-1.40728,-1.40727,-1.40726,-1.40725,-1.40724,-1.40723,-1.40723,-1.40722,-1.40721,-1.4072,-1.4072,-1.40719,-1.40718,-1.40717,-1.40717,-1.40716,-1.40715,-1.40715,-1.40715,-1.40709,-1.40703,-1.40697,-1.40689,-1.4068,-1.40669,-1.40656,-1.40643,-1.4063,-1.40617,-1.40605,-1.40595,-1.40585,-1.40577,-1.4057,-1.40564,-1.40558,-1.40554,-1.4055,-1.40547,-1.40544,-1.40541,-1.40538,-1.40536,-1.40534,-1.40532,-1.4053,-1.40529,-1.40527,-1.40525,-1.40524,-1.40522,-1.40521,-1.40519,-1.40518,-1.40517,-1.40515,-1.40514,-1.40513,-1.40512,-1.40511,-1.40509,-1.40508,-1.40507,-1.40506,-1.40505,-1.40504,-1.40503,-1.40502,-1.40502,-1.40495,-1.40488,-1.40481,-1.40471,-1.40459,-1.40444,-1.40428,-1.40411,-1.40394,-1.40378,-1.40363,-1.4035,-1.40338,-1.40328,-1.40319,-1.40312,-1.40305,-1.40299,-1.40294,-1.40289,-1.40284,-1.40281,-1.40277,-1.40274,-1.4027,-1.40267,-1.40265,-1.40262,-1.4026,-1.40257,-1.40255,-1.40253,-1.40251,-1.40249,-1.40247,-1.40246,-1.40244,-1.40242,-1.40241,-1.4024,-1.40238,-1.40237,-1.40236,-1.40235,-1.40234,-1.40233,-1.40232,-1.40231,-1.4023]
32Ã—26 Array{Float64,2}:
  0.434525    -0.130731     0.187176   -0.17593     0.649064     0.149517     0.141088    -0.112637    0.23117    -0.0152046   -0.565413    -0.730563    -0.0470257  -0.628058    -0.186429   -0.274349     0.154583    -0.442046    -0.613857   -0.713379    -0.34439     -0.164336   -0.341497      0.369477    -0.0246573  -0.473666  
  0.184429    -0.00422168  -0.096857   -0.436472    0.344099    -0.0990925   -0.00495624  -0.213273   -0.0508323   0.841844    -0.576546     0.222437    -0.276628   -0.579245    -0.614463    0.378036    -0.500477    -0.174231    -0.466534   -0.0732746    0.0532016   -0.325231   -0.283958      0.22765     -0.362125    0.305661  
  0.457064     0.185499    -0.436853    0.13805     0.869942    -0.350024    -0.0624142    0.28531     0.381774   -0.236828     0.0656242   -0.757526     0.272348   -0.0950556   -0.105033    0.329564     0.473064    -0.431447    -0.945558    0.330713    -0.18472      0.394572   -0.087702     -0.507198    -0.156992    0.319383  
  0.639888    -0.289024     0.225171    0.410258    0.366136     0.151065     0.276298     0.282692    0.16068    -0.00880541   0.428302     0.564496     0.176695   -0.699208    -0.216494   -0.260639     0.181708    -0.900737     0.0356727   0.165014     0.1131      -0.141651   -0.471609     -0.210524    -0.642421   -0.140883  
  0.354829    -0.0359813   -0.123685   -0.661298   -0.27526      0.370373     0.564316     0.0754551  -0.422252   -0.150319    -0.101203     0.272983    -0.601171   -0.492509     0.0478835  -0.402578    -0.207278     0.763269    -0.494053   -0.374966    -0.133778    -0.967793    0.84552      -0.0401597   -0.465447   -0.45876   
  1.04406     -0.00586991   0.06543     0.581884    0.463875    -0.239664     0.281812     0.451901    0.339081    0.364563     0.190742     0.441165    -0.316398    0.0902673    0.678653   -0.140327     0.00223256   0.390386     0.164026   -0.249943    -0.00993884  -0.456942    1.00637      -0.137673    -0.195932    0.717612  
  0.483406     0.829249     0.787617   -0.175225   -0.184956    -0.303036     0.263575    -0.473841    0.0255985  -0.00460481   0.0845965    0.00685118   0.350122    0.561269    -0.279272    0.442403    -0.467191     0.131349    -0.0846155   0.193859    -0.47921      0.135455    0.849601     -0.105095     0.105504   -0.272723  
  0.359048    -0.222124     0.708832    0.0631332   0.982232    -0.804956    -0.0274881   -0.295901    0.69917    -0.0719914   -1.00193      0.00773654  -0.317433   -0.0827171    0.201323    0.482522    -0.517044     0.607204    -0.23253     0.128719    -0.626578     0.400301    0.200524     -0.050089    -0.62019    -0.320683  
  0.269865     0.51592      0.188611   -0.144981   -0.0327819   -0.28774      0.332719    -0.0637233   0.165912   -0.117675    -0.0270518   -0.0917153    0.301775   -0.610078     0.161565    0.57586      0.384497     0.0795463    0.309575   -0.407421    -0.179382    -0.346852   -0.422136     -0.272544     0.0214355  -0.624638  
  0.218832     0.52893     -0.275471    0.163705    0.272919    -0.156177    -0.0639277   -0.546738   -0.634028   -0.214544     0.379383     0.00593022   0.49849    -0.158622     0.397311   -0.0468096    0.593586    -0.139947     0.495877    0.538301    -0.132761    -0.418086   -0.298946     -0.146228    -0.183227    0.32779   
 -0.126507    -0.192813    -0.216112   -0.123047   -0.147076     0.144568     0.109982     0.317417   -0.500606    0.22152     -0.216252     0.460152     0.326614   -0.552063     0.235139   -0.22839      0.356549    -0.397182    -0.115528   -0.485857     0.0209554   -0.375356    0.298059      1.02024     -0.59821     0.418848  
  0.226437     0.459942    -0.371054   -0.636611   -0.311991    -0.328375     0.429517    -0.326505   -0.233828    0.0924091    0.215069     0.260443     0.141057   -0.431082    -0.119918    0.124957    -0.0642625   -0.352007     0.358401   -0.109139    -0.444565     0.610242    0.285449      0.709397    -0.616096    0.119228  
 -0.397427    -0.405165     0.0825854  -0.056105   -0.145182     0.197198    -0.388268     0.0451498   0.200518    0.169666    -0.542639    -0.0995961   -0.538307    0.591447    -0.153221   -0.129976    -0.621136     0.287912    -0.647536   -0.052589     0.10208      0.101734    0.397643      0.00807649   0.263236    0.127261  
 -0.232078     0.243971     0.0362011   0.0143919  -0.143339    -0.0950725   -0.0504475   -0.20745    -0.109242   -0.242617     0.0739593   -0.0452269    0.217477    0.00157511   0.278988    0.181312     0.326953     0.22649      0.57563    -0.0595083   -0.0908754    0.0503452  -0.260331     -0.00457355   0.180415   -0.0774374 
 -0.0634957    0.118475    -0.226286    0.0128727  -0.716196    -0.290154    -0.0387755    0.77178    -0.12711     0.131115     0.0259978   -0.0322336   -0.922862   -0.120013    -0.494107   -0.255227    -0.153153    -0.359989    -0.239363    0.190802    -0.15728      0.274843    0.000500429  -0.431683    -0.0328194  -0.38656   
 -0.402758     0.0900873   -0.388536   -0.343055   -0.386523     0.507995    -0.0280041    0.230271   -0.284872   -0.0793996    0.593304    -0.228403     0.480456   -0.310423    -0.272897    0.0672509   -0.0554327   -0.958071     0.145739    0.0792176    0.377749     0.279837   -0.761963     -0.250145     0.55042    -0.589453  
 -0.0868059    0.107929    -0.138124   -0.272741    0.0818846   -0.0394362   -0.0221083   -0.150116   -0.0652433   0.177274    -0.180754    -0.117777     0.0533051  -0.142031    -0.151254    0.0787255   -0.0339355   -0.0901264   -0.160864   -0.112746    -0.0743399    0.0559429  -0.136451     -0.0294108    0.186458   -0.126106  
  0.00831642  -0.182063     0.0092957   0.192472   -0.00510407   0.156056     0.0487016    0.169849    0.0622939  -0.0248396    0.163927     0.214621     0.0561727  -0.0343998    0.0437761  -0.00446601   0.0979993   -0.245228     0.121042    0.00130535   0.150323    -0.0745376  -0.105694      0.0426577   -0.0518295   0.0914086 
 -0.0465216    0.100115     0.182096    0.15236    -0.200475    -0.0315707    0.115502     0.0413306  -0.276921   -0.21087     -0.354149    -0.124791    -0.547497   -0.0237088    0.379717   -0.466837     0.0877244    0.286727     0.012307   -0.195761    -0.508518    -0.383721    0.393969      0.183333    -0.19485     0.103693  
  0.15038     -0.121908     0.354562   -0.153949    0.181809    -0.225577     0.0152941    0.237674    0.279239    0.111064    -0.437716     0.00918398  -0.165845    0.126797     0.118567    0.31439     -0.0802743    0.502545    -0.206482   -0.258623    -0.0232269   -0.260834    0.432201      0.296593    -0.444999    0.150236  
 -0.361575    -0.10327     -0.653548   -0.230756   -0.501102     0.628785    -0.111199    -0.17617    -0.296128    0.291329     0.0161655   -0.139103     0.172603    0.324475    -0.316713   -0.531577     0.162464     0.29361      0.270131   -0.337738    -0.046883    -0.571891    0.00309815    0.510797     0.684822    0.435677  
 -0.72896     -0.560588    -0.579709    0.539597    0.197501     0.566422    -0.173763     0.654555   -0.208962    0.410101    -0.248998     0.131705    -0.402793   -0.304346     0.238725   -0.26123      0.543312    -0.23093      0.0600331   0.195606     0.656407    -0.124063   -0.420089      0.0675743    0.484053    0.531731  
 -0.157886    -0.485556     0.472352   -0.136744   -0.0634542    0.572795     0.101399     0.477242    0.735347    0.1319      -0.183464     0.0378336   -0.414335   -0.00684337  -0.234541    0.0719872   -0.273679     0.0314644   -0.374018   -0.432591     0.232797    -0.0319881   0.134767     -0.202911     0.379232   -0.0862384 
 -0.187899    -0.091417     0.651182    0.0210301  -0.459818     0.255013     0.680649     0.211729   -0.0964785   0.0906815   -0.128292     0.349146    -0.243843    0.24901      0.52156     0.135468     0.0912638    0.224593     0.503001   -0.651167     0.0361272    0.0583505  -0.0132264     0.671413     0.175749   -0.21656   
  0.276492    -0.0824567    0.0806329  -0.167974   -0.184242    -0.141275    -0.313001    -0.406521    0.316274   -0.500118     0.287769    -0.223706     0.289082    0.520252    -0.341233   -0.0543723   -0.381404     0.478694    -0.288238    0.210972    -0.0838326   -0.107101   -0.033304     -0.375722     0.171414   -0.428185  
 -0.118282    -0.355346     0.13768    -0.21234     0.300607    -0.00310527  -1.13605     -0.41823     0.0465483  -0.209233     0.117872    -0.407829     0.599583    0.0866621   -0.406459    0.175823    -0.325523    -0.00914783  -0.182278    0.456458     0.525245    -0.463648   -0.11067      -0.409895    -0.241197    0.0991469 
 -0.228824     0.140374    -0.564805   -0.265583    0.235146    -0.0242337    0.0636148   -0.252085   -0.470783   -0.0825557   -0.506088    -0.381273    -0.387027    0.238509     0.111409    0.199345    -0.232752     0.39117     -0.318385    0.56475     -0.0185066    0.475323   -0.0234453    -0.0589164    0.429055   -0.0179066 
 -0.531803    -0.397298    -0.536358   -0.202675   -0.184789    -0.49498     -0.622471    -0.100743    0.284143   -0.168234    -0.509119     0.0321895    0.178538   -0.0446168   -0.210303   -0.356682     0.0265431    0.169326    -0.0481005  -0.134019     0.128198     0.567123   -0.302154     -0.0358304   -0.132887    0.0295543 
 -0.405316    -0.0577094   -0.281308    0.298873   -0.401459    -0.0829743   -0.144828     0.279645   -0.0331265  -0.00875675   0.440301     0.682755    -0.198608    0.461947     0.188869    0.274572    -0.270465     0.26485      0.468439    0.800815     0.316693     0.0586584   0.129364     -0.619943     0.138715    0.534571  
 -0.0421522    0.490647     0.287314    0.0808092  -0.0692654   -0.090159    -0.0372312    0.129005   -0.561947    0.180079     0.471112     0.161171    -0.0547763   0.26661      0.408661    0.388414    -0.20606     -0.183885     0.123644    0.24846      0.17163      0.191619    0.311403     -0.210747     0.316927   -0.0821164 
 -0.538646     0.157392     0.209724    0.0635097  -0.175171     0.146257    -0.423313    -0.450416    0.320099   -0.0992271    0.0168141   -0.114229     0.456902    0.0820185    0.404666   -0.0901022    0.19131      0.394975     0.613917    0.118162    -0.29117      0.232184   -0.128288     -0.398724     0.603122   -0.00245106
  0.0185731    0.0585333    0.264992    0.904691    0.467614    -0.097831    -0.181526    -0.0644474   0.624683   -0.0317809   -0.00111889  -0.213327     0.211746    0.568975     0.0428165   0.270813     0.0278933   -0.333374     0.551437    0.373247     0.126897     0.586037   -0.622561     -0.1644       0.319284   -0.0788964 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.402289
INFO: iteration 2, average log likelihood -1.402281
INFO: iteration 3, average log likelihood -1.402273
INFO: iteration 4, average log likelihood -1.402265
INFO: iteration 5, average log likelihood -1.402257
INFO: iteration 6, average log likelihood -1.402250
INFO: iteration 7, average log likelihood -1.402243
INFO: iteration 8, average log likelihood -1.402237
INFO: iteration 9, average log likelihood -1.402231
INFO: iteration 10, average log likelihood -1.402224
INFO: EM with 100000 data points 10 iterations avll -1.402224
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.091638e+05
      1       6.910143e+05      -2.181495e+05 |       32
      2       6.791973e+05      -1.181698e+04 |       32
      3       6.743546e+05      -4.842713e+03 |       32
      4       6.717313e+05      -2.623264e+03 |       32
      5       6.700851e+05      -1.646266e+03 |       32
      6       6.689318e+05      -1.153298e+03 |       32
      7       6.680190e+05      -9.128045e+02 |       32
      8       6.673169e+05      -7.020571e+02 |       32
      9       6.667593e+05      -5.576239e+02 |       32
     10       6.663262e+05      -4.331053e+02 |       32
     11       6.659724e+05      -3.537862e+02 |       32
     12       6.656734e+05      -2.989932e+02 |       32
     13       6.654140e+05      -2.593653e+02 |       32
     14       6.651759e+05      -2.381082e+02 |       32
     15       6.649547e+05      -2.211982e+02 |       32
     16       6.647289e+05      -2.258382e+02 |       32
     17       6.645264e+05      -2.025235e+02 |       32
     18       6.643541e+05      -1.722468e+02 |       32
     19       6.642207e+05      -1.334001e+02 |       32
     20       6.641068e+05      -1.139373e+02 |       32
     21       6.639880e+05      -1.188057e+02 |       32
     22       6.638744e+05      -1.135318e+02 |       32
     23       6.637693e+05      -1.051682e+02 |       32
     24       6.636733e+05      -9.594074e+01 |       32
     25       6.635781e+05      -9.520000e+01 |       32
     26       6.634807e+05      -9.746913e+01 |       32
     27       6.633962e+05      -8.448768e+01 |       32
     28       6.633128e+05      -8.334065e+01 |       32
     29       6.632328e+05      -7.999010e+01 |       32
     30       6.631594e+05      -7.348667e+01 |       32
     31       6.630886e+05      -7.073038e+01 |       32
     32       6.630168e+05      -7.183947e+01 |       32
     33       6.629456e+05      -7.121686e+01 |       32
     34       6.628808e+05      -6.477270e+01 |       32
     35       6.628248e+05      -5.603137e+01 |       32
     36       6.627706e+05      -5.415534e+01 |       32
     37       6.627215e+05      -4.905951e+01 |       32
     38       6.626804e+05      -4.118831e+01 |       32
     39       6.626414e+05      -3.894771e+01 |       32
     40       6.626031e+05      -3.835337e+01 |       32
     41       6.625532e+05      -4.984879e+01 |       32
     42       6.624989e+05      -5.434075e+01 |       32
     43       6.624445e+05      -5.432051e+01 |       32
     44       6.623909e+05      -5.365446e+01 |       32
     45       6.623360e+05      -5.492498e+01 |       32
     46       6.622897e+05      -4.622297e+01 |       32
     47       6.622499e+05      -3.985501e+01 |       32
     48       6.622067e+05      -4.323938e+01 |       32
     49       6.621595e+05      -4.718012e+01 |       32
     50       6.621118e+05      -4.765937e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 662111.8122868638)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.413756
INFO: iteration 2, average log likelihood -1.408913
INFO: iteration 3, average log likelihood -1.407656
INFO: iteration 4, average log likelihood -1.406765
INFO: iteration 5, average log likelihood -1.405841
INFO: iteration 6, average log likelihood -1.404938
INFO: iteration 7, average log likelihood -1.404238
INFO: iteration 8, average log likelihood -1.403799
INFO: iteration 9, average log likelihood -1.403540
INFO: iteration 10, average log likelihood -1.403377
INFO: iteration 11, average log likelihood -1.403261
INFO: iteration 12, average log likelihood -1.403170
INFO: iteration 13, average log likelihood -1.403096
INFO: iteration 14, average log likelihood -1.403032
INFO: iteration 15, average log likelihood -1.402976
INFO: iteration 16, average log likelihood -1.402926
INFO: iteration 17, average log likelihood -1.402880
INFO: iteration 18, average log likelihood -1.402839
INFO: iteration 19, average log likelihood -1.402800
INFO: iteration 20, average log likelihood -1.402765
INFO: iteration 21, average log likelihood -1.402731
INFO: iteration 22, average log likelihood -1.402700
INFO: iteration 23, average log likelihood -1.402671
INFO: iteration 24, average log likelihood -1.402644
INFO: iteration 25, average log likelihood -1.402618
INFO: iteration 26, average log likelihood -1.402594
INFO: iteration 27, average log likelihood -1.402571
INFO: iteration 28, average log likelihood -1.402550
INFO: iteration 29, average log likelihood -1.402529
INFO: iteration 30, average log likelihood -1.402510
INFO: iteration 31, average log likelihood -1.402492
INFO: iteration 32, average log likelihood -1.402475
INFO: iteration 33, average log likelihood -1.402458
INFO: iteration 34, average log likelihood -1.402443
INFO: iteration 35, average log likelihood -1.402428
INFO: iteration 36, average log likelihood -1.402413
INFO: iteration 37, average log likelihood -1.402399
INFO: iteration 38, average log likelihood -1.402386
INFO: iteration 39, average log likelihood -1.402373
INFO: iteration 40, average log likelihood -1.402361
INFO: iteration 41, average log likelihood -1.402349
INFO: iteration 42, average log likelihood -1.402338
INFO: iteration 43, average log likelihood -1.402326
INFO: iteration 44, average log likelihood -1.402316
INFO: iteration 45, average log likelihood -1.402305
INFO: iteration 46, average log likelihood -1.402295
INFO: iteration 47, average log likelihood -1.402285
INFO: iteration 48, average log likelihood -1.402275
INFO: iteration 49, average log likelihood -1.402265
INFO: iteration 50, average log likelihood -1.402255
INFO: EM with 100000 data points 50 iterations avll -1.402255
59.0 data points per parameter
32Ã—26 Array{Float64,2}:
 -0.101544     0.350539    0.32308     -0.08738     0.294546    0.0462171   -0.16429    -0.193098     0.013027    0.447268     0.299911     0.334511     0.716008    0.0844817   0.320814    0.569027     0.20086     -0.314392    0.211778     -0.23759     0.313442    -0.0399484   0.0458679   0.076982    0.022867     0.336522  
 -0.478447    -0.455593   -0.305553     0.163566   -0.546857   -0.0870073   -0.905963    0.176808     0.126435    0.0529954    0.185402     0.320743     0.333844    0.0182804  -0.18653    -0.45009     -0.232451    -0.33496     0.377704      0.213472    0.239793     0.393354    0.0837353  -0.16482    -0.0918158    0.150149  
  0.0929964    0.10324    -0.589317    -0.833282   -0.436761    0.00494867   0.264346    0.040785    -0.29951     0.18838      0.188963     0.446221     0.495101   -0.720139   -0.090971    0.01058      0.155549    -0.323958    0.119186     -0.435018   -0.304249    -0.0138756   0.456665    0.89518    -0.480829     0.229053  
 -0.073796     0.0135989  -0.0884358   -0.0762686  -0.0207496  -0.0370687   -0.0313874   0.00668605  -0.0410493   0.0742508   -0.00187496   0.0374951    0.0562087  -0.0753576  -0.0675088   0.0710706    0.0023907   -0.115595    0.000799435  -0.0129906   0.00199301   0.0446984  -0.113235   -0.068316    0.0837722   -0.0589593 
  0.196688    -0.293788    0.411882    -0.121943    0.887807   -0.517005    -0.0673817  -0.127519     0.670831    0.145292    -1.10806     -0.139698    -0.427145   -0.140828    0.175964    0.462555    -0.487719     0.63456    -0.326972     -0.0981993  -0.436932     0.302084    0.1828      0.140126   -0.408803    -0.0777432 
  0.239217    -0.167469   -0.185023    -0.107178    0.0773397  -0.238536    -0.279616   -0.00638049  -0.322882    0.250068    -0.25437      0.192096    -0.454427   -0.19109     0.0732709  -0.264356    -0.439392     0.199185   -0.29167      -0.150985    0.0948406   -0.400322    0.533593    0.251332   -0.632664     0.523614  
 -0.245377    -0.524886    0.73287     -0.0958279  -0.580509    0.140643     0.318329    0.384533     0.0439137   0.0252574   -0.247239     0.13752     -0.16191     0.612455    0.215305    0.0879493   -0.221782     0.214727    0.118808     -0.451364    0.375404     0.244627    0.148472    0.724958   -0.00739331  -0.107813  
 -0.108183    -0.142005   -0.332538    -0.325469   -0.478977   -0.092568     0.203906    0.441983    -0.129022    0.0595362   -0.344625    -0.0305779   -0.971283   -0.408247   -0.563397   -0.212532    -0.257509     0.0248616  -0.340723      0.0417176  -0.252978     0.238843    0.0062537  -0.158827   -0.0322759   -0.258335  
 -0.790265    -0.610753   -0.657467     0.305254    0.070807    0.553044    -0.245406    0.571419    -0.201864    0.311268    -0.346784     0.1719      -0.379486   -0.269171    0.0875534  -0.279413     0.57491     -0.0672092  -0.125949      0.0663699   0.656449    -0.161677   -0.385741    0.133086    0.400659     0.508468  
 -0.119363    -0.0717604   0.452061    -0.330194   -0.124583    0.483544    -0.0393671   0.431453     0.578498    0.329707    -0.273642     0.0856537   -0.41172     0.125454   -0.404393    0.0300157   -0.410537     0.169147   -0.901873     -0.565562    0.00676637  -0.154266    0.460806   -0.518806    0.409754    -0.2151    
  0.635384     0.597385    0.839224    -0.154214   -0.0729493  -0.553368     0.374729   -0.619092     0.0114926  -0.0266965   -0.0225203    0.0126781    0.371942    0.389107   -0.315025    0.375459    -0.50725      0.03781     0.0516675     0.322316   -0.926329     0.0734058   0.529481   -0.0127701  -0.118454    -0.456849  
 -0.0598339    0.10507     0.314285    -0.0185267   0.0172689   0.144635    -0.713813   -0.461483    -0.0418007  -0.479873     0.412198    -0.419291     0.668821   -0.0529499   0.28499     0.195813     0.156666     0.0767786   0.313749      0.573034    0.14317     -0.772841   -0.134354   -0.757023    0.217053    -0.217085  
 -0.338681     0.175918   -0.340584    -0.271282   -0.610522    0.437793     0.0430503   0.374138    -0.315279   -0.0206174    0.641777    -0.127376     0.383352   -0.26603    -0.402478    0.135646     0.00900319  -1.03572     0.108193      0.154487    0.446211     0.307998   -0.703568   -0.324239    0.474101    -0.615619  
 -0.149073     0.473565   -0.0169727    0.494453   -0.501266    0.0606163    0.264193    0.381139    -0.632126    0.194867     0.420071     0.503788    -0.457051    0.153701    0.618804    0.343361    -0.0296551   -0.160256    0.595011      0.305861   -0.0141761    0.0948927   0.40627    -0.101184    0.310357     0.179748  
  0.00484295   0.0458142   0.132109     0.646949    0.415319   -0.124967    -0.027901    0.224973     0.507307   -0.157548    -0.0233814   -0.155973     0.0060757   0.317941    0.110619    0.32545     -0.0326249   -0.361934    0.273635      0.439727    0.258376     0.624547   -0.594558   -0.26181     0.207417    -0.182844  
  0.555823    -0.0773279  -0.0205184    0.157477    0.715012    0.0126767    0.0308497   0.187828     0.179574    0.0395227    0.250227     0.0918607    0.0686708  -0.640479   -0.255294   -0.0660305    0.0231756   -0.917771   -0.364236      0.149736   -0.0216234   -0.0702846  -0.336056   -0.266268   -0.587898    -0.10471   
 -0.49263      0.646412   -0.0539165   -0.120948   -0.425852   -0.381921    -0.431939   -0.62476     -0.227447   -0.269644    -0.253312    -0.410166    -0.187106   -0.121034    0.14673    -0.0324677   -0.299815     0.161368    0.3732       -0.870937   -0.563513    -0.262801   -0.343595    0.435528    0.641516    -0.490177  
 -0.0753801    0.35917    -0.456564    -0.322546    0.236119    0.0302794   -0.0450274  -0.341297    -0.600792   -0.102037    -0.128035    -0.457096    -0.17665     0.357204    0.0804097   0.193575    -0.343068     0.195177   -0.608066      0.622492    0.107254     0.357492    0.269332   -0.14013     0.388922    -0.0152811 
  0.35218     -0.235375    0.544912     0.298136   -0.16583     0.0989203    0.534136    0.215164     0.270255    0.0404321    0.219951     0.533149     0.110974   -0.650069   -0.0840608   0.0869883    0.542378    -0.0792893   0.695229     -0.359627   -0.0111921   -0.489934   -0.748586   -0.198095   -0.0673418   -0.357882  
 -0.537936     0.305731   -0.489416    -0.257463   -0.395195    0.466329     0.174853   -0.173169    -0.266095   -0.0015503    0.00933045  -0.356641     0.232435    0.513111   -0.205025   -0.404916     0.873986     0.591995    0.771125     -0.160296   -0.23738     -0.239837   -0.47674     0.112589    0.754329     0.245842  
  1.05799     -0.0221521   0.0937834    0.605545    0.429624   -0.226631     0.363325    0.561989     0.485056    0.283545     0.195825     0.326933    -0.313777    0.208191    0.464905   -0.143869     0.138423     0.394091    0.0194089     0.012974    0.0102216   -0.589977    1.08699    -0.273269   -0.355232     0.529053  
  0.196645    -0.161765    0.226493     0.043039    0.253665    0.1797       0.31105     0.121006     0.0441703   0.279073    -0.515458    -0.141997    -0.221469   -0.480521   -0.140378   -0.182001     0.234265    -0.565294   -0.329407     -0.524998   -0.278913    -0.261896   -0.178297    0.62425    -0.317629    -0.112423  
 -0.181108     0.266188   -0.00846589  -0.148056   -0.31028    -0.453383    -0.312256   -0.10421      0.22932    -0.249657     0.338805     0.25242      0.201507    0.444014    0.241003    0.00225066  -0.191194     0.603083    0.532706      0.362028    0.0126527    0.635622    0.068122   -0.882963    0.40267     -0.056935  
 -0.278       -0.102661   -0.0109142    0.0995894  -0.12814    -0.0236267   -0.479404   -0.174548     0.127295   -0.087786    -0.159575    -0.151816    -0.0333502   0.502556   -0.0812356   0.0962169   -0.237729     0.261529   -0.0524327     0.176092    0.078812     0.141646    0.0364977  -0.0910888   0.24031      0.0874587 
 -0.0165536    0.241109    0.28891     -0.216436   -0.273845    0.224489     0.606539   -0.0490426   -0.280637    0.0504758   -0.238347     0.237305    -0.22733    -0.116092    0.592419    0.00192367   0.127071     0.558533    0.231635     -0.694716   -0.174344    -0.329706    0.400697    0.434561   -0.0450506   -0.149967  
 -0.483992     0.0535956   0.295314     0.016773   -0.217843   -0.494065    -0.389099   -0.149218    -0.215218    0.380822     0.159698     0.212345    -0.522016    0.431234   -0.585546    0.623383    -0.702261     0.275172   -0.0974692     1.13642     0.515042    -0.595105   -0.269191   -0.943897   -0.305594     0.3346    
 -0.20044     -0.399107    0.0665324    0.244484   -0.0669416   0.465546    -0.30816    -0.305402     0.345855    0.00137813  -0.0378053    0.0660795   -0.110072    0.514444   -0.0352086  -0.119831    -0.234332     0.412129    0.083058      0.205655    0.0425593   -0.111571    0.152053    0.126744    0.347682     0.236932  
  0.0880601    0.152788    0.312473    -0.0398781  -0.0674212  -0.0444193    0.29072     0.236889     0.0914612  -0.198113    -0.229132     0.00419561  -0.29555    -0.112096    0.335516    0.0849163    0.191809     0.352244   -0.0541733    -0.316531   -0.210229    -0.347864    0.352827    0.115589   -0.25858     -0.0988165 
  0.385       -0.138338   -0.228541    -0.0235241   0.525108    0.64652      0.27067    -0.0505406   -0.0934797  -0.0547682    0.343873    -0.389668     0.170865   -0.23276     0.448515   -0.458157    -0.175717    -0.452688   -0.120637     -0.419287    0.0246012   -0.250853   -0.168178   -0.022665    0.758126    -0.00535473
  0.175042    -0.276392   -0.119099    -0.522204    0.4764     -0.18288     -0.367047   -0.300611     0.565709   -0.281189    -0.197039    -0.449819     0.625094   -0.0252481  -0.769137   -0.0290665   -0.249567     0.125617   -0.616923     -0.0575615   0.218272    -0.126644   -0.439241   -0.120706   -0.233181    -0.228202  
  0.192541     0.536897   -0.327621    -0.0728138   0.403313   -0.513595     0.0396363  -0.0277353    0.239532   -0.0995609   -0.435899    -0.513244     0.447358   -0.372611    0.24999     0.513372     1.29358     -0.0202889  -0.32718      -0.053396   -0.485888     0.557731   -0.12921    -0.520603    0.180065     0.160699  
 -0.0204617    0.227928   -0.367185     0.15726     0.279413   -0.388639     0.0986826  -0.477577    -0.835552   -0.22449     -0.0153242    0.0215552    0.369045   -0.251127    0.255854   -0.108364     0.390247    -0.0774923   0.502202      0.595317   -0.164953     0.0695772  -0.558277    0.439622   -0.549728     0.352429  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.402246
INFO: iteration 2, average log likelihood -1.402237
INFO: iteration 3, average log likelihood -1.402227
INFO: iteration 4, average log likelihood -1.402218
INFO: iteration 5, average log likelihood -1.402209
INFO: iteration 6, average log likelihood -1.402200
INFO: iteration 7, average log likelihood -1.402191
INFO: iteration 8, average log likelihood -1.402183
INFO: iteration 9, average log likelihood -1.402174
INFO: iteration 10, average log likelihood -1.402165
INFO: EM with 100000 data points 10 iterations avll -1.402165
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
