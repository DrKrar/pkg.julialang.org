>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.7.0
INFO: Installing JLD v0.6.6
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.1
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.3.0
INFO: Installing ScikitLearnBase v0.2.0
INFO: Installing StaticArrays v0.1.0
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
WARNING: takebuf_string(b) is deprecated, use String(take!(b)) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in takebuf_string(::Base.AbstractIOBuffer{Array{UInt8,1}}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/BinDeps/src/dependencies.jl:887 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::SubString{String}) at ./sysimg.jl:14
 in evalfile(::SubString{String}, ::Array{String,1}) at ./loading.jl:572 (repeats 2 times)
 in cd(::##2#4, ::String) at ./file.jl:69
 in (::##1#3)(::IOStream) at ./none:13
 in open(::##1#3, ::String, ::String) at ./iostream.jl:152
 in eval(::Module, ::Any) at ./boot.jl:236
 in process_options(::Base.JLOptions) at ./client.jl:248
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/Rmath/deps/build.jl, in expression starting on line 39
INFO: Building HDF5
INFO: Package database updated
INFO: METADATA is out-of-date â€” you may not have the latest version of GaussianMixtures
INFO: Use `Pkg.update()` to get the latest versions of your packages

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1268
Commit 01ef3c3 (2016-11-17 17:28 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-101-generic #148-Ubuntu SMP Thu Oct 20 22:08:32 UTC 2016 x86_64 x86_64
Memory: 2.939281463623047 GB (658.546875 MB free)
Uptime: 27554.0 sec
Load Avg:  0.94189453125  0.93798828125  0.9853515625
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3499 MHz    1730600 s       7052 s     149336 s     544311 s         74 s
#2  3499 MHz     715750 s       1901 s      90578 s    1846559 s          3 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.4
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.7.0
 - JLD                           0.6.6
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.1
 - Rmath                         0.1.4
 - SHA                           0.3.0
 - ScikitLearnBase               0.2.0
 - StaticArrays                  0.1.0
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: takebuf_string(b) is deprecated, use String(take!(b)) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in takebuf_string(::Base.AbstractIOBuffer{Array{UInt8,1}}) at ./deprecated.jl:50
 in #_write#17(::Array{Any,1}, ::Function, ::JLD.JldFile, ::String, ::Array{Any,1}, ::JLD.JldWriteSession) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:587
 in #write#14(::Array{Any,1}, ::Function, ::JLD.JldFile, ::String, ::Array{Any,1}, ::JLD.JldWriteSession) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:529
 in #jldopen#9(::Bool, ::Bool, ::Bool, ::Function, ::String, ::Bool, ::Bool, ::Bool, ::Bool, ::Bool) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:198
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::String, ::Bool, ::Bool, ::Bool, ::Bool, ::Bool) at ./<missing>:0
 in #jldopen#10(::Bool, ::Bool, ::Bool, ::Function, ::String, ::String) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:253
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::String, ::String) at ./<missing>:0
 in #jldopen#11(::Array{Any,1}, ::Function, ::JLD.##34#35{String,Array{Float64,2},Tuple{}}, ::String, ::Vararg{String,N}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:263
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::Function, ::String, ::String) at ./<missing>:0
 in #save#33(::Bool, ::Bool, ::Function, ::FileIO.File{FileIO.DataFormat{:JLD}}, ::String, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:1217
 in save(::FileIO.File{FileIO.DataFormat{:JLD}}, ::String, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:1214
 in #save#14(::Array{Any,1}, ::Function, ::String, ::String, ::Vararg{Any,N}) at /home/vagrant/.julia/v0.6/FileIO/src/loadsave.jl:54
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:8 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##776#778{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##776#778{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##776#778{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##776#778{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##776#778{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##776#778{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-2.677951411338442e6,[67641.2,32358.8],
[23976.5 7257.91 12242.1; -24179.5 -7140.99 -12120.4],

Array{Float64,2}[
[62108.6 -7250.39 -3598.02; -7250.39 48117.5 12324.6; -3598.02 12324.6 53386.1],

[37931.3 7319.36 3495.24; 7319.36 51656.0 -12305.3; 3495.24 -12305.3 46962.8]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.910389e+03
      1       1.242211e+03      -6.681778e+02 |        8
      2       1.006344e+03      -2.358669e+02 |        7
      3       9.161544e+02      -9.018965e+01 |        3
      4       8.978373e+02      -1.831712e+01 |        2
      5       8.959214e+02      -1.915892e+00 |        0
      6       8.959214e+02       0.000000e+00 |        0
K-means converged with 6 iterations (objv = 895.9213684048873)
INFO: K-means with 272 data points using 6 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.082804
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.902491
INFO: iteration 2, lowerbound -3.785918
INFO: iteration 3, lowerbound -3.637376
INFO: iteration 4, lowerbound -3.426767
INFO: iteration 5, lowerbound -3.168480
INFO: iteration 6, lowerbound -2.902778
INFO: iteration 7, lowerbound -2.679582
INFO: dropping number of Gaussions to 6
INFO: iteration 8, lowerbound -2.517261
INFO: iteration 9, lowerbound -2.416068
INFO: dropping number of Gaussions to 4
INFO: iteration 10, lowerbound -2.354525
INFO: iteration 11, lowerbound -2.319480
INFO: dropping number of Gaussions to 3
INFO: iteration 12, lowerbound -2.311390
INFO: dropping number of Gaussions to 2
INFO: iteration 13, lowerbound -2.302928
INFO: iteration 14, lowerbound -2.299263
INFO: iteration 15, lowerbound -2.299258
INFO: iteration 16, lowerbound -2.299255
INFO: iteration 17, lowerbound -2.299254
INFO: iteration 18, lowerbound -2.299253
INFO: iteration 19, lowerbound -2.299253
INFO: iteration 20, lowerbound -2.299253
INFO: iteration 21, lowerbound -2.299253
INFO: iteration 22, lowerbound -2.299253
INFO: iteration 23, lowerbound -2.299253
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: 45 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Fri 18 Nov 2016 01:08:56 PM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Fri 18 Nov 2016 01:08:58 PM UTC: K-means with 272 data points using 6 iterations
11.3 data points per parameter
,Fri 18 Nov 2016 01:08:59 PM UTC: EM with 272 data points 0 iterations avll -2.082804
5.8 data points per parameter
,Fri 18 Nov 2016 01:09:00 PM UTC: GMM converted to Variational GMM
,Fri 18 Nov 2016 01:09:02 PM UTC: iteration 1, lowerbound -3.902491
,Fri 18 Nov 2016 01:09:02 PM UTC: iteration 2, lowerbound -3.785918
,Fri 18 Nov 2016 01:09:02 PM UTC: iteration 3, lowerbound -3.637376
,Fri 18 Nov 2016 01:09:02 PM UTC: iteration 4, lowerbound -3.426767
,Fri 18 Nov 2016 01:09:02 PM UTC: iteration 5, lowerbound -3.168480
,Fri 18 Nov 2016 01:09:02 PM UTC: iteration 6, lowerbound -2.902778
,Fri 18 Nov 2016 01:09:02 PM UTC: iteration 7, lowerbound -2.679582
,Fri 18 Nov 2016 01:09:03 PM UTC: dropping number of Gaussions to 6
,Fri 18 Nov 2016 01:09:03 PM UTC: iteration 8, lowerbound -2.517261
,Fri 18 Nov 2016 01:09:03 PM UTC: iteration 9, lowerbound -2.416068
,Fri 18 Nov 2016 01:09:03 PM UTC: dropping number of Gaussions to 4
,Fri 18 Nov 2016 01:09:03 PM UTC: iteration 10, lowerbound -2.354525
,Fri 18 Nov 2016 01:09:03 PM UTC: iteration 11, lowerbound -2.319480
,Fri 18 Nov 2016 01:09:03 PM UTC: dropping number of Gaussions to 3
,Fri 18 Nov 2016 01:09:03 PM UTC: iteration 12, lowerbound -2.311390
,Fri 18 Nov 2016 01:09:03 PM UTC: dropping number of Gaussions to 2
,Fri 18 Nov 2016 01:09:03 PM UTC: iteration 13, lowerbound -2.302928
,Fri 18 Nov 2016 01:09:03 PM UTC: iteration 14, lowerbound -2.299263
,Fri 18 Nov 2016 01:09:03 PM UTC: iteration 15, lowerbound -2.299258
,Fri 18 Nov 2016 01:09:03 PM UTC: iteration 16, lowerbound -2.299255
,Fri 18 Nov 2016 01:09:03 PM UTC: iteration 17, lowerbound -2.299254
,Fri 18 Nov 2016 01:09:03 PM UTC: iteration 18, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:03 PM UTC: iteration 19, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:03 PM UTC: iteration 20, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:04 PM UTC: iteration 21, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:04 PM UTC: iteration 22, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:04 PM UTC: iteration 23, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:04 PM UTC: iteration 24, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:04 PM UTC: iteration 25, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:04 PM UTC: iteration 26, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:04 PM UTC: iteration 27, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:04 PM UTC: iteration 28, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:04 PM UTC: iteration 29, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:04 PM UTC: iteration 30, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:04 PM UTC: iteration 31, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:04 PM UTC: iteration 32, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:04 PM UTC: iteration 33, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:04 PM UTC: iteration 34, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:05 PM UTC: iteration 35, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:05 PM UTC: iteration 36, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:05 PM UTC: iteration 37, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:05 PM UTC: iteration 38, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:05 PM UTC: iteration 39, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:05 PM UTC: iteration 40, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:05 PM UTC: iteration 41, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:05 PM UTC: iteration 42, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:05 PM UTC: iteration 43, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:05 PM UTC: iteration 44, lowerbound -2.299253
,Fri 18 Nov 2016 01:09:05 PM UTC: 45 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
Î± = [95.9549,178.045]
Î² = [95.9549,178.045]
m = [2.00023 53.852; 4.2503 79.2869]
Î½ = [97.9549,180.045]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.375876 -0.00895312; 0.0 0.0127487],

[0.184042 -0.00764405; 0.0 0.00858171]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -0.9911494818752646
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.991149481875265
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.991149481875265
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 99999.99999999997
avll from stats: -1.0099570712468628
avll from llpg:  -1.0099570712468626
avll direct:     -1.0099570712468628
sum posterior: 100000.0
32Ã—26 Array{Float64,2}:
 -0.0490438    0.0553457   -0.0528444   -0.0766458   -0.115827     -0.2697       0.0207669    -0.113129    -0.111572      0.0750932   -0.0716158  -0.0816009   -0.13586      0.00192119  -0.140393   -0.0174898   -0.283428     0.0937911    -0.0390291    0.105041    -0.0229922   0.151266     0.0652067    0.0206535    0.117927    -0.217982  
  0.0140295   -0.065521     0.126987    -0.128701    -0.0369072    -0.194712     0.191213     -0.0497815   -0.0607316    -0.0284027    0.0796117  -0.0106134   -0.0321256    0.00957147   0.0256343   0.114453    -0.106375     0.0122177     0.00430934   0.154114     0.0964127   0.0603254   -0.00158894  -0.103848    -0.1013      -0.106685  
  0.0312316   -0.0674881    0.0929402   -0.0118148    0.183771      0.0606921    0.0705817     0.116537     0.126001      0.083495     0.0397993   0.0384111    0.0377749   -0.0208186   -0.0584858   0.0270132   -0.17062      0.0276923    -0.125201     0.110756    -0.227178    0.115756     0.0627934   -0.00650308   0.0666219    0.134051  
  0.0354293    0.0156847    0.172944    -0.0190389    0.0709069     0.144235     0.17849      -0.164653     0.156539      0.0329704   -0.074779   -0.22371     -0.0348637   -0.0764243    0.0832999   0.014421    -0.0108407   -0.00179095    0.0875404    0.00533603  -0.0400704   0.116904    -0.0859862    0.0709734   -0.0808495   -0.0117524 
  0.0185473   -0.1159       0.116988    -0.0447064   -0.0151447     0.185924    -0.0879564    -0.01074      0.00166924   -0.163441     0.0704198   0.00895877   0.172144    -0.0681295   -0.0935688   0.111709     0.0903864   -0.0474472    -0.0458807    0.121661    -0.0516313  -0.0949668    0.165792    -0.0873601   -0.24516      0.107713  
 -0.0656796   -0.0796234    0.129363    -0.0868992   -0.12362       0.0761135   -0.0166364     0.0739333   -0.0332918     0.136864    -0.0617342  -0.227518     0.0206268   -0.114554     0.0194117   0.0319137   -0.168489     0.0426117    -0.0892462   -0.0152874   -0.0574278   0.100772     0.0345781    0.151152    -0.175225    -0.164164  
  0.0764535   -0.0852058   -0.0603757    0.0903446    0.054887      0.0222045   -0.0151684     0.0237173   -0.108074      0.073833    -0.0109993  -0.0380268    0.0282588    0.00682659  -0.161817   -0.00353527   0.0281385   -0.052466     -0.0480738    0.0481207    0.113297    0.0387378    0.0400257    0.0828341    0.00481599  -0.0246649 
  0.109406     0.0333156   -0.210455    -0.201309     0.0381941     0.0833005   -0.142815     -0.0976389   -0.0211924     0.0430465    0.0696959   0.0833725    0.00690621   0.00599335   0.0222515  -0.0991214   -0.0771891    0.110011      0.0255084    0.0888656    0.179717    0.0187152    0.121965     0.116171     0.107344     0.031921  
 -0.0896965    0.0514076   -0.00338195  -0.0200907   -0.0101748     0.0104646   -0.0911315     0.0826265    0.0102192     0.107834     0.267033   -0.0720086   -0.0997741   -0.149141     0.195956   -0.119248    -0.0168831   -0.0166228    -0.174514    -0.0805838    0.0200693  -0.0972944   -0.0343683    0.0319551   -0.18931     -0.0333692 
 -0.0858567   -0.119338     0.0358819   -0.015539    -0.0965965     0.0196708   -0.000288254  -0.140039    -0.0750006    -0.0859477   -0.0314474  -0.0448301   -0.0109659   -0.0798256    0.0182355  -0.071839    -0.0387962   -0.0870355    -0.0715083    0.0301471    0.133118   -0.130173     0.149083    -0.16575      0.131749     0.0169618 
 -0.0525644   -0.233655    -0.0183525    0.102427     0.0158941     0.00723109   0.0141131     0.154708     0.117697     -0.0826004   -0.161828   -0.0699088    0.215966     0.0855177    0.0433746  -0.00133549  -0.0231327   -0.171045     -0.0401682   -0.0175856    0.0795315  -0.182506    -0.0114365    0.18417      0.188846     0.0537538 
  0.0572999   -0.00328727  -0.0554299   -0.0636396   -0.0976309     0.0900469   -0.144363      0.105087     0.0195416    -0.0289276   -0.0867307  -0.168999    -0.0873564    0.0217663   -0.0800243   0.00907335  -0.0255316   -0.271622      0.108218     0.192878     0.0165159   0.0444669    0.0484794   -0.0618716    0.0351135   -0.21368   
 -0.0374545    0.0119038   -0.0566       0.0363396   -0.00316349    0.0451218    0.0351733    -0.0433775    0.0805058    -0.0513159    0.084651    0.0276251    0.0737439    0.101242     0.108714   -0.0499561    0.105348     0.0221189     0.111536     0.0264236    0.010458   -0.0663425   -0.0253438    0.0566175    0.00256844   0.0219145 
  0.0227554    0.0123569    0.0370691   -0.0805673    0.0664338     0.04468      0.0862117    -0.13443     -0.000218218   0.113251     0.0264647  -0.0433949    0.0932511    0.105491    -0.0915889   0.0571724    0.0500349    0.0657514    -0.0570957    0.116365     0.127037   -0.067486    -0.0835193   -0.137207    -0.0942037    0.0883348 
 -0.0902982   -0.183298    -0.173693     0.0170445   -0.0796107     0.0371219    0.105961      0.159926    -0.0496091     0.151124    -0.0516706   0.0295642    0.0757458    0.0344144   -0.113079   -0.0412861   -0.0215768   -0.000394675  -0.0445318   -0.0234518    0.111719    0.0337862    0.0816189   -0.0496376   -0.0622046    0.0295908 
  0.0806303   -0.0263224   -0.00685585   0.180069    -0.0322366    -0.0435342    0.122495      0.0248351   -0.0761505    -0.180115     0.160824   -0.186869     0.154751     0.0862852   -0.0423052   0.172964    -0.0200115   -0.0256259     0.0715976   -0.0464632   -0.237159   -0.098143     0.0436191   -0.047771    -0.0256834    0.090403  
 -0.0854907    0.0254143    0.105877     0.275205     0.069792      0.0241483   -0.086219     -0.131318     0.212954      0.127534    -0.0521084   0.0844988   -0.00974831  -0.00283814   0.0760877  -0.0420063   -0.0174573   -0.00473072   -0.0778986   -0.0736772   -0.071611    0.0950325    0.105865     0.224433    -0.0329367   -0.170356  
  0.0407447   -0.132513     0.123905    -0.0666537   -0.089692     -0.0750693    0.0590628    -0.0204945   -0.0354944    -0.115703    -0.0713272   0.106569    -0.166111     0.00517438  -0.0355796  -0.0962471    0.187002    -0.149214     -0.19467      0.0169105    0.0181365  -0.00510561  -0.149185    -0.00612672  -0.116769     0.0688077 
  0.113195     0.041549     0.0111503   -0.00102908   0.00844183    0.131769    -0.037142      0.0609928    0.0261655     0.00412101  -0.0260065  -0.0719152    0.108315     0.0390604    0.216595   -0.0175329   -0.0719082    0.189278     -0.0969552   -0.0577406    0.0396547  -0.215891     0.0567539   -0.0491217    0.0603665    0.0461202 
 -0.161434     0.0036561    0.206113     0.0671923    0.102551     -0.0421961   -0.0529284    -0.0526399   -0.0569544    -0.209665    -0.0312371   0.180439    -0.0744233   -0.0307788   -0.121373   -0.0801587   -0.207355     0.12781       0.0221206    0.0122693   -0.0154081  -0.0490571    0.0418623    0.0242867    0.0500681   -0.0178353 
  0.00901804  -0.0952459    0.106295     0.132588     0.141765     -0.114263    -0.126773     -0.023518    -0.0718165    -0.103347     0.0727828  -0.119993     0.00242069  -0.0403229    0.087525   -0.0480595    0.0509292   -0.0872071     0.0527622    0.0145469   -0.0612251  -0.0316911    0.037454     0.00476368  -0.0114657   -0.00611655
  0.0206991   -0.0850099    0.0567345    0.0848101    0.000542155   0.201173     0.255609      0.0489004   -0.139917     -0.0902479    0.129124   -0.086007    -0.0135226   -0.0635492   -0.0664408   0.078431    -0.00768354  -0.131198     -0.150436     0.042101     0.196334    0.14317      0.0598488    0.0330989   -0.0568182   -0.0492454 
 -0.185583     0.190187     0.021732     0.120936     0.0247006    -0.0593253   -0.00466221    0.0892773    0.0700061     0.274089     0.0531757  -0.0316203   -0.0190082    0.0258752   -0.0699413   0.0335909    0.0930205    0.00758656    0.0506201   -0.0607317    0.0808362   0.00974749  -0.0552126   -0.0671156   -0.0952135   -0.141211  
 -0.0757764    0.199027     0.146083     0.0284014    0.169653     -0.0171523   -0.0101061     0.0544615    0.0963673     0.0868382    0.0962037  -0.130209    -0.0963342    0.0583212   -0.141425    0.0411083   -0.193618     0.0902722     0.137707     0.00789679  -0.043455    0.0559865   -0.0396172    0.0721301    0.00790146   0.0218512 
 -0.0480069    0.06746      0.00684137  -0.0307361   -0.0447021    -0.0760098   -0.0320101     0.127232     0.0937338     0.121882     0.126783   -0.00819739   0.117251    -0.139003     0.0293729  -0.0067524    0.0245683   -0.12794      -0.138875    -0.0329182   -0.0262795   0.0333359    0.0999969   -0.0744177   -0.0452615   -0.0931435 
  0.0118431    0.0088978   -0.0680095    0.0213152    0.0233651    -0.051575     0.000311883  -0.0608676   -0.0748196     0.00712034  -0.0579826   0.00344679   0.042665     0.0638802   -0.0147001  -0.0134728    0.00268244  -0.226022     -0.214165     0.156961     0.089983    0.0755488    0.00285557   0.0930061    0.0322596   -0.0168784 
 -0.153609     0.0465048   -0.136194    -0.0486616   -0.0311609     0.13702      0.00925727   -0.150738     0.104018     -0.0289635    0.107753    0.00228349  -0.0902285   -0.0602184   -0.141165   -0.00956642  -0.0210796    0.0170972    -0.0553788   -0.0319984   -0.0549686  -0.167139    -0.100424    -0.049819    -0.0844019    0.149383  
  0.0457294   -0.143746     0.0513702   -0.0399293   -0.0173632     0.122901    -0.136765      0.0269437    0.09564      -0.0835404    0.0374217   0.182784    -0.0364196   -0.240586    -0.104968    0.0345221   -0.0334233    0.018765      0.201398    -0.0181433    0.0337743   0.0226632   -0.0205435    0.094062    -0.0762724    0.0781651 
  0.131718     0.00289928  -0.174713     0.106056     0.106306      0.0230406    0.0314749    -0.156883     0.0869924    -0.00831887  -0.0518971   0.141784    -0.131912    -0.133252    -0.0475065  -0.0473601   -0.0941911   -0.0249189     0.0809968   -0.126728    -0.107007   -0.0956684   -0.00184513   0.0595504   -0.068149    -0.0729964 
  0.0452092   -0.143737    -0.135951    -0.0373849   -0.0856005    -0.185961     0.0517552     0.00613826   0.127847     -0.149816    -0.164658   -0.0163034   -0.170879    -0.0635815    0.0673253  -0.0398307    0.0178474    0.159221     -0.0756623    0.0273123    0.122529    0.103968     0.0884015   -0.0369384    0.0288593    0.174177  
 -0.168961     0.0983752    0.0369617    0.0494876   -0.0241605     0.0211129   -0.139376     -0.105516     0.0261255    -0.143881     0.0763559  -0.0109754    0.15093     -0.0427306    0.0337279  -0.0214293   -0.0924422    0.1137        0.0459661   -0.0323823    0.0643832  -0.233298    -0.0108428    0.0427975    0.0127122    0.119509  
 -0.0540515    0.208062    -0.00967487  -0.0231364   -0.112936     -0.136395     0.0639612    -0.0709844   -0.110972     -0.122225    -0.221769    0.0161907   -0.00545304  -0.0425836    0.0561328   0.0652105   -0.180913    -0.0516331     0.0849306    0.00560057   0.0460205   0.0759417   -0.121938     0.0231004    0.0385405    0.0553514 kind diag, method split
0: avll = -1.3982638421639426
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.398329
INFO: iteration 2, average log likelihood -1.398229
INFO: iteration 3, average log likelihood -1.397249
INFO: iteration 4, average log likelihood -1.388689
INFO: iteration 5, average log likelihood -1.373790
INFO: iteration 6, average log likelihood -1.369268
INFO: iteration 7, average log likelihood -1.367649
INFO: iteration 8, average log likelihood -1.366562
INFO: iteration 9, average log likelihood -1.365908
INFO: iteration 10, average log likelihood -1.365459
INFO: iteration 11, average log likelihood -1.365075
INFO: iteration 12, average log likelihood -1.364707
INFO: iteration 13, average log likelihood -1.364358
INFO: iteration 14, average log likelihood -1.364058
INFO: iteration 15, average log likelihood -1.363819
INFO: iteration 16, average log likelihood -1.363639
INFO: iteration 17, average log likelihood -1.363510
INFO: iteration 18, average log likelihood -1.363426
INFO: iteration 19, average log likelihood -1.363380
INFO: iteration 20, average log likelihood -1.363354
INFO: iteration 21, average log likelihood -1.363338
INFO: iteration 22, average log likelihood -1.363329
INFO: iteration 23, average log likelihood -1.363322
INFO: iteration 24, average log likelihood -1.363317
INFO: iteration 25, average log likelihood -1.363314
INFO: iteration 26, average log likelihood -1.363312
INFO: iteration 27, average log likelihood -1.363310
INFO: iteration 28, average log likelihood -1.363309
INFO: iteration 29, average log likelihood -1.363308
INFO: iteration 30, average log likelihood -1.363307
INFO: iteration 31, average log likelihood -1.363307
INFO: iteration 32, average log likelihood -1.363306
INFO: iteration 33, average log likelihood -1.363306
INFO: iteration 34, average log likelihood -1.363306
INFO: iteration 35, average log likelihood -1.363306
INFO: iteration 36, average log likelihood -1.363305
INFO: iteration 37, average log likelihood -1.363305
INFO: iteration 38, average log likelihood -1.363305
INFO: iteration 39, average log likelihood -1.363305
INFO: iteration 40, average log likelihood -1.363305
INFO: iteration 41, average log likelihood -1.363305
INFO: iteration 42, average log likelihood -1.363305
INFO: iteration 43, average log likelihood -1.363305
INFO: iteration 44, average log likelihood -1.363305
INFO: iteration 45, average log likelihood -1.363305
INFO: iteration 46, average log likelihood -1.363305
INFO: iteration 47, average log likelihood -1.363305
INFO: iteration 48, average log likelihood -1.363305
INFO: iteration 49, average log likelihood -1.363305
INFO: iteration 50, average log likelihood -1.363305
INFO: EM with 100000 data points 50 iterations avll -1.363305
952.4 data points per parameter
1: avll = [-1.39833,-1.39823,-1.39725,-1.38869,-1.37379,-1.36927,-1.36765,-1.36656,-1.36591,-1.36546,-1.36508,-1.36471,-1.36436,-1.36406,-1.36382,-1.36364,-1.36351,-1.36343,-1.36338,-1.36335,-1.36334,-1.36333,-1.36332,-1.36332,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.363406
INFO: iteration 2, average log likelihood -1.363295
INFO: iteration 3, average log likelihood -1.362844
INFO: iteration 4, average log likelihood -1.359512
INFO: iteration 5, average log likelihood -1.350116
INFO: iteration 6, average log likelihood -1.341474
INFO: iteration 7, average log likelihood -1.337711
INFO: iteration 8, average log likelihood -1.335908
INFO: iteration 9, average log likelihood -1.334639
INFO: iteration 10, average log likelihood -1.333495
INFO: iteration 11, average log likelihood -1.332307
INFO: iteration 12, average log likelihood -1.331061
INFO: iteration 13, average log likelihood -1.329850
INFO: iteration 14, average log likelihood -1.328806
INFO: iteration 15, average log likelihood -1.327924
INFO: iteration 16, average log likelihood -1.327063
INFO: iteration 17, average log likelihood -1.326109
INFO: iteration 18, average log likelihood -1.325107
INFO: iteration 19, average log likelihood -1.324210
INFO: iteration 20, average log likelihood -1.323430
INFO: iteration 21, average log likelihood -1.322719
INFO: iteration 22, average log likelihood -1.322088
INFO: iteration 23, average log likelihood -1.321564
INFO: iteration 24, average log likelihood -1.321164
INFO: iteration 25, average log likelihood -1.320888
INFO: iteration 26, average log likelihood -1.320702
INFO: iteration 27, average log likelihood -1.320578
INFO: iteration 28, average log likelihood -1.320499
INFO: iteration 29, average log likelihood -1.320447
INFO: iteration 30, average log likelihood -1.320412
INFO: iteration 31, average log likelihood -1.320388
INFO: iteration 32, average log likelihood -1.320371
INFO: iteration 33, average log likelihood -1.320359
INFO: iteration 34, average log likelihood -1.320349
INFO: iteration 35, average log likelihood -1.320341
INFO: iteration 36, average log likelihood -1.320335
INFO: iteration 37, average log likelihood -1.320329
INFO: iteration 38, average log likelihood -1.320325
INFO: iteration 39, average log likelihood -1.320320
INFO: iteration 40, average log likelihood -1.320317
INFO: iteration 41, average log likelihood -1.320313
INFO: iteration 42, average log likelihood -1.320310
INFO: iteration 43, average log likelihood -1.320307
INFO: iteration 44, average log likelihood -1.320304
INFO: iteration 45, average log likelihood -1.320301
INFO: iteration 46, average log likelihood -1.320299
INFO: iteration 47, average log likelihood -1.320296
INFO: iteration 48, average log likelihood -1.320294
INFO: iteration 49, average log likelihood -1.320292
INFO: iteration 50, average log likelihood -1.320290
INFO: EM with 100000 data points 50 iterations avll -1.320290
473.9 data points per parameter
2: avll = [-1.36341,-1.3633,-1.36284,-1.35951,-1.35012,-1.34147,-1.33771,-1.33591,-1.33464,-1.3335,-1.33231,-1.33106,-1.32985,-1.32881,-1.32792,-1.32706,-1.32611,-1.32511,-1.32421,-1.32343,-1.32272,-1.32209,-1.32156,-1.32116,-1.32089,-1.3207,-1.32058,-1.3205,-1.32045,-1.32041,-1.32039,-1.32037,-1.32036,-1.32035,-1.32034,-1.32033,-1.32033,-1.32032,-1.32032,-1.32032,-1.32031,-1.32031,-1.32031,-1.3203,-1.3203,-1.3203,-1.3203,-1.32029,-1.32029,-1.32029]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.320430
INFO: iteration 2, average log likelihood -1.320286
INFO: iteration 3, average log likelihood -1.319703
INFO: iteration 4, average log likelihood -1.314080
INFO: iteration 5, average log likelihood -1.295034
INFO: iteration 6, average log likelihood -1.277989
INFO: iteration 7, average log likelihood -1.269719
INFO: iteration 8, average log likelihood -1.265517
INFO: iteration 9, average log likelihood -1.263621
INFO: iteration 10, average log likelihood -1.262700
INFO: iteration 11, average log likelihood -1.262151
INFO: iteration 12, average log likelihood -1.261758
INFO: iteration 13, average log likelihood -1.261417
INFO: iteration 14, average log likelihood -1.261066
INFO: iteration 15, average log likelihood -1.260683
INFO: iteration 16, average log likelihood -1.260283
INFO: iteration 17, average log likelihood -1.259928
INFO: iteration 18, average log likelihood -1.259706
INFO: iteration 19, average log likelihood -1.259574
INFO: iteration 20, average log likelihood -1.259470
INFO: iteration 21, average log likelihood -1.259370
INFO: iteration 22, average log likelihood -1.259264
INFO: iteration 23, average log likelihood -1.259142
INFO: iteration 24, average log likelihood -1.258993
INFO: iteration 25, average log likelihood -1.258806
INFO: iteration 26, average log likelihood -1.258570
INFO: iteration 27, average log likelihood -1.258276
INFO: iteration 28, average log likelihood -1.257919
INFO: iteration 29, average log likelihood -1.257481
INFO: iteration 30, average log likelihood -1.256922
INFO: iteration 31, average log likelihood -1.256123
INFO: iteration 32, average log likelihood -1.255134
INFO: iteration 33, average log likelihood -1.254311
INFO: iteration 34, average log likelihood -1.253929
INFO: iteration 35, average log likelihood -1.253788
INFO: iteration 36, average log likelihood -1.253715
INFO: iteration 37, average log likelihood -1.253663
INFO: iteration 38, average log likelihood -1.253622
INFO: iteration 39, average log likelihood -1.253591
INFO: iteration 40, average log likelihood -1.253567
INFO: iteration 41, average log likelihood -1.253547
INFO: iteration 42, average log likelihood -1.253532
INFO: iteration 43, average log likelihood -1.253519
INFO: iteration 44, average log likelihood -1.253507
INFO: iteration 45, average log likelihood -1.253498
INFO: iteration 46, average log likelihood -1.253489
INFO: iteration 47, average log likelihood -1.253480
INFO: iteration 48, average log likelihood -1.253471
INFO: iteration 49, average log likelihood -1.253462
INFO: iteration 50, average log likelihood -1.253454
INFO: EM with 100000 data points 50 iterations avll -1.253454
236.4 data points per parameter
3: avll = [-1.32043,-1.32029,-1.3197,-1.31408,-1.29503,-1.27799,-1.26972,-1.26552,-1.26362,-1.2627,-1.26215,-1.26176,-1.26142,-1.26107,-1.26068,-1.26028,-1.25993,-1.25971,-1.25957,-1.25947,-1.25937,-1.25926,-1.25914,-1.25899,-1.25881,-1.25857,-1.25828,-1.25792,-1.25748,-1.25692,-1.25612,-1.25513,-1.25431,-1.25393,-1.25379,-1.25372,-1.25366,-1.25362,-1.25359,-1.25357,-1.25355,-1.25353,-1.25352,-1.25351,-1.2535,-1.25349,-1.25348,-1.25347,-1.25346,-1.25345]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.253635
INFO: iteration 2, average log likelihood -1.253413
INFO: iteration 3, average log likelihood -1.252084
INFO: iteration 4, average log likelihood -1.238139
INFO: iteration 5, average log likelihood -1.199818
WARNING: Variances had to be floored 1 9
INFO: iteration 6, average log likelihood -1.168733
INFO: iteration 7, average log likelihood -1.185477
INFO: iteration 8, average log likelihood -1.168772
WARNING: Variances had to be floored 1
INFO: iteration 9, average log likelihood -1.159440
WARNING: Variances had to be floored 9
INFO: iteration 10, average log likelihood -1.168802
INFO: iteration 11, average log likelihood -1.166208
WARNING: Variances had to be floored 1
INFO: iteration 12, average log likelihood -1.154006
WARNING: Variances had to be floored 7
INFO: iteration 13, average log likelihood -1.164982
WARNING: Variances had to be floored 13
INFO: iteration 14, average log likelihood -1.159071
WARNING: Variances had to be floored 1
INFO: iteration 15, average log likelihood -1.161306
WARNING: Variances had to be floored 9
INFO: iteration 16, average log likelihood -1.165694
INFO: iteration 17, average log likelihood -1.161825
WARNING: Variances had to be floored 1
INFO: iteration 18, average log likelihood -1.149090
INFO: iteration 19, average log likelihood -1.160637
INFO: iteration 20, average log likelihood -1.148854
WARNING: Variances had to be floored 1 9 13
INFO: iteration 21, average log likelihood -1.139642
WARNING: Variances had to be floored 7
INFO: iteration 22, average log likelihood -1.176384
INFO: iteration 23, average log likelihood -1.161388
WARNING: Variances had to be floored 1
INFO: iteration 24, average log likelihood -1.148118
WARNING: Variances had to be floored 9
INFO: iteration 25, average log likelihood -1.158274
INFO: iteration 26, average log likelihood -1.155455
WARNING: Variances had to be floored 1
INFO: iteration 27, average log likelihood -1.142979
INFO: iteration 28, average log likelihood -1.155444
INFO: iteration 29, average log likelihood -1.144819
WARNING: Variances had to be floored 1 9
INFO: iteration 30, average log likelihood -1.137128
WARNING: Variances had to be floored 7
INFO: iteration 31, average log likelihood -1.160879
INFO: iteration 32, average log likelihood -1.152573
WARNING: Variances had to be floored 1
INFO: iteration 33, average log likelihood -1.142529
INFO: iteration 34, average log likelihood -1.155491
WARNING: Variances had to be floored 9
INFO: iteration 35, average log likelihood -1.142557
WARNING: Variances had to be floored 1
INFO: iteration 36, average log likelihood -1.146696
INFO: iteration 37, average log likelihood -1.156177
INFO: iteration 38, average log likelihood -1.145724
WARNING: Variances had to be floored 1
INFO: iteration 39, average log likelihood -1.139657
WARNING: Variances had to be floored 7 9
INFO: iteration 40, average log likelihood -1.152132
INFO: iteration 41, average log likelihood -1.157395
WARNING: Variances had to be floored 1
INFO: iteration 42, average log likelihood -1.143446
INFO: iteration 43, average log likelihood -1.156359
INFO: iteration 44, average log likelihood -1.144732
WARNING: Variances had to be floored 1 9
INFO: iteration 45, average log likelihood -1.135517
INFO: iteration 46, average log likelihood -1.160984
INFO: iteration 47, average log likelihood -1.146598
WARNING: Variances had to be floored 1 7
INFO: iteration 48, average log likelihood -1.140255
INFO: iteration 49, average log likelihood -1.159349
WARNING: Variances had to be floored 9
INFO: iteration 50, average log likelihood -1.142689
INFO: EM with 100000 data points 50 iterations avll -1.142689
118.1 data points per parameter
4: avll = [-1.25363,-1.25341,-1.25208,-1.23814,-1.19982,-1.16873,-1.18548,-1.16877,-1.15944,-1.1688,-1.16621,-1.15401,-1.16498,-1.15907,-1.16131,-1.16569,-1.16182,-1.14909,-1.16064,-1.14885,-1.13964,-1.17638,-1.16139,-1.14812,-1.15827,-1.15545,-1.14298,-1.15544,-1.14482,-1.13713,-1.16088,-1.15257,-1.14253,-1.15549,-1.14256,-1.1467,-1.15618,-1.14572,-1.13966,-1.15213,-1.1574,-1.14345,-1.15636,-1.14473,-1.13552,-1.16098,-1.1466,-1.14025,-1.15935,-1.14269]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 2
INFO: iteration 1, average log likelihood -1.148090
WARNING: Variances had to be floored 1 2
INFO: iteration 2, average log likelihood -1.142221
WARNING: Variances had to be floored 1 2 17
INFO: iteration 3, average log likelihood -1.138349
WARNING: Variances had to be floored 1 2 13
INFO: iteration 4, average log likelihood -1.122401
WARNING: Variances had to be floored 1 2 7 8 14 15 17 25 31
INFO: iteration 5, average log likelihood -1.061094
WARNING: Variances had to be floored 1 2 5 25
INFO: iteration 6, average log likelihood -1.078051
WARNING: Variances had to be floored 1 2 13 15 17 18 25 31
INFO: iteration 7, average log likelihood -1.046924
WARNING: Variances had to be floored 1 2 5 7 8 10 14 25
INFO: iteration 8, average log likelihood -1.065212
WARNING: Variances had to be floored 1 2 15 25 31
INFO: iteration 9, average log likelihood -1.065923
WARNING: Variances had to be floored 1 2 5 17 18 25
INFO: iteration 10, average log likelihood -1.052345
WARNING: Variances had to be floored 1 2 7 8 10 14 15 25 31
INFO: iteration 11, average log likelihood -1.048620
WARNING: Variances had to be floored 1 2 5 25
INFO: iteration 12, average log likelihood -1.072212
WARNING: Variances had to be floored 1 2 15 17 18 25 31
INFO: iteration 13, average log likelihood -1.039615
WARNING: Variances had to be floored 1 2 5 7 8 10 14 25
INFO: iteration 14, average log likelihood -1.053287
WARNING: Variances had to be floored 1 2 15 17 25 31
INFO: iteration 15, average log likelihood -1.053929
WARNING: Variances had to be floored 1 2 5 18 25
INFO: iteration 16, average log likelihood -1.058311
WARNING: Variances had to be floored 1 2 7 8 10 13 14 15 17 25 31
INFO: iteration 17, average log likelihood -1.033777
WARNING: Variances had to be floored 1 2 5 25
INFO: iteration 18, average log likelihood -1.077995
WARNING: Variances had to be floored 1 2 15 18 25 31
INFO: iteration 19, average log likelihood -1.042616
WARNING: Variances had to be floored 1 2 5 7 8 10 12 14 17 25
INFO: iteration 20, average log likelihood -1.042747
WARNING: Variances had to be floored 1 2 15 18 25 31
INFO: iteration 21, average log likelihood -1.066159
WARNING: Variances had to be floored 1 2 13 25
INFO: iteration 22, average log likelihood -1.068952
WARNING: Variances had to be floored 1 2 5 7 8 15 17 18 25 31
INFO: iteration 23, average log likelihood -1.025833
WARNING: Variances had to be floored 1 2 10 14 25
INFO: iteration 24, average log likelihood -1.072276
WARNING: Variances had to be floored 1 2 12 15 25 31
INFO: iteration 25, average log likelihood -1.049114
WARNING: Variances had to be floored 1 2 5 7 8 13 17 18 25
INFO: iteration 26, average log likelihood -1.047259
WARNING: Variances had to be floored 1 2 14 15 25 31
INFO: iteration 27, average log likelihood -1.063354
WARNING: Variances had to be floored 1 2 10 25
INFO: iteration 28, average log likelihood -1.059484
WARNING: Variances had to be floored 1 2 5 7 8 12 15 17 18 25 31
INFO: iteration 29, average log likelihood -1.024099
WARNING: Variances had to be floored 1 2 14 25
INFO: iteration 30, average log likelihood -1.083079
WARNING: Variances had to be floored 1 2 15 25 31
INFO: iteration 31, average log likelihood -1.051716
WARNING: Variances had to be floored 1 2 5 7 8 10 17 18 25
INFO: iteration 32, average log likelihood -1.037738
WARNING: Variances had to be floored 1 2 12 14 15 25 31
INFO: iteration 33, average log likelihood -1.055283
WARNING: Variances had to be floored 1 2 18 25
INFO: iteration 34, average log likelihood -1.061599
WARNING: Variances had to be floored 1 2 5 7 8 13 15 17 25 31
INFO: iteration 35, average log likelihood -1.030753
WARNING: Variances had to be floored 1 2 10 14 25
INFO: iteration 36, average log likelihood -1.073396
WARNING: Variances had to be floored 1 2 12 15 18 25 31
INFO: iteration 37, average log likelihood -1.049958
WARNING: Variances had to be floored 1 2 5 7 8 25
INFO: iteration 38, average log likelihood -1.059844
WARNING: Variances had to be floored 1 2 14 15 17 18 25 31
INFO: iteration 39, average log likelihood -1.044540
WARNING: Variances had to be floored 1 2 5 10 25
INFO: iteration 40, average log likelihood -1.064715
WARNING: Variances had to be floored 1 2 7 8 12 15 18 25 31
INFO: iteration 41, average log likelihood -1.037660
WARNING: Variances had to be floored 1 2 14 25
INFO: iteration 42, average log likelihood -1.070959
WARNING: Variances had to be floored 1 2 5 15 18 25 31
INFO: iteration 43, average log likelihood -1.038277
WARNING: Variances had to be floored 1 2 7 8 10 25
INFO: iteration 44, average log likelihood -1.050918
WARNING: Variances had to be floored 1 2 5 12 13 14 15 18 25 31
INFO: iteration 45, average log likelihood -1.036933
WARNING: Variances had to be floored 1 2 17 25
INFO: iteration 46, average log likelihood -1.080296
WARNING: Variances had to be floored 1 2 5 7 8 15 18 25 31
INFO: iteration 47, average log likelihood -1.043378
WARNING: Variances had to be floored 1 2 10 14 17 25
INFO: iteration 48, average log likelihood -1.070355
WARNING: Variances had to be floored 1 2 5 15 18 25 31
INFO: iteration 49, average log likelihood -1.050480
WARNING: Variances had to be floored 1 2 7 8 12 13 17 25
INFO: iteration 50, average log likelihood -1.051800
INFO: EM with 100000 data points 50 iterations avll -1.051800
59.0 data points per parameter
5: avll = [-1.14809,-1.14222,-1.13835,-1.1224,-1.06109,-1.07805,-1.04692,-1.06521,-1.06592,-1.05234,-1.04862,-1.07221,-1.03961,-1.05329,-1.05393,-1.05831,-1.03378,-1.07799,-1.04262,-1.04275,-1.06616,-1.06895,-1.02583,-1.07228,-1.04911,-1.04726,-1.06335,-1.05948,-1.0241,-1.08308,-1.05172,-1.03774,-1.05528,-1.0616,-1.03075,-1.0734,-1.04996,-1.05984,-1.04454,-1.06471,-1.03766,-1.07096,-1.03828,-1.05092,-1.03693,-1.0803,-1.04338,-1.07035,-1.05048,-1.0518]
[-1.39826,-1.39833,-1.39823,-1.39725,-1.38869,-1.37379,-1.36927,-1.36765,-1.36656,-1.36591,-1.36546,-1.36508,-1.36471,-1.36436,-1.36406,-1.36382,-1.36364,-1.36351,-1.36343,-1.36338,-1.36335,-1.36334,-1.36333,-1.36332,-1.36332,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36331,-1.36341,-1.3633,-1.36284,-1.35951,-1.35012,-1.34147,-1.33771,-1.33591,-1.33464,-1.3335,-1.33231,-1.33106,-1.32985,-1.32881,-1.32792,-1.32706,-1.32611,-1.32511,-1.32421,-1.32343,-1.32272,-1.32209,-1.32156,-1.32116,-1.32089,-1.3207,-1.32058,-1.3205,-1.32045,-1.32041,-1.32039,-1.32037,-1.32036,-1.32035,-1.32034,-1.32033,-1.32033,-1.32032,-1.32032,-1.32032,-1.32031,-1.32031,-1.32031,-1.3203,-1.3203,-1.3203,-1.3203,-1.32029,-1.32029,-1.32029,-1.32043,-1.32029,-1.3197,-1.31408,-1.29503,-1.27799,-1.26972,-1.26552,-1.26362,-1.2627,-1.26215,-1.26176,-1.26142,-1.26107,-1.26068,-1.26028,-1.25993,-1.25971,-1.25957,-1.25947,-1.25937,-1.25926,-1.25914,-1.25899,-1.25881,-1.25857,-1.25828,-1.25792,-1.25748,-1.25692,-1.25612,-1.25513,-1.25431,-1.25393,-1.25379,-1.25372,-1.25366,-1.25362,-1.25359,-1.25357,-1.25355,-1.25353,-1.25352,-1.25351,-1.2535,-1.25349,-1.25348,-1.25347,-1.25346,-1.25345,-1.25363,-1.25341,-1.25208,-1.23814,-1.19982,-1.16873,-1.18548,-1.16877,-1.15944,-1.1688,-1.16621,-1.15401,-1.16498,-1.15907,-1.16131,-1.16569,-1.16182,-1.14909,-1.16064,-1.14885,-1.13964,-1.17638,-1.16139,-1.14812,-1.15827,-1.15545,-1.14298,-1.15544,-1.14482,-1.13713,-1.16088,-1.15257,-1.14253,-1.15549,-1.14256,-1.1467,-1.15618,-1.14572,-1.13966,-1.15213,-1.1574,-1.14345,-1.15636,-1.14473,-1.13552,-1.16098,-1.1466,-1.14025,-1.15935,-1.14269,-1.14809,-1.14222,-1.13835,-1.1224,-1.06109,-1.07805,-1.04692,-1.06521,-1.06592,-1.05234,-1.04862,-1.07221,-1.03961,-1.05329,-1.05393,-1.05831,-1.03378,-1.07799,-1.04262,-1.04275,-1.06616,-1.06895,-1.02583,-1.07228,-1.04911,-1.04726,-1.06335,-1.05948,-1.0241,-1.08308,-1.05172,-1.03774,-1.05528,-1.0616,-1.03075,-1.0734,-1.04996,-1.05984,-1.04454,-1.06471,-1.03766,-1.07096,-1.03828,-1.05092,-1.03693,-1.0803,-1.04338,-1.07035,-1.05048,-1.0518]
32Ã—26 Array{Float64,2}:
  0.0291491   -0.14927      0.054539    -0.0395527   0.000809104   0.172724   -0.182811      0.0104911    0.120332     0.16352      0.0830038    0.126289     0.0325438    -0.114194    -0.104531    0.0298001   -0.0358128    0.0281603    0.280879    -0.0262417    0.0315654    0.0187679   -0.426333     0.0795667   -0.0699856    0.0635224 
  0.1242      -0.139125     0.0572351   -0.0396154  -0.0126976     0.116577   -0.0484003     0.0486846    0.0836139   -0.292993    -0.00421433   0.257052    -0.0974245    -0.338007    -0.104401    0.0310028   -0.0324071    0.060372     0.244578    -0.010005     0.033312     0.0255526    0.419186     0.0161165   -0.0983167    0.0973508 
  0.0168557   -0.0111868   -0.0570097    0.093434   -0.506603     -0.0641154   0.0129424    -0.0547229   -0.0630545    0.00861777  -0.0539764   -0.0391124   -0.0263976     0.0867871   -0.0174432   0.0899321    0.010674    -0.244807    -0.480691     0.205174     0.082932     0.0910941    0.0814304    0.0972475    0.0693216   -0.00618887
  0.0121083    0.00380417  -0.0651131   -0.0506323   0.493713     -0.0752184  -0.0159548    -0.114972    -0.0880731   -0.0137423   -0.0659442    0.0403171    0.104097      0.0529568   -0.0347693  -0.112899    -0.00505173  -0.176811     0.00878126   0.135195     0.0926899    0.0997276   -0.0803598    0.0925056   -0.01679     -0.02565   
  0.0536121   -0.127433     0.131026    -0.0632397  -0.0806278    -0.0736697   0.0624029    -0.0203971   -0.0155012   -0.135145    -0.0720266    0.0764038   -0.140476      0.00512742  -0.0311339  -0.0791746    0.184225    -0.140551    -0.189968     0.00497279   0.0145147    0.00291125  -0.13747      0.00360527  -0.114691     0.0711393 
  0.0596324    0.0461467    0.187692    -0.0190645   0.0825581     0.145434    0.174626     -0.148293     0.161007    -0.032863    -0.0767233   -0.244498    -0.0248041    -0.076457     0.0834317   0.0136758   -0.0262894    0.0133073    0.0847093    0.011207    -0.0398491    0.114922    -0.0657015    0.0538278   -0.0842402   -0.0380103 
 -0.0531435    0.0663447   -0.0725531   -0.0765072  -0.12269      -0.252059    0.0240128    -0.1209      -0.111988     0.0748224   -0.109081    -0.0963772   -0.112777      0.00163233  -0.144886   -0.0162383   -0.283196     0.0883726   -0.0370826    0.109873     0.00992341   0.140373     0.0679334    0.0512277    0.0975447   -0.217924  
  0.0695438   -0.0864858   -0.0564963    0.086052    0.0764193     0.0214383  -0.0154353    -0.00628803  -0.104084     0.0636503   -0.0121315    0.040308     0.0389305     0.006975    -0.162141   -0.00619976   0.0378928   -0.0691551   -0.0481673    0.0150512    0.128033     0.0419862    0.00921057   0.0757027    0.00912068  -0.0253554 
  0.020346    -0.066113     0.14664     -0.148182   -0.0533074    -0.193243    0.20812      -0.0465971   -0.0548677   -0.0235727    0.0680656   -0.0219774   -0.0529308     0.00634579   0.0491338   0.113353    -0.118733     0.0369085    0.004574     0.139911     0.0983818    0.073527    -0.00729259  -0.0985848   -0.0592231   -0.107297  
 -0.0727157    0.071894     0.0197581   -0.0474556  -0.0392669    -0.0606001  -0.000120181   0.120109     0.120224     0.119796     0.117611     0.00592792   0.114646     -0.107155     0.0286038  -0.0104675    0.0211222   -0.133023    -0.167692    -0.0335431   -0.027188     0.0372438    0.0855452   -0.0478961   -0.0245893   -0.0896368 
  0.0114541   -0.0780922    0.0671076    0.0831329  -0.00249549    0.193471    0.261365      0.0593581   -0.112837    -0.109097     0.111864    -0.0652532   -0.0109767    -0.0631149   -0.0811568   0.0776585   -0.00661067  -0.119838    -0.199181     0.0287482    0.184163     0.142663     0.0878483    0.036334    -0.0572007   -0.0548553 
  0.126764     0.0141865   -0.174067     0.0963422   0.0969652     0.0269589   0.0205631    -0.165856     0.066971    -0.00506953  -0.043046     0.129423    -0.135089     -0.130613    -0.0491282  -0.0472297   -0.0908496   -0.0252131    0.0666138   -0.120943    -0.102548    -0.0960716    0.00340836   0.088012    -0.0689226   -0.0683176 
 -0.178939    -0.0126243    0.18635      0.113631    0.0806208    -0.0414859  -0.0650246    -0.0456117   -0.0248366   -0.209078    -0.0113547    0.11488     -0.0358806    -0.0384131   -0.082141   -0.0502619   -0.223093     0.115211     0.0287137    0.0123604   -0.00138802  -0.0593007    0.0288856    0.0156365    0.0481424    0.0127561 
 -0.204125    -0.0486347    0.235389     0.0195165   0.163052     -0.0253503  -0.0204477    -0.00523453  -0.0660313   -0.199932    -0.0137758    0.242755    -0.169169     -0.0457382   -0.147301   -0.108104    -0.173739     0.152692     0.0334374    0.0331296   -0.0479672    0.0184401    0.0365888    0.0238527    0.050738    -0.0495319 
 -0.0477049    0.205073    -0.00819779  -0.0157819  -0.137474     -0.139243    0.0462926    -0.0801542   -0.107207    -0.121112    -0.224587     0.0256593   -0.0121892    -0.0145777    0.0442662   0.0608432   -0.191784    -0.0398827    0.0776127    0.00427178   0.0468801    0.0717406   -0.142655     0.0184702    0.0403512    0.05555   
 -0.0918817   -0.124327     0.0365386   -0.0166499  -0.0703353     0.035593    0.00460495   -0.11027     -0.0678719   -0.0942668   -0.0288497   -0.0256312    0.000820627  -0.0487085    0.020923   -0.0798411   -0.0665348   -0.085548    -0.0989197    0.0542639    0.138151    -0.150797     0.169985    -0.169543     0.137743     0.0224311 
 -0.172723     0.0533314    0.0257575    0.0487711  -0.0265544     0.0434073  -0.142596     -0.10105      0.00257376  -0.116061     0.057521    -0.0208204    0.148495     -0.045025     0.0507463  -0.0192397   -0.156396     0.0882657    0.0489791   -0.0288517    0.0692634   -0.241611    -0.0206055    0.0504078    0.0112834    0.168662  
 -0.0515428   -0.237676    -0.0161633    0.102593   -0.00791908    0.0217797   0.0136616     0.175023     0.121224    -0.101954    -0.176387    -0.0685829    0.198275      0.0684368    0.0423043  -0.0285647    0.0016342   -0.173135    -0.0100363    0.00068067   0.0754283   -0.155172    -0.0119477    0.188653     0.181483     0.049621  
  0.0143615   -0.104645     0.116068    -0.0509872   0.0666734     0.226804   -0.103377      0.0260766   -0.139582    -0.2215      -0.368793     0.0509901    0.156386     -0.00834148  -0.0841216  -0.15842      0.0785158   -0.0950315   -0.0313209    0.12751     -0.0517797   -0.0869131    0.155377    -0.257636    -0.304788     0.119135  
  0.0313638   -0.136738     0.122736    -0.0519131  -0.120153      0.105434   -0.0670143    -0.034225     0.205814    -0.086049     0.581498    -0.0155915    0.196916     -0.147283    -0.0949972   0.379986     0.10228      0.0574597   -0.01874      0.131387    -0.0511437   -0.103034     0.171148     0.0755489   -0.184282     0.14061   
 -0.116        0.0418644   -0.0280685    0.10936     0.0382428     0.0843433  -0.0469448    -0.154662     0.16005      0.0342867    0.0395605    0.0410321   -0.0473424     0.00172906  -0.0457365  -0.028732    -0.0122592    0.011677    -0.050751    -0.0516286   -0.0267142   -0.00717685   0.0174299    0.107187    -0.0516577   -0.0362249 
  0.0266713    0.128765     0.0462183    0.0307427   0.092332      0.0621378  -0.0170782     0.0636425    0.0557657    0.0367975    0.0157857   -0.095574     0.0094182     0.0490609    0.0400372   0.0195454   -0.134611     0.143756     0.00974991  -0.0274789    0.0155921   -0.103395     0.0156398    0.00434172   0.05777      0.0282397 
  0.0786046   -0.0394794    0.0558845   -0.0501568  -0.154565      0.0881506  -0.138802      0.148255     0.198147    -0.677629    -0.099439    -0.177647    -0.0714229    -0.107835    -0.0686439   0.00584206  -0.0762475   -0.272395     0.0393252    0.171174     0.0181513    0.0353644    0.0665612    0.0142092   -0.039293    -0.136891  
  0.0139831    0.0356739   -0.159205    -0.0593452  -0.0337307     0.0699549  -0.130136      0.149757    -0.100492     0.703937    -0.0198207   -0.138307    -0.0992198     0.144655    -0.0808396   0.012074     0.0185723   -0.269584     0.144105     0.177404     0.0130391    0.0515516    0.0193716   -0.167456     0.202065    -0.271739  
 -0.183866     0.197823     0.0152886    0.127241    0.0168579    -0.0709076  -0.0548063     0.0500839    0.0705827    0.254962     0.0520375   -0.0339544   -0.0242189     0.0273483   -0.0287173   0.037509     0.1308      -0.00333275   0.0542314   -0.06178      0.113589    -0.0246741   -0.0558158   -0.0636747   -0.0868915   -0.12614   
  0.017542    -0.0495044    0.0697366   -0.0168379   0.190103      0.0613424   0.0435399     0.133122     0.127128     0.0673913    0.034003     0.0377215    0.0194722    -0.0237982   -0.0490021   0.0164311   -0.161452     0.0294833   -0.0701902    0.03656     -0.227877     0.118229     0.0467888    0.00167105   0.0705877    0.108216  
  0.00260952  -0.0377495   -0.0157074   -0.0433695   0.00690301    0.0299284  -0.0574874    -0.0185557   -0.0141117    0.0148456    0.0435518   -0.0479347    0.0201592    -0.0105078    0.0488082  -0.0361905   -0.0357519    0.0286967    0.0297346    0.032572     0.0241341    0.00842094   0.0413263    0.0866708   -0.0249686   -0.0201151 
 -0.102559     0.0349882    0.00496129  -0.031943   -0.00673991    0.0154966  -0.103709      0.0997188    0.00707169   0.0888708    0.261137    -0.0640866   -0.0444222    -0.15555      0.185149   -0.117298    -0.0103621    0.0256626   -0.168465    -0.0871231    0.0195556   -0.123268    -0.0419397    0.0394385   -0.109269    -0.041044  
  0.00955586  -0.00583878   0.0322107   -0.0800389   0.0596108     0.0494756   0.0854755    -0.108514    -0.0146704    0.0758337    0.0288542   -0.0392088    0.0905865     0.0621264   -0.110954    0.0946277    0.0690638    0.0960126   -0.0537778    0.130735     0.118417    -0.0597027   -0.132091    -0.128093    -0.0910328    0.0890112 
  0.0802241   -0.0294797   -0.0187882    0.183877   -0.0251063    -0.0485822   0.122801      0.0590832   -0.0814375   -0.184131     0.153853    -0.17409      0.0800471     0.0676697   -0.0415352   0.187488    -0.0053886   -0.0307399    0.0756594   -0.0446863   -0.240146    -0.0936289    0.0680107   -0.033346    -0.028014     0.0927798 
 -0.0941376   -0.188963    -0.157228     0.0142061  -0.120438      0.0379773   0.10011       0.158261    -0.0374722    0.146803    -0.0564774    0.0348024    0.0698301     0.0223656   -0.13551    -0.0590148   -0.0201976   -0.00230642  -0.0448869   -0.0289456    0.09025      0.0339721    0.0825416   -0.0489276   -0.0722971    0.0371793 
  0.040814    -0.123932    -0.12849     -0.0347208  -0.0921742    -0.192464    0.0386757    -0.0811838    0.132376    -0.164243    -0.16651     -0.0115941   -0.106019     -0.0642413    0.0917178  -0.0411037    0.0255042    0.158098    -0.0771533    0.110102     0.123918     0.109311     0.0801305   -0.0314714    0.0198771    0.175691  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 2 10 15 18 25 31
INFO: iteration 1, average log likelihood -1.054578
WARNING: Variances had to be floored 1 2 5 10 14 15 17 18 25 31
INFO: iteration 2, average log likelihood -1.039380
WARNING: Variances had to be floored 1 2 5 7 8 10 15 18 25 31
INFO: iteration 3, average log likelihood -1.034679
WARNING: Variances had to be floored 1 2 5 10 12 13 15 17 18 25 31
INFO: iteration 4, average log likelihood -1.033708
WARNING: Variances had to be floored 1 2 10 14 15 18 25 31
INFO: iteration 5, average log likelihood -1.050384
WARNING: Variances had to be floored 1 2 5 7 8 10 15 17 18 25 31
INFO: iteration 6, average log likelihood -1.030667
WARNING: Variances had to be floored 1 2 5 10 14 15 18 25 31
INFO: iteration 7, average log likelihood -1.045539
WARNING: Variances had to be floored 1 2 5 10 12 15 17 18 25 31
INFO: iteration 8, average log likelihood -1.031138
WARNING: Variances had to be floored 1 2 5 7 8 10 14 15 18 25 31
INFO: iteration 9, average log likelihood -1.038463
WARNING: Variances had to be floored 1 2 5 10 15 18 25 31
INFO: iteration 10, average log likelihood -1.040936
INFO: EM with 100000 data points 10 iterations avll -1.040936
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.678169e+05
      1       6.747380e+05      -1.930789e+05 |       32
      2       6.414665e+05      -3.327148e+04 |       32
      3       6.258341e+05      -1.563239e+04 |       32
      4       6.171772e+05      -8.656969e+03 |       32
      5       6.120513e+05      -5.125850e+03 |       32
      6       6.080577e+05      -3.993622e+03 |       32
      7       6.050099e+05      -3.047799e+03 |       32
      8       6.031033e+05      -1.906623e+03 |       32
      9       6.018227e+05      -1.280527e+03 |       32
     10       6.008683e+05      -9.544815e+02 |       32
     11       6.001733e+05      -6.949788e+02 |       32
     12       5.996891e+05      -4.841768e+02 |       32
     13       5.992594e+05      -4.297403e+02 |       32
     14       5.987900e+05      -4.693886e+02 |       32
     15       5.982883e+05      -5.016626e+02 |       32
     16       5.977233e+05      -5.649980e+02 |       32
     17       5.972497e+05      -4.736455e+02 |       32
     18       5.969487e+05      -3.009336e+02 |       32
     19       5.968032e+05      -1.455583e+02 |       32
     20       5.967313e+05      -7.183558e+01 |       32
     21       5.966929e+05      -3.846265e+01 |       32
     22       5.966717e+05      -2.113475e+01 |       31
     23       5.966598e+05      -1.198400e+01 |       26
     24       5.966526e+05      -7.127442e+00 |       28
     25       5.966444e+05      -8.180106e+00 |       29
     26       5.966359e+05      -8.552860e+00 |       29
     27       5.966281e+05      -7.828397e+00 |       26
     28       5.966217e+05      -6.339772e+00 |       26
     29       5.966158e+05      -5.936931e+00 |       26
     30       5.966102e+05      -5.624963e+00 |       27
     31       5.966023e+05      -7.886574e+00 |       29
     32       5.965939e+05      -8.374538e+00 |       26
     33       5.965856e+05      -8.343605e+00 |       29
     34       5.965776e+05      -7.923280e+00 |       30
     35       5.965707e+05      -6.974163e+00 |       30
     36       5.965617e+05      -8.917572e+00 |       27
     37       5.965528e+05      -8.931597e+00 |       30
     38       5.965432e+05      -9.566968e+00 |       29
     39       5.965328e+05      -1.041895e+01 |       28
     40       5.965214e+05      -1.141102e+01 |       28
     41       5.965118e+05      -9.655359e+00 |       28
     42       5.965045e+05      -7.262104e+00 |       22
     43       5.965015e+05      -2.999986e+00 |       20
     44       5.964992e+05      -2.283748e+00 |       22
     45       5.964965e+05      -2.711811e+00 |       24
     46       5.964931e+05      -3.371508e+00 |       23
     47       5.964907e+05      -2.402887e+00 |       22
     48       5.964892e+05      -1.488521e+00 |       15
     49       5.964884e+05      -8.834972e-01 |       11
     50       5.964876e+05      -7.592078e-01 |       13
K-means terminated without convergence after 50 iterations (objv = 596487.5990259671)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.313982
INFO: iteration 2, average log likelihood -1.278444
INFO: iteration 3, average log likelihood -1.247517
INFO: iteration 4, average log likelihood -1.214276
INFO: iteration 5, average log likelihood -1.170948
WARNING: Variances had to be floored 17 19
INFO: iteration 6, average log likelihood -1.111010
WARNING: Variances had to be floored 10 11 13 15 27 30
INFO: iteration 7, average log likelihood -1.079778
WARNING: Variances had to be floored 4 16 23 26
INFO: iteration 8, average log likelihood -1.103767
WARNING: Variances had to be floored 5 9
INFO: iteration 9, average log likelihood -1.117090
WARNING: Variances had to be floored 12
INFO: iteration 10, average log likelihood -1.097805
WARNING: Variances had to be floored 8 11 15 17 19 27
INFO: iteration 11, average log likelihood -1.050419
WARNING: Variances had to be floored 4 13 16 23 26
INFO: iteration 12, average log likelihood -1.095294
WARNING: Variances had to be floored 9
INFO: iteration 13, average log likelihood -1.119402
WARNING: Variances had to be floored 5 10 12
INFO: iteration 14, average log likelihood -1.079896
WARNING: Variances had to be floored 8 15 16 26
INFO: iteration 15, average log likelihood -1.076634
WARNING: Variances had to be floored 4 17 19 23
INFO: iteration 16, average log likelihood -1.074360
WARNING: Variances had to be floored 5 9 10 11 13 27
INFO: iteration 17, average log likelihood -1.060921
WARNING: Variances had to be floored 12
INFO: iteration 18, average log likelihood -1.113394
WARNING: Variances had to be floored 26
INFO: iteration 19, average log likelihood -1.077034
WARNING: Variances had to be floored 4 5 8 9 11 15 17 19 27
INFO: iteration 20, average log likelihood -1.037211
WARNING: Variances had to be floored 12 16 23
INFO: iteration 21, average log likelihood -1.101693
WARNING: Variances had to be floored 7 10 13 26
INFO: iteration 22, average log likelihood -1.089933
WARNING: Variances had to be floored 11
INFO: iteration 23, average log likelihood -1.086429
WARNING: Variances had to be floored 4 5 9 12 15 19
INFO: iteration 24, average log likelihood -1.038136
WARNING: Variances had to be floored 13 16 17 23 27
INFO: iteration 25, average log likelihood -1.078747
WARNING: Variances had to be floored 10 26
INFO: iteration 26, average log likelihood -1.109790
INFO: iteration 27, average log likelihood -1.081965
WARNING: Variances had to be floored 4 5 8 9 11 12 15 19 27
INFO: iteration 28, average log likelihood -1.023935
WARNING: Variances had to be floored 7 13 17 23 26
INFO: iteration 29, average log likelihood -1.099480
WARNING: Variances had to be floored 10
INFO: iteration 30, average log likelihood -1.110791
WARNING: Variances had to be floored 11 16
INFO: iteration 31, average log likelihood -1.064308
WARNING: Variances had to be floored 4 5 7 9 12 13 19 23 26
INFO: iteration 32, average log likelihood -1.044354
WARNING: Variances had to be floored 15 27
INFO: iteration 33, average log likelihood -1.108781
WARNING: Variances had to be floored 10 17
INFO: iteration 34, average log likelihood -1.098821
WARNING: Variances had to be floored 16
INFO: iteration 35, average log likelihood -1.076036
WARNING: Variances had to be floored 4 5 9 11 12 19 23
INFO: iteration 36, average log likelihood -1.038321
WARNING: Variances had to be floored 7 13 15 26 27
INFO: iteration 37, average log likelihood -1.074140
WARNING: Variances had to be floored 10 17
INFO: iteration 38, average log likelihood -1.105369
WARNING: Variances had to be floored 5 8 16
INFO: iteration 39, average log likelihood -1.074817
WARNING: Variances had to be floored 4 7 9 12 15 27
INFO: iteration 40, average log likelihood -1.057056
WARNING: Variances had to be floored 13 26
INFO: iteration 41, average log likelihood -1.092751
WARNING: Variances had to be floored 10 11 17 19
INFO: iteration 42, average log likelihood -1.073173
WARNING: Variances had to be floored 5 23
INFO: iteration 43, average log likelihood -1.067480
WARNING: Variances had to be floored 4 9 12 13 15 16 26 27
INFO: iteration 44, average log likelihood -1.037629
WARNING: Variances had to be floored 8 11 19
INFO: iteration 45, average log likelihood -1.112145
WARNING: Variances had to be floored 5 17
INFO: iteration 46, average log likelihood -1.098542
WARNING: Variances had to be floored 7 10 12
INFO: iteration 47, average log likelihood -1.053820
WARNING: Variances had to be floored 4 9 13 15 23 26 27
INFO: iteration 48, average log likelihood -1.042374
WARNING: Variances had to be floored 5 16
INFO: iteration 49, average log likelihood -1.108190
WARNING: Variances had to be floored 8 11 12 17 19
INFO: iteration 50, average log likelihood -1.083526
INFO: EM with 100000 data points 50 iterations avll -1.083526
59.0 data points per parameter
32Ã—26 Array{Float64,2}:
 -0.0833966    0.00592493   0.0672787    0.260538     0.0865202    0.00811471  -0.082318     -0.145         0.250337     0.122383     -0.0246466   0.0668708   -0.0162655     0.0361026    0.0279472  -0.0383462   -0.0139606   -0.00127985  -0.0641989   -0.0813398   -0.0559106    0.0938809   0.101894      0.219499    -0.0490454   -0.191345  
  0.0146055   -0.0442816    0.0729842   -0.0145391    0.189476     0.0613293    0.0423336     0.136729      0.125821     0.0753857     0.0315367   0.0356769    0.0176525    -0.0225597   -0.0499068   0.01554     -0.162549     0.0287344   -0.0672118    0.0403823   -0.231925     0.11124     0.0454559     6.40858e-5   0.0660632    0.102747  
  0.0600702   -0.0433041    0.159135    -0.0392479   -0.0028472    0.0401914    0.120028     -0.0881712     0.0756431   -0.0821933    -0.0729955  -0.0821213   -0.0774514    -0.0366849    0.0278152  -0.0290999    0.0758773   -0.0637417   -0.0458063    0.00650107  -0.00675472   0.0577884  -0.103275      0.0309573   -0.098563     0.0147553 
  0.00546937  -0.0991844    0.113529     0.150229     0.170314    -0.119001    -0.127929     -0.00245683   -0.0854753   -0.102084      0.0797823  -0.121594     0.0100194    -0.0411807    0.0872122  -0.0452683    0.0749452   -0.10324      0.0581068    0.018339    -0.112906    -0.0373356   0.0349598    -0.00109541   0.00673005  -0.00770163
 -0.187619     0.251184     0.0427269    0.140476     0.0188268   -0.152371    -0.0992977     0.044193      0.0577898    0.269703      0.0439559  -0.0445511   -0.0389344     0.0252958    0.0172886   0.0545914    0.210314     0.00343938   0.0680525   -0.0967146    0.0996563   -0.0184346  -0.0449311    -0.0606421   -0.0828279   -0.123924  
  0.0786318    0.0293713   -0.0217908   -0.0214044   -0.046562     0.103978    -0.0846344     0.109172      0.0298999    0.00252187   -0.0484688  -0.119714    -0.000976276   0.0299695    0.0583207  -0.00443577  -0.0419578   -0.0482817   -0.00356828   0.0631191    0.0394984   -0.0799139   0.0497467    -0.0655048    0.0948293   -0.0804883 
  0.0735015    0.00221417  -0.00749555  -0.00355167  -0.106055    -0.0293156    0.0205136    -0.0172862    -0.149159    -0.0916755    -0.184119    0.0266299   -0.0454815    -0.0744775   -0.0477693  -0.0265551   -0.130355    -0.0654334   -0.00789627   0.0067297    0.0688649    0.0363862  -0.0468688    -0.0184523    0.004608    -0.101397  
 -0.172634     0.055297     0.0317773    0.0489598   -0.0319204    0.0310588   -0.144431     -0.103008      0.00044187  -0.115724      0.0731843  -0.0255389    0.146688     -0.0418788    0.0574057  -0.0228686   -0.153838     0.0873473    0.0523613   -0.0268931    0.0681219   -0.243097   -0.0138843     0.0509947    0.0119137    0.167672  
 -0.0695783    0.205025     0.119207     0.0420849    0.200803    -0.02263     -0.00871749    0.0746121     0.0889864    0.0730544     0.0999513  -0.138762    -0.0805242     0.0571849   -0.129675    0.0600902   -0.230048     0.073631     0.130432     0.0221324   -0.0537221    0.0476624  -0.0375331     0.0825689    0.0187018   -0.00384627
 -0.0143919   -0.0721401   -0.0462047    0.03404      0.149026    -0.0377506   -0.0438314    -0.0052553     0.0102159   -0.00222127   -0.0127472   0.0801447   -0.0832714    -0.00395587  -0.0680371  -0.0426825   -0.0818936    0.0223169    0.0976814   -0.00906434  -0.054939    -0.0252903  -0.00917881    0.0826262   -0.0290752   -0.0273879 
  0.125652     0.0137458   -0.173664     0.0954353    0.0947136    0.0273444    0.0199892    -0.163464      0.0657853   -0.00559747   -0.0447465   0.128808    -0.135376     -0.130742    -0.0492223  -0.0472179   -0.089009    -0.0248571    0.0672538   -0.12178     -0.101365    -0.0970453   0.00186314    0.0874207   -0.0679371   -0.0663362 
 -0.109725    -0.189064    -0.159395     0.0158603   -0.119725     0.0423273    0.105105      0.158601     -0.045969     0.153453     -0.0586043   0.0368242    0.0743118     0.0267193   -0.134418   -0.0584896   -0.0259974   -0.00279623  -0.0453967   -0.0329099    0.087956     0.0337054   0.0817479    -0.0586082   -0.0724612    0.0316243 
  0.00402017  -0.0813249   -0.0375185    0.0964519    0.0941975    0.0658346   -0.00399751    0.0287231    -0.0453172    0.0516044     0.0205816   0.0245113    0.0548432     0.0199645   -0.130058   -0.0142797    0.0340916   -0.0818573   -0.0264662    0.0152429    0.134544     0.07276    -0.008116      0.0659082    0.0152194   -0.034693  
  0.0144332   -0.00449678  -0.0601751    0.0195837    0.00245633  -0.0693772   -0.00147127   -0.083303     -0.075251     0.000299933  -0.059286    0.00261457   0.0413716     0.0678524   -0.0275416  -0.0123676    0.00259327  -0.207628    -0.232815     0.169801     0.0888428    0.0936547   0.000605988   0.0941443    0.0259809   -0.015991  
  0.0974657   -0.135614     0.0451046   -0.0271765    0.00297052   0.117676    -0.110714      0.0233355     0.0681719   -0.0487347     0.0371623   0.279186    -0.0299635    -0.204138    -0.108184    0.0235172   -0.032799     0.0506721    0.227323    -0.00753415   0.0370817    0.0265041   0.00127955    0.0570233   -0.0778135    0.0664742 
 -0.231537    -0.0457531    0.195449     0.0953227    0.0989802   -0.0355181   -0.0543394    -0.0346648    -0.0487082   -0.208685     -0.0142862   0.251346    -0.108241     -0.0425863   -0.171376   -0.0861914   -0.240069     0.121158     0.0166577    0.0385438   -0.0194144   -0.0299294   0.0350064     0.0182327    0.0425599   -0.00933359
 -0.0708517    0.0736773    0.0225196   -0.056116    -0.0424381   -0.0593038   -0.000431362   0.120704      0.127564     0.124491      0.120029    0.00779873   0.116575     -0.109527     0.0273018  -0.00998401   0.0219622   -0.13312     -0.163822    -0.0354029   -0.0264453    0.0370433   0.0859271    -0.048364    -0.0179723   -0.0879437 
  0.0146737    0.0013979    0.0371581   -0.0800195    0.0635923    0.0497144    0.0873574    -0.103915     -0.0170365    0.0721004     0.0292189  -0.0413232    0.0989171     0.0686672   -0.105399    0.0983653    0.0658669    0.097956    -0.0571333    0.139591     0.125855    -0.0560319  -0.127871     -0.127035    -0.0911835    0.0882064 
 -0.044145     0.209147    -0.00912589  -0.0152884   -0.132795    -0.142894     0.0461586    -0.0792923    -0.106657    -0.121848     -0.22939     0.0243417   -0.00940493   -0.0107187    0.0449194   0.064514    -0.197563    -0.0449364    0.0848296    0.00831673   0.0453617    0.0751015  -0.151641      0.0229205    0.0380329    0.0589437 
 -0.0456012   -0.0365162    0.0418802   -0.0202253   -0.0581533    0.0563043    0.00896401   -0.00175079    0.0182464    0.0373564     0.0129556  -0.130938     0.0400983    -0.0103281    0.0499427   0.0059568   -0.0487438    0.0378302    0.00496298   0.00521054  -0.010831     0.0241248   0.00900674    0.0968023   -0.0849453   -0.0519133 
 -0.0828626   -0.117725     0.0340708   -0.0139431   -0.0744996    0.032701     0.00604146   -0.10501      -0.0677656   -0.0971354    -0.0367532  -0.0294893    0.00680074   -0.0476717    0.0189963  -0.0768349   -0.069837    -0.0869491   -0.0899713    0.0472582    0.132934    -0.141453    0.161223     -0.164474     0.135999     0.0282121 
 -0.0989894    0.0246922    0.00759764  -0.0273361   -0.00732801   0.0140282   -0.102783      0.10291       0.00606072   0.0921836     0.263917   -0.0723099   -0.0501693    -0.160491     0.184029   -0.116015    -0.0104682    0.0269264   -0.16947     -0.0921136    0.0192328   -0.115377   -0.0401361     0.0317019   -0.118383    -0.0430007 
 -0.129425    -0.0265115    0.0774927    0.0312773    0.0495525    0.00606128  -0.0678398    -0.103212      0.0120355   -0.167047      0.0693353  -0.123125     0.131435     -0.0256727    0.0470848  -0.0351806   -0.256893     0.0739186    0.0233038   -0.00851669   0.0362229   -0.355816   -0.0267362     0.0401535    0.0301805    0.0387405 
  0.0782487   -0.0329866   -0.0172907    0.187159    -0.0315945   -0.0496105    0.12259       0.0592376    -0.0776546   -0.184567      0.1535     -0.176891     0.0830091     0.0667309   -0.040678    0.185366    -0.00775526  -0.0308875    0.0759642   -0.0522339   -0.234982    -0.0945864   0.0668919    -0.026471    -0.0252764    0.0927123 
  0.0192134   -0.120033     0.112011    -0.0473124   -0.023522     0.168523    -0.0827244    -0.000551802   0.0312672   -0.156022      0.0816351   0.0159922    0.178304     -0.0728544   -0.0836361   0.0921049    0.0889999   -0.0356498   -0.0216568    0.124819    -0.0485124   -0.0947678   0.160151     -0.0920856   -0.231724     0.125444  
 -0.0585533    0.0606158   -0.0684854   -0.0631939   -0.106566    -0.236266     0.0154631    -0.119359     -0.106894     0.0755178    -0.109578   -0.17101     -0.109933      0.00352013  -0.145461   -0.0163555   -0.253763     0.0685536   -0.039739     0.105863     0.0260972    0.144999    0.0651339     0.057965     0.0930956   -0.208461  
 -0.0604654   -0.245473    -0.00797618   0.102284    -0.0396024    0.0200484    0.000422777   0.204967      0.128129    -0.118514     -0.282109   -0.0645681    0.190355      0.0793731    0.0365235  -0.0201303    0.0138018   -0.158464    -0.0242716   -0.0364181    0.0796849   -0.259721   -0.0171581     0.22596      0.176548     0.0388602 
  0.103521     0.0263787   -0.232472    -0.226368     0.0379328    0.094406    -0.140117     -0.0745122    -0.0254463    0.0684402     0.0807285   0.154924     0.00209573    0.0181598    0.023539   -0.10798     -0.0744163    0.120152     0.0486577    0.0986905    0.192369     0.025353    0.124774      0.112516     0.0993928    0.0292018 
  0.0213409   -0.065146     0.148393    -0.145289    -0.0503829   -0.19234      0.210313     -0.0439459    -0.0514159   -0.0218247     0.0697154  -0.0189087   -0.0512598     0.00716863   0.0495875   0.112467    -0.120896     0.0383555    0.00197517   0.141115     0.098742     0.0737645  -0.00633653   -0.0997064   -0.0620965   -0.114436  
  0.0421929   -0.121572    -0.12452     -0.0312424   -0.0866155   -0.175934     0.0422448    -0.0674018     0.126462    -0.152047     -0.156308   -0.0131038   -0.0951145    -0.0584954    0.0804833  -0.0383397    0.0218862    0.157039    -0.0760115    0.0935296    0.126455     0.0929665   0.0777653    -0.0346662    0.0126227    0.165392  
  0.0123999   -0.0784463    0.0642831    0.0857832   -0.00134025   0.194093     0.269069      0.0698497    -0.111476    -0.11377       0.11574    -0.068096    -0.0112046    -0.0633093   -0.0731643   0.0776549   -0.00290548  -0.118663    -0.20325      0.0336709    0.186914     0.142822    0.0936627     0.0349936   -0.0576811   -0.0523748 
 -0.146924     0.0778311   -0.124054    -0.030852    -0.0425503    0.156255     0.0097589    -0.146863      0.0923852   -0.0364355     0.0912196   0.00294814  -0.0665211    -0.0515469   -0.115741   -0.00984882   0.0113941    0.0143678   -0.0411903   -0.031802     0.0204254   -0.14844    -0.0920196    -0.0421625   -0.0666857    0.135609  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.085105
WARNING: Variances had to be floored 4 5 7 9 13 15 26 27
INFO: iteration 2, average log likelihood -1.017793
WARNING: Variances had to be floored 10 16 17 19 23 27
INFO: iteration 3, average log likelihood -1.036549
WARNING: Variances had to be floored 4 5 8 9 12 13 15 26
INFO: iteration 4, average log likelihood -1.025305
WARNING: Variances had to be floored 7 16 27
INFO: iteration 5, average log likelihood -1.040277
WARNING: Variances had to be floored 4 5 9 10 13 15 17 19 26
INFO: iteration 6, average log likelihood -1.023910
WARNING: Variances had to be floored 11 12 16 23 27
INFO: iteration 7, average log likelihood -1.033659
WARNING: Variances had to be floored 4 5 7 8 9 13 15 26
INFO: iteration 8, average log likelihood -1.030918
WARNING: Variances had to be floored 10 17 27
INFO: iteration 9, average log likelihood -1.044790
WARNING: Variances had to be floored 4 5 9 12 13 15 16 19 23 26 27
INFO: iteration 10, average log likelihood -1.015063
INFO: EM with 100000 data points 10 iterations avll -1.015063
59.0 data points per parameter
32Ã—26 Array{Float64,2}:
  0.155025    -0.0945912   -0.133448    -0.0839497    0.176134    -0.0716321   -0.111838   -0.100902     0.0437613   -0.130714     0.0675536   -0.00177405   0.0257569   0.195911    -0.113218   -0.110442   -0.0533363    -0.0199786    0.048733      0.0928047   -0.0715256   0.0531176     0.0309247    0.204129    -0.0387918   -0.175127   
 -0.10695     -0.0605666    0.111312    -0.0872102   -0.0580719    0.059532    -0.0318497  -0.0355583    0.199729     0.0632868    0.161836     0.104018    -0.115368    0.122959     0.115217   -0.0169775   0.112631      0.02272      0.0708551     0.0621289    0.171055   -0.0936016     0.105595    -0.183974     0.018353    -0.0200771  
  0.0199646   -0.0609586    0.0943543   -0.193108     0.156923    -0.0703396    0.0963254  -0.0469047   -0.109938    -0.151058     0.012642     0.104134     0.146723   -0.0876962    0.143206    0.143492   -0.0996064    -0.139696    -0.135178      0.0271528   -0.0387946  -0.0633049    -0.00161822  -0.0800084    0.106654     0.04349    
 -0.0905981    0.066698     0.0528395   -0.141041    -0.128161     0.114094    -0.0953666   0.0550753    0.119369     0.00255246   0.150075    -0.0051873    0.297731    0.123762    -0.10632     0.0595876  -0.0183185    -0.00127608  -0.0551327     0.239777     0.0146227  -0.0261422    -0.00331848  -0.0253821   -0.0913444   -0.13451    
 -0.0546751   -0.242673     0.0774347    0.057535    -0.227654     0.0717222   -0.04897    -0.0894964    0.0338252   -0.0883897    0.052111     0.0763783   -0.114239   -0.158438    -0.0132218  -0.109527   -0.0508045    -0.0448234    0.00982709    0.0122862   -0.101303    0.00936277    0.0624103    0.0392718   -0.0502253    0.0411928  
 -0.0419191   -0.0655568    0.0724529   -0.0341105    0.197991     0.0133182    0.0217586   0.154807     0.059515    -0.00489052   0.0379468   -0.0143189   -0.121848    0.170873    -0.0435643   0.0540604  -0.15673       0.202416    -0.0417481     0.0374499    0.157502    0.0220428     0.141058    -0.0659174    0.0262203   -0.0886517  
 -0.0740546    0.0710585    0.122806     0.0640843    0.0423455    0.09586     -0.120574   -0.0498564   -0.184906     0.10293      0.116382    -0.114114     0.22039     0.0838964    0.0408612   0.0306517  -0.0692205     0.0549776   -0.129527      0.0181493   -0.021136   -0.136147      0.027183    -0.110059    -0.166146    -0.0742969  
 -0.0107233   -0.140169    -0.0133368    0.0116392   -0.00869758   0.196122    -0.144301   -0.0236355    0.0868559   -0.0514977    0.075127     0.226809    -0.0344482  -0.0267811    0.0782478  -0.0805328  -0.0213976    -0.0104987   -0.151541     -0.0538278    0.060826   -0.023592     -0.070917    -0.045035    -0.0177007    0.0145124  
 -0.105784     0.0523733    0.0632276    0.0517464   -0.0469729    0.0723542   -0.0232174  -0.0714621    0.034743     0.103328     0.0282841    0.0462748    0.0245431  -0.004655     0.0335476  -0.0969688  -0.0790015     0.0252074   -0.145832      0.0419632    0.0220231   0.0181695     0.00493973   0.159263     0.0292646    0.0162496  
  0.0955688    0.121407     0.100154    -0.0991367   -0.274306    -0.162611     0.101217   -0.0301239    0.0566354   -0.128297     0.0458756    0.184084     0.0967184   0.015084    -0.0498289  -0.134355    0.086138     -0.00866338  -0.0860989    -0.198633     0.05051     0.047154      0.0771933   -0.0595995   -0.161371    -0.0679217  
 -0.0640175    0.138823     0.0992883    0.0716778    0.0457652   -0.00184766   0.0312179   0.0892304    0.0975297    0.0979967    0.151984     0.0638574    0.089761    0.114504    -0.1624      0.0606691   0.0526844    -0.196322    -0.0542601    -0.0447299   -0.0154198   0.176812     -0.101615     0.0277253   -0.0603734    0.0779601  
  0.00800669  -0.0332618    0.00795939  -0.100602    -0.00291033  -0.0120462    0.0522066  -0.0541857    0.130326    -0.0444105    0.118125    -0.176458    -0.233884    0.00912546   0.110307    0.0588793  -0.180336      0.0438682   -0.221158     -0.0544183    0.141875    0.177414      0.1041       0.191488     0.116445     0.0431246  
 -0.00285731   0.214887     0.13838     -0.0955594    0.0235068    0.0283511   -0.0288774   0.203425     0.0217097    0.275715    -0.0317285    0.0410508    0.132817   -0.0869742    0.144014    0.0278587  -0.208148     -0.03491     -0.0355291    -0.0383836    0.160834   -0.0683995     0.265735     0.0907243    0.0595358    0.0284559  
  0.0718597    0.0864818   -0.0233645   -0.0728621   -0.0484199    0.130285     0.12572    -0.0302434    0.0743128   -0.111557    -0.0527302   -0.0645717    0.0688659   0.0241526    0.0170223   0.149969   -0.0756328     0.00713395  -0.0682478    -0.109896     0.0154223   0.0346046     0.163592    -0.00750891  -0.0367575   -0.00180025 
  0.277402    -0.0158659    0.13935     -0.0662042   -0.0633557    0.0532908   -0.154419    0.00443412   0.145712    -0.123134     0.0353238   -0.120408     0.0750308  -0.0372888   -0.0570666   0.158247    0.10304       0.0839569    0.164936      0.0775306   -0.120762    0.127454     -0.0880092    0.241323    -0.00811852   0.00475063 
  0.0922966    0.012828    -0.0202861    0.112766     0.00370567  -0.179175     0.147994    0.0216965   -0.175061     0.00278539   0.0708946    0.0786688   -0.104476    0.107888    -0.0684283   0.0509667  -0.277791     -0.0399556    0.263401     -0.108495     0.110656   -0.0826735    -0.11854     -0.22956      0.221742     0.074291   
  0.0845705    0.125072     0.149142     0.113156    -0.0733889   -0.0199957   -0.0491702  -0.153383    -0.138185    -0.0359894    0.0258447   -0.215344    -0.0727313  -0.0484114   -0.0362969   0.0262917  -0.000104479   0.00966891  -0.0279777    -0.108854    -0.0355811   0.00360268   -0.0631842    0.0327635   -0.0350493   -0.0162321  
  0.0779745    0.0562084    0.131861    -0.0344717   -0.0173602    0.087465    -0.050024   -0.120656    -0.0508304   -0.0142988    0.112718    -0.06479     -0.035386   -0.0866407    0.269371    0.070633   -0.0556506    -0.0413831    0.129168      0.133459    -0.0247102   0.163135     -0.0402831   -0.0834305    0.242267     0.005144   
 -0.177508    -0.172842    -0.0161336    0.196305     0.166709     0.00423223   0.0565566  -0.0208226   -0.0204628   -0.0941718    0.0558632    0.0789087    0.0133324   0.0804417    0.0199929  -0.0841543   0.176436     -0.237393    -0.0137615     0.22177      0.125509   -0.000781756  -0.0417946    0.0642627   -0.0557431   -0.0555246  
 -0.0144365    0.139413     0.15191     -0.0261979   -0.0475943   -0.125718     0.0146361  -0.0163319    0.0425085    0.219378    -0.090146    -0.0655719    0.0603556  -0.0243875    0.0913258  -0.0853774  -0.177681      0.0502951   -0.00683003    0.12669     -0.012412   -0.0511976    -0.0361574   -0.0334818    0.00950403  -0.0243581  
 -0.198054     0.165739    -0.0204032   -0.0406037   -0.0770561    0.0720092    0.0102484  -0.0879363    0.0688109   -0.0467998   -0.163715    -0.124418    -0.162616    0.0490331   -0.124458    0.161916    0.0144129     0.145708    -0.155286     -0.0452906    0.0612959   0.074467     -0.0438987   -0.0339197   -0.0493208    0.0257337  
  0.0192104   -0.184438    -0.0471       0.183484     0.0981853   -0.0357911   -0.0227539   0.0533471    0.00981303   0.00270837  -0.0243281    0.0245589    0.0185663  -0.0971698   -0.107014    0.16054    -0.164896      0.0844179   -0.000748443  -0.00250849  -0.0995034   0.0678915     0.0318421    0.110531    -0.0357929   -0.0898428  
  0.0274768   -0.00524688   0.136704    -0.0516781   -0.0652914   -0.118794     0.100528    0.0628783   -0.0972315   -0.106275     0.133345     0.0272517   -0.0640567  -0.0968587   -0.0480331   0.0441227  -0.132975      0.045874    -0.0248903     0.00577283   0.15808    -0.0216534     0.137732     0.071097     0.0725714   -0.0595975  
 -0.164539     0.087449    -0.0910487    0.226759    -0.0522201   -0.054401    -0.017869    0.0542606    0.131585    -0.058536    -0.0632429   -0.0614064    0.0370358   0.0200643    0.0895179   0.0478388   0.00736651   -0.213357     0.0475142    -0.0669762    0.126701    0.0546194     0.0668102   -0.133642    -0.082358     0.022552   
  0.00298574   0.0853304    0.0682714    0.0920278    0.0612994   -0.159964    -0.0863634  -0.108771    -0.163422    -0.0594646    0.0129973    0.196934    -0.0837362  -0.052851    -0.187822    0.0579465   0.182669     -0.220333     0.0452766    -0.0576918   -0.0469666  -0.0277476    -0.0460098    0.130867    -0.090051    -0.114521   
 -0.0422131    0.0781147    0.0864616   -0.238951     0.0029475   -0.00347844  -0.198559    0.0356545   -0.0236241    0.0776922   -0.119188     0.0714617    0.0349571   0.0488259    0.063594    0.0589012  -0.0366259     0.0753732    0.0162825     0.035296     0.0462134   0.0608273     0.135825    -0.199361    -0.0703342    0.0237687  
 -0.13165      0.126763    -0.0447923    0.0275253    0.0268069    0.0547819   -0.120961    0.0340304   -0.14632     -0.0260958    0.164386     0.107835     0.0868411   0.113156     0.316339    0.0793458  -0.0864866    -0.254039     0.0903465     0.1593      -0.0554191   0.099952      0.142807    -0.301539     0.0125214    0.00186802 
 -0.132        0.0736696    0.0312992    0.0733409   -0.00451126   0.128878     0.0591301  -0.119862     0.103742    -0.0944791    0.119317     0.141804    -0.0380471  -0.12304      0.089666    0.0611296   0.105307      0.0126103   -0.0731382    -0.00209105  -0.107328    0.00507297    0.00667791  -0.10152     -0.0125829   -0.121036   
 -0.0244298   -0.0770591   -0.0354953    0.0629165   -0.111716     0.0437523    0.0544746  -0.212722    -0.156753     0.0941078   -0.108708     0.0174077   -0.119798   -0.165028    -0.0495119  -0.104877    0.037235     -0.16283     -0.016231     -0.0422861    0.148868    0.0203558    -0.0285201    0.181063    -0.0688666   -0.0945902  
  0.166441     0.0861284    0.164746    -0.133243     0.159138     0.217995    -0.0586021  -0.0101714    0.0988467   -0.145758    -0.00651259  -0.0670861   -0.020112    0.0611556    0.114748   -0.0406694  -0.0193156    -0.0104659    0.0790897    -0.0462426    0.155548    0.13631       0.0678093   -0.0483719    0.0132803   -0.0439735  
 -0.0455039   -0.150769    -0.144124     0.00432955  -0.0239104   -0.0270645   -0.045279    0.0197578    0.0754165   -0.0941693   -0.0140953   -0.0201657   -0.0228377   0.00830145  -0.0914018  -0.0413717  -0.118759      0.0637807    0.00429541    0.145377    -0.0973258   0.000202401  -0.0854386   -0.0560209    0.0772579    0.000503679
  0.134479    -0.0833429    0.10427      0.0275935    0.0271097    0.0365241   -0.243241    0.0130836   -0.0318978   -0.0650122   -0.0317317    0.0815344    0.0807133  -0.0897338    0.0837804   0.128837   -0.117871      0.16661      0.0687844    -0.110624     0.0552179  -0.0235018     0.164046     0.114979    -0.196177     0.0596496  kind full, method split
0: avll = -1.425309830524776
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.425330
INFO: iteration 2, average log likelihood -1.425252
INFO: iteration 3, average log likelihood -1.425205
INFO: iteration 4, average log likelihood -1.425158
INFO: iteration 5, average log likelihood -1.425105
INFO: iteration 6, average log likelihood -1.425045
INFO: iteration 7, average log likelihood -1.424973
INFO: iteration 8, average log likelihood -1.424878
INFO: iteration 9, average log likelihood -1.424727
INFO: iteration 10, average log likelihood -1.424443
INFO: iteration 11, average log likelihood -1.423894
INFO: iteration 12, average log likelihood -1.422966
INFO: iteration 13, average log likelihood -1.421802
INFO: iteration 14, average log likelihood -1.420831
INFO: iteration 15, average log likelihood -1.420288
INFO: iteration 16, average log likelihood -1.420057
INFO: iteration 17, average log likelihood -1.419968
INFO: iteration 18, average log likelihood -1.419934
INFO: iteration 19, average log likelihood -1.419922
INFO: iteration 20, average log likelihood -1.419916
INFO: iteration 21, average log likelihood -1.419914
INFO: iteration 22, average log likelihood -1.419913
INFO: iteration 23, average log likelihood -1.419912
INFO: iteration 24, average log likelihood -1.419912
INFO: iteration 25, average log likelihood -1.419912
INFO: iteration 26, average log likelihood -1.419911
INFO: iteration 27, average log likelihood -1.419911
INFO: iteration 28, average log likelihood -1.419911
INFO: iteration 29, average log likelihood -1.419911
INFO: iteration 30, average log likelihood -1.419911
INFO: iteration 31, average log likelihood -1.419911
INFO: iteration 32, average log likelihood -1.419910
INFO: iteration 33, average log likelihood -1.419910
INFO: iteration 34, average log likelihood -1.419910
INFO: iteration 35, average log likelihood -1.419910
INFO: iteration 36, average log likelihood -1.419910
INFO: iteration 37, average log likelihood -1.419910
INFO: iteration 38, average log likelihood -1.419910
INFO: iteration 39, average log likelihood -1.419910
INFO: iteration 40, average log likelihood -1.419910
INFO: iteration 41, average log likelihood -1.419910
INFO: iteration 42, average log likelihood -1.419910
INFO: iteration 43, average log likelihood -1.419910
INFO: iteration 44, average log likelihood -1.419910
INFO: iteration 45, average log likelihood -1.419910
INFO: iteration 46, average log likelihood -1.419910
INFO: iteration 47, average log likelihood -1.419910
INFO: iteration 48, average log likelihood -1.419910
INFO: iteration 49, average log likelihood -1.419910
INFO: iteration 50, average log likelihood -1.419910
INFO: EM with 100000 data points 50 iterations avll -1.419910
952.4 data points per parameter
1: avll = [-1.42533,-1.42525,-1.4252,-1.42516,-1.42511,-1.42505,-1.42497,-1.42488,-1.42473,-1.42444,-1.42389,-1.42297,-1.4218,-1.42083,-1.42029,-1.42006,-1.41997,-1.41993,-1.41992,-1.41992,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.419929
INFO: iteration 2, average log likelihood -1.419849
INFO: iteration 3, average log likelihood -1.419801
INFO: iteration 4, average log likelihood -1.419752
INFO: iteration 5, average log likelihood -1.419698
INFO: iteration 6, average log likelihood -1.419639
INFO: iteration 7, average log likelihood -1.419576
INFO: iteration 8, average log likelihood -1.419514
INFO: iteration 9, average log likelihood -1.419456
INFO: iteration 10, average log likelihood -1.419405
INFO: iteration 11, average log likelihood -1.419363
INFO: iteration 12, average log likelihood -1.419329
INFO: iteration 13, average log likelihood -1.419302
INFO: iteration 14, average log likelihood -1.419281
INFO: iteration 15, average log likelihood -1.419263
INFO: iteration 16, average log likelihood -1.419249
INFO: iteration 17, average log likelihood -1.419235
INFO: iteration 18, average log likelihood -1.419223
INFO: iteration 19, average log likelihood -1.419212
INFO: iteration 20, average log likelihood -1.419201
INFO: iteration 21, average log likelihood -1.419191
INFO: iteration 22, average log likelihood -1.419182
INFO: iteration 23, average log likelihood -1.419173
INFO: iteration 24, average log likelihood -1.419164
INFO: iteration 25, average log likelihood -1.419157
INFO: iteration 26, average log likelihood -1.419150
INFO: iteration 27, average log likelihood -1.419143
INFO: iteration 28, average log likelihood -1.419138
INFO: iteration 29, average log likelihood -1.419133
INFO: iteration 30, average log likelihood -1.419128
INFO: iteration 31, average log likelihood -1.419124
INFO: iteration 32, average log likelihood -1.419121
INFO: iteration 33, average log likelihood -1.419118
INFO: iteration 34, average log likelihood -1.419115
INFO: iteration 35, average log likelihood -1.419112
INFO: iteration 36, average log likelihood -1.419110
INFO: iteration 37, average log likelihood -1.419107
INFO: iteration 38, average log likelihood -1.419105
INFO: iteration 39, average log likelihood -1.419103
INFO: iteration 40, average log likelihood -1.419101
INFO: iteration 41, average log likelihood -1.419098
INFO: iteration 42, average log likelihood -1.419096
INFO: iteration 43, average log likelihood -1.419094
INFO: iteration 44, average log likelihood -1.419092
INFO: iteration 45, average log likelihood -1.419090
INFO: iteration 46, average log likelihood -1.419088
INFO: iteration 47, average log likelihood -1.419085
INFO: iteration 48, average log likelihood -1.419083
INFO: iteration 49, average log likelihood -1.419081
INFO: iteration 50, average log likelihood -1.419079
INFO: EM with 100000 data points 50 iterations avll -1.419079
473.9 data points per parameter
2: avll = [-1.41993,-1.41985,-1.4198,-1.41975,-1.4197,-1.41964,-1.41958,-1.41951,-1.41946,-1.41941,-1.41936,-1.41933,-1.4193,-1.41928,-1.41926,-1.41925,-1.41924,-1.41922,-1.41921,-1.4192,-1.41919,-1.41918,-1.41917,-1.41916,-1.41916,-1.41915,-1.41914,-1.41914,-1.41913,-1.41913,-1.41912,-1.41912,-1.41912,-1.41911,-1.41911,-1.41911,-1.41911,-1.41911,-1.4191,-1.4191,-1.4191,-1.4191,-1.41909,-1.41909,-1.41909,-1.41909,-1.41909,-1.41908,-1.41908,-1.41908]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.419087
INFO: iteration 2, average log likelihood -1.419019
INFO: iteration 3, average log likelihood -1.418956
INFO: iteration 4, average log likelihood -1.418880
INFO: iteration 5, average log likelihood -1.418787
INFO: iteration 6, average log likelihood -1.418675
INFO: iteration 7, average log likelihood -1.418552
INFO: iteration 8, average log likelihood -1.418431
INFO: iteration 9, average log likelihood -1.418322
INFO: iteration 10, average log likelihood -1.418233
INFO: iteration 11, average log likelihood -1.418163
INFO: iteration 12, average log likelihood -1.418109
INFO: iteration 13, average log likelihood -1.418067
INFO: iteration 14, average log likelihood -1.418033
INFO: iteration 15, average log likelihood -1.418006
INFO: iteration 16, average log likelihood -1.417984
INFO: iteration 17, average log likelihood -1.417965
INFO: iteration 18, average log likelihood -1.417949
INFO: iteration 19, average log likelihood -1.417935
INFO: iteration 20, average log likelihood -1.417923
INFO: iteration 21, average log likelihood -1.417912
INFO: iteration 22, average log likelihood -1.417902
INFO: iteration 23, average log likelihood -1.417893
INFO: iteration 24, average log likelihood -1.417885
INFO: iteration 25, average log likelihood -1.417877
INFO: iteration 26, average log likelihood -1.417869
INFO: iteration 27, average log likelihood -1.417862
INFO: iteration 28, average log likelihood -1.417854
INFO: iteration 29, average log likelihood -1.417846
INFO: iteration 30, average log likelihood -1.417839
INFO: iteration 31, average log likelihood -1.417831
INFO: iteration 32, average log likelihood -1.417823
INFO: iteration 33, average log likelihood -1.417816
INFO: iteration 34, average log likelihood -1.417808
INFO: iteration 35, average log likelihood -1.417799
INFO: iteration 36, average log likelihood -1.417791
INFO: iteration 37, average log likelihood -1.417783
INFO: iteration 38, average log likelihood -1.417774
INFO: iteration 39, average log likelihood -1.417766
INFO: iteration 40, average log likelihood -1.417758
INFO: iteration 41, average log likelihood -1.417749
INFO: iteration 42, average log likelihood -1.417741
INFO: iteration 43, average log likelihood -1.417732
INFO: iteration 44, average log likelihood -1.417724
INFO: iteration 45, average log likelihood -1.417716
INFO: iteration 46, average log likelihood -1.417708
INFO: iteration 47, average log likelihood -1.417700
INFO: iteration 48, average log likelihood -1.417692
INFO: iteration 49, average log likelihood -1.417685
INFO: iteration 50, average log likelihood -1.417677
INFO: EM with 100000 data points 50 iterations avll -1.417677
236.4 data points per parameter
3: avll = [-1.41909,-1.41902,-1.41896,-1.41888,-1.41879,-1.41867,-1.41855,-1.41843,-1.41832,-1.41823,-1.41816,-1.41811,-1.41807,-1.41803,-1.41801,-1.41798,-1.41796,-1.41795,-1.41793,-1.41792,-1.41791,-1.4179,-1.41789,-1.41788,-1.41788,-1.41787,-1.41786,-1.41785,-1.41785,-1.41784,-1.41783,-1.41782,-1.41782,-1.41781,-1.4178,-1.41779,-1.41778,-1.41777,-1.41777,-1.41776,-1.41775,-1.41774,-1.41773,-1.41772,-1.41772,-1.41771,-1.4177,-1.41769,-1.41768,-1.41768]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.417677
INFO: iteration 2, average log likelihood -1.417619
INFO: iteration 3, average log likelihood -1.417567
INFO: iteration 4, average log likelihood -1.417508
INFO: iteration 5, average log likelihood -1.417438
INFO: iteration 6, average log likelihood -1.417354
INFO: iteration 7, average log likelihood -1.417256
INFO: iteration 8, average log likelihood -1.417149
INFO: iteration 9, average log likelihood -1.417038
INFO: iteration 10, average log likelihood -1.416930
INFO: iteration 11, average log likelihood -1.416829
INFO: iteration 12, average log likelihood -1.416737
INFO: iteration 13, average log likelihood -1.416654
INFO: iteration 14, average log likelihood -1.416578
INFO: iteration 15, average log likelihood -1.416511
INFO: iteration 16, average log likelihood -1.416451
INFO: iteration 17, average log likelihood -1.416399
INFO: iteration 18, average log likelihood -1.416353
INFO: iteration 19, average log likelihood -1.416314
INFO: iteration 20, average log likelihood -1.416280
INFO: iteration 21, average log likelihood -1.416250
INFO: iteration 22, average log likelihood -1.416224
INFO: iteration 23, average log likelihood -1.416201
INFO: iteration 24, average log likelihood -1.416180
INFO: iteration 25, average log likelihood -1.416161
INFO: iteration 26, average log likelihood -1.416144
INFO: iteration 27, average log likelihood -1.416128
INFO: iteration 28, average log likelihood -1.416113
INFO: iteration 29, average log likelihood -1.416098
INFO: iteration 30, average log likelihood -1.416085
INFO: iteration 31, average log likelihood -1.416072
INFO: iteration 32, average log likelihood -1.416059
INFO: iteration 33, average log likelihood -1.416046
INFO: iteration 34, average log likelihood -1.416034
INFO: iteration 35, average log likelihood -1.416022
INFO: iteration 36, average log likelihood -1.416011
INFO: iteration 37, average log likelihood -1.415999
INFO: iteration 38, average log likelihood -1.415988
INFO: iteration 39, average log likelihood -1.415977
INFO: iteration 40, average log likelihood -1.415966
INFO: iteration 41, average log likelihood -1.415955
INFO: iteration 42, average log likelihood -1.415945
INFO: iteration 43, average log likelihood -1.415935
INFO: iteration 44, average log likelihood -1.415925
INFO: iteration 45, average log likelihood -1.415915
INFO: iteration 46, average log likelihood -1.415905
INFO: iteration 47, average log likelihood -1.415896
INFO: iteration 48, average log likelihood -1.415887
INFO: iteration 49, average log likelihood -1.415878
INFO: iteration 50, average log likelihood -1.415870
INFO: EM with 100000 data points 50 iterations avll -1.415870
118.1 data points per parameter
4: avll = [-1.41768,-1.41762,-1.41757,-1.41751,-1.41744,-1.41735,-1.41726,-1.41715,-1.41704,-1.41693,-1.41683,-1.41674,-1.41665,-1.41658,-1.41651,-1.41645,-1.4164,-1.41635,-1.41631,-1.41628,-1.41625,-1.41622,-1.4162,-1.41618,-1.41616,-1.41614,-1.41613,-1.41611,-1.4161,-1.41608,-1.41607,-1.41606,-1.41605,-1.41603,-1.41602,-1.41601,-1.416,-1.41599,-1.41598,-1.41597,-1.41596,-1.41594,-1.41593,-1.41592,-1.41591,-1.41591,-1.4159,-1.41589,-1.41588,-1.41587]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.415870
INFO: iteration 2, average log likelihood -1.415811
INFO: iteration 3, average log likelihood -1.415756
INFO: iteration 4, average log likelihood -1.415690
INFO: iteration 5, average log likelihood -1.415608
INFO: iteration 6, average log likelihood -1.415503
INFO: iteration 7, average log likelihood -1.415373
INFO: iteration 8, average log likelihood -1.415219
INFO: iteration 9, average log likelihood -1.415050
INFO: iteration 10, average log likelihood -1.414874
INFO: iteration 11, average log likelihood -1.414702
INFO: iteration 12, average log likelihood -1.414541
INFO: iteration 13, average log likelihood -1.414395
INFO: iteration 14, average log likelihood -1.414265
INFO: iteration 15, average log likelihood -1.414150
INFO: iteration 16, average log likelihood -1.414049
INFO: iteration 17, average log likelihood -1.413961
INFO: iteration 18, average log likelihood -1.413882
INFO: iteration 19, average log likelihood -1.413813
INFO: iteration 20, average log likelihood -1.413751
INFO: iteration 21, average log likelihood -1.413696
INFO: iteration 22, average log likelihood -1.413646
INFO: iteration 23, average log likelihood -1.413600
INFO: iteration 24, average log likelihood -1.413559
INFO: iteration 25, average log likelihood -1.413521
INFO: iteration 26, average log likelihood -1.413487
INFO: iteration 27, average log likelihood -1.413454
INFO: iteration 28, average log likelihood -1.413425
INFO: iteration 29, average log likelihood -1.413397
INFO: iteration 30, average log likelihood -1.413371
INFO: iteration 31, average log likelihood -1.413346
INFO: iteration 32, average log likelihood -1.413323
INFO: iteration 33, average log likelihood -1.413301
INFO: iteration 34, average log likelihood -1.413280
INFO: iteration 35, average log likelihood -1.413259
INFO: iteration 36, average log likelihood -1.413240
INFO: iteration 37, average log likelihood -1.413222
INFO: iteration 38, average log likelihood -1.413204
INFO: iteration 39, average log likelihood -1.413187
INFO: iteration 40, average log likelihood -1.413170
INFO: iteration 41, average log likelihood -1.413154
INFO: iteration 42, average log likelihood -1.413138
INFO: iteration 43, average log likelihood -1.413123
INFO: iteration 44, average log likelihood -1.413108
INFO: iteration 45, average log likelihood -1.413093
INFO: iteration 46, average log likelihood -1.413079
INFO: iteration 47, average log likelihood -1.413065
INFO: iteration 48, average log likelihood -1.413051
INFO: iteration 49, average log likelihood -1.413038
INFO: iteration 50, average log likelihood -1.413024
INFO: EM with 100000 data points 50 iterations avll -1.413024
59.0 data points per parameter
5: avll = [-1.41587,-1.41581,-1.41576,-1.41569,-1.41561,-1.4155,-1.41537,-1.41522,-1.41505,-1.41487,-1.4147,-1.41454,-1.41439,-1.41426,-1.41415,-1.41405,-1.41396,-1.41388,-1.41381,-1.41375,-1.4137,-1.41365,-1.4136,-1.41356,-1.41352,-1.41349,-1.41345,-1.41342,-1.4134,-1.41337,-1.41335,-1.41332,-1.4133,-1.41328,-1.41326,-1.41324,-1.41322,-1.4132,-1.41319,-1.41317,-1.41315,-1.41314,-1.41312,-1.41311,-1.41309,-1.41308,-1.41307,-1.41305,-1.41304,-1.41302]
[-1.42531,-1.42533,-1.42525,-1.4252,-1.42516,-1.42511,-1.42505,-1.42497,-1.42488,-1.42473,-1.42444,-1.42389,-1.42297,-1.4218,-1.42083,-1.42029,-1.42006,-1.41997,-1.41993,-1.41992,-1.41992,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41991,-1.41993,-1.41985,-1.4198,-1.41975,-1.4197,-1.41964,-1.41958,-1.41951,-1.41946,-1.41941,-1.41936,-1.41933,-1.4193,-1.41928,-1.41926,-1.41925,-1.41924,-1.41922,-1.41921,-1.4192,-1.41919,-1.41918,-1.41917,-1.41916,-1.41916,-1.41915,-1.41914,-1.41914,-1.41913,-1.41913,-1.41912,-1.41912,-1.41912,-1.41911,-1.41911,-1.41911,-1.41911,-1.41911,-1.4191,-1.4191,-1.4191,-1.4191,-1.41909,-1.41909,-1.41909,-1.41909,-1.41909,-1.41908,-1.41908,-1.41908,-1.41909,-1.41902,-1.41896,-1.41888,-1.41879,-1.41867,-1.41855,-1.41843,-1.41832,-1.41823,-1.41816,-1.41811,-1.41807,-1.41803,-1.41801,-1.41798,-1.41796,-1.41795,-1.41793,-1.41792,-1.41791,-1.4179,-1.41789,-1.41788,-1.41788,-1.41787,-1.41786,-1.41785,-1.41785,-1.41784,-1.41783,-1.41782,-1.41782,-1.41781,-1.4178,-1.41779,-1.41778,-1.41777,-1.41777,-1.41776,-1.41775,-1.41774,-1.41773,-1.41772,-1.41772,-1.41771,-1.4177,-1.41769,-1.41768,-1.41768,-1.41768,-1.41762,-1.41757,-1.41751,-1.41744,-1.41735,-1.41726,-1.41715,-1.41704,-1.41693,-1.41683,-1.41674,-1.41665,-1.41658,-1.41651,-1.41645,-1.4164,-1.41635,-1.41631,-1.41628,-1.41625,-1.41622,-1.4162,-1.41618,-1.41616,-1.41614,-1.41613,-1.41611,-1.4161,-1.41608,-1.41607,-1.41606,-1.41605,-1.41603,-1.41602,-1.41601,-1.416,-1.41599,-1.41598,-1.41597,-1.41596,-1.41594,-1.41593,-1.41592,-1.41591,-1.41591,-1.4159,-1.41589,-1.41588,-1.41587,-1.41587,-1.41581,-1.41576,-1.41569,-1.41561,-1.4155,-1.41537,-1.41522,-1.41505,-1.41487,-1.4147,-1.41454,-1.41439,-1.41426,-1.41415,-1.41405,-1.41396,-1.41388,-1.41381,-1.41375,-1.4137,-1.41365,-1.4136,-1.41356,-1.41352,-1.41349,-1.41345,-1.41342,-1.4134,-1.41337,-1.41335,-1.41332,-1.4133,-1.41328,-1.41326,-1.41324,-1.41322,-1.4132,-1.41319,-1.41317,-1.41315,-1.41314,-1.41312,-1.41311,-1.41309,-1.41308,-1.41307,-1.41305,-1.41304,-1.41302]
32Ã—26 Array{Float64,2}:
 -0.10091    -0.111469    0.403997    -0.214337   -0.566088    -0.44722     0.303932    -0.900446    -0.0982933   -0.0816712  -0.488485     0.0109461  -0.130079   -0.0591441    0.529509     0.091892   -0.298349   -0.272706    0.32787      0.0636816    0.394808    0.0390992   0.268635   -0.228902   -0.215478      0.333248 
 -1.23983    -0.240233   -0.112135     0.466865    0.978232    -0.427549    0.543572    -0.40896      0.213389     0.212065    0.23708      0.391633    0.217096   -0.576681     0.0885852   -0.139388    0.623051   -1.22323    -0.24621     -0.135221    -0.211324    0.622226   -0.181421    0.149385   -0.000974828   0.373643 
 -0.303649   -0.270903   -0.137462    -0.389624    0.318829     0.0395647  -0.44561      0.399541    -0.223224     0.0988397  -0.163168    -0.0285489   0.414587   -0.450426    -0.380788     0.48228     0.0520803  -0.428574    0.0506243   -0.223708     0.374991   -0.438823    0.311976    0.401331    0.0468546     0.0558231
 -0.725325    0.294179    0.0326029    0.244922    0.220054    -0.200698   -0.0958655    0.415232    -0.241127     0.322895    0.210764     0.332216    0.284832   -0.232618     0.337601     0.589802   -0.465778   -0.643369    0.038302    -0.327939     0.211149   -0.345683    0.497221   -0.311831   -0.304983      0.0233142
  0.0142723   0.453353   -0.334745    -0.183811   -0.114275    -0.907802   -0.0108337    0.0842188    0.149851    -0.582207   -0.0283555    0.57432    -0.10126    -0.19691     -0.500216    -0.232117   -0.269644    0.0123522  -0.709756     0.0364686    0.279998    0.450206    0.483108    0.152369    0.192537      0.516507 
  0.275199   -0.243269    0.311107    -0.454829   -0.0412341    0.443068    0.115353     0.288722    -0.194408    -0.411027   -0.0985116    0.840376   -0.19247    -0.111773    -0.394328    -0.616168    0.356253    0.0504126  -0.206813    -0.0435391    0.244533    0.346156    0.492314    0.0194821  -0.194348      0.157305 
 -0.153356    0.341627    0.0566231   -0.116324    0.321808    -0.102397    0.0638951    0.134903    -0.237932     0.0209138   0.227599    -0.0811844  -0.0614401   0.103142     0.339277    -0.496355   -0.483168    0.427519   -0.780783    -0.076384    -0.153177    0.120677    0.0737211   0.225645    0.404733      0.363854 
  0.124557    0.526538    0.292321    -0.276987    0.629544    -0.80571    -0.00512772   0.255844    -0.34178      0.221488    0.230909    -0.108714   -0.351308   -0.0304787    0.604564     0.280417    0.486786   -0.0175856  -0.18232      0.294182     0.111261    0.238988   -0.0735026   0.127184    0.292905      0.109784 
 -0.09986     0.0757526  -0.0641982    0.0599959   0.0872662    0.0822116  -0.15965     -0.0298549    0.0200734   -0.0526721  -0.0812729    0.0209568  -0.672512    0.43976      0.279665    -0.0952105  -0.0350778  -0.0412507  -0.206982    -0.15768      0.349258   -0.173394   -0.152818   -0.270737    0.268415     -0.0702903
 -0.0472846   0.0656731  -0.435811     0.0846462  -0.00210229  -0.11842    -0.0552914    0.0762837    0.10039      0.0260998   0.185804    -0.0719493   0.32457    -0.103072     0.0387284    0.160319   -0.0383064  -0.0166389  -0.291297    -0.00186267   0.196196   -0.0914888   0.0300073   0.122009    0.0521569    -0.0998879
  0.0998538  -0.0934332   0.249815    -0.195365   -0.0837449   -0.0214846   0.0174742   -0.0456652   -0.124024    -0.0740839  -0.247901    -0.0258128   0.161397    0.115222    -0.132987    -0.228204   -0.270246   -0.0353403   0.158923     0.0206436   -0.283012   -0.051441   -0.0167954  -0.0117915  -0.00440622    0.0265637
  0.0444246   0.0111706   0.35031     -0.0276335   0.033247     0.194292   -0.0236661    0.00454411   0.0688379   -0.0161906   0.182748     0.0153577  -0.241435   -0.086693     0.00694776   0.216749    0.225743    0.16194     0.22012     -0.0554451    0.140405    0.271411    0.115644    0.105192   -0.158136      0.137226 
 -0.0896581  -0.0925143  -0.330311     0.283673    0.13675      0.111232   -0.0528927   -0.05303     -0.307339    -0.366738    0.233779    -0.0507915   0.266987   -0.00105005  -0.915168     0.15819    -0.235889    0.126951    0.310809     0.347939    -0.310062    0.257536    0.181052   -0.0715895  -0.252014     -0.427837 
  0.187642    0.203157    0.220071     0.425831   -0.0615734    0.165745    0.166622    -0.267706     0.256338    -0.0479168   0.165729    -0.287358    0.149182    0.214699     0.314085     0.414923   -0.220664    0.0378539   0.479628     0.336324    -0.620028   -0.0548212  -0.424672   -0.462757   -0.0232961    -0.38727  
 -0.121478    0.504582   -0.101441     0.843978   -0.222548     0.0529056   0.00271073  -0.5652       0.5034       0.160624    0.283317    -0.0996422   0.165719   -0.293182     0.408988     0.352071    0.388807    0.197151   -0.364242    -0.320085     0.224835   -0.33278    -0.0957869   0.148014   -0.0356908    -0.103389 
  0.508915   -0.0753799   0.33049      0.317744   -0.418655    -0.288482    0.190438     0.271144     0.415083     0.641488   -0.173659     0.147087    0.742599    0.022985    -0.162613     0.104878    0.266159   -0.371953   -0.448762     0.185048    -0.368261   -0.0409936  -0.0216564   0.193021   -0.311717     -0.392265 
 -0.615483    0.0307837  -0.224491     0.0861427   0.468465     0.0104056   0.0520935   -0.228125     0.00972295  -0.183705    0.00363183   0.0373071  -0.181229    0.0154261   -0.00566195  -0.0990801  -0.365448    0.0233172   0.069394    -0.115891     0.0443882  -0.0203328  -0.0565134   0.0165952  -0.0749517     0.171243 
  0.514149    0.0708346   0.20061     -0.0171226  -0.426731    -0.0633667  -0.0266247    0.353313    -0.16385      0.280483    0.177429    -0.089907    0.151825   -0.266875     0.218166     0.256072    0.509048   -0.101863   -0.189966     0.0374653    0.258652   -0.110774    0.156181   -0.0510499   0.218482     -0.295437 
 -0.343487   -0.115494   -0.739186     0.318209   -0.281136     0.284769    0.650633     0.0923554    0.0721027    0.0169177  -0.036118     0.536625   -0.830578   -0.227336     0.165299     0.781058   -0.343646    0.263328   -0.314295     0.11486      0.85686     0.340035    0.70202    -0.142171   -0.567297     -0.204329 
 -0.317238   -1.04273    -0.186252     0.213042    0.0875462    0.56104     0.511149    -0.00288831   0.103945    -0.252818    0.170968    -0.0311907   0.043715   -0.251682     0.0204071    0.0856321  -0.682439    0.618009   -0.325654    -0.0231088    0.385663   -0.249242    0.0674673  -0.134625    0.395995     -0.275475 
  0.392256   -0.303604   -0.263713    -0.105619   -0.86881     -0.280082    0.116372    -0.466256    -0.0959305   -0.308279   -0.283662     0.068609   -0.185706    0.877057     0.0397834   -0.211769    0.145461    0.179309   -0.243625    -0.167224     0.10053    -0.449775   -0.0329446  -0.25531     0.433553     -0.639554 
 -0.0587727   0.637418    0.00701673   0.0761691  -0.0507154   -0.674937   -0.480305    -0.0833819   -0.0930389    0.29155    -0.210077     0.447547   -0.194007    1.26528      0.0597125   -0.116598    0.0895054  -0.29719     0.550958    -0.415523     0.0309282   0.655248   -0.0310901   0.0859448  -0.373773      0.676273 
  0.240076    0.858278    0.304029     0.1381     -0.0656479    0.307827   -0.21508      0.455149    -0.00441127  -0.365703    0.41199      0.636598   -0.136208    0.407388     0.0410864   -0.405701   -0.0142829  -0.322373    0.0923659    0.149093    -0.939023    0.0596233   0.0573516  -0.723166   -0.079657     -0.373932 
  0.532438    0.495966   -0.149139     0.205095   -0.211179    -0.46498    -0.52804      0.171901    -0.160573    -0.0158889   0.0266056   -0.320165    0.177254    0.673518    -0.272328     0.607709   -0.775563    0.734582   -0.0727785   -0.0242282   -0.336201   -0.364569    0.0386007   0.164451   -0.292593     -0.286765 
  0.52733     0.102956   -0.28843     -0.534253   -0.538969     0.148155   -0.615023    -0.0133746    0.279596    -0.280498    0.556088     0.0091385   0.98386    -0.506528    -0.634765    -0.458679   -0.26571     0.410332    0.10338      0.127495     0.010928    0.484786    0.302563    0.386136   -0.479975      0.449113 
 -0.281933   -0.434839   -0.0655494    0.324806   -0.592625     1.39694    -0.416871    -0.201944     1.05874     -0.196614   -0.140713     0.359736    0.659351    0.550592    -0.692369    -0.344125   -0.47611     0.0037246   0.0973448   -0.302821    -0.360308   -0.547496   -0.474812    0.181927   -0.438281      0.117625 
 -0.19127    -0.414369    0.248471    -0.446881    0.204515     0.0129025   0.0778568   -0.392141    -0.338793    -0.224841   -0.38554     -0.433938    0.0491449  -0.0253617   -0.389947    -0.453706    0.0152352   0.103082    0.517333    -0.341041    -0.102278   -0.103435   -0.202257    0.394237   -0.0412409     0.367524 
  0.423166   -0.378544    0.894429    -0.218936    0.0968447   -0.189549   -0.420325     0.0234349    0.15274     -0.128195    0.0626176   -0.512776    0.687563    0.173794    -0.56979     -0.509798    0.116757    0.108395    0.232437     0.0218518   -0.901671   -0.0978256  -0.504104    0.358879    0.525799      0.265331 
 -0.262529   -0.0752927  -0.0148438   -0.266594   -0.146796     0.184363   -0.31297      0.413097     0.00561612   0.301313    0.467373    -0.360577   -0.178833   -0.390294     0.900518     0.212649    0.164763   -0.640083    0.00202733   0.624952     0.219068    0.492684    0.160178   -0.584542    0.404045     -0.316767 
  0.274615    0.46061     0.574255    -0.443265    0.133825     0.808145   -0.0206869    0.0578549    0.360223     0.0913536  -0.225015    -0.258843   -0.109496   -1.53605      0.15181      0.0309101  -0.158425   -0.0573196   0.222014     0.654062    -0.22667     0.206829    0.230653   -0.16848    -0.181287      0.14525  
  0.345293   -0.579498    0.314648    -0.312306   -0.0902712    0.602655    0.057307    -0.723238     0.280121     0.0135562   0.110879    -0.030575   -0.727742    0.440518     0.16913     -0.455222    0.289597    0.242966    0.261852     0.460619     0.337345    0.78357    -0.618249   -0.161516   -0.00948871    0.109781 
  0.074096   -0.549692    0.236977     0.355923    0.151932     0.66536     0.105094     0.0183093   -0.108965     0.299203   -0.203183    -0.351596   -0.211027    0.293241     0.304599     0.419857    0.489426   -0.15003     0.504193    -0.2818       0.237845   -0.533186   -0.596916    0.191211    0.0205759    -0.772321 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.413011
INFO: iteration 2, average log likelihood -1.412998
INFO: iteration 3, average log likelihood -1.412985
INFO: iteration 4, average log likelihood -1.412973
INFO: iteration 5, average log likelihood -1.412960
INFO: iteration 6, average log likelihood -1.412948
INFO: iteration 7, average log likelihood -1.412935
INFO: iteration 8, average log likelihood -1.412923
INFO: iteration 9, average log likelihood -1.412911
INFO: iteration 10, average log likelihood -1.412899
INFO: EM with 100000 data points 10 iterations avll -1.412899
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.676426e+05
      1       7.074390e+05      -2.602036e+05 |       32
      2       6.955402e+05      -1.189874e+04 |       32
      3       6.906200e+05      -4.920172e+03 |       32
      4       6.881223e+05      -2.497769e+03 |       32
      5       6.865888e+05      -1.533464e+03 |       32
      6       6.854581e+05      -1.130694e+03 |       32
      7       6.846128e+05      -8.453357e+02 |       32
      8       6.839290e+05      -6.837682e+02 |       32
      9       6.833484e+05      -5.806123e+02 |       32
     10       6.828728e+05      -4.756113e+02 |       32
     11       6.824693e+05      -4.035164e+02 |       32
     12       6.821508e+05      -3.184862e+02 |       32
     13       6.818650e+05      -2.857816e+02 |       32
     14       6.816093e+05      -2.556876e+02 |       32
     15       6.813664e+05      -2.429572e+02 |       32
     16       6.811553e+05      -2.110233e+02 |       32
     17       6.809542e+05      -2.011190e+02 |       32
     18       6.807657e+05      -1.884897e+02 |       32
     19       6.806034e+05      -1.622980e+02 |       32
     20       6.804686e+05      -1.348834e+02 |       32
     21       6.803550e+05      -1.135337e+02 |       32
     22       6.802480e+05      -1.070079e+02 |       32
     23       6.801462e+05      -1.017718e+02 |       32
     24       6.800449e+05      -1.013160e+02 |       32
     25       6.799325e+05      -1.123865e+02 |       32
     26       6.798162e+05      -1.163199e+02 |       32
     27       6.797071e+05      -1.091398e+02 |       32
     28       6.796062e+05      -1.009075e+02 |       32
     29       6.795108e+05      -9.537711e+01 |       32
     30       6.794098e+05      -1.009752e+02 |       32
     31       6.793111e+05      -9.866999e+01 |       32
     32       6.792181e+05      -9.301173e+01 |       32
     33       6.791343e+05      -8.388038e+01 |       32
     34       6.790577e+05      -7.651430e+01 |       32
     35       6.789793e+05      -7.845363e+01 |       32
     36       6.789073e+05      -7.198017e+01 |       32
     37       6.788423e+05      -6.499740e+01 |       32
     38       6.787945e+05      -4.785675e+01 |       32
     39       6.787488e+05      -4.562225e+01 |       32
     40       6.787026e+05      -4.622577e+01 |       32
     41       6.786614e+05      -4.119632e+01 |       32
     42       6.786293e+05      -3.213719e+01 |       32
     43       6.785982e+05      -3.104902e+01 |       32
     44       6.785720e+05      -2.617799e+01 |       32
     45       6.785426e+05      -2.946672e+01 |       32
     46       6.785141e+05      -2.843137e+01 |       32
     47       6.784860e+05      -2.811485e+01 |       32
     48       6.784556e+05      -3.040383e+01 |       32
     49       6.784274e+05      -2.823453e+01 |       32
     50       6.784011e+05      -2.625866e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 678401.1333796441)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.425252
INFO: iteration 2, average log likelihood -1.420202
INFO: iteration 3, average log likelihood -1.418887
INFO: iteration 4, average log likelihood -1.417950
INFO: iteration 5, average log likelihood -1.416929
INFO: iteration 6, average log likelihood -1.415908
INFO: iteration 7, average log likelihood -1.415141
INFO: iteration 8, average log likelihood -1.414688
INFO: iteration 9, average log likelihood -1.414432
INFO: iteration 10, average log likelihood -1.414270
INFO: iteration 11, average log likelihood -1.414152
INFO: iteration 12, average log likelihood -1.414056
INFO: iteration 13, average log likelihood -1.413974
INFO: iteration 14, average log likelihood -1.413900
INFO: iteration 15, average log likelihood -1.413833
INFO: iteration 16, average log likelihood -1.413770
INFO: iteration 17, average log likelihood -1.413710
INFO: iteration 18, average log likelihood -1.413654
INFO: iteration 19, average log likelihood -1.413599
INFO: iteration 20, average log likelihood -1.413547
INFO: iteration 21, average log likelihood -1.413497
INFO: iteration 22, average log likelihood -1.413449
INFO: iteration 23, average log likelihood -1.413403
INFO: iteration 24, average log likelihood -1.413359
INFO: iteration 25, average log likelihood -1.413317
INFO: iteration 26, average log likelihood -1.413276
INFO: iteration 27, average log likelihood -1.413238
INFO: iteration 28, average log likelihood -1.413202
INFO: iteration 29, average log likelihood -1.413167
INFO: iteration 30, average log likelihood -1.413134
INFO: iteration 31, average log likelihood -1.413103
INFO: iteration 32, average log likelihood -1.413074
INFO: iteration 33, average log likelihood -1.413046
INFO: iteration 34, average log likelihood -1.413020
INFO: iteration 35, average log likelihood -1.412995
INFO: iteration 36, average log likelihood -1.412971
INFO: iteration 37, average log likelihood -1.412948
INFO: iteration 38, average log likelihood -1.412927
INFO: iteration 39, average log likelihood -1.412906
INFO: iteration 40, average log likelihood -1.412886
INFO: iteration 41, average log likelihood -1.412867
INFO: iteration 42, average log likelihood -1.412849
INFO: iteration 43, average log likelihood -1.412832
INFO: iteration 44, average log likelihood -1.412815
INFO: iteration 45, average log likelihood -1.412799
INFO: iteration 46, average log likelihood -1.412784
INFO: iteration 47, average log likelihood -1.412769
INFO: iteration 48, average log likelihood -1.412755
INFO: iteration 49, average log likelihood -1.412741
INFO: iteration 50, average log likelihood -1.412728
INFO: EM with 100000 data points 50 iterations avll -1.412728
59.0 data points per parameter
32Ã—26 Array{Float64,2}:
  1.0278      0.131221   -0.227462    -0.150229   -0.464583      0.139155    -0.709587     0.346158    -0.194675     0.0108692    0.0109157  -0.484758     0.116561    0.901434    -0.400257     0.351663   -0.635208     0.985599    0.0666988  -0.0725299   -0.1852     -0.377622     0.000457064   0.128647    -0.214495    -0.445104  
 -0.0552402   0.603191   -0.0229648   -0.137417   -0.0455076    -0.31093     -0.115839    -0.113471     0.00717202  -0.460194     0.029175    0.37244      0.0950708   0.104025    -0.36045     -0.362236   -0.411017     0.20728     0.0457736   0.157238    -0.384235    0.264306     0.381732     -0.0643743   -0.34084      0.483511  
 -0.416319    0.29649    -0.179484    -0.189816   -0.204295      0.183433    -0.509515     0.319076    -0.0433686    0.00317785   0.321097    0.249817    -0.187082   -0.170088     0.71533      0.0515971  -0.195546    -0.866677    0.0360497   0.217808     0.127563    0.190748     0.365457     -0.987713     0.332686    -0.240811  
  0.20867     0.31543     0.111035    -0.215592    0.0641496    -0.039294    -0.150994     0.334081    -0.0419382   -0.382739     0.410045    0.208677     0.196623   -0.14416     -0.618226    -0.232593   -0.207802     0.345721    0.0314321   0.14275     -0.433785    0.400811     0.257132      0.137068    -0.14241      0.350884  
  0.0871732   0.892312    0.403353    -0.0966083   0.632862     -0.499314    -0.387183     0.505117    -0.366742    -0.0176187   -0.070824    0.704431    -0.475937    0.572077     0.328697     0.0794611   0.316034    -0.441701    0.153716    0.101341    -0.243157    0.389147    -0.343009     -0.0106531    0.188765     0.00344102
 -0.392321   -0.162319   -0.216759     0.288491    0.120126      0.862022     0.359108    -0.385421    -0.192585    -0.0190388    0.360919   -0.162044    -0.0167523  -0.516715     0.169106    -0.234188    0.46193      0.326084    0.483025   -0.465514     0.22525     0.154414    -0.0980963     0.550991     0.300674    -0.0468729 
  0.264553    0.430944    0.191465     0.313904   -0.143947      0.179579    -0.01236      0.39285      0.215122     0.0509591    0.44422     0.885453    -0.211654    0.0874296    0.39302      0.445423    0.366285     0.252746   -0.495543   -0.391915     0.36669    -0.188077     0.310192     -0.121182    -0.104326     0.14547   
  0.488878    0.284161   -0.00631439   0.143914   -0.730656     -0.336456    -0.330957    -0.853778     0.115205    -0.2049      -0.165159   -0.504748     0.465467   -0.432127     0.00325911   0.43838     0.0399598    0.266791    0.131509    0.17657      0.196795   -0.0285271   -0.126319      0.27234     -0.00640512   0.0674488 
  0.0378917   0.294618   -0.0577474   -0.021626   -0.000115248  -0.086282    -0.17662     -0.128059     0.130536    -0.0174352    0.196243   -0.170834    -0.368219    0.466078     0.438148    -0.532589   -0.0572273    0.306439   -0.817254   -0.0935544    0.0772402   0.256475    -0.234397      0.0903617    0.43472      0.247865  
 -0.250584   -0.0600871  -0.211023    -0.467019    0.178283      0.177305    -0.396588     0.561309    -0.250033    -0.24552     -0.246545   -0.385408     0.476376   -0.559091    -0.352056     0.132685    0.127762    -0.159034   -0.123252   -0.101375     0.224352   -0.837134     0.324314      0.240078     0.731213     0.0894835 
 -0.0926711   0.104159    0.0291411    0.0282031   0.110848     -0.235855     0.0616119    0.00745237  -0.0123463    0.0444883    0.0503006  -0.00742462   0.0540644  -0.0553602    0.0987992    0.110512   -0.0564854   -0.0827439  -0.0912315  -0.00256978   0.0323983   0.0549108    0.0704399     0.0387441   -0.0431077    0.0402121 
 -1.48874    -0.167568    0.0918651    0.629234    1.05437      -0.573864     0.752162    -0.290409     0.346138     0.2217       0.214045    0.641547     0.0781636  -0.72614      0.171711     0.0641208   0.794777    -1.35032    -0.271995   -0.190908    -0.190254    0.653951    -0.016906     -0.0461191    0.0323077    0.489632  
 -0.459018    0.4244     -0.322903     0.864334    0.196974     -0.49174     -0.21239     -0.170171    -0.029138     0.04594      0.382412   -0.195638    -0.0619638   0.240326     0.00841923   0.405253   -0.201764     0.0630171  -0.376433   -0.258447     0.0187164  -0.542383     0.0957904    -0.107941    -0.120488    -0.443186  
 -0.145627    0.0417019  -0.0607556   -0.57894     0.281172     -0.809833     0.389593     0.118492    -0.23935     -0.349394    -0.245131    0.262587    -0.0194278  -0.499324    -0.0339623   -0.193921   -0.126995     0.205615   -0.737693   -0.222349     0.700858    0.0434298    0.441803      0.35748      0.40658      0.331354  
  0.0465022  -0.18245     0.36087      0.424821    0.033805      0.183135     0.350584    -0.481452     0.300663     0.465919    -0.52364    -0.329463    -0.122749    0.175503     0.687538     0.150751    0.483735    -0.47823    -0.0721541  -0.311404     0.189948   -0.548337    -0.733003     -0.132567     0.00852575  -0.531227  
 -0.532007   -0.186037    0.295259    -0.073998    0.357055      0.168143    -0.0170722    0.0470159   -0.206915     0.192406    -0.195271    0.0519754    0.170205   -0.324754    -0.0826428    0.762467   -0.226982    -0.469658    0.839644   -0.216432     0.259506   -0.359095     0.270438     -0.0223931   -0.667534     0.105343  
  0.28917    -0.859376   -0.0492276   -0.531406   -0.0827466     0.473588    -0.296261    -0.0766209   -0.063661    -0.139088    -0.157037    0.724883    -0.205398    0.0679686   -0.595569    -0.57653     0.441782    -0.221179   -0.402203    0.280049     0.61967     0.632767     0.332115      0.154885    -0.174413    -0.101564  
  0.0579271  -0.123198   -0.00490943   0.123153   -0.0520078     0.00870123   0.10232      0.130845    -0.189339     0.0843131    0.19494    -0.196005    -0.184632    0.0829627    0.249743     0.217891    0.211112     0.0183155  -0.0542284  -0.0210849    0.186353   -0.149344    -0.118666     -0.210887     0.309506    -0.394475  
  0.0638267  -0.245094    0.113819    -0.102671   -0.0719676     0.412319    -0.11468     -0.0646591   -0.0144235   -0.130882    -0.134351   -0.106991    -0.300462    0.294962    -0.0172627   -0.107898   -0.00211883   0.105992    0.162038   -0.103461     0.132408   -0.147903    -0.136323     -0.0975283    0.172213    -0.0960464 
  0.231914    0.143428    0.147552     0.413287   -0.235466      0.557465     0.064307    -0.215792     0.23074     -0.191882     0.197653   -0.133327     0.0834105   0.251466     0.0169482    0.175573   -0.060222     0.0576522   0.575047    0.337712    -0.544459    0.0487998   -0.32728      -0.438556    -0.129368    -0.568331  
 -0.207611   -0.769353    0.0781821    0.40435     0.296935      0.622725     0.467971     0.166582     0.0962989   -0.273576     0.316185    0.350686     0.0223302  -0.341023     0.224162     0.360454   -1.23796      0.434776   -0.572343    0.208818    -0.0763646  -0.16176     -0.160022     -0.0369627    0.593917    -0.383021  
 -0.325027   -0.475514   -0.261892     0.188984   -0.272138      1.35219     -0.301368    -0.278743     0.971525    -0.209146    -0.120807    0.314424     0.376706    0.510118    -0.634591    -0.453614   -0.412927     0.0720675   0.103748   -0.271004    -0.179126   -0.402675    -0.519485      0.0584418   -0.347863     0.197719  
  0.0234924   0.440704   -0.05521      0.295814   -0.331324     -0.482361    -0.423661     0.477683     0.361223     0.569705    -0.0303852   0.280336     0.995777   -0.310166    -0.144355     0.354298   -0.00562561  -0.564166   -0.562523   -0.371195    -0.334986   -0.370998     0.245888      0.947241    -0.281663    -0.22172   
 -0.508669   -0.408531   -0.709112     0.301687   -0.230591      0.342039     0.745247    -0.0168736    0.11579     -0.076757    -0.155282    0.183123    -0.593841   -0.258158    -0.0936628    0.50923    -0.43639      0.469911   -0.256869    0.0584238    0.928621    0.291132     0.747313     -0.116363    -0.576933    -0.0143725 
  0.363071   -0.05523     0.322851     0.365568   -0.376745     -0.424094     0.308378     0.33206      0.233178     0.417363    -0.010156    0.0962155    0.502779    0.130787    -0.254334     0.10818     0.265888    -0.316184   -0.379167    0.49531     -0.501452    0.00362997   0.0772633    -0.0695523   -0.13214     -0.519232  
  0.378153    0.410186    0.622368    -0.388105   -0.0323119     0.774711    -0.00246729   0.0962697    0.441129     0.19158     -0.0447151  -0.208357    -0.112879   -1.18152      0.404081    -0.217387    0.0850427   -0.149855    0.031704    0.590098    -0.120153    0.41318      0.198907     -0.187932    -0.130783     0.0999986 
  0.178508   -0.385977    0.567489    -0.315066   -0.128871      0.279512     0.399112    -0.814004    -0.0459585   -0.215956    -0.0823065   0.141237    -0.845323    0.411033     0.397098    -0.267677    0.0841553    0.193203    0.518819    0.30197      0.364947    0.612281    -0.356786     -0.47046     -0.148899     0.117619  
  0.340286   -0.249648   -0.200606    -0.253109   -0.269785      0.556685    -0.213235     0.157278     0.0568839    0.23735     -0.17664     0.247725     0.773373   -0.356441    -0.280224     0.0760969  -0.176181    -0.138822    0.228108   -0.267546     0.236518   -0.0421242    0.367886      0.202614    -0.515935    -0.0723658 
 -0.0453603  -0.06541     0.00858346  -0.243756    0.264932     -0.450392    -0.115614     0.180777    -0.130169     0.493608     0.487657   -0.646556    -0.197053   -0.220557     0.707979     0.519855    0.293802    -0.105542    0.0377942   0.404949     0.281625    0.429802    -0.034648     -0.00407591   0.222024     0.118306  
 -0.828931   -0.469533   -0.919819     0.0705197   0.591938     -0.148374    -0.244385    -0.394973    -0.357883    -0.286404     0.313351   -0.158272     0.305562   -0.00201991  -0.814257    -0.195947   -0.0647989   -0.581425   -0.138601    0.397362     0.0213898   0.239507    -0.442735      0.537302    -0.179876    -0.156338  
  0.119245   -0.627827    0.826888    -0.352311    0.225881     -0.0700813   -0.173563    -0.166756    -0.0844102   -0.121152    -0.169696   -0.519604     0.377696    0.138147    -0.505585    -0.608195    0.165685     0.0571218   0.438799   -0.175366    -0.615872   -0.0564325   -0.430561      0.358908     0.221397     0.333837  
 -0.0670188  -0.0160544   0.125319    -0.186499   -0.249168     -0.807875     0.279055    -0.507962    -0.256847    -0.0527989   -0.560888    0.143115    -0.108549    0.576423     0.127881    -0.166083   -0.423166    -0.106254    0.144224   -0.20532      0.0115477  -0.0933024    0.170442     -0.00884042  -0.144766     0.255399  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.412716
INFO: iteration 2, average log likelihood -1.412704
INFO: iteration 3, average log likelihood -1.412693
INFO: iteration 4, average log likelihood -1.412682
INFO: iteration 5, average log likelihood -1.412672
INFO: iteration 6, average log likelihood -1.412662
INFO: iteration 7, average log likelihood -1.412652
INFO: iteration 8, average log likelihood -1.412643
INFO: iteration 9, average log likelihood -1.412634
INFO: iteration 10, average log likelihood -1.412626
INFO: EM with 100000 data points 10 iterations avll -1.412626
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
