>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.6.6
INFO: Installing JLD v0.6.5
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.0
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.2.0
INFO: Installing StaticArrays v0.0.11
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1156
Commit 9747250 (2016-10-29 17:44 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-100-generic #147-Ubuntu SMP Tue Oct 18 16:48:51 UTC 2016 x86_64 x86_64
Memory: 2.939289093017578 GB (653.94921875 MB free)
Uptime: 23976.0 sec
Load Avg:  1.00830078125  1.02294921875  1.029296875
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3501 MHz    1446534 s       5745 s     182650 s     513595 s         51 s
#2  3501 MHz     644924 s         88 s      86869 s    1587079 s          1 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.3
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.6.6
 - JLD                           0.6.5
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.0
 - Rmath                         0.1.4
 - SHA                           0.2.1
 - ScikitLearnBase               0.2.0
 - StaticArrays                  0.0.11
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-4.526244326863894e6,[320.317,99679.7],
[-35.6716 -133.612 -956.668; 161.365 391.316 442.198],

Array{Float64,2}[
[314.396 21.3259 106.939; 21.3259 442.173 357.111; 106.939 357.111 2888.17],

[99262.1 -210.641 147.614; -210.641 1.00541e5 24.2851; 147.614 24.2851 96378.2]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.552555e+03
      1       1.213747e+03      -3.388080e+02 |        7
      2       1.087427e+03      -1.263200e+02 |        6
      3       1.015012e+03      -7.241420e+01 |        2
      4       9.722988e+02      -4.271354e+01 |        0
      5       9.722988e+02       0.000000e+00 |        0
K-means converged with 5 iterations (objv = 972.2987889641472)
INFO: K-means with 272 data points using 5 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.080163
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.800644
INFO: iteration 2, lowerbound -3.677427
INFO: iteration 3, lowerbound -3.538845
INFO: iteration 4, lowerbound -3.360819
INFO: dropping number of Gaussions to 7
INFO: iteration 5, lowerbound -3.147177
INFO: iteration 6, lowerbound -2.925897
INFO: iteration 7, lowerbound -2.742047
INFO: dropping number of Gaussions to 6
INFO: iteration 8, lowerbound -2.607980
INFO: dropping number of Gaussions to 5
INFO: iteration 9, lowerbound -2.513534
INFO: dropping number of Gaussions to 4
INFO: iteration 10, lowerbound -2.442973
INFO: iteration 11, lowerbound -2.397572
INFO: dropping number of Gaussions to 3
INFO: iteration 12, lowerbound -2.361298
INFO: iteration 13, lowerbound -2.330504
INFO: iteration 14, lowerbound -2.312375
INFO: iteration 15, lowerbound -2.307582
INFO: dropping number of Gaussions to 2
INFO: iteration 16, lowerbound -2.302921
INFO: iteration 17, lowerbound -2.299260
INFO: iteration 18, lowerbound -2.299256
INFO: iteration 19, lowerbound -2.299254
INFO: iteration 20, lowerbound -2.299254
INFO: iteration 21, lowerbound -2.299253
INFO: iteration 22, lowerbound -2.299253
INFO: iteration 23, lowerbound -2.299253
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Mon 31 Oct 2016 11:09:33 AM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Mon 31 Oct 2016 11:09:35 AM UTC: K-means with 272 data points using 5 iterations
11.3 data points per parameter
,Mon 31 Oct 2016 11:09:36 AM UTC: EM with 272 data points 0 iterations avll -2.080163
5.8 data points per parameter
,Mon 31 Oct 2016 11:09:37 AM UTC: GMM converted to Variational GMM
,Mon 31 Oct 2016 11:09:38 AM UTC: iteration 1, lowerbound -3.800644
,Mon 31 Oct 2016 11:09:38 AM UTC: iteration 2, lowerbound -3.677427
,Mon 31 Oct 2016 11:09:39 AM UTC: iteration 3, lowerbound -3.538845
,Mon 31 Oct 2016 11:09:39 AM UTC: iteration 4, lowerbound -3.360819
,Mon 31 Oct 2016 11:09:39 AM UTC: dropping number of Gaussions to 7
,Mon 31 Oct 2016 11:09:39 AM UTC: iteration 5, lowerbound -3.147177
,Mon 31 Oct 2016 11:09:39 AM UTC: iteration 6, lowerbound -2.925897
,Mon 31 Oct 2016 11:09:39 AM UTC: iteration 7, lowerbound -2.742047
,Mon 31 Oct 2016 11:09:39 AM UTC: dropping number of Gaussions to 6
,Mon 31 Oct 2016 11:09:39 AM UTC: iteration 8, lowerbound -2.607980
,Mon 31 Oct 2016 11:09:39 AM UTC: dropping number of Gaussions to 5
,Mon 31 Oct 2016 11:09:39 AM UTC: iteration 9, lowerbound -2.513534
,Mon 31 Oct 2016 11:09:39 AM UTC: dropping number of Gaussions to 4
,Mon 31 Oct 2016 11:09:39 AM UTC: iteration 10, lowerbound -2.442973
,Mon 31 Oct 2016 11:09:39 AM UTC: iteration 11, lowerbound -2.397572
,Mon 31 Oct 2016 11:09:39 AM UTC: dropping number of Gaussions to 3
,Mon 31 Oct 2016 11:09:39 AM UTC: iteration 12, lowerbound -2.361298
,Mon 31 Oct 2016 11:09:39 AM UTC: iteration 13, lowerbound -2.330504
,Mon 31 Oct 2016 11:09:39 AM UTC: iteration 14, lowerbound -2.312375
,Mon 31 Oct 2016 11:09:40 AM UTC: iteration 15, lowerbound -2.307582
,Mon 31 Oct 2016 11:09:40 AM UTC: dropping number of Gaussions to 2
,Mon 31 Oct 2016 11:09:40 AM UTC: iteration 16, lowerbound -2.302921
,Mon 31 Oct 2016 11:09:40 AM UTC: iteration 17, lowerbound -2.299260
,Mon 31 Oct 2016 11:09:40 AM UTC: iteration 18, lowerbound -2.299256
,Mon 31 Oct 2016 11:09:40 AM UTC: iteration 19, lowerbound -2.299254
,Mon 31 Oct 2016 11:09:40 AM UTC: iteration 20, lowerbound -2.299254
,Mon 31 Oct 2016 11:09:40 AM UTC: iteration 21, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:40 AM UTC: iteration 22, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:40 AM UTC: iteration 23, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:40 AM UTC: iteration 24, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:40 AM UTC: iteration 25, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:40 AM UTC: iteration 26, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:40 AM UTC: iteration 27, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:40 AM UTC: iteration 28, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:41 AM UTC: iteration 29, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:41 AM UTC: iteration 30, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:41 AM UTC: iteration 31, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:41 AM UTC: iteration 32, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:41 AM UTC: iteration 33, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:41 AM UTC: iteration 34, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:41 AM UTC: iteration 35, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:41 AM UTC: iteration 36, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:41 AM UTC: iteration 37, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:41 AM UTC: iteration 38, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:41 AM UTC: iteration 39, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:41 AM UTC: iteration 40, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:41 AM UTC: iteration 41, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:42 AM UTC: iteration 42, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:42 AM UTC: iteration 43, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:42 AM UTC: iteration 44, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:42 AM UTC: iteration 45, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:42 AM UTC: iteration 46, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:42 AM UTC: iteration 47, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:42 AM UTC: iteration 48, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:42 AM UTC: iteration 49, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:42 AM UTC: iteration 50, lowerbound -2.299253
,Mon 31 Oct 2016 11:09:42 AM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.9549,178.045]
β = [95.9549,178.045]
m = [2.00023 53.852; 4.2503 79.2869]
ν = [97.9549,180.045]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.375876 -0.00895312; 0.0 0.0127487],

[0.184042 -0.00764405; 0.0 0.00858171]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 99999.99999999996
avll from stats: -0.9652849537356334
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9652849537355267
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9652849537355268
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
WARNING: is is deprecated, use === instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in is(::Array{Float64,2}, ::Vararg{Array{Float64,2},N}) at ./deprecated.jl:30
 in _rcopy!(::Array{Float64,2}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/PDMats/src/utils.jl:10
 in unwhiten!(::Array{Float64,2}, ::PDMats.PDMat{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/PDMats/src/pdmat.jl:60
 in rand(::Distributions.MvNormal{Float64,PDMats.PDMat{Float64,Array{Float64,2}},Array{Float64,1}}, ::Int64) at /home/vagrant/.julia/v0.6/Distributions/src/multivariates.jl:18
 in rand(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:60
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -1.0203294331802901
avll from llpg:  -1.0203294331802897
avll direct:     -1.0203294331802897
sum posterior: 100000.0
32×26 Array{Float64,2}:
 -0.0754433    0.0518839    -0.0140575   -0.0342296   0.162325      0.138126     0.173761    -0.0916066    -0.0449154     0.086763    -0.0304763     0.0158904   0.209785    -0.0207454    0.00812522   0.213644     0.0138856   -0.161967     0.183079    -0.0842455     0.00266564    0.055716    -0.119531    -0.0595486    0.00978103  -0.103029  
 -0.161224    -0.140044      0.233451    -0.0062448   0.19131       0.0809785    0.00826591   0.0975511    -0.0503869     0.0467961   -0.0995023     0.140456    0.0994857    0.0384856    0.177395     0.0192728   -0.115403     0.133113     0.132138     0.0645405    -0.0306371     0.166359     0.10156      0.0600697   -0.0373931    0.0993777 
 -0.0605108   -0.190793      0.0601732   -0.0174017   0.0748378     0.06793     -0.024762     0.0687054     0.00183791   -0.0295487   -0.0426233     0.0639704   0.022571    -0.0319045   -0.240552    -0.0379923    0.00815257   0.110642     0.106625    -0.156538     -0.102279     -0.0565035    0.126621     0.0909565   -0.0183477   -0.0796602 
 -0.13551      0.0639341     0.0973635   -0.100616    0.10966      -0.0741057    0.0885389    0.228796     -0.0946749    -0.138961    -0.0631959    -0.0270285  -0.194358    -0.0647513    0.153447     0.0129131   -0.0255121    0.146987    -0.186636     0.055441     -0.169115      0.0692183   -0.147636     0.0630839    0.0452325    0.033146  
  0.0515875    0.266397      0.0614104    0.0152506   0.158849     -0.0325436    0.112946    -0.114083      0.0387539     0.138765     0.000774134   0.0662539  -0.189533    -0.0939712    0.0714388    0.0986085    0.129563    -0.0725619   -0.107845    -0.0839418     0.0297932     0.0418606   -0.0826336    0.0783096   -0.00364295   0.00129153
  0.136152     0.0212624    -0.0196853    0.0203269   0.118166      0.0454114    0.0306545    0.0224405     0.0431986     0.0587122    0.0272424     0.110397    0.146109    -0.118868     0.149828     0.0989148    0.0623453    0.00448066   0.0734365   -0.0985501     0.120132     -0.0797913    0.0240744    0.0755128    0.112015     0.0189247 
 -0.204332     0.13217      -0.027058     0.0116884  -0.0633559     0.0119412   -0.0175509    0.00710608    0.0121781    -0.0278596   -0.00284354   -0.0780804   0.107198     0.0123199    0.0322177   -0.00927399  -0.0183424   -0.0390193   -0.126427    -0.00835778   -0.0910311     0.0614939    0.0778331   -0.0467322   -0.0881247    0.0736629 
 -0.0824336    0.184218      0.0138426   -0.0569898   0.118167     -0.107302    -0.0202461    0.0771486    -0.0223064     0.234474     0.100326      0.0460565  -0.0761241    0.114815     0.0763196    0.00051414   0.176288     0.00299143   0.122162    -0.00338991   -0.00790785   -0.00901258  -0.0266626   -0.039439    -0.0657327    0.116332  
 -0.0196041    0.0144024     0.0348452   -0.159863   -0.00714865    0.0830692   -0.120469     0.0239025     0.0901527    -0.134612     0.1199       -0.0490794   0.00799041  -0.0237594    0.0689736    0.207613    -0.1817       0.0163795   -0.0651727   -0.0517599     0.00259141   -0.027113     0.0910326   -0.0148032   -0.106824     0.0506542 
 -0.0123632   -0.0960773    -0.0130141   -0.0231191   0.0124692    -0.00304171  -0.111547     0.0824325     0.0255448    -0.0744656    0.115962     -0.0624665  -0.0683006   -0.0411491   -0.243673    -0.164539    -0.0109183   -0.00595041  -0.0957477    0.0811803    -0.0325065    -0.110366    -0.090074    -0.00260806   0.0821321   -0.113816  
  0.0629224   -0.000718794   0.208943    -0.214664   -0.0893587     0.148455     0.0838551   -0.0178669     0.0756959     0.150484     0.23171       0.0833744   0.155225     0.0707405    0.0471303   -0.226908     0.092495    -0.289702    -0.134557    -0.088765      0.0369442     0.112396     0.113046     0.141881    -0.12239     -0.00723575
  0.187571    -0.151734      0.152113    -0.0799437  -0.0427987    -0.0541422   -0.0963824   -0.179671      0.0336107    -0.10763     -0.0125796    -0.0308389  -0.0482687    0.114676    -0.0570941   -0.0364673    0.0922566   -0.0358128    0.104383     0.124668     -0.109737      0.0910987   -0.0297048   -0.0164474    0.00936534  -0.254118  
 -0.0161372   -0.0583042    -0.00383244   0.0110705  -0.00391687   -0.235753     0.0201393    0.0972338     0.148591     -0.0731115    0.0813487    -0.0666574  -0.0266986    0.00169619  -0.0721531    0.0440554   -0.0470091    0.144883    -0.0256865    0.0646446     0.0483739    -0.140756    -0.115192    -0.128475     0.0246844   -0.141019  
 -0.00606567  -0.0812368    -0.169961     0.0410384   0.0795777     0.0331836    0.053786     0.132507      0.0485229     0.0520962    0.161898     -0.0558815  -0.071642     0.0220902    0.00175779   0.0685271    0.123796    -0.0209722    0.156755     0.14431      -0.0138898     0.091255     0.0417826    0.0488607    0.0608762   -0.0491331 
  0.0199783    0.0296811     0.0503363    0.0238304  -0.0769422    -0.0316689   -0.19178      0.0232873     0.237784     -0.0726253    0.0524002    -0.0140885  -0.0184556   -0.0577664   -0.01781      0.0740831    0.189338    -0.0187346   -0.0726655   -0.0197172    -0.0102758    -0.0528547    0.0415271   -0.0296467    0.0909891    0.0644994 
 -0.221641     0.0796059    -0.0386571   -0.0568741   0.178837      0.189842     0.242389    -0.0799048    -0.0510403     0.131326    -0.152397      0.102354    0.121167     0.00318321  -0.115143    -0.0264981   -0.149027     0.0700241    0.0467504    0.143642      0.128901     -0.0800282   -0.0406573   -0.184384     0.0548433    0.0427556 
 -0.208209     0.0878852    -0.055584    -0.067218    0.0100349    -0.148471     0.0729716    0.0224242     0.0737205     0.0475182    0.0437124    -0.0813466   0.136379     0.0274792    0.0411743   -0.150965     0.0840625    0.147698    -0.163194     0.107185     -0.141806     -0.0239877    0.0049609    0.07212      0.129422    -0.0022142 
 -0.106884    -0.0801391     0.0598826   -0.083462   -0.0200441     0.0413721    0.144181     0.0443883    -0.126267      0.124642    -0.0737307     0.127454   -0.0103815   -0.156213     0.074826     0.0509692   -0.0106628    0.134189     0.131182    -0.212592      0.102994     -0.180781     0.0600602   -0.0154348    0.0890033   -0.0675641 
  0.0309848   -0.000301183  -0.0254149    0.125331    0.105466      0.148827    -0.00541258   0.000403235   0.0218937    -0.0041727    0.177357      0.0141572  -0.0135086   -0.0621648   -0.217183     0.0503025   -0.112648    -0.083082     0.188557     0.0685973    -0.000454962  -0.0819723   -0.00681163   0.0564031   -0.0615207   -0.0966754 
 -0.104212    -0.0829724    -0.0217758    0.0850016  -0.0514103    -0.0456479    0.0407447    0.0782512    -0.1141        0.116209     0.0649294    -0.0190061   0.0915044    0.063385     0.0477051   -0.00365962   0.0121288    0.106467    -0.100404     0.0694409    -0.0905878    -0.0115445   -0.0886104    0.00866824  -0.0875584   -0.163751  
 -0.00409653  -0.1811       -0.0196108    0.0722829   0.09575       0.138567     0.0266432    0.0902022    -0.0459477    -0.1419       0.0830036     0.0980241   0.168873     0.0051913   -0.136849     0.176697     0.0978182    0.173504    -0.0909987   -0.0485838    -0.0983663     0.0360776    0.0234145    0.0284395   -0.0928135    0.136473  
  0.174889    -0.0828094     0.0708019   -0.183979   -0.181345     -0.244929    -0.0880142    0.00460363    0.0379952    -0.0260916    0.0170835    -0.0179512   0.112159     0.117581    -0.0454238    0.0247971    0.11417      0.0251504    0.00202915  -0.131151     -0.0195946    -0.10229     -0.0846676   -0.0384033   -0.0329975    0.133848  
 -0.0330979    0.042579     -0.0979609   -0.0805543  -0.0101399    -0.0294791   -0.0381447    0.0697193    -0.050209      0.196347    -0.0889176    -0.0446591  -0.0576845    0.136395     0.0555456   -0.0619208   -0.162275    -0.0742287    0.10196     -0.000462377  -0.0855238     0.234028    -0.136334     0.0237573   -0.0825974   -0.0201403 
  0.0141652   -0.155608     -0.0801455   -0.059729    0.051416      0.0614088    0.316425     0.0387424    -0.00796446   -0.156358     0.000175549  -0.0774502  -0.0462236   -0.0303767    0.0290128    0.0451484    0.120452     0.0581522   -0.00287951   0.00555959   -0.0448318     0.124249     0.145692     0.0263464   -0.118337     0.0441515 
 -0.0716378    0.0747517     0.161129     0.063627   -0.0518744     0.0737958   -0.124292    -0.0332329    -0.236136     -0.137447     0.0790961     0.0204514  -0.0216091    0.0720831    0.0063076   -0.029596    -0.177268     0.0192503    0.133158     0.197818     -0.0870686     0.0727393   -0.196233     0.0826321   -0.0800211    0.0613491 
 -0.109419    -0.0104425     0.0245648   -0.110051    0.0340034    -0.0785374    0.0689226   -0.0527366    -0.0265644    -0.15121      0.0333444    -0.145708    0.16324     -0.0576822   -0.0168888   -0.0570111    0.136632    -0.0829974   -0.0727223    0.0482821     0.070526      0.225747     0.00274027  -0.070991    -0.0433373   -0.045345  
  0.0688112   -0.0420407    -0.0130485   -0.0169198   0.000552621  -0.0152477   -0.00184744   0.071474     -0.065845     -0.0212385   -0.00479346    0.178852   -0.0620386    0.283106    -0.13889     -0.0512799    0.20988     -0.096068    -0.0295199   -0.0577105     0.117719      0.121999     0.0840372    0.0111685   -0.0251835    0.0501559 
 -0.0796224   -0.0406435    -0.10094     -0.159136   -0.0644738     0.0160466   -0.120055    -0.0730686    -0.0272633     0.0258914   -0.133667      0.0222106   0.0474268    0.0427223   -0.145769    -0.00319777  -0.1777       0.0092371   -0.0678244   -0.134203     -0.102113      0.146299    -0.0261033    0.186146     0.0204498   -0.142857  
 -0.137303    -0.100561     -0.0907564   -0.0926281  -0.0940555     0.0242341   -0.0217779    0.0528529     0.000183926  -0.00146975  -0.0518132     0.0844706   0.0428371    0.013934    -0.190724    -0.204154    -0.161608    -0.0787197   -0.0350225    0.175592     -0.03489      -0.119501    -0.0112543   -0.115486     0.127383    -0.142826  
  0.165423    -0.183608      0.15488     -0.105794   -0.14522       0.169744    -0.161782    -0.0534459     0.141171     -0.269567    -0.167095     -0.150137    0.0367422    0.100515     0.0862903   -0.0609836    0.115528     0.0448066   -0.143664     0.135361     -0.0234668    -0.0961649   -0.0208886    0.0136388    0.0118039    0.0894701 
 -0.0168968    0.0430453     0.0638033    0.0525171   0.0354785     0.120358    -0.202947     0.193309     -0.137332     -0.128687     0.00831316   -0.115136    0.0504437    0.0506087   -0.136669    -0.0477241   -0.0807753   -0.0646334   -0.047216     0.0110736    -0.0330353     0.195751    -0.0556112   -0.134342     0.0112506   -0.0137791 
  0.15419     -0.0516729    -0.0714768   -0.0333763   0.0410884    -0.0764006   -0.0478873   -0.0261294    -0.0102616    -0.0857001    0.00121957   -0.141465    0.0299893    0.0970061   -0.0385252    0.332655    -0.0564717    0.0735922   -0.210696    -0.0718566    -0.00594308    0.0343773   -0.0409632   -0.245885    -0.077787    -0.0298421 kind diag, method split
0: avll = -1.357254509544217
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.357320
INFO: iteration 2, average log likelihood -1.357258
INFO: iteration 3, average log likelihood -1.356791
INFO: iteration 4, average log likelihood -1.351214
INFO: iteration 5, average log likelihood -1.334120
INFO: iteration 6, average log likelihood -1.325416
INFO: iteration 7, average log likelihood -1.323238
INFO: iteration 8, average log likelihood -1.322265
INFO: iteration 9, average log likelihood -1.321742
INFO: iteration 10, average log likelihood -1.321444
INFO: iteration 11, average log likelihood -1.321262
INFO: iteration 12, average log likelihood -1.321143
INFO: iteration 13, average log likelihood -1.321058
INFO: iteration 14, average log likelihood -1.320986
INFO: iteration 15, average log likelihood -1.320919
INFO: iteration 16, average log likelihood -1.320862
INFO: iteration 17, average log likelihood -1.320820
INFO: iteration 18, average log likelihood -1.320792
INFO: iteration 19, average log likelihood -1.320774
INFO: iteration 20, average log likelihood -1.320763
INFO: iteration 21, average log likelihood -1.320755
INFO: iteration 22, average log likelihood -1.320749
INFO: iteration 23, average log likelihood -1.320745
INFO: iteration 24, average log likelihood -1.320741
INFO: iteration 25, average log likelihood -1.320738
INFO: iteration 26, average log likelihood -1.320735
INFO: iteration 27, average log likelihood -1.320733
INFO: iteration 28, average log likelihood -1.320730
INFO: iteration 29, average log likelihood -1.320729
INFO: iteration 30, average log likelihood -1.320727
INFO: iteration 31, average log likelihood -1.320725
INFO: iteration 32, average log likelihood -1.320724
INFO: iteration 33, average log likelihood -1.320722
INFO: iteration 34, average log likelihood -1.320721
INFO: iteration 35, average log likelihood -1.320719
INFO: iteration 36, average log likelihood -1.320718
INFO: iteration 37, average log likelihood -1.320717
INFO: iteration 38, average log likelihood -1.320715
INFO: iteration 39, average log likelihood -1.320714
INFO: iteration 40, average log likelihood -1.320713
INFO: iteration 41, average log likelihood -1.320712
INFO: iteration 42, average log likelihood -1.320711
INFO: iteration 43, average log likelihood -1.320710
INFO: iteration 44, average log likelihood -1.320709
INFO: iteration 45, average log likelihood -1.320708
INFO: iteration 46, average log likelihood -1.320706
INFO: iteration 47, average log likelihood -1.320705
INFO: iteration 48, average log likelihood -1.320704
INFO: iteration 49, average log likelihood -1.320702
INFO: iteration 50, average log likelihood -1.320701
INFO: EM with 100000 data points 50 iterations avll -1.320701
952.4 data points per parameter
1: avll = [-1.35732,-1.35726,-1.35679,-1.35121,-1.33412,-1.32542,-1.32324,-1.32226,-1.32174,-1.32144,-1.32126,-1.32114,-1.32106,-1.32099,-1.32092,-1.32086,-1.32082,-1.32079,-1.32077,-1.32076,-1.32075,-1.32075,-1.32074,-1.32074,-1.32074,-1.32073,-1.32073,-1.32073,-1.32073,-1.32073,-1.32073,-1.32072,-1.32072,-1.32072,-1.32072,-1.32072,-1.32072,-1.32072,-1.32071,-1.32071,-1.32071,-1.32071,-1.32071,-1.32071,-1.32071,-1.32071,-1.32071,-1.3207,-1.3207,-1.3207]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.320784
INFO: iteration 2, average log likelihood -1.320692
INFO: iteration 3, average log likelihood -1.320244
INFO: iteration 4, average log likelihood -1.315331
INFO: iteration 5, average log likelihood -1.299887
INFO: iteration 6, average log likelihood -1.289429
INFO: iteration 7, average log likelihood -1.285870
INFO: iteration 8, average log likelihood -1.284215
INFO: iteration 9, average log likelihood -1.282958
INFO: iteration 10, average log likelihood -1.281860
INFO: iteration 11, average log likelihood -1.280912
INFO: iteration 12, average log likelihood -1.280132
INFO: iteration 13, average log likelihood -1.279509
INFO: iteration 14, average log likelihood -1.279017
INFO: iteration 15, average log likelihood -1.278618
INFO: iteration 16, average log likelihood -1.278273
INFO: iteration 17, average log likelihood -1.277954
INFO: iteration 18, average log likelihood -1.277666
INFO: iteration 19, average log likelihood -1.277417
INFO: iteration 20, average log likelihood -1.277210
INFO: iteration 21, average log likelihood -1.277047
INFO: iteration 22, average log likelihood -1.276925
INFO: iteration 23, average log likelihood -1.276835
INFO: iteration 24, average log likelihood -1.276765
INFO: iteration 25, average log likelihood -1.276710
INFO: iteration 26, average log likelihood -1.276664
INFO: iteration 27, average log likelihood -1.276624
INFO: iteration 28, average log likelihood -1.276587
INFO: iteration 29, average log likelihood -1.276552
INFO: iteration 30, average log likelihood -1.276518
INFO: iteration 31, average log likelihood -1.276484
INFO: iteration 32, average log likelihood -1.276450
INFO: iteration 33, average log likelihood -1.276417
INFO: iteration 34, average log likelihood -1.276383
INFO: iteration 35, average log likelihood -1.276350
INFO: iteration 36, average log likelihood -1.276317
INFO: iteration 37, average log likelihood -1.276284
INFO: iteration 38, average log likelihood -1.276251
INFO: iteration 39, average log likelihood -1.276219
INFO: iteration 40, average log likelihood -1.276188
INFO: iteration 41, average log likelihood -1.276158
INFO: iteration 42, average log likelihood -1.276128
INFO: iteration 43, average log likelihood -1.276099
INFO: iteration 44, average log likelihood -1.276071
INFO: iteration 45, average log likelihood -1.276046
INFO: iteration 46, average log likelihood -1.276025
INFO: iteration 47, average log likelihood -1.276006
INFO: iteration 48, average log likelihood -1.275990
INFO: iteration 49, average log likelihood -1.275976
INFO: iteration 50, average log likelihood -1.275964
INFO: EM with 100000 data points 50 iterations avll -1.275964
473.9 data points per parameter
2: avll = [-1.32078,-1.32069,-1.32024,-1.31533,-1.29989,-1.28943,-1.28587,-1.28422,-1.28296,-1.28186,-1.28091,-1.28013,-1.27951,-1.27902,-1.27862,-1.27827,-1.27795,-1.27767,-1.27742,-1.27721,-1.27705,-1.27693,-1.27683,-1.27677,-1.27671,-1.27666,-1.27662,-1.27659,-1.27655,-1.27652,-1.27648,-1.27645,-1.27642,-1.27638,-1.27635,-1.27632,-1.27628,-1.27625,-1.27622,-1.27619,-1.27616,-1.27613,-1.2761,-1.27607,-1.27605,-1.27602,-1.27601,-1.27599,-1.27598,-1.27596]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.276094
INFO: iteration 2, average log likelihood -1.275939
INFO: iteration 3, average log likelihood -1.275441
INFO: iteration 4, average log likelihood -1.271137
INFO: iteration 5, average log likelihood -1.257574
INFO: iteration 6, average log likelihood -1.246007
INFO: iteration 7, average log likelihood -1.241206
INFO: iteration 8, average log likelihood -1.239365
INFO: iteration 9, average log likelihood -1.238179
INFO: iteration 10, average log likelihood -1.237168
INFO: iteration 11, average log likelihood -1.236216
INFO: iteration 12, average log likelihood -1.235339
INFO: iteration 13, average log likelihood -1.234538
INFO: iteration 14, average log likelihood -1.233771
INFO: iteration 15, average log likelihood -1.232983
INFO: iteration 16, average log likelihood -1.232141
INFO: iteration 17, average log likelihood -1.231187
INFO: iteration 18, average log likelihood -1.230135
INFO: iteration 19, average log likelihood -1.229017
INFO: iteration 20, average log likelihood -1.227830
INFO: iteration 21, average log likelihood -1.226359
INFO: iteration 22, average log likelihood -1.224834
INFO: iteration 23, average log likelihood -1.224092
INFO: iteration 24, average log likelihood -1.223863
INFO: iteration 25, average log likelihood -1.223748
INFO: iteration 26, average log likelihood -1.223669
INFO: iteration 27, average log likelihood -1.223602
INFO: iteration 28, average log likelihood -1.223537
INFO: iteration 29, average log likelihood -1.223472
INFO: iteration 30, average log likelihood -1.223400
INFO: iteration 31, average log likelihood -1.223319
INFO: iteration 32, average log likelihood -1.223242
INFO: iteration 33, average log likelihood -1.223174
INFO: iteration 34, average log likelihood -1.223115
INFO: iteration 35, average log likelihood -1.223064
INFO: iteration 36, average log likelihood -1.223020
INFO: iteration 37, average log likelihood -1.222983
INFO: iteration 38, average log likelihood -1.222951
INFO: iteration 39, average log likelihood -1.222924
INFO: iteration 40, average log likelihood -1.222900
INFO: iteration 41, average log likelihood -1.222880
INFO: iteration 42, average log likelihood -1.222862
INFO: iteration 43, average log likelihood -1.222845
INFO: iteration 44, average log likelihood -1.222831
INFO: iteration 45, average log likelihood -1.222817
INFO: iteration 46, average log likelihood -1.222805
INFO: iteration 47, average log likelihood -1.222794
INFO: iteration 48, average log likelihood -1.222784
INFO: iteration 49, average log likelihood -1.222774
INFO: iteration 50, average log likelihood -1.222766
INFO: EM with 100000 data points 50 iterations avll -1.222766
236.4 data points per parameter
3: avll = [-1.27609,-1.27594,-1.27544,-1.27114,-1.25757,-1.24601,-1.24121,-1.23937,-1.23818,-1.23717,-1.23622,-1.23534,-1.23454,-1.23377,-1.23298,-1.23214,-1.23119,-1.23013,-1.22902,-1.22783,-1.22636,-1.22483,-1.22409,-1.22386,-1.22375,-1.22367,-1.2236,-1.22354,-1.22347,-1.2234,-1.22332,-1.22324,-1.22317,-1.22312,-1.22306,-1.22302,-1.22298,-1.22295,-1.22292,-1.2229,-1.22288,-1.22286,-1.22285,-1.22283,-1.22282,-1.22281,-1.22279,-1.22278,-1.22277,-1.22277]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.222933
INFO: iteration 2, average log likelihood -1.222584
INFO: iteration 3, average log likelihood -1.219779
INFO: iteration 4, average log likelihood -1.202597
INFO: iteration 5, average log likelihood -1.167279
INFO: iteration 6, average log likelihood -1.141215
WARNING: Variances had to be floored 9
INFO: iteration 7, average log likelihood -1.127266
WARNING: Variances had to be floored 3
INFO: iteration 8, average log likelihood -1.132601
WARNING: Variances had to be floored 11
INFO: iteration 9, average log likelihood -1.138554
INFO: iteration 10, average log likelihood -1.138187
WARNING: Variances had to be floored 9
INFO: iteration 11, average log likelihood -1.128155
INFO: iteration 12, average log likelihood -1.135974
INFO: iteration 13, average log likelihood -1.127673
WARNING: Variances had to be floored 3
INFO: iteration 14, average log likelihood -1.121044
WARNING: Variances had to be floored 9 11
INFO: iteration 15, average log likelihood -1.132059
INFO: iteration 16, average log likelihood -1.145805
INFO: iteration 17, average log likelihood -1.130891
INFO: iteration 18, average log likelihood -1.122262
WARNING: Variances had to be floored 3 9
INFO: iteration 19, average log likelihood -1.116575
INFO: iteration 20, average log likelihood -1.140924
INFO: iteration 21, average log likelihood -1.128173
WARNING: Variances had to be floored 11
INFO: iteration 22, average log likelihood -1.120368
WARNING: Variances had to be floored 3 9
INFO: iteration 23, average log likelihood -1.124184
INFO: iteration 24, average log likelihood -1.143496
INFO: iteration 25, average log likelihood -1.129211
WARNING: Variances had to be floored 9
INFO: iteration 26, average log likelihood -1.121476
WARNING: Variances had to be floored 3
INFO: iteration 27, average log likelihood -1.127425
WARNING: Variances had to be floored 11
INFO: iteration 28, average log likelihood -1.134422
INFO: iteration 29, average log likelihood -1.132847
WARNING: Variances had to be floored 9
INFO: iteration 30, average log likelihood -1.121697
WARNING: Variances had to be floored 3
INFO: iteration 31, average log likelihood -1.128585
INFO: iteration 32, average log likelihood -1.135585
INFO: iteration 33, average log likelihood -1.125292
WARNING: Variances had to be floored 9
INFO: iteration 34, average log likelihood -1.118905
WARNING: Variances had to be floored 3 11
INFO: iteration 35, average log likelihood -1.126732
INFO: iteration 36, average log likelihood -1.142624
INFO: iteration 37, average log likelihood -1.127989
WARNING: Variances had to be floored 9
INFO: iteration 38, average log likelihood -1.120245
WARNING: Variances had to be floored 3
INFO: iteration 39, average log likelihood -1.128241
INFO: iteration 40, average log likelihood -1.135374
INFO: iteration 41, average log likelihood -1.125034
WARNING: Variances had to be floored 9 11
INFO: iteration 42, average log likelihood -1.118344
WARNING: Variances had to be floored 3
INFO: iteration 43, average log likelihood -1.135269
INFO: iteration 44, average log likelihood -1.137620
WARNING: Variances had to be floored 9
INFO: iteration 45, average log likelihood -1.125825
INFO: iteration 46, average log likelihood -1.130701
WARNING: Variances had to be floored 3
INFO: iteration 47, average log likelihood -1.121925
INFO: iteration 48, average log likelihood -1.131466
WARNING: Variances had to be floored 9 11
INFO: iteration 49, average log likelihood -1.121976
INFO: iteration 50, average log likelihood -1.138099
INFO: EM with 100000 data points 50 iterations avll -1.138099
118.1 data points per parameter
4: avll = [-1.22293,-1.22258,-1.21978,-1.2026,-1.16728,-1.14122,-1.12727,-1.1326,-1.13855,-1.13819,-1.12815,-1.13597,-1.12767,-1.12104,-1.13206,-1.1458,-1.13089,-1.12226,-1.11658,-1.14092,-1.12817,-1.12037,-1.12418,-1.1435,-1.12921,-1.12148,-1.12742,-1.13442,-1.13285,-1.1217,-1.12859,-1.13559,-1.12529,-1.11891,-1.12673,-1.14262,-1.12799,-1.12024,-1.12824,-1.13537,-1.12503,-1.11834,-1.13527,-1.13762,-1.12582,-1.1307,-1.12192,-1.13147,-1.12198,-1.1381]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 5 6
INFO: iteration 1, average log likelihood -1.124613
WARNING: Variances had to be floored 5 6 17
INFO: iteration 2, average log likelihood -1.119283
WARNING: Variances had to be floored 5 6 18
INFO: iteration 3, average log likelihood -1.116052
WARNING: Variances had to be floored 5 6 22 28
INFO: iteration 4, average log likelihood -1.103728
WARNING: Variances had to be floored 5 6 18 21
INFO: iteration 5, average log likelihood -1.070328
WARNING: Variances had to be floored 3 5 6 7 28
INFO: iteration 6, average log likelihood -1.033592
WARNING: Variances had to be floored 5 6 9 17 18 22 24 29
INFO: iteration 7, average log likelihood -1.022514
WARNING: Variances had to be floored 5 6 10 14 21 28
INFO: iteration 8, average log likelihood -1.027544
WARNING: Variances had to be floored 5 6 7 18 29
INFO: iteration 9, average log likelihood -1.038586
WARNING: Variances had to be floored 3 5 6 17 21 28
INFO: iteration 10, average log likelihood -1.030546
WARNING: Variances had to be floored 5 6 18 22 29
INFO: iteration 11, average log likelihood -1.027008
WARNING: Variances had to be floored 5 6 7 9 10 14 21 28
INFO: iteration 12, average log likelihood -1.012610
WARNING: Variances had to be floored 5 6 17 18 29
INFO: iteration 13, average log likelihood -1.033515
WARNING: Variances had to be floored 3 5 6 21 22 23 28
INFO: iteration 14, average log likelihood -1.015238
WARNING: Variances had to be floored 5 6 7 17 18 24 29
INFO: iteration 15, average log likelihood -1.027754
WARNING: Variances had to be floored 5 6 14 28
INFO: iteration 16, average log likelihood -1.028029
WARNING: Variances had to be floored 5 6 10 18 21 22 29
INFO: iteration 17, average log likelihood -1.012084
WARNING: Variances had to be floored 3 5 6 7 17 23 28
INFO: iteration 18, average log likelihood -1.015574
WARNING: Variances had to be floored 5 6 18 29
INFO: iteration 19, average log likelihood -1.034614
WARNING: Variances had to be floored 5 6 17 21 22 24 28
INFO: iteration 20, average log likelihood -1.006717
WARNING: Variances had to be floored 5 6 7 10 14 18 23 29
INFO: iteration 21, average log likelihood -1.005975
WARNING: Variances had to be floored 3 5 6 28
INFO: iteration 22, average log likelihood -1.044103
WARNING: Variances had to be floored 5 6 17 18 21 22 29
INFO: iteration 23, average log likelihood -1.012017
WARNING: Variances had to be floored 5 6 7 14 23 28
INFO: iteration 24, average log likelihood -1.012542
WARNING: Variances had to be floored 3 5 6 10 17 18 29
INFO: iteration 25, average log likelihood -1.025535
WARNING: Variances had to be floored 5 6 9 21 22 24 28
INFO: iteration 26, average log likelihood -1.023152
WARNING: Variances had to be floored 5 6 7 18 23 29
INFO: iteration 27, average log likelihood -1.017862
WARNING: Variances had to be floored 3 5 6 17 28
INFO: iteration 28, average log likelihood -1.021532
WARNING: Variances had to be floored 5 6 10 12 18 21 22 29
INFO: iteration 29, average log likelihood -1.001309
WARNING: Variances had to be floored 5 6 7 9 28
INFO: iteration 30, average log likelihood -1.024455
WARNING: Variances had to be floored 3 5 6 17 18 23 29
INFO: iteration 31, average log likelihood -1.014998
WARNING: Variances had to be floored 5 6 21 22 24 28
INFO: iteration 32, average log likelihood -1.017397
WARNING: Variances had to be floored 5 6 7 10 18 29
INFO: iteration 33, average log likelihood -1.006952
WARNING: Variances had to be floored 5 6 9 17 23 28
INFO: iteration 34, average log likelihood -1.015107
WARNING: Variances had to be floored 3 5 6 18 21 22 29
INFO: iteration 35, average log likelihood -1.020161
WARNING: Variances had to be floored 5 6 7 17 28
INFO: iteration 36, average log likelihood -1.015996
WARNING: Variances had to be floored 5 6 10 14 18 23 29
INFO: iteration 37, average log likelihood -1.005382
WARNING: Variances had to be floored 3 5 6 17 21 22 24 28
INFO: iteration 38, average log likelihood -1.024660
WARNING: Variances had to be floored 5 6 7 18 29
INFO: iteration 39, average log likelihood -1.021330
WARNING: Variances had to be floored 5 6 12 23 28
INFO: iteration 40, average log likelihood -1.009086
WARNING: Variances had to be floored 5 6 10 14 17 18 21 22 29
INFO: iteration 41, average log likelihood -1.009114
WARNING: Variances had to be floored 3 5 6 7 28
INFO: iteration 42, average log likelihood -1.027753
WARNING: Variances had to be floored 5 6 18 23 29
INFO: iteration 43, average log likelihood -1.019235
WARNING: Variances had to be floored 5 6 9 17 21 22 24 28
INFO: iteration 44, average log likelihood -1.007171
WARNING: Variances had to be floored 3 5 6 7 10 18 29
INFO: iteration 45, average log likelihood -1.007937
WARNING: Variances had to be floored 5 6 23 28
INFO: iteration 46, average log likelihood -1.026656
WARNING: Variances had to be floored 5 6 17 18 21 22 29
INFO: iteration 47, average log likelihood -1.013058
WARNING: Variances had to be floored 3 5 6 7 9 28
INFO: iteration 48, average log likelihood -1.006025
WARNING: Variances had to be floored 5 6 10 18 23 29
INFO: iteration 49, average log likelihood -1.013020
WARNING: Variances had to be floored 5 6 7 17 21 22 24 28
INFO: iteration 50, average log likelihood -1.017680
INFO: EM with 100000 data points 50 iterations avll -1.017680
59.0 data points per parameter
5: avll = [-1.12461,-1.11928,-1.11605,-1.10373,-1.07033,-1.03359,-1.02251,-1.02754,-1.03859,-1.03055,-1.02701,-1.01261,-1.03351,-1.01524,-1.02775,-1.02803,-1.01208,-1.01557,-1.03461,-1.00672,-1.00597,-1.0441,-1.01202,-1.01254,-1.02553,-1.02315,-1.01786,-1.02153,-1.00131,-1.02446,-1.015,-1.0174,-1.00695,-1.01511,-1.02016,-1.016,-1.00538,-1.02466,-1.02133,-1.00909,-1.00911,-1.02775,-1.01923,-1.00717,-1.00794,-1.02666,-1.01306,-1.00603,-1.01302,-1.01768]
[-1.35725,-1.35732,-1.35726,-1.35679,-1.35121,-1.33412,-1.32542,-1.32324,-1.32226,-1.32174,-1.32144,-1.32126,-1.32114,-1.32106,-1.32099,-1.32092,-1.32086,-1.32082,-1.32079,-1.32077,-1.32076,-1.32075,-1.32075,-1.32074,-1.32074,-1.32074,-1.32073,-1.32073,-1.32073,-1.32073,-1.32073,-1.32073,-1.32072,-1.32072,-1.32072,-1.32072,-1.32072,-1.32072,-1.32072,-1.32071,-1.32071,-1.32071,-1.32071,-1.32071,-1.32071,-1.32071,-1.32071,-1.32071,-1.3207,-1.3207,-1.3207,-1.32078,-1.32069,-1.32024,-1.31533,-1.29989,-1.28943,-1.28587,-1.28422,-1.28296,-1.28186,-1.28091,-1.28013,-1.27951,-1.27902,-1.27862,-1.27827,-1.27795,-1.27767,-1.27742,-1.27721,-1.27705,-1.27693,-1.27683,-1.27677,-1.27671,-1.27666,-1.27662,-1.27659,-1.27655,-1.27652,-1.27648,-1.27645,-1.27642,-1.27638,-1.27635,-1.27632,-1.27628,-1.27625,-1.27622,-1.27619,-1.27616,-1.27613,-1.2761,-1.27607,-1.27605,-1.27602,-1.27601,-1.27599,-1.27598,-1.27596,-1.27609,-1.27594,-1.27544,-1.27114,-1.25757,-1.24601,-1.24121,-1.23937,-1.23818,-1.23717,-1.23622,-1.23534,-1.23454,-1.23377,-1.23298,-1.23214,-1.23119,-1.23013,-1.22902,-1.22783,-1.22636,-1.22483,-1.22409,-1.22386,-1.22375,-1.22367,-1.2236,-1.22354,-1.22347,-1.2234,-1.22332,-1.22324,-1.22317,-1.22312,-1.22306,-1.22302,-1.22298,-1.22295,-1.22292,-1.2229,-1.22288,-1.22286,-1.22285,-1.22283,-1.22282,-1.22281,-1.22279,-1.22278,-1.22277,-1.22277,-1.22293,-1.22258,-1.21978,-1.2026,-1.16728,-1.14122,-1.12727,-1.1326,-1.13855,-1.13819,-1.12815,-1.13597,-1.12767,-1.12104,-1.13206,-1.1458,-1.13089,-1.12226,-1.11658,-1.14092,-1.12817,-1.12037,-1.12418,-1.1435,-1.12921,-1.12148,-1.12742,-1.13442,-1.13285,-1.1217,-1.12859,-1.13559,-1.12529,-1.11891,-1.12673,-1.14262,-1.12799,-1.12024,-1.12824,-1.13537,-1.12503,-1.11834,-1.13527,-1.13762,-1.12582,-1.1307,-1.12192,-1.13147,-1.12198,-1.1381,-1.12461,-1.11928,-1.11605,-1.10373,-1.07033,-1.03359,-1.02251,-1.02754,-1.03859,-1.03055,-1.02701,-1.01261,-1.03351,-1.01524,-1.02775,-1.02803,-1.01208,-1.01557,-1.03461,-1.00672,-1.00597,-1.0441,-1.01202,-1.01254,-1.02553,-1.02315,-1.01786,-1.02153,-1.00131,-1.02446,-1.015,-1.0174,-1.00695,-1.01511,-1.02016,-1.016,-1.00538,-1.02466,-1.02133,-1.00909,-1.00911,-1.02775,-1.01923,-1.00717,-1.00794,-1.02666,-1.01306,-1.00603,-1.01302,-1.01768]
32×26 Array{Float64,2}:
 -0.130861    -0.115386     -0.0799217   -0.0806706   -0.0987334     0.0294669    -0.0192143   0.0505608   -0.00606022   0.00669591   -0.10378      0.0856201    0.0653161    0.011545    -0.187943    -0.207324    -0.183526    -0.0813501   -0.0248847    0.17738     -0.00951811  -0.0913557     0.0193825    -0.081739     0.0991356    -0.135766  
  0.00667952  -0.0754003    -0.0333959   -0.0330353   -0.01106       0.0260319     0.109548    0.0232081    0.0826893   -0.130942      0.0257838   -0.0540571   -0.0432248   -0.0460293    0.0114744    0.0636766    0.138497     0.0273187   -0.0778794   -0.0169396   -0.0230785    0.0413421     0.0935976     0.023316    -0.033716      0.0538275 
 -0.0268592    0.0429599    -0.0904077   -0.0798308   -0.0141839    -0.0212885    -0.0420943   0.0638793   -0.0428676    0.177549     -0.0826914   -0.0745761   -0.0473888    0.0926592    0.0698346   -0.0509526   -0.169505    -0.0815154    0.0614673    0.01346     -0.0842717    0.230812     -0.130692      0.0387968   -0.0735771    -0.0116849 
 -0.0857534    0.052404     -0.0174993   -0.0268697    0.191795      0.136614      0.206879   -0.143819    -0.0727523    0.0914347    -0.0503255   -0.0116116    0.239648    -0.00279955   0.0401132    0.217489    -0.0307844   -0.119844     0.180252    -0.0901258    0.00324586   0.0553705    -0.110418     -0.0755151    0.0250753    -0.09089   
 -0.246778     0.130383     -0.0673063    0.0115653   -0.00894664    0.0127707    -0.0132068  -0.316716    -0.417586    -0.000520934  -0.00315287  -0.1268       0.155713     0.299292     0.130767    -0.209699    -0.00987664  -0.076532    -0.126394    -0.0035346   -0.0549716    0.0976965     0.0932542     0.0354392   -0.13739       0.0642435 
 -0.200603     0.130626      0.0214961    0.00613883  -0.115244     -0.112055     -0.0166037   0.388443     0.4265      -0.0667904    -0.0019118   -0.065158     0.0606802   -0.294652     0.0299723    0.156691    -0.0181596   -0.0929284   -0.126249    -0.0139634   -0.10512      0.0165448     0.0441461    -0.115093    -0.0480795     0.0820865 
 -0.0129221   -0.101363     -0.0136377   -0.0167394    0.0384641    -0.000171302  -0.0986501   0.0946805    0.0322884   -0.0692243     0.0923032   -0.0445749   -0.058595    -0.100109    -0.23928     -0.16945     -0.0276136    0.00500364  -0.10406      0.0745142   -0.0344274   -0.125511     -0.0912476    -0.00156955   0.0798621    -0.111538  
 -0.125241     0.0749207     0.143346    -0.107166     0.0941026    -0.0541497     0.0875631   0.227083    -0.0747478   -0.143538     -0.073466    -0.028978    -0.16492     -0.0770052    0.157528    -0.0192064    0.0422123    0.158218    -0.185485     0.0552982   -0.13978      0.0632701    -0.146987      0.0612844    0.0593291     0.0267925 
 -0.00313794  -0.0650316    -0.161555     0.048799     0.051909      0.0435328     0.0575163   0.124785     0.0571643    0.0516869     0.166979    -0.0699203   -0.0944918    0.0135102    0.00686652   0.0712061    0.122834    -0.0325348    0.149196     0.175297    -0.0113216    0.0521836     0.03499       0.00129873   0.0596835    -0.118098  
 -0.0524675   -0.171938      0.0555529   -0.083997     0.0568013     0.0441466    -0.0517721   0.0751035    0.0208806   -0.0492719    -0.0397016    0.0591653    0.012833    -0.0443574   -0.225965    -0.024574     0.0359322    0.11168      0.0953482   -0.169818    -0.0975464   -0.000817311   0.127553      0.0695068   -0.00409168   -0.107376  
 -0.0242054   -0.000679125   0.00358744   0.0105461    0.0661588     0.0184199     0.0145696  -0.0273804    0.00446433  -0.0634268     0.101032    -0.0629566    0.0855598   -0.0583941   -0.118677    -0.00683765   0.0279121   -0.0872469    0.0658789    0.067256     0.0273534    0.0486723     0.000558552   0.0136894   -0.0521661    -0.0492912 
  0.0545258   -0.0381875     0.160055    -0.00553719  -0.0445981     0.012775     -0.108237   -0.105479    -0.0965307   -0.127879      0.0333949    0.00892367  -0.0357529    0.0917731   -0.0184314   -0.0423171   -0.0386102   -0.0148569    0.12014      0.159921    -0.0964162    0.0819228    -0.112749      0.0357482   -0.0359657    -0.0767193 
 -0.0782794   -0.051202     -0.104502    -0.142203    -0.00972246    0.0151042    -0.0686661  -0.0593055   -0.0289745    0.0385577    -0.129647    -0.00438362   0.0433823    0.0470368   -0.15872      0.00584117  -0.171468     0.0340228   -0.0425055   -0.105226    -0.105478     0.138253     -0.0253261     0.230151     0.0498588    -0.145656  
 -0.00781612  -0.0938784     0.0318594    0.0727329    0.0755789     0.131444     -0.0459545   0.151171    -0.107247    -0.141493      0.0663435    0.02958      0.121309     0.031387    -0.152116     0.0877778    0.0179848    0.00661739  -0.0798836   -0.00503771  -0.0608493    0.0932832    -0.0107471    -0.00897147  -0.0508316     0.0834381 
 -0.0234505   -0.0692415     0.124691     0.00183014   0.174664      0.0661671    -0.0129558   0.0657295   -0.00610532   0.0547931    -0.0442075    0.131497     0.129198    -0.0302145    0.172622     0.0345037   -0.0311922    0.076443     0.102877    -0.0283615    0.0397244    0.044988      0.0363419     0.0486112    0.031876      0.0573413 
 -0.124878    -0.0828792     0.058983    -0.102906    -0.00495302    0.0398054     0.162406    0.0458892   -0.120026     0.12624      -0.0688233    0.124457    -0.0158568   -0.161492     0.0751029    0.0291318   -0.0141732    0.134813     0.138655    -0.201287     0.0897484   -0.178341      0.0692417    -0.0145905    0.0959753    -0.112675  
 -0.0421688    0.0714905     0.0325301    0.0507756    0.0368099     0.127166     -0.22324     0.111538     0.00515786  -0.10784      -0.0484336   -0.129921     0.0274396    0.0301607   -0.138837    -0.113743    -0.0808569   -0.0509678   -0.023469     0.108777    -0.0304021    0.244168     -0.0410231    -0.182227     0.0136431    -0.00833644
 -0.218635     0.0724571    -0.0382714   -0.0664183    0.160194      0.188959      0.250115   -0.0788227   -0.055181     0.130785     -0.170411     0.0967978    0.117158     0.00308348  -0.128674    -0.0462707   -0.168937     0.0650717    0.0369318    0.125167     0.216652    -0.0859141    -0.0335849    -0.178438     0.0535246     0.0680656 
  0.086969     0.368373      0.0612631    0.0151885    0.204962     -0.0330972     0.108539   -0.133425     0.035608     0.126375     -0.814081     0.0982021   -0.167402    -0.091369     0.0698836    0.101655     0.129023    -0.0720667   -0.118503    -0.0801085    0.057093     0.124261     -0.138178      0.0801815   -0.0363828     0.0401788 
  0.0108552    0.196085      0.0574805    0.0254836    0.150873     -0.0480074     0.0962909  -0.121545     0.0432092    0.101332      0.8365       0.0403322   -0.236763    -0.0873611    0.0424856    0.0945827    0.147102    -0.0745935   -0.110383    -0.100496    -0.0847141    0.0247291    -0.0674102     0.0789897    0.065857     -0.0162809 
 -0.199981     0.0662378    -0.051323    -0.0978825   -0.000857557  -0.13789       0.0734193   0.0225725    0.102488     0.0274499     0.0260331   -0.0662291    0.137649     0.0369798    0.0963597   -0.150279     0.109213     0.143722    -0.133295     0.110357    -0.122997    -0.0225552     0.000487264   0.0588702    0.106472      0.0269967 
  0.219421    -0.12823       0.0663709   -0.177218    -0.183861     -0.248415     -0.0687232   0.00314886   0.0275855   -0.0276442     0.0310759   -0.0221057    0.112003     0.146791    -0.0426678   -0.00667288   0.134296     0.0299379    0.00243467  -0.12341     -0.0400867   -0.0989005    -0.068117     -0.104695    -0.0525839     0.135488  
 -0.0476934   -0.0729463     0.00104083   0.0548404   -0.00502987   -0.239666      0.0204188   0.081492     0.154931    -0.0663972     0.0807636   -0.0561844   -0.0124116    0.014043    -0.0431026    0.0729258   -0.0423919    0.145828    -0.0195035    0.0581242    0.0656353   -0.153552     -0.134849     -0.118263     0.0548283    -0.126387  
 -0.0740626    0.17746       0.014332    -0.0798474    0.0910347    -0.109041     -0.0163979   0.0799407   -0.0206194    0.251414      0.110862     0.112159    -0.0753469    0.107954     0.0501977    0.0219148    0.215571    -0.00518136   0.115877    -0.0119817   -0.0420981    0.0095847    -0.0423778    -0.0393764   -0.0592617     0.155894  
  0.113103    -0.121163      0.180177    -0.21364     -0.00380319    0.147621      0.118624   -0.0133487    0.0766764    0.194946      0.229748    -0.03306      0.0653775    0.0450539    0.0530272   -0.741628     0.0756269   -0.290851    -0.140976    -0.124881     0.0457982   -0.0711346     0.109118      0.176213    -0.120482     -0.0043235 
  0.0231364   -0.0355557     0.235629    -0.200611    -0.160023      0.152672      0.102086   -0.0274545    0.0753214    0.128613      0.238392     0.208562     0.29503      0.0933433    0.0423951    0.274552     0.110268    -0.307295    -0.153425    -0.0595857    0.0296004    0.279666      0.156974      0.0559926   -0.124091     -0.00899239
  0.152008    -0.0579103    -0.0711812   -0.0596721    0.0313911    -0.0706635    -0.0485794  -0.0230292   -0.0050427   -0.0926584     0.00505105  -0.135129     0.0214466    0.0960415   -0.0330919    0.346991    -0.060196     0.0557303   -0.149303    -0.0707712   -0.00152696   0.0219246    -0.0614138    -0.27059     -0.0763781    -0.0301367 
 -0.103176    -0.0867721    -0.0211923    0.0865503   -0.0490749    -0.0476715     0.0410777   0.0721698   -0.116826     0.11576       0.062732    -0.0300974    0.0900055    0.0864217    0.0587931   -0.00095284   0.0156509    0.116798    -0.0922123    0.069991    -0.0907621   -0.0110471    -0.0970429    -0.00376655  -0.0877503    -0.164066  
  0.151024    -0.178904      0.16269     -0.106371    -0.134126      0.157192     -0.166315   -0.0509174    0.138403    -0.284783     -0.15814     -0.154383     0.0348804    0.0958262    0.0770732   -0.0492782    0.104219     0.0431265   -0.131015     0.10517     -0.0228305   -0.0728672     0.0039284     0.0155927   -0.000179313   0.111186  
 -0.0320548    0.016361      0.0451386   -0.122799    -0.00771171    0.0686756    -0.154813    0.0132669    0.0618305   -0.138143      0.124001    -0.0457872    0.00633848  -0.0286658    0.0679634    0.212171    -0.181461     0.0212296   -0.0701525   -0.0483441   -0.00463277  -0.0350482     0.0708256    -0.0105706   -0.103194      0.052328  
 -0.0553336    0.00836068   -0.0188073   -0.026641     0.0184689    -0.0721688     0.0524827   0.0504952    0.00929774  -0.0647311    -0.0144711    0.167938    -0.0879855    0.23267     -0.0514781   -0.0425072    0.207003    -0.0065042   -0.00863418  -0.0435139   -0.151051     0.0414721     0.0581609    -0.0242303   -0.0796241     0.0825436 
  0.128994    -0.043571     -0.00804034   0.00523494  -0.0779008     3.83745e-5   -0.0556313   0.0742111   -0.138434     0.00789926   -0.00190088   0.163409    -0.0723189    0.301647    -0.162294    -0.0615974    0.207236    -0.116893    -0.0721904   -0.0618842    0.253282     0.112567      0.1189        0.024039     0.000791941   0.0272793 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 3 5 6 18 29
INFO: iteration 1, average log likelihood -1.019381
WARNING: Variances had to be floored 3 5 6 9 18 23 28 29
INFO: iteration 2, average log likelihood -0.989611
WARNING: Variances had to be floored 3 5 6 7 9 10 17 18 21 22 29
INFO: iteration 3, average log likelihood -0.989692
WARNING: Variances had to be floored 3 5 6 18 23 28 29
INFO: iteration 4, average log likelihood -1.001428
WARNING: Variances had to be floored 3 5 6 9 18 29
INFO: iteration 5, average log likelihood -0.999364
WARNING: Variances had to be floored 3 5 6 7 9 10 17 18 21 22 23 24 28 29
INFO: iteration 6, average log likelihood -0.969001
WARNING: Variances had to be floored 3 5 6 18 29
INFO: iteration 7, average log likelihood -1.020653
WARNING: Variances had to be floored 3 5 6 9 18 23 28 29
INFO: iteration 8, average log likelihood -0.990028
WARNING: Variances had to be floored 3 5 6 7 9 10 14 17 18 21 22 28 29
INFO: iteration 9, average log likelihood -0.983869
WARNING: Variances had to be floored 3 5 6 18 23 29
INFO: iteration 10, average log likelihood -1.011754
INFO: EM with 100000 data points 10 iterations avll -1.011754
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.040310e+05
      1       6.091794e+05      -1.948516e+05 |       32
      2       5.861031e+05      -2.307635e+04 |       32
      3       5.691942e+05      -1.690884e+04 |       32
      4       5.569614e+05      -1.223278e+04 |       32
      5       5.502998e+05      -6.661693e+03 |       32
      6       5.470634e+05      -3.236327e+03 |       32
      7       5.452233e+05      -1.840150e+03 |       32
      8       5.442589e+05      -9.644269e+02 |       32
      9       5.437225e+05      -5.363343e+02 |       32
     10       5.432915e+05      -4.310287e+02 |       32
     11       5.427999e+05      -4.915952e+02 |       32
     12       5.422194e+05      -5.804953e+02 |       32
     13       5.415961e+05      -6.233212e+02 |       32
     14       5.410997e+05      -4.963480e+02 |       32
     15       5.407426e+05      -3.570864e+02 |       32
     16       5.404066e+05      -3.360172e+02 |       32
     17       5.399321e+05      -4.745414e+02 |       32
     18       5.392232e+05      -7.089149e+02 |       32
     19       5.384671e+05      -7.561004e+02 |       32
     20       5.378312e+05      -6.359160e+02 |       32
     21       5.374604e+05      -3.707623e+02 |       32
     22       5.372952e+05      -1.652138e+02 |       32
     23       5.372287e+05      -6.646697e+01 |       32
     24       5.372032e+05      -2.550262e+01 |       31
     25       5.371867e+05      -1.650641e+01 |       30
     26       5.371783e+05      -8.432455e+00 |       30
     27       5.371733e+05      -4.986829e+00 |       25
     28       5.371704e+05      -2.839469e+00 |       23
     29       5.371685e+05      -1.906876e+00 |       23
     30       5.371665e+05      -2.080727e+00 |       20
     31       5.371646e+05      -1.887463e+00 |       20
     32       5.371634e+05      -1.194564e+00 |       15
     33       5.371623e+05      -1.101427e+00 |       18
     34       5.371608e+05      -1.444683e+00 |       21
     35       5.371592e+05      -1.673042e+00 |       22
     36       5.371576e+05      -1.524262e+00 |       15
     37       5.371562e+05      -1.437281e+00 |       20
     38       5.371540e+05      -2.199504e+00 |       21
     39       5.371517e+05      -2.302403e+00 |       20
     40       5.371500e+05      -1.685147e+00 |       21
     41       5.371481e+05      -1.942522e+00 |       18
     42       5.371464e+05      -1.693847e+00 |       19
     43       5.371445e+05      -1.910180e+00 |       22
     44       5.371431e+05      -1.313783e+00 |       20
     45       5.371418e+05      -1.357172e+00 |       19
     46       5.371405e+05      -1.317988e+00 |       20
     47       5.371393e+05      -1.172823e+00 |       17
     48       5.371384e+05      -8.536232e-01 |       15
     49       5.371377e+05      -7.288056e-01 |       13
     50       5.371372e+05      -5.092929e-01 |       11
K-means terminated without convergence after 50 iterations (objv = 537137.2058453213)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.265970
INFO: iteration 2, average log likelihood -1.234396
INFO: iteration 3, average log likelihood -1.210180
INFO: iteration 4, average log likelihood -1.180504
WARNING: Variances had to be floored 30
INFO: iteration 5, average log likelihood -1.131857
WARNING: Variances had to be floored 19 21 31
INFO: iteration 6, average log likelihood -1.085522
WARNING: Variances had to be floored 8 22 24
INFO: iteration 7, average log likelihood -1.070943
WARNING: Variances had to be floored 1 11 14 30
INFO: iteration 8, average log likelihood -1.034696
WARNING: Variances had to be floored 5 7 13 17 21 28
INFO: iteration 9, average log likelihood -1.036071
WARNING: Variances had to be floored 19 22
INFO: iteration 10, average log likelihood -1.073657
WARNING: Variances had to be floored 8 30 31
INFO: iteration 11, average log likelihood -1.053610
WARNING: Variances had to be floored 1 11 14
INFO: iteration 12, average log likelihood -1.038199
WARNING: Variances had to be floored 5 7 17 21
INFO: iteration 13, average log likelihood -1.029668
WARNING: Variances had to be floored 19 22 24 28 30
INFO: iteration 14, average log likelihood -1.034820
WARNING: Variances had to be floored 8 13 31
INFO: iteration 15, average log likelihood -1.053373
WARNING: Variances had to be floored 1 11 14
INFO: iteration 16, average log likelihood -1.034589
WARNING: Variances had to be floored 5 7 21 23 30
INFO: iteration 17, average log likelihood -1.027515
WARNING: Variances had to be floored 19 22 28
INFO: iteration 18, average log likelihood -1.051817
WARNING: Variances had to be floored 8 30 31
INFO: iteration 19, average log likelihood -1.036043
WARNING: Variances had to be floored 1 5 11 13 14 23
INFO: iteration 20, average log likelihood -1.010814
WARNING: Variances had to be floored 7 21 22
INFO: iteration 21, average log likelihood -1.038975
WARNING: Variances had to be floored 19 24 28 30
INFO: iteration 22, average log likelihood -1.029862
WARNING: Variances had to be floored 5 8 17 23 31
INFO: iteration 23, average log likelihood -1.034511
WARNING: Variances had to be floored 1 11 21 22
INFO: iteration 24, average log likelihood -1.038964
WARNING: Variances had to be floored 7 19 30
INFO: iteration 25, average log likelihood -1.046475
WARNING: Variances had to be floored 23 28
INFO: iteration 26, average log likelihood -1.037523
WARNING: Variances had to be floored 1 5 8 21 22 24 30 31
INFO: iteration 27, average log likelihood -0.983780
WARNING: Variances had to be floored 11 13 14 19
INFO: iteration 28, average log likelihood -1.066246
WARNING: Variances had to be floored 7 28
INFO: iteration 29, average log likelihood -1.069519
WARNING: Variances had to be floored 8 22 30
INFO: iteration 30, average log likelihood -1.032285
WARNING: Variances had to be floored 1 5 19 21 23 24
INFO: iteration 31, average log likelihood -1.014879
WARNING: Variances had to be floored 11 17 31
INFO: iteration 32, average log likelihood -1.051915
WARNING: Variances had to be floored 7 14 22 28 30
INFO: iteration 33, average log likelihood -1.021908
WARNING: Variances had to be floored 1 5 8 19 24
INFO: iteration 34, average log likelihood -1.034380
WARNING: Variances had to be floored 11 21
INFO: iteration 35, average log likelihood -1.061758
WARNING: Variances had to be floored 13 22 30 31
INFO: iteration 36, average log likelihood -1.030561
WARNING: Variances had to be floored 7 8 19 23 28
INFO: iteration 37, average log likelihood -1.022218
WARNING: Variances had to be floored 1 5 11 14 24
INFO: iteration 38, average log likelihood -1.033612
WARNING: Variances had to be floored 17 21 22 30
INFO: iteration 39, average log likelihood -1.052414
WARNING: Variances had to be floored 31
INFO: iteration 40, average log likelihood -1.054881
WARNING: Variances had to be floored 5 7 8 19 23 28
INFO: iteration 41, average log likelihood -1.002401
WARNING: Variances had to be floored 1 11 13 21 22 24 30
INFO: iteration 42, average log likelihood -1.024199
WARNING: Variances had to be floored 14
INFO: iteration 43, average log likelihood -1.078280
WARNING: Variances had to be floored 31
INFO: iteration 44, average log likelihood -1.033020
WARNING: Variances had to be floored 5 7 8 11 19 21 22 23 28 30
INFO: iteration 45, average log likelihood -0.977818
WARNING: Variances had to be floored 1 14 17
INFO: iteration 46, average log likelihood -1.079883
WARNING: Variances had to be floored 24
INFO: iteration 47, average log likelihood -1.065991
WARNING: Variances had to be floored 5 19 22 30 31
INFO: iteration 48, average log likelihood -1.017278
WARNING: Variances had to be floored 1 7 8 11 13 28
INFO: iteration 49, average log likelihood -1.027240
WARNING: Variances had to be floored 14 21 23 24
INFO: iteration 50, average log likelihood -1.051310
INFO: EM with 100000 data points 50 iterations avll -1.051310
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.089988    -0.0843582   -0.0363117    0.0924037   -0.0303456    -0.0581937     0.0251327     0.0713511   -0.109494     0.113779     0.0433075   -0.0122111    0.110746     0.0715746    0.0915583    0.0056276   0.00191073   0.22001     -0.1477       0.0647759   -0.0516962    -0.0257774    -0.108469    -0.024624    -0.0602954   -0.118802  
 -0.00519231  -0.168357    -0.0931407   -0.0638298    0.0369246     0.0665027     0.29789       0.0295761   -0.013049    -0.159514    -0.00152961  -0.08057     -0.0479442   -0.0289519    0.0373818    0.0578296   0.114494     0.0585285   -0.0352982    0.00103918  -0.0623459     0.126344      0.126355     0.0527661   -0.148046     0.0488288 
  0.0997431   -0.0104022   -0.0153237   -0.0303236   -0.132757     -0.0971978    -0.0155536     0.0684407   -0.25102      0.0124338   -0.00382762   0.182913    -0.01669      0.289956    -0.198864    -0.0677172   0.228102    -0.0188624   -0.0252798   -0.088704    -0.262354      0.0463794     0.0502671    0.131173     0.0745693   -0.0593933 
 -0.0763995   -0.0516986   -0.105986    -0.123391    -0.00918134    0.0159416    -0.0662046    -0.0522364   -0.0354275    0.0394513   -0.128146    -0.003149     0.0463637    0.0492156   -0.153751     0.0103894  -0.167321     0.0410233   -0.0506361   -0.097556    -0.10525       0.13168      -0.0246168    0.217795     0.0397881   -0.144802  
  0.412955    -0.175365     0.0409292   -0.143402    -0.140514     -0.24953      -0.0930817     0.010816     0.0109601   -0.025734     0.0277344   -0.075294     0.0981476    0.19547     -0.0621314    0.144508    0.0821634    0.0246457   -0.00488766  -0.0906592   -0.010782     -0.136062     -0.0900252   -0.14701     -0.0651605    0.0995511 
  0.177062    -0.144888     0.146818    -0.0809402   -0.0436751    -0.0503427    -0.0951118    -0.170877     0.0552217   -0.092579    -0.00090142   0.00320053  -0.0429242    0.11355     -0.0496377   -0.0736959   0.108585    -0.0616547    0.0963082    0.120859    -0.108018      0.07702      -0.0273765   -0.0165511    0.00667199  -0.213371  
  0.0165803   -0.165091    -0.0131711    0.0918433    0.109152      0.131519      0.0213425     0.0997546   -0.0287528   -0.110346     0.10618      0.127409     0.160285    -0.0280729   -0.131762     0.16186     0.118932     0.0816632   -0.0648305   -0.071311    -0.016272     -0.000399437   0.0187846    0.0216276   -0.041347     0.116896  
 -0.0399271   -0.0634573   -0.0034687    0.0553016   -0.00454913   -0.251693      0.0125555     0.0836833    0.147244    -0.0781443    0.0889056   -0.0458068   -0.0104102    0.00544971  -0.0491452    0.0747387  -0.0468772    0.157062    -0.0150611    0.0546911    0.0671296    -0.172516     -0.195219    -0.118874     0.08372     -0.131345  
 -0.0872327    0.0534383   -0.0213779   -0.0246533    0.182703      0.136614      0.199039     -0.135864    -0.0749906    0.0919168   -0.044846    -0.0115975    0.245788     0.00314127   0.0360121    0.211508   -0.0321315   -0.108795     0.16661     -0.0845612    0.000368692   0.0553891    -0.112878    -0.070345     0.0231616   -0.0910542 
  0.0506979    0.287572     0.0595058    0.020063     0.179761     -0.0397595     0.103364     -0.127723     0.0393869    0.114365    -0.0425352    0.0708505   -0.200116    -0.0893606    0.0577943    0.0981879   0.137595    -0.0731893   -0.114552    -0.0889648   -0.00998143    0.0766737    -0.103878     0.0795666    0.011451     0.0151952 
 -0.035615     0.0505742   -0.0736822   -0.0708496   -0.010028     -0.0198961    -0.0354888     0.067637    -0.038114     0.184803    -0.0515701   -0.0709457   -0.0890244    0.143157     0.0805097   -0.0886082  -0.153106    -0.141146     0.0949064    0.0239331   -0.0639233     0.230349     -0.172634     0.0344577   -0.0985099   -0.0428208 
 -0.130246    -0.115774    -0.0785576   -0.0777269   -0.0966839     0.0277226    -0.0182581     0.0516396   -0.00940769   0.00983457  -0.102817     0.085671     0.0636945    0.0105361   -0.188642    -0.20441    -0.180217    -0.0803407   -0.0265869    0.174289    -0.012625     -0.0892022     0.00967631  -0.0785794    0.0935799   -0.136634  
 -0.192522     0.11269     -0.0228021   -0.00171683  -0.0234313    -0.0945101    -0.00934838    0.0451979   -0.0456979    0.0229535   -0.00331512  -0.105274     0.110297    -0.0386947    0.0194454    0.0081909  -0.0294953   -0.207192    -0.107522    -0.00765451  -0.0279335     0.0413349     0.128569    -0.0745137   -0.0317819    0.0241667 
 -0.060603     0.0643932    0.178215     0.0613606   -0.0513418     0.0763891    -0.120773     -0.0472      -0.232406    -0.159887     0.0630446    0.0159182   -0.0286209    0.0722872    0.0264964   -0.0115037  -0.176863     0.00494451   0.134077     0.195883    -0.0874619     0.0876684    -0.200413     0.0809104   -0.0829398    0.0447637 
  0.0652422   -0.0777203    0.208365    -0.206488    -0.0858172     0.150515      0.109359     -0.0205105    0.0758895    0.160652     0.233805     0.0925134    0.184404     0.070517     0.047411    -0.213476    0.0928542   -0.298306    -0.145227    -0.0901591    0.0364391     0.111326      0.134166     0.113437    -0.122329    -0.00729606
 -0.141119     0.126671    -0.00741726  -0.0741551    0.129066      0.0327306     0.102302      0.0102352   -0.0328679    0.189553    -0.0253729    0.0960777    0.0162897    0.0509376   -0.0362791   -0.0163368   0.0270562    0.0286395    0.0773365    0.0541151    0.0769956    -0.0336192    -0.039604    -0.105827    -0.00429302   0.109653  
  0.0332925   -0.0011152   -0.0108828    0.117466     0.0933355     0.133835     -0.00762057   -0.00212037   0.0180884    0.00361777   0.162416    -0.00430628   0.00104839  -0.0558514   -0.206898     0.0493602  -0.108948    -0.0697096    0.186232     0.0694242   -0.039954     -0.116267     -0.0223717    0.084167    -0.0621735   -0.0769859 
  0.0158764   -0.0278475   -0.0124337    0.00665727  -0.00515704    0.000384465  -0.000986317   0.0634367   -0.0347731   -0.0239112   -0.00985038   0.164574    -0.0996182    0.274001    -0.0919348   -0.0504866   0.195473    -0.0882604   -0.0521583   -0.0427572    0.17857       0.108278      0.118357    -0.0361682   -0.0719209    0.0810481 
 -0.0602434   -0.185176     0.0563764   -0.0902333    0.068502      0.055922     -0.028799      0.0804092   -0.00572243  -0.062577    -0.0491771    0.0642098    0.0232728   -0.0317147   -0.238898    -0.0297916   0.0134591    0.135744     0.102979    -0.173675    -0.102758      0.0159374     0.127452     0.0874346   -0.0218892   -0.102179  
 -0.1253      -0.0817949    0.0580129   -0.103039    -0.00659595    0.0395046     0.162112      0.0448617   -0.120475     0.12609     -0.0680098    0.123744    -0.0162034   -0.161121     0.0751332    0.0286898  -0.0143125    0.13354      0.139137    -0.197478     0.0885514    -0.177339      0.0686601   -0.0147814    0.0955825   -0.112387  
 -0.224114     0.131352    -0.0217855    0.00875014  -0.0658829    -0.0497439    -0.0151267     0.0477669    0.0164638   -0.0353367   -0.00306244  -0.0994762    0.10739     -0.00496078   0.0774299   -0.0193273  -0.0147158   -0.0849729   -0.126506    -0.00914596  -0.0826914     0.0587349     0.0696536   -0.0403343   -0.0869418    0.0732148 
 -0.156026    -0.137803     0.22704     -0.00819071   0.223469      0.0877917    -0.00353969    0.082736    -0.0440676    0.0629549   -0.0840564    0.142884     0.104456     0.0332377    0.196054     0.039669   -0.123326     0.182873     0.11779      0.0193879   -0.02632       0.125285      0.0894346    0.0586965   -0.0370242    0.0979635 
  0.0665381   -0.0820845    0.0635547   -0.175814    -0.178157     -0.236618     -0.0336291     0.00512013   0.050035    -0.0193441    0.026192     0.00435765   0.11198      0.113923    -0.0364253   -0.0698624   0.122655     0.0483835    0.00812381  -0.119469    -0.0323863    -0.070117     -0.0570623   -0.0809377   -0.0471215    0.132436  
 -0.200382     0.0597764   -0.0545088   -0.102123     0.000401207  -0.137609      0.0744143     0.022187     0.104311     0.0275458    0.0234163   -0.0664429    0.137708     0.0398616    0.105819    -0.148965    0.10568      0.140217    -0.132873     0.1148      -0.125403     -0.0238543    -0.00283798   0.0588624    0.113231     0.0318191 
 -0.110592    -0.00476697   0.0164057   -0.0997755    0.0315798    -0.116906      0.0546666    -0.0507037   -0.020029    -0.129772     0.0325285   -0.126358     0.174493    -0.0611653   -0.0282151   -0.0633756   0.158871    -0.0832167   -0.0686569    0.0640449    0.0924791     0.216815      0.022151    -0.0659016   -0.042925     0.00349178
  0.010779     0.0359597    0.0296674    0.0146388   -0.0792061    -0.0374326    -0.151941      0.0248719    0.227809    -0.055489     0.0436351   -0.00488639  -0.0164683   -0.0613568   -0.0249245    0.072512    0.170849    -0.00440302  -0.109451    -0.0297357    0.00112694   -0.084873      0.0310869   -0.0169202    0.0975347    0.0564257 
  0.139743    -0.0593847   -0.069139    -0.0535247    0.0292417    -0.0694943    -0.0446007    -0.0224156   -0.00838874  -0.0827419    0.00547013  -0.130304     0.0285421    0.0955721   -0.0341444    0.331324   -0.056925     0.0623038   -0.146511    -0.0643875   -0.0063785     0.0200129    -0.0668338   -0.25786     -0.0768156   -0.0362192 
  0.219424     0.0284116   -0.0312758   -0.0227915    0.0988277     0.10499       0.015392      0.0285784    0.0221346    0.0815336    0.0198785    0.109445     0.16786     -0.14271      0.299522     0.0522106   0.0579098   -0.0229686    0.0646637   -0.0796914    0.0761072    -0.0397402     0.015193     0.0653886    0.091567    -0.00518478
 -0.0681075   -0.0107301    0.0614411   -0.0636818    0.0717038    -0.0233961    -0.00169981    0.167789    -0.0369113   -0.104443     0.0112846   -0.0457949   -0.11659     -0.0929359   -0.0303754   -0.0954873   0.00353835   0.080473    -0.1474       0.0658763   -0.0894559    -0.0276242    -0.121423     0.0359966    0.0654705   -0.0457191 
 -0.0255912    0.0273654    0.0558456    0.0472137    0.0360596     0.118568     -0.150629      0.174279    -0.127246    -0.12366      0.0113902   -0.0808468    0.0391742    0.0618575   -0.163424    -0.0434152  -0.0668034   -0.0913758   -0.0447964    0.0797904   -0.0340479     0.1728       -0.0543561   -0.0768697    0.00318643  -0.0119713 
 -0.006426    -0.0736516   -0.170371     0.0473756    0.0502534     0.0478771     0.0797422     0.123652     0.0366714    0.0646649    0.16687     -0.0706881   -0.0960124    0.00620236  -0.00265627   0.0686412   0.118624    -0.0275421    0.158532     0.178293    -0.0186353     0.0631354     0.0473434    0.0176946    0.0561213   -0.135667  
  0.0795228   -0.0977741    0.114105    -0.114718    -0.0805688     0.131803     -0.162374     -0.026184     0.0976338   -0.23007     -0.0392757   -0.109877     0.023096     0.0436405    0.0797585    0.0642575  -0.0262677    0.0364591   -0.112663     0.0453825   -0.0128128    -0.0503258     0.0418654    0.00281025  -0.0462678    0.0858441 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 17 19 22 30
INFO: iteration 1, average log likelihood -1.048386
WARNING: Variances had to be floored 5 11 17 19 22 28 30 31
INFO: iteration 2, average log likelihood -0.994497
WARNING: Variances had to be floored 1 7 8 17 19 21 23 24 30 31
INFO: iteration 3, average log likelihood -0.974688
WARNING: Variances had to be floored 5 11 13 14 17 19 22 28 30 31
INFO: iteration 4, average log likelihood -1.009009
WARNING: Variances had to be floored 17 19 22 30
INFO: iteration 5, average log likelihood -1.010489
WARNING: Variances had to be floored 1 5 7 8 11 17 19 21 22 23 28 30 31
INFO: iteration 6, average log likelihood -0.964424
WARNING: Variances had to be floored 14 17 19 22 24 30
INFO: iteration 7, average log likelihood -1.022184
WARNING: Variances had to be floored 5 11 17 19 22 28 30 31
INFO: iteration 8, average log likelihood -1.002306
WARNING: Variances had to be floored 1 7 8 17 19 21 22 30 31
INFO: iteration 9, average log likelihood -0.982511
WARNING: Variances had to be floored 5 11 17 19 22 24 28 30
INFO: iteration 10, average log likelihood -1.005111
INFO: EM with 100000 data points 10 iterations avll -1.005111
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.268625    -0.00724368  -0.111943    -0.0440896    0.212764    0.143585     0.21842     -0.183574    -0.0111205    0.116199     0.0571831   -0.121033    -0.286846     0.0806499   -0.0787399   -0.054125    -0.0201332  -0.0329954    0.0832126    0.0683958    0.0537333    -0.168453    -0.0145955    0.0877554    0.0114794    0.0827831 
  0.00338744   0.00624919  -0.027348    -0.0504765   -0.0948088  -0.0198207    0.0857929   -0.0794215   -0.0380236   -0.0450513   -0.0195475    0.0702277   -0.0770593    0.0481304    0.00946483  -0.0398066    0.0720769  -0.12546     -0.0633225    0.0104018   -0.16518       0.0484819    0.0262364    0.0890593   -0.0577414    0.0711602 
 -0.0876548   -0.03863     -0.00993259   0.184865    -0.118723   -0.0789111   -0.0173868    0.0187404    0.050067    -0.167404     0.0553942    0.097904    -0.0103992    0.136784     0.056477     0.129975    -0.0697142  -0.114786    -0.00407794   0.112775    -0.0231726    -0.0989293   -0.0277043    0.0858781    0.0432581    0.0202255 
 -0.249781    -0.0292736    0.0486677   -0.0463116    0.224249   -0.127142     0.125261    -0.167996     0.113748     0.0221847    0.0171134   -0.0513037    0.0586151    0.0503054   -0.155006    -0.158184    -0.161472   -0.0786053    0.00308763   0.128995    -0.0131569    -0.00536244  -0.199504    -0.10296      0.0396247   -0.0948241 
  0.041855    -0.068824    -0.0346121    0.216721     0.183533   -0.0918527   -0.0578128   -0.153333    -0.325299     0.082132     0.149331     0.163701     0.100028     0.07134      0.108216    -0.150943    -0.1041      0.0774563   -0.0822767    0.0839742    0.116057      0.0899781   -0.00877737  -0.0936944   -0.0171331   -0.0103684 
  0.0831213    0.096912    -0.0128064   -0.0174249   -0.0796873   0.0524815   -0.102313     0.039344    -0.0295745   -0.0107354    0.0573554    0.104252    -0.0508277    0.0906561    0.119445    -0.0973411   -0.0499785   0.288483     0.0207459    0.104352     0.00980672    0.0419511    0.055646    -0.023635     0.0724104   -0.200008  
 -0.0924939   -0.0424736    0.308532    -0.023419    -0.0839732   0.255858    -0.091752    -0.184851    -0.0266558    0.00476188   0.00171975  -0.0205718   -0.0572138    0.235306     0.0577613    0.206369     0.0991396  -0.11107      0.0464057    0.224493     0.0339817    -0.0861644   -0.0149882    0.037771    -0.0572176    0.212535  
  0.192033    -0.0822678    0.11004      0.0334592    0.111688   -0.156559     0.089311    -0.118584     0.0173812   -0.0748422   -0.0996284   -0.179062    -0.0765594   -0.0285591   -0.00514112  -0.0859954    0.142158   -0.118069     0.170062     0.0693283    0.0754199     0.0528435    0.0258194   -0.180414    -0.0996671   -0.179696  
 -0.129425     0.1478      -0.090453    -0.0395432    0.133294   -0.0779575    0.179713     0.0641873    0.170037     0.0269616    0.137592     0.0443687   -0.121889    -0.172953     0.0462151   -0.00130546  -0.0361838  -0.00772904   0.040715    -0.0666733   -0.000714699  -0.0500908   -0.193849     0.15695     -0.0281027    0.145992  
  0.135793     0.0222776   -0.162932    -0.0255276    0.0158022  -0.040676    -0.0757531    0.103208     0.110177    -0.0813846   -0.0773518   -0.0989714   -0.0208538    0.1973       0.0241545   -0.0963742    0.163177    0.0187746   -0.122629    -0.0899597   -0.0132762     0.0221796    0.235749    -0.168521    -0.0573651    0.03079   
  0.0676914   -0.173572    -0.039084     0.103227    -0.0780308   0.044201    -0.0704758   -0.188216     0.0956199   -0.108733    -0.105126     0.0301943   -0.0124078   -0.129318     0.0489283   -0.0180721   -0.0653807  -0.00589434  -0.0590818    0.110166     0.121716     -0.0488838   -0.0729318    0.102567    -0.178333     0.0440512 
 -0.102704    -0.0695443    0.13335     -0.0180853    0.149402    0.0429551   -0.0682041    0.0698657    0.114798     0.177807    -0.0602591   -0.0465495   -0.033658     0.0579287   -0.0314077    0.1381      -0.0529477   0.0100333   -0.0182676    0.22989     -0.0701691    -0.139751     0.054082     0.15815      0.103098     0.0217825 
 -0.157557    -0.0356633    0.00117149   0.104339     0.0461145   0.00370467  -0.0754174   -0.213953    -0.0503001    0.0662181   -0.0159574    0.0862913    0.0228429    0.102066     0.0299189    0.0693235    0.0233778  -0.196134    -0.0933913    0.0564807   -0.111731     -0.127087    -0.0819106    0.0216564    0.0363646   -0.0209623 
  0.0811616   -0.143415     0.0903073    0.0294709    0.069461    0.0552011   -0.0573003   -0.0130671    0.017938    -0.0836347    0.0716947   -0.0705567   -0.00550518   0.0521658    0.168743    -0.0088744    0.0169712  -0.0773709    0.109093    -0.20527     -0.00138005    0.123105     0.0674479    0.258792     0.0402461   -0.122159  
  0.0480673    0.0368236    0.122034    -0.0134831    0.0589708  -0.0277853    0.00333114   0.106869     0.148326     0.0280517    0.063194     0.0983823   -0.204892    -0.00612311   0.0553507    0.0589877   -0.0466701   0.00225455  -0.0491306    0.0944316   -0.0520396     0.0654286    0.0252293    0.0581194   -0.187832     0.077864  
 -0.0839728   -0.0841864    0.087063     0.0170522   -0.127385    0.0920637   -0.0271817    0.193555    -0.0622025   -0.0390157   -0.10918      0.0981715   -0.0362359    0.0732281    0.0268802    0.0729526    0.0506617  -0.0405549   -0.131395     0.0487439   -0.0120414    -0.0640608    0.240381     0.0791903   -0.144706    -0.140749  
  0.0644953    0.0967801   -0.0677629    0.200854    -0.0156551   0.148751    -0.0249079   -0.080832    -0.128998    -0.142677     0.00231813  -0.111311     0.0595046   -0.0761909   -0.0508016    0.0573407   -0.104501   -0.149561     0.118481     0.0362736   -0.182007     -0.166597    -0.103654    -0.0661887   -0.0245897    0.0286276 
 -0.118003     0.116118    -0.103604     0.0533437    0.202763   -0.0693006   -0.00441399   0.0774931   -0.0792799   -0.0335946   -0.166673     0.0705123    0.0905778   -0.0749999    0.101605     0.052903     0.0248374   0.101233     0.00543058  -0.00497415   0.0715616    -0.0691659   -0.0323861   -0.157027    -0.0323284    0.0211652 
 -0.13344     -0.213218     0.0364749    0.00558585  -0.176691    0.0571636    0.0309229   -0.167813    -0.0584677   -0.099782     0.218957     0.0225836    0.0678752   -0.0843502   -0.0130948   -0.00431575   0.0115157   0.100522     0.061831    -0.0374326    0.0494265     0.197176    -0.149792    -0.00420746  -0.116188     0.0699293 
  0.0497262    0.0950003   -0.00440534  -0.181754     0.0583435   0.0296905    0.0461135   -0.193389     0.0584837   -0.00987481  -0.231125    -0.056951    -0.0807598    0.0477275   -0.0128265    0.186957    -0.0773457   0.146709     0.138989    -0.0368194    0.1651       -0.0604175   -0.169253     0.0104567    0.0552901    0.00403027
  0.0481823   -0.183533     0.0417648    0.0981121    0.0354995  -0.0262834    0.00814781  -0.11379      0.107016    -0.189975     0.0600584   -0.167667     0.225893    -0.00291481  -0.132473     0.0366645   -0.0277219   0.0407998   -0.110119    -0.176226    -0.0475702    -0.148186    -0.220242    -0.0130051    0.00315044  -0.0442891 
 -0.194192     0.143132    -0.0608736   -0.052445    -0.0031771  -0.0589896    0.102916     0.134529     0.0662087    0.135974    -0.091211     0.0976103    0.0966477    0.00116922  -0.12067      0.11774     -0.0847337  -0.0442049   -0.00459365  -0.263814    -0.0837747    -0.0109763    0.0598335   -0.141697     0.0782657   -0.00382734
 -0.320036    -0.0442611   -0.00788256  -0.0215887   -0.107776    0.116175    -0.171605    -0.00667033   0.150782     0.183909    -0.223713    -0.00495525   0.0545982    0.00534705  -0.122557     0.0049173   -0.0507673   0.141368     0.0726138    0.0714094   -0.0942017     0.0395449    0.0516226   -0.00204234  -0.109021    -0.0010924 
  0.00748707   0.0153801    0.0918244    0.119357    -0.0937157  -0.0764534    0.00464158   0.0407545    0.103346    -0.0576591   -0.111143     0.0992094    0.0755501   -0.0680405   -0.0714716    0.0899718    0.0218056   0.0471891   -0.108164     0.19148      0.0830126     0.189764    -0.166128    -0.100814    -0.0943919   -0.125529  
  0.126357     0.0434866    0.128771    -0.297695     0.0505945  -0.019746     0.0690281   -0.0421801   -0.103078    -0.0773697   -0.0770913   -0.0784832   -0.291442    -0.0866588    0.125743     0.0381921    0.0085834   0.0147007   -0.0293013    0.115756     0.100431     -0.011056    -0.127835    -0.0809118    0.0132187   -0.0276845 
 -0.00490518   0.233809    -0.0335314   -0.0852332    0.0747554   0.208038    -0.094976    -0.0532141   -0.185659     0.158891     0.0587598    0.0715144   -0.0814851   -0.0816161    0.0326454    0.0163044    0.0516662  -0.0289222    0.0674984    0.0270057    0.0597353     0.158186    -0.0708214    0.138382    -0.00421406   0.060917  
  0.13506     -0.0290241   -0.148794     0.0789305   -0.0534656  -0.0918626    0.0636616    0.179813    -0.155738    -0.136528    -0.256103    -0.153671    -0.105131     0.117275    -0.025181     0.0908323    0.0771822   0.0577893    0.0429521    0.164121    -0.103331      0.0249429    0.0487103    0.0268886    0.0216252    0.0379568 
 -0.118693    -0.24873      0.0598189    0.0819232    0.0264287   0.0355167    0.0800943   -0.0687496    0.0120809   -0.0226438   -0.0051442    0.0171179   -0.127367     0.0370544    0.00983321  -0.0735504    0.144157    0.0895795    0.0784425    0.180413     0.031843      0.121357     0.0678269   -0.020571     0.0427645   -0.114453  
 -0.0478935   -0.0511047   -0.0199558    0.0151628    0.0113464   0.0824997    0.0697006    0.0205897    0.255903     0.0744865   -0.0966574    0.0996808    0.0674944    0.0394779    0.0131955    0.140114     0.0127177   0.0115992   -0.0659535   -0.00339256  -0.00731297   -0.127713    -0.113725    -0.09986      0.03883      0.0849997 
  0.0502948    0.103584    -0.0377943   -0.0307411    0.0731665   0.0136407    0.113893     0.0521039   -0.173566    -0.00158464  -0.0520946   -0.108215    -0.0492087    0.225448    -0.0879151   -0.0741076   -0.0260445  -0.0580039   -0.0592678   -0.10751      0.0605539     0.0913073   -0.0068591   -0.139292     0.133236     0.046838  
 -0.0609707   -0.0557455    0.118588    -0.0712198   -0.0362799   0.180922    -0.164547    -0.0448505   -0.00302283  -0.0608594    0.199125    -0.18128      0.0453274   -0.0740898   -0.116848     0.0822802   -0.0621681  -0.0684726   -0.150115     0.0300751   -0.0765486    -0.00870763  -0.0186356    0.160633     0.0596972   -0.0475483 
  0.175484     0.122783     0.0648188    0.0988305    0.154024    0.0887735   -0.207347     0.0590665   -0.0245549    0.0728914   -0.172234     0.166889    -0.102376     0.108649     0.0799175    0.00372022   0.121868    0.0223281    0.168368     0.177944    -0.0631496    -0.0371713   -0.146189    -0.0587188   -0.0265228    0.189538  kind full, method split
0: avll = -1.4194048831740074
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.419424
INFO: iteration 2, average log likelihood -1.419355
INFO: iteration 3, average log likelihood -1.419300
INFO: iteration 4, average log likelihood -1.419234
INFO: iteration 5, average log likelihood -1.419152
INFO: iteration 6, average log likelihood -1.419049
INFO: iteration 7, average log likelihood -1.418910
INFO: iteration 8, average log likelihood -1.418688
INFO: iteration 9, average log likelihood -1.418282
INFO: iteration 10, average log likelihood -1.417550
INFO: iteration 11, average log likelihood -1.416475
INFO: iteration 12, average log likelihood -1.415360
INFO: iteration 13, average log likelihood -1.414581
INFO: iteration 14, average log likelihood -1.414188
INFO: iteration 15, average log likelihood -1.414022
INFO: iteration 16, average log likelihood -1.413956
INFO: iteration 17, average log likelihood -1.413929
INFO: iteration 18, average log likelihood -1.413919
INFO: iteration 19, average log likelihood -1.413914
INFO: iteration 20, average log likelihood -1.413912
INFO: iteration 21, average log likelihood -1.413911
INFO: iteration 22, average log likelihood -1.413910
INFO: iteration 23, average log likelihood -1.413910
INFO: iteration 24, average log likelihood -1.413909
INFO: iteration 25, average log likelihood -1.413909
INFO: iteration 26, average log likelihood -1.413909
INFO: iteration 27, average log likelihood -1.413909
INFO: iteration 28, average log likelihood -1.413908
INFO: iteration 29, average log likelihood -1.413908
INFO: iteration 30, average log likelihood -1.413908
INFO: iteration 31, average log likelihood -1.413908
INFO: iteration 32, average log likelihood -1.413908
INFO: iteration 33, average log likelihood -1.413908
INFO: iteration 34, average log likelihood -1.413908
INFO: iteration 35, average log likelihood -1.413907
INFO: iteration 36, average log likelihood -1.413907
INFO: iteration 37, average log likelihood -1.413907
INFO: iteration 38, average log likelihood -1.413907
INFO: iteration 39, average log likelihood -1.413907
INFO: iteration 40, average log likelihood -1.413907
INFO: iteration 41, average log likelihood -1.413907
INFO: iteration 42, average log likelihood -1.413907
INFO: iteration 43, average log likelihood -1.413907
INFO: iteration 44, average log likelihood -1.413907
INFO: iteration 45, average log likelihood -1.413907
INFO: iteration 46, average log likelihood -1.413907
INFO: iteration 47, average log likelihood -1.413907
INFO: iteration 48, average log likelihood -1.413907
INFO: iteration 49, average log likelihood -1.413907
INFO: iteration 50, average log likelihood -1.413907
INFO: EM with 100000 data points 50 iterations avll -1.413907
952.4 data points per parameter
1: avll = [-1.41942,-1.41936,-1.4193,-1.41923,-1.41915,-1.41905,-1.41891,-1.41869,-1.41828,-1.41755,-1.41648,-1.41536,-1.41458,-1.41419,-1.41402,-1.41396,-1.41393,-1.41392,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.413926
INFO: iteration 2, average log likelihood -1.413856
INFO: iteration 3, average log likelihood -1.413799
INFO: iteration 4, average log likelihood -1.413732
INFO: iteration 5, average log likelihood -1.413653
INFO: iteration 6, average log likelihood -1.413565
INFO: iteration 7, average log likelihood -1.413477
INFO: iteration 8, average log likelihood -1.413397
INFO: iteration 9, average log likelihood -1.413332
INFO: iteration 10, average log likelihood -1.413282
INFO: iteration 11, average log likelihood -1.413245
INFO: iteration 12, average log likelihood -1.413218
INFO: iteration 13, average log likelihood -1.413197
INFO: iteration 14, average log likelihood -1.413180
INFO: iteration 15, average log likelihood -1.413165
INFO: iteration 16, average log likelihood -1.413150
INFO: iteration 17, average log likelihood -1.413136
INFO: iteration 18, average log likelihood -1.413121
INFO: iteration 19, average log likelihood -1.413105
INFO: iteration 20, average log likelihood -1.413087
INFO: iteration 21, average log likelihood -1.413068
INFO: iteration 22, average log likelihood -1.413048
INFO: iteration 23, average log likelihood -1.413025
INFO: iteration 24, average log likelihood -1.413002
INFO: iteration 25, average log likelihood -1.412978
INFO: iteration 26, average log likelihood -1.412955
INFO: iteration 27, average log likelihood -1.412932
INFO: iteration 28, average log likelihood -1.412911
INFO: iteration 29, average log likelihood -1.412891
INFO: iteration 30, average log likelihood -1.412874
INFO: iteration 31, average log likelihood -1.412860
INFO: iteration 32, average log likelihood -1.412848
INFO: iteration 33, average log likelihood -1.412838
INFO: iteration 34, average log likelihood -1.412830
INFO: iteration 35, average log likelihood -1.412824
INFO: iteration 36, average log likelihood -1.412819
INFO: iteration 37, average log likelihood -1.412815
INFO: iteration 38, average log likelihood -1.412812
INFO: iteration 39, average log likelihood -1.412809
INFO: iteration 40, average log likelihood -1.412807
INFO: iteration 41, average log likelihood -1.412805
INFO: iteration 42, average log likelihood -1.412804
INFO: iteration 43, average log likelihood -1.412803
INFO: iteration 44, average log likelihood -1.412802
INFO: iteration 45, average log likelihood -1.412801
INFO: iteration 46, average log likelihood -1.412800
INFO: iteration 47, average log likelihood -1.412799
INFO: iteration 48, average log likelihood -1.412799
INFO: iteration 49, average log likelihood -1.412798
INFO: iteration 50, average log likelihood -1.412798
INFO: EM with 100000 data points 50 iterations avll -1.412798
473.9 data points per parameter
2: avll = [-1.41393,-1.41386,-1.4138,-1.41373,-1.41365,-1.41357,-1.41348,-1.4134,-1.41333,-1.41328,-1.41325,-1.41322,-1.4132,-1.41318,-1.41316,-1.41315,-1.41314,-1.41312,-1.4131,-1.41309,-1.41307,-1.41305,-1.41303,-1.413,-1.41298,-1.41295,-1.41293,-1.41291,-1.41289,-1.41287,-1.41286,-1.41285,-1.41284,-1.41283,-1.41282,-1.41282,-1.41281,-1.41281,-1.41281,-1.41281,-1.41281,-1.4128,-1.4128,-1.4128,-1.4128,-1.4128,-1.4128,-1.4128,-1.4128,-1.4128]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.412811
INFO: iteration 2, average log likelihood -1.412764
INFO: iteration 3, average log likelihood -1.412728
INFO: iteration 4, average log likelihood -1.412688
INFO: iteration 5, average log likelihood -1.412640
INFO: iteration 6, average log likelihood -1.412582
INFO: iteration 7, average log likelihood -1.412512
INFO: iteration 8, average log likelihood -1.412431
INFO: iteration 9, average log likelihood -1.412339
INFO: iteration 10, average log likelihood -1.412239
INFO: iteration 11, average log likelihood -1.412134
INFO: iteration 12, average log likelihood -1.412029
INFO: iteration 13, average log likelihood -1.411931
INFO: iteration 14, average log likelihood -1.411844
INFO: iteration 15, average log likelihood -1.411770
INFO: iteration 16, average log likelihood -1.411711
INFO: iteration 17, average log likelihood -1.411662
INFO: iteration 18, average log likelihood -1.411623
INFO: iteration 19, average log likelihood -1.411591
INFO: iteration 20, average log likelihood -1.411564
INFO: iteration 21, average log likelihood -1.411541
INFO: iteration 22, average log likelihood -1.411520
INFO: iteration 23, average log likelihood -1.411501
INFO: iteration 24, average log likelihood -1.411484
INFO: iteration 25, average log likelihood -1.411469
INFO: iteration 26, average log likelihood -1.411455
INFO: iteration 27, average log likelihood -1.411441
INFO: iteration 28, average log likelihood -1.411429
INFO: iteration 29, average log likelihood -1.411417
INFO: iteration 30, average log likelihood -1.411406
INFO: iteration 31, average log likelihood -1.411396
INFO: iteration 32, average log likelihood -1.411386
INFO: iteration 33, average log likelihood -1.411377
INFO: iteration 34, average log likelihood -1.411369
INFO: iteration 35, average log likelihood -1.411360
INFO: iteration 36, average log likelihood -1.411353
INFO: iteration 37, average log likelihood -1.411345
INFO: iteration 38, average log likelihood -1.411338
INFO: iteration 39, average log likelihood -1.411332
INFO: iteration 40, average log likelihood -1.411325
INFO: iteration 41, average log likelihood -1.411319
INFO: iteration 42, average log likelihood -1.411313
INFO: iteration 43, average log likelihood -1.411308
INFO: iteration 44, average log likelihood -1.411302
INFO: iteration 45, average log likelihood -1.411297
INFO: iteration 46, average log likelihood -1.411292
INFO: iteration 47, average log likelihood -1.411287
INFO: iteration 48, average log likelihood -1.411282
INFO: iteration 49, average log likelihood -1.411277
INFO: iteration 50, average log likelihood -1.411273
INFO: EM with 100000 data points 50 iterations avll -1.411273
236.4 data points per parameter
3: avll = [-1.41281,-1.41276,-1.41273,-1.41269,-1.41264,-1.41258,-1.41251,-1.41243,-1.41234,-1.41224,-1.41213,-1.41203,-1.41193,-1.41184,-1.41177,-1.41171,-1.41166,-1.41162,-1.41159,-1.41156,-1.41154,-1.41152,-1.4115,-1.41148,-1.41147,-1.41145,-1.41144,-1.41143,-1.41142,-1.41141,-1.4114,-1.41139,-1.41138,-1.41137,-1.41136,-1.41135,-1.41135,-1.41134,-1.41133,-1.41133,-1.41132,-1.41131,-1.41131,-1.4113,-1.4113,-1.41129,-1.41129,-1.41128,-1.41128,-1.41127]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.411277
INFO: iteration 2, average log likelihood -1.411224
INFO: iteration 3, average log likelihood -1.411177
INFO: iteration 4, average log likelihood -1.411124
INFO: iteration 5, average log likelihood -1.411060
INFO: iteration 6, average log likelihood -1.410983
INFO: iteration 7, average log likelihood -1.410892
INFO: iteration 8, average log likelihood -1.410788
INFO: iteration 9, average log likelihood -1.410674
INFO: iteration 10, average log likelihood -1.410557
INFO: iteration 11, average log likelihood -1.410442
INFO: iteration 12, average log likelihood -1.410334
INFO: iteration 13, average log likelihood -1.410236
INFO: iteration 14, average log likelihood -1.410149
INFO: iteration 15, average log likelihood -1.410075
INFO: iteration 16, average log likelihood -1.410012
INFO: iteration 17, average log likelihood -1.409958
INFO: iteration 18, average log likelihood -1.409912
INFO: iteration 19, average log likelihood -1.409873
INFO: iteration 20, average log likelihood -1.409837
INFO: iteration 21, average log likelihood -1.409806
INFO: iteration 22, average log likelihood -1.409778
INFO: iteration 23, average log likelihood -1.409752
INFO: iteration 24, average log likelihood -1.409727
INFO: iteration 25, average log likelihood -1.409705
INFO: iteration 26, average log likelihood -1.409683
INFO: iteration 27, average log likelihood -1.409663
INFO: iteration 28, average log likelihood -1.409644
INFO: iteration 29, average log likelihood -1.409626
INFO: iteration 30, average log likelihood -1.409608
INFO: iteration 31, average log likelihood -1.409591
INFO: iteration 32, average log likelihood -1.409575
INFO: iteration 33, average log likelihood -1.409559
INFO: iteration 34, average log likelihood -1.409543
INFO: iteration 35, average log likelihood -1.409528
INFO: iteration 36, average log likelihood -1.409514
INFO: iteration 37, average log likelihood -1.409499
INFO: iteration 38, average log likelihood -1.409485
INFO: iteration 39, average log likelihood -1.409472
INFO: iteration 40, average log likelihood -1.409458
INFO: iteration 41, average log likelihood -1.409445
INFO: iteration 42, average log likelihood -1.409432
INFO: iteration 43, average log likelihood -1.409419
INFO: iteration 44, average log likelihood -1.409406
INFO: iteration 45, average log likelihood -1.409394
INFO: iteration 46, average log likelihood -1.409382
INFO: iteration 47, average log likelihood -1.409370
INFO: iteration 48, average log likelihood -1.409358
INFO: iteration 49, average log likelihood -1.409347
INFO: iteration 50, average log likelihood -1.409336
INFO: EM with 100000 data points 50 iterations avll -1.409336
118.1 data points per parameter
4: avll = [-1.41128,-1.41122,-1.41118,-1.41112,-1.41106,-1.41098,-1.41089,-1.41079,-1.41067,-1.41056,-1.41044,-1.41033,-1.41024,-1.41015,-1.41008,-1.41001,-1.40996,-1.40991,-1.40987,-1.40984,-1.40981,-1.40978,-1.40975,-1.40973,-1.4097,-1.40968,-1.40966,-1.40964,-1.40963,-1.40961,-1.40959,-1.40957,-1.40956,-1.40954,-1.40953,-1.40951,-1.4095,-1.40949,-1.40947,-1.40946,-1.40944,-1.40943,-1.40942,-1.40941,-1.40939,-1.40938,-1.40937,-1.40936,-1.40935,-1.40934]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.409332
INFO: iteration 2, average log likelihood -1.409263
INFO: iteration 3, average log likelihood -1.409193
INFO: iteration 4, average log likelihood -1.409109
INFO: iteration 5, average log likelihood -1.409000
INFO: iteration 6, average log likelihood -1.408862
INFO: iteration 7, average log likelihood -1.408693
INFO: iteration 8, average log likelihood -1.408501
INFO: iteration 9, average log likelihood -1.408300
INFO: iteration 10, average log likelihood -1.408104
INFO: iteration 11, average log likelihood -1.407923
INFO: iteration 12, average log likelihood -1.407762
INFO: iteration 13, average log likelihood -1.407622
INFO: iteration 14, average log likelihood -1.407502
INFO: iteration 15, average log likelihood -1.407399
INFO: iteration 16, average log likelihood -1.407311
INFO: iteration 17, average log likelihood -1.407236
INFO: iteration 18, average log likelihood -1.407170
INFO: iteration 19, average log likelihood -1.407113
INFO: iteration 20, average log likelihood -1.407062
INFO: iteration 21, average log likelihood -1.407017
INFO: iteration 22, average log likelihood -1.406977
INFO: iteration 23, average log likelihood -1.406940
INFO: iteration 24, average log likelihood -1.406906
INFO: iteration 25, average log likelihood -1.406875
INFO: iteration 26, average log likelihood -1.406845
INFO: iteration 27, average log likelihood -1.406817
INFO: iteration 28, average log likelihood -1.406791
INFO: iteration 29, average log likelihood -1.406765
INFO: iteration 30, average log likelihood -1.406740
INFO: iteration 31, average log likelihood -1.406716
INFO: iteration 32, average log likelihood -1.406693
INFO: iteration 33, average log likelihood -1.406669
INFO: iteration 34, average log likelihood -1.406646
INFO: iteration 35, average log likelihood -1.406624
INFO: iteration 36, average log likelihood -1.406602
INFO: iteration 37, average log likelihood -1.406581
INFO: iteration 38, average log likelihood -1.406560
INFO: iteration 39, average log likelihood -1.406540
INFO: iteration 40, average log likelihood -1.406521
INFO: iteration 41, average log likelihood -1.406502
INFO: iteration 42, average log likelihood -1.406485
INFO: iteration 43, average log likelihood -1.406468
INFO: iteration 44, average log likelihood -1.406452
INFO: iteration 45, average log likelihood -1.406437
INFO: iteration 46, average log likelihood -1.406422
INFO: iteration 47, average log likelihood -1.406408
INFO: iteration 48, average log likelihood -1.406395
INFO: iteration 49, average log likelihood -1.406382
INFO: iteration 50, average log likelihood -1.406370
INFO: EM with 100000 data points 50 iterations avll -1.406370
59.0 data points per parameter
5: avll = [-1.40933,-1.40926,-1.40919,-1.40911,-1.409,-1.40886,-1.40869,-1.4085,-1.4083,-1.4081,-1.40792,-1.40776,-1.40762,-1.4075,-1.4074,-1.40731,-1.40724,-1.40717,-1.40711,-1.40706,-1.40702,-1.40698,-1.40694,-1.40691,-1.40687,-1.40685,-1.40682,-1.40679,-1.40677,-1.40674,-1.40672,-1.40669,-1.40667,-1.40665,-1.40662,-1.4066,-1.40658,-1.40656,-1.40654,-1.40652,-1.4065,-1.40648,-1.40647,-1.40645,-1.40644,-1.40642,-1.40641,-1.40639,-1.40638,-1.40637]
[-1.4194,-1.41942,-1.41936,-1.4193,-1.41923,-1.41915,-1.41905,-1.41891,-1.41869,-1.41828,-1.41755,-1.41648,-1.41536,-1.41458,-1.41419,-1.41402,-1.41396,-1.41393,-1.41392,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41391,-1.41393,-1.41386,-1.4138,-1.41373,-1.41365,-1.41357,-1.41348,-1.4134,-1.41333,-1.41328,-1.41325,-1.41322,-1.4132,-1.41318,-1.41316,-1.41315,-1.41314,-1.41312,-1.4131,-1.41309,-1.41307,-1.41305,-1.41303,-1.413,-1.41298,-1.41295,-1.41293,-1.41291,-1.41289,-1.41287,-1.41286,-1.41285,-1.41284,-1.41283,-1.41282,-1.41282,-1.41281,-1.41281,-1.41281,-1.41281,-1.41281,-1.4128,-1.4128,-1.4128,-1.4128,-1.4128,-1.4128,-1.4128,-1.4128,-1.4128,-1.41281,-1.41276,-1.41273,-1.41269,-1.41264,-1.41258,-1.41251,-1.41243,-1.41234,-1.41224,-1.41213,-1.41203,-1.41193,-1.41184,-1.41177,-1.41171,-1.41166,-1.41162,-1.41159,-1.41156,-1.41154,-1.41152,-1.4115,-1.41148,-1.41147,-1.41145,-1.41144,-1.41143,-1.41142,-1.41141,-1.4114,-1.41139,-1.41138,-1.41137,-1.41136,-1.41135,-1.41135,-1.41134,-1.41133,-1.41133,-1.41132,-1.41131,-1.41131,-1.4113,-1.4113,-1.41129,-1.41129,-1.41128,-1.41128,-1.41127,-1.41128,-1.41122,-1.41118,-1.41112,-1.41106,-1.41098,-1.41089,-1.41079,-1.41067,-1.41056,-1.41044,-1.41033,-1.41024,-1.41015,-1.41008,-1.41001,-1.40996,-1.40991,-1.40987,-1.40984,-1.40981,-1.40978,-1.40975,-1.40973,-1.4097,-1.40968,-1.40966,-1.40964,-1.40963,-1.40961,-1.40959,-1.40957,-1.40956,-1.40954,-1.40953,-1.40951,-1.4095,-1.40949,-1.40947,-1.40946,-1.40944,-1.40943,-1.40942,-1.40941,-1.40939,-1.40938,-1.40937,-1.40936,-1.40935,-1.40934,-1.40933,-1.40926,-1.40919,-1.40911,-1.409,-1.40886,-1.40869,-1.4085,-1.4083,-1.4081,-1.40792,-1.40776,-1.40762,-1.4075,-1.4074,-1.40731,-1.40724,-1.40717,-1.40711,-1.40706,-1.40702,-1.40698,-1.40694,-1.40691,-1.40687,-1.40685,-1.40682,-1.40679,-1.40677,-1.40674,-1.40672,-1.40669,-1.40667,-1.40665,-1.40662,-1.4066,-1.40658,-1.40656,-1.40654,-1.40652,-1.4065,-1.40648,-1.40647,-1.40645,-1.40644,-1.40642,-1.40641,-1.40639,-1.40638,-1.40637]
32×26 Array{Float64,2}:
 -0.432011      0.123617    0.0913389    0.272931    -0.455859    -0.114968    -0.667972     0.358453    0.195829    -0.731325    0.363461    0.155274    0.259499   -0.448957    0.388035    -0.151603   -0.301674   -0.132892    0.124476    0.68576    -0.04915     -0.0505759   -0.0678251   0.0136672  -0.663612   -0.259519 
 -0.113778     -0.451274   -0.150727     0.30273     -0.583951    -0.296667    -0.107765     0.202145   -0.226636    -0.225454   -0.162596    0.50782     0.0413327  -0.222622   -0.00182499   0.0665017  -0.124103   -0.100513   -0.774069    0.311287    0.272633     0.0916893    0.50434     0.384662    0.354402    0.34403  
 -0.375031     -0.458158   -0.145219    -0.101194    -0.720405     0.334162     0.453717    -0.177688    0.0382035   -0.301266    0.0560548  -0.0106864   0.076548    0.507289    0.200496    -0.28587    -0.0549711   0.234069   -0.298516   -0.305001   -0.212902    -0.249657     0.168131    0.253379   -0.415835    0.507499 
  0.0180144    -0.114349   -0.243728     0.618276    -0.621698     0.00198786   0.261726     0.178489   -0.157372    -0.421007   -0.68231    -0.0863213  -0.635915    0.0304229   0.0857584   -0.355245   -0.437578   -0.13635    -0.0320428   0.0753595  -0.438862     0.0516312    0.359158   -0.10014    -0.227499    0.0925181
  0.239288      0.124219    0.305489    -0.0968029    0.36059     -0.173849    -0.233168     0.374598   -0.242771    -0.157601   -0.0214753  -0.0826263   0.0421606  -0.506782   -0.0710742    0.312604    0.40918     0.0676419  -0.194867    0.221525    0.00496278   0.454335    -0.16636    -0.147645    0.100022   -0.211892 
  0.0410666    -0.140162    0.157177    -0.0618311   -0.074329     0.113863    -0.0613182    0.24811     0.112729    -0.113403   -0.332763    0.0271331  -0.0582533   0.25933     0.143484     0.32852     0.179619   -0.258476   -0.11833    -0.155081   -0.164056    -0.0429872    0.48385     0.0900602   0.196876   -0.0783359
 -0.459207      0.102654   -0.112036     0.528911     0.216378     0.0998388    0.311663    -0.122263    0.0764829   -0.262535   -0.684587    0.0700784   0.548946   -0.112362    0.289356    -0.212171    0.0340868   0.852725   -0.26429    -0.255951    0.0890176   -0.0926098   -0.19631    -0.180788    0.350051   -0.598611 
 -0.000975905   0.320914    0.0330658    0.231704     0.59664     -0.0637287    0.360122     0.161346   -0.258901    -0.320319    0.175051   -0.262091   -0.392253    0.175679    0.0479963    0.164405   -0.119174    0.468274    0.0791513   0.0189821   0.0708399    0.11772     -0.34812     0.194326    0.304616   -0.753222 
  0.243336     -0.46639    -0.0455164   -0.00794846   0.568581    -0.310315    -0.345335    -0.924208   -0.112018     0.128604   -0.194468    0.122362    0.352297   -0.020992   -0.228873    -0.218047   -0.536449   -0.619233   -0.0722229  -0.132317   -0.216661    -0.597083     0.163875   -0.308398    0.0791267  -0.153714 
 -0.301626     -0.64845     0.453984    -0.478264     0.445581    -0.383213    -0.124517    -0.612787   -0.27989     -0.215697   -0.182719   -0.192084    0.595905    0.304224    0.248941     0.108322    0.463578   -0.01402    -0.23929    -0.489767    0.803865    -0.201851    -0.398407    0.0807246   0.54855    -0.201468 
  0.13282       0.0548964   0.0104413    0.14552      0.0673897   -0.00453432   0.159164    -0.274741   -0.375651     0.0431309  -0.117017    0.202722   -0.0991517   0.0323752  -0.242361    -0.0043327  -0.0811672  -0.173105    0.254366    0.0390388  -0.132561    -0.127373    -0.301885   -0.100636   -0.0591856   0.349809 
 -0.0944903    -0.0342071  -0.186601     0.0554682    0.0157269   -0.108338    -0.0933499   -0.143035    0.301482     0.073852    0.115161    0.0518512   0.0163513  -0.135746    0.0239425   -0.087777   -0.164282    0.0615007  -0.176736   -0.0772787   0.127753    -0.0711665   -0.0267581  -0.0567958  -0.086723   -0.0630416
 -0.295118      0.179898    0.0393942   -0.0539139   -0.106724     0.539628    -0.0589064    0.171131   -0.12039     -0.111824    0.355685   -0.531299   -0.346304    0.148539   -0.0972478   -0.128797    0.0921221   0.400725    0.219665    0.217563   -0.250174     0.305366    -0.235844    0.227404   -0.112146    0.307044 
  0.0886044     0.12526     0.215459    -0.097142    -0.111848    -0.0732896   -0.136447    -0.345885    0.299332    -0.0309985   0.274745   -0.390053    0.339938    0.438138    0.159937     0.023897   -0.0297125   0.0611347   0.0882747   0.437961    0.456702    -0.074951    -0.237211    0.490044   -0.340338   -0.049952 
  0.136386      0.313873   -0.164931    -0.582279     0.238387    -0.429609    -0.183344     0.262692    0.148235     0.506578    0.46943     0.153213   -0.416692   -0.373123   -0.54088      0.379169   -0.0808757  -0.373769    0.345624    0.229942    0.180537     0.0237352   -0.0422893  -0.0839834  -0.178792    0.453858 
 -0.0724709    -0.013779    0.132751    -0.0745137   -0.0689808    0.532849    -0.00764411  -0.172369    0.154646     0.796556    0.460046    0.311282   -0.232134   -0.0252856   0.0934895   -0.0998388  -0.331686    0.21185     0.507341    0.384001   -0.210839    -0.483891     0.109225   -0.635907   -0.273631    0.181228 
  0.168242     -0.0942287   0.203169     0.0862064    0.1043      -0.291361    -0.069037     0.449644   -0.331405    -0.826249   -0.203351    0.146427    0.088348    0.195212    0.248855     0.183161   -0.179151    0.192219   -0.455244    0.0375187   0.0733329    0.140479    -0.0961003   0.100908    0.354823   -0.392014 
 -0.192508     -0.0397861  -0.11691      0.0680374   -0.15513     -0.0429852   -0.00839407  -0.283599    0.0561722    0.281831   -0.0555733  -0.0582574  -0.0834247  -0.095915   -0.0602722   -0.173157   -0.146922   -0.191693    0.0113179   0.15912     0.0253769   -0.177771     0.157414    0.0969847  -0.134413    0.277004 
 -0.263825     -0.44809     0.0368767   -0.283631    -0.0444301    0.416151    -0.196727    -0.351144    0.0185335   -0.0580638  -0.539203    0.415341    0.806744    0.288409    0.368875    -1.05252     0.16598    -0.213038    0.39086     0.0901504  -0.44952      0.76124      0.304368    0.50548     0.551851   -0.0680502
 -0.206293      0.284964   -0.0247701   -0.02521     -0.114749    -0.169352     0.555694     0.106813    0.0160415   -0.182733   -0.592958   -0.32894    -0.0999741  -0.0792872   0.12204      0.428143    0.8374     -0.0479736  -0.0371374  -0.0785886   0.205701     0.748693    -0.183302    0.614322   -0.149916    0.179394 
 -0.0504529     0.48378     0.173733     0.367749     0.114658     0.135038    -0.105114     0.94219     0.352359     0.126619    0.0619581   0.385973   -0.445245   -0.979546   -0.110972     0.351447    0.181236   -0.0610389  -0.277844   -0.148238   -0.151065    -0.0118502    0.476556   -0.614041    0.205225   -0.765137 
  0.615973     -0.23693     0.219281     0.0186037   -0.144678    -0.222378    -0.279656     0.456423   -0.00259213   0.251603   -0.426855    0.335928    0.43419     0.282828    0.428345     1.08837     0.990945   -0.34496    -0.231695   -0.30248    -0.0351103   -0.0627119    0.335212   -0.455744   -0.105459   -0.109108 
 -0.559724      0.210837   -0.0177277   -0.455153     0.20236      0.155976    -0.25776     -0.130677    0.578111     0.551408   -0.332938   -0.335613    0.132371    0.02231     0.290601    -0.0772437   0.123546   -0.367282    0.106784   -0.0841066   0.514978     0.034859     0.993798   -0.0934038  -0.0523917  -0.144219 
  0.607522     -0.054252   -0.0769501   -0.140672     0.281207     0.356698    -0.0248168   -0.131995    0.584346     0.521895    0.156426    0.0577307  -0.227626    0.6447     -0.0957594    0.234577    0.0811583  -0.518255   -0.13485    -0.311488    0.288527     0.00702895   0.34641     0.51006     0.436089   -0.122204 
  0.0807447     0.27964    -0.139129    -0.0867457   -0.118772     0.235319    -0.0465       0.0378496   0.546176    -0.136975    0.622094   -0.775168   -0.759264    0.365374    0.231931     0.486294    0.0442843   0.206331    0.0354125  -0.378248    0.100058    -0.0231696   -0.225742   -0.260559   -0.764307   -0.145826 
  0.304545     -0.136326    0.778174    -0.6433       0.0419586    1.13121     -0.362679     0.0691354   0.477515     0.198329    0.179129   -0.290868    0.369916    0.163531    0.490743     0.64187     0.0269286   0.238698    0.718933   -0.206246   -0.111147    -0.475582    -0.275666   -0.530235   -0.352828    0.20636  
  0.75323      -0.529073    0.408921    -0.0191409    0.367982     0.298342    -0.532192     0.183616   -0.758619    -0.0924108   0.178701    0.481324   -0.317328   -0.502844    0.952743    -0.541372   -0.0156099  -0.204791    0.831055    0.110918   -0.523579     0.389746    -0.461331   -0.183265   -0.118759   -0.392974 
  0.50975      -0.178313    0.0765458   -0.190126    -0.326919     0.672851     0.430308     0.0819146  -0.582071    -0.58641     0.349008   -0.461731   -0.156758    0.33926     0.332842     0.129818    0.959841   -0.262143    0.588245    0.996528   -0.404126     0.162456     0.287323   -0.0958437  -0.234201   -0.0909142
 -0.0926439    -0.204039   -0.0275841    0.0459588    0.00543386  -0.11148      0.197965    -0.428108   -0.342137    -0.133691    0.448328   -0.11873    -0.117476    0.0715856  -0.463438    -0.117979   -0.55471     0.583736    0.0388002   0.588858   -0.189802    -0.285578    -0.956596    0.264976    0.105977    0.396648 
 -0.0812281     0.589652   -0.229014     0.398261     0.266114    -0.276882    -0.239902    -0.0757041   0.349577     0.326808   -0.137281   -0.189571   -0.246991   -0.108797   -0.675808    -0.0678489  -0.666834    0.531749   -0.53858    -0.461357    0.258538    -0.162947    -0.257272   -0.0140314   0.0235673   0.29093  
 -0.167431      0.0208779  -0.00882169  -0.147213     0.312476    -0.404766     0.656597    -0.13926    -0.89552      0.215229   -0.532345    0.373398    0.035911   -0.216546   -0.487101    -0.113732    0.119264   -0.0949036   0.630824   -0.094076   -0.341206    -0.145518    -0.405027   -0.294624    0.174628    0.304446 
  0.128549      0.460064   -0.381616     0.33019      0.454798     0.271718     0.424331    -0.153231    0.317382     0.428193    0.0553948   0.399702    0.198504    0.0711271  -0.478182    -0.360655    0.191993    0.166028    0.297192    0.204739   -0.25777      0.344077    -1.03934    -0.295033   -0.308662   -0.0613434INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.406358
INFO: iteration 2, average log likelihood -1.406347
INFO: iteration 3, average log likelihood -1.406336
INFO: iteration 4, average log likelihood -1.406325
INFO: iteration 5, average log likelihood -1.406315
INFO: iteration 6, average log likelihood -1.406304
INFO: iteration 7, average log likelihood -1.406295
INFO: iteration 8, average log likelihood -1.406285
INFO: iteration 9, average log likelihood -1.406276
INFO: iteration 10, average log likelihood -1.406267
INFO: EM with 100000 data points 10 iterations avll -1.406267
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.807811e+05
      1       7.002843e+05      -2.804968e+05 |       32
      2       6.870837e+05      -1.320058e+04 |       32
      3       6.826004e+05      -4.483367e+03 |       32
      4       6.802148e+05      -2.385598e+03 |       32
      5       6.787044e+05      -1.510356e+03 |       32
      6       6.775931e+05      -1.111325e+03 |       32
      7       6.766869e+05      -9.061678e+02 |       32
      8       6.759294e+05      -7.575036e+02 |       32
      9       6.752878e+05      -6.416654e+02 |       32
     10       6.747618e+05      -5.259735e+02 |       32
     11       6.742855e+05      -4.763151e+02 |       32
     12       6.738734e+05      -4.120447e+02 |       32
     13       6.735106e+05      -3.628421e+02 |       32
     14       6.731794e+05      -3.311600e+02 |       32
     15       6.728756e+05      -3.038008e+02 |       32
     16       6.725871e+05      -2.885575e+02 |       32
     17       6.723386e+05      -2.484415e+02 |       32
     18       6.720884e+05      -2.502106e+02 |       32
     19       6.718710e+05      -2.173888e+02 |       32
     20       6.716838e+05      -1.871922e+02 |       32
     21       6.715216e+05      -1.622660e+02 |       32
     22       6.713592e+05      -1.623675e+02 |       32
     23       6.711849e+05      -1.742827e+02 |       32
     24       6.710177e+05      -1.672175e+02 |       32
     25       6.708658e+05      -1.519452e+02 |       32
     26       6.707179e+05      -1.478369e+02 |       32
     27       6.705750e+05      -1.429125e+02 |       32
     28       6.704510e+05      -1.240206e+02 |       32
     29       6.703286e+05      -1.223561e+02 |       32
     30       6.702086e+05      -1.200296e+02 |       32
     31       6.701061e+05      -1.024573e+02 |       32
     32       6.700054e+05      -1.007870e+02 |       32
     33       6.699197e+05      -8.561544e+01 |       32
     34       6.698427e+05      -7.701567e+01 |       32
     35       6.697681e+05      -7.460130e+01 |       32
     36       6.696893e+05      -7.884961e+01 |       32
     37       6.696159e+05      -7.334112e+01 |       32
     38       6.695582e+05      -5.773637e+01 |       32
     39       6.695081e+05      -5.009431e+01 |       32
     40       6.694630e+05      -4.514883e+01 |       32
     41       6.694213e+05      -4.167262e+01 |       32
     42       6.693811e+05      -4.019267e+01 |       32
     43       6.693500e+05      -3.111538e+01 |       32
     44       6.693200e+05      -3.001687e+01 |       32
     45       6.692936e+05      -2.633134e+01 |       32
     46       6.692686e+05      -2.501413e+01 |       32
     47       6.692423e+05      -2.631299e+01 |       32
     48       6.692140e+05      -2.827782e+01 |       32
     49       6.691829e+05      -3.112282e+01 |       32
     50       6.691571e+05      -2.583034e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 669157.0654371368)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.418367
INFO: iteration 2, average log likelihood -1.413447
INFO: iteration 3, average log likelihood -1.412158
INFO: iteration 4, average log likelihood -1.411248
INFO: iteration 5, average log likelihood -1.410268
INFO: iteration 6, average log likelihood -1.409251
INFO: iteration 7, average log likelihood -1.408435
INFO: iteration 8, average log likelihood -1.407929
INFO: iteration 9, average log likelihood -1.407645
INFO: iteration 10, average log likelihood -1.407475
INFO: iteration 11, average log likelihood -1.407359
INFO: iteration 12, average log likelihood -1.407271
INFO: iteration 13, average log likelihood -1.407198
INFO: iteration 14, average log likelihood -1.407135
INFO: iteration 15, average log likelihood -1.407080
INFO: iteration 16, average log likelihood -1.407031
INFO: iteration 17, average log likelihood -1.406985
INFO: iteration 18, average log likelihood -1.406944
INFO: iteration 19, average log likelihood -1.406905
INFO: iteration 20, average log likelihood -1.406869
INFO: iteration 21, average log likelihood -1.406835
INFO: iteration 22, average log likelihood -1.406803
INFO: iteration 23, average log likelihood -1.406772
INFO: iteration 24, average log likelihood -1.406743
INFO: iteration 25, average log likelihood -1.406716
INFO: iteration 26, average log likelihood -1.406689
INFO: iteration 27, average log likelihood -1.406664
INFO: iteration 28, average log likelihood -1.406640
INFO: iteration 29, average log likelihood -1.406616
INFO: iteration 30, average log likelihood -1.406594
INFO: iteration 31, average log likelihood -1.406573
INFO: iteration 32, average log likelihood -1.406552
INFO: iteration 33, average log likelihood -1.406532
INFO: iteration 34, average log likelihood -1.406513
INFO: iteration 35, average log likelihood -1.406494
INFO: iteration 36, average log likelihood -1.406477
INFO: iteration 37, average log likelihood -1.406460
INFO: iteration 38, average log likelihood -1.406443
INFO: iteration 39, average log likelihood -1.406427
INFO: iteration 40, average log likelihood -1.406411
INFO: iteration 41, average log likelihood -1.406396
INFO: iteration 42, average log likelihood -1.406382
INFO: iteration 43, average log likelihood -1.406368
INFO: iteration 44, average log likelihood -1.406354
INFO: iteration 45, average log likelihood -1.406341
INFO: iteration 46, average log likelihood -1.406328
INFO: iteration 47, average log likelihood -1.406315
INFO: iteration 48, average log likelihood -1.406303
INFO: iteration 49, average log likelihood -1.406291
INFO: iteration 50, average log likelihood -1.406279
INFO: EM with 100000 data points 50 iterations avll -1.406279
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.215731    0.332103     -0.412238   -0.0947104    0.0184081    0.389115    0.157616     -0.00617528   0.0834982   -0.175464     0.310215    -0.73916    -0.572472     0.351298     0.16077     0.0311103   0.0545983   0.547279    0.0304942  -0.280399     0.107582    0.0341045   -0.2619      0.163637   -0.484749    0.0301687 
 -0.18341    -0.0072322     0.147765   -0.293307     0.140191    -0.365452    0.544824     -0.172241    -0.755311     0.262006    -0.522883     0.347407    0.148789    -0.186136    -0.541455    0.0891283   0.145999   -0.210526    0.549492   -0.0994364   -0.305853   -0.241555    -0.292443   -0.259106    0.238308    0.483609  
 -0.19988     0.0695331    -0.29302    -0.193908    -0.147811    -0.138826   -0.0877764    -0.35005      0.175458     0.413796    -0.495843    -0.0448395  -0.0176076    0.161937     0.11477    -0.193322    0.202032   -0.832518    0.080202    0.121034     0.270472    0.124502     0.696514    0.515147    0.079089    0.281938  
  0.396479   -0.123565      0.081291   -0.125574    -0.353828     0.637996    0.431169      0.179465    -0.430764    -0.524595     0.261482    -0.427027   -0.262724     0.210635     0.312535    0.238524    1.09128    -0.218388    0.521844    0.792515    -0.435856    0.327443     0.0544314   0.0379511  -0.254877    0.0106288 
  0.0415352  -0.949865     -0.152705    0.348469     0.047319     0.22299     0.260141     -0.532528     0.190676     0.482203    -0.273482     0.208341   -0.363553     0.00525272   0.118301   -0.284905   -0.34029    -0.264403   -0.483982   -0.349659    -0.292281   -0.372749     0.25333    -0.0998289   0.122534    0.100596  
 -0.172125   -0.735761      0.130203   -0.427159     0.508986    -0.771735   -0.415719     -0.836772    -0.451017    -0.475308    -0.405151    -0.473918    0.295778     0.254023     0.410743    0.117234    0.470619   -0.429351   -0.445462   -0.496029     0.690104   -0.0430573   -0.111097    0.271739    0.285967   -0.258183  
  0.59299    -0.174847      0.116585   -0.0968241   -0.0841892   -0.0536906  -0.199058      0.339894     0.250863     0.437643    -0.474151     0.383136    0.425928     0.232391     0.496436    0.798055    0.815272   -0.62063    -0.166188   -0.426386     0.093813   -0.0163384    0.632653   -0.367015   -0.0304783  -0.157418  
 -0.744392    0.0922579    -0.108891   -0.0344503    0.0307761    0.179277   -0.0695703    -0.283508     0.18015     -0.27674      0.548039    -0.355789   -0.0339595   -0.346814    -0.649783   -0.619573   -0.835877    0.339408   -0.101051    0.376906    -0.0962028  -0.349886    -0.360418    0.187804   -0.070473    0.315939  
  0.0426251   0.216344     -0.0853698   0.296989     0.36217     -0.727463   -0.282402     -0.382351     0.504734     0.281026    -0.371373     0.311107    0.18221     -0.186136    -0.439909   -0.330149   -0.919817   -0.066958   -0.427858   -0.680564     0.431028   -0.6281      -0.0667391  -0.291726    0.0433102  -0.00904551
  0.0221286   0.000745866   0.18936    -0.1832      -0.300544     0.0602945  -0.0818351    -0.323744     0.377011     0.177684     0.102699    -0.184486    0.234092     0.482679     0.0778557   0.0172988  -0.0811016  -0.221368    0.0626045   0.213756     0.277315   -0.165646     0.185381    0.436584   -0.163657    0.231002  
  0.163778    0.930972     -0.493116    0.79612     -0.513434     0.432452    0.569848      0.453782     0.258907     0.606399    -0.261833     0.454575   -0.485688    -0.239252    -0.572516   -0.219432   -0.48227     0.12146     0.187101    0.354861    -0.497044    0.292831     0.0495432  -0.246841   -0.257722    0.182884  
 -0.0133186   0.222489      0.0397069   0.00150577  -0.0139298   -0.148824    0.0170039     0.22443      0.174661    -0.00599209   0.12801     -0.0253623  -0.0616358   -0.0436834   -0.102383    0.41111     0.0508452   0.146517   -0.203298    0.093823     0.236323   -0.0103696   -0.115453    0.0133873  -0.0173169  -0.0804419 
 -0.279331   -0.0725715    -0.0822447   0.152102    -0.516934     0.269747   -0.384923      0.376539     0.450384    -0.302319     0.245316     0.106209   -0.173962    -0.454017     0.562009    0.151422   -0.350018   -0.230117   -0.156027    0.42592      0.152324   -0.044583     0.4929      0.288968   -0.301052   -0.217122  
 -0.0271319  -0.137341      0.534405    0.118193    -0.30186      0.741906   -0.0200018     0.0184274   -0.0378632    0.380775     0.537461     0.326285   -0.141896     0.00315916   0.218562    0.0831055  -0.399641    0.480142    0.673456    0.753517    -0.308954   -0.594106    -0.0753568  -0.52788    -0.083085    0.181238  
  0.160431   -0.0767295    -0.295766    0.179277    -0.0894234   -0.313942    0.0349636    -0.679633    -0.57454      0.048375     0.132641    -0.172708   -0.248588     0.440715    -0.349851   -0.180729   -0.723835    0.407912    0.0287584   0.386467    -0.249859   -0.00858667  -0.861323    0.370968    0.0649631   0.527623  
  0.493833   -0.497774     -0.0725641   0.308784     0.689357    -0.0666217   0.0131615     0.207923    -1.01192     -0.442322    -0.23251      0.693567   -0.240483    -0.197807     0.170151   -0.227218   -0.167774    0.019186   -0.0146758   0.137618    -0.665422    0.164626    -0.111401   -0.21703     0.313586   -0.41136   
  0.0792495   0.204103     -0.296318   -0.49474      0.376831    -0.0891833  -0.12364       0.0650855    0.203921     0.665506     0.40242      0.0935178  -0.461319    -0.239473    -0.357624    0.151876   -0.0614203  -0.180103    0.256713    0.0855507    0.0253054   0.0979127    9.692e-5   -0.182306   -0.199966    0.236853  
 -0.571885    0.508843      0.0277486   0.330436     0.502928     0.457809   -0.18091       0.638891     0.484639     0.210745    -0.329646     0.138846    0.00921892  -0.424962     0.103293    0.0144558   0.313262    0.181458   -0.121034   -0.253935     0.13468     0.113374     0.638999   -0.750816    0.283536   -0.778514  
  0.326876    0.295543      0.259433    0.265392     0.115469    -0.590907   -0.272837      0.348104     0.0227141   -0.0163029    0.172673    -0.168656   -0.0371256   -0.570491    -0.269559    0.674229    0.138365   -0.0492173  -0.211032    0.295709     0.078785    0.0348167   -0.401367   -0.167817    0.0543115  -0.239665  
  0.176876   -0.0470222    -0.0609594   0.218253     0.225767    -0.012928   -0.258843      0.24047     -0.00556275  -0.297258    -0.49155     -0.170806   -0.413949     0.166054    -0.151166    0.258399    0.0093054  -0.154793   -0.383452   -0.521671    -0.326816    0.192805     0.540865    0.30167     0.313458   -0.182072  
 -0.268275   -0.568396      0.146096   -0.384396     0.0567063    0.614736   -0.385245     -0.43758      0.144672     0.0621374   -0.309487     0.370143    0.723432     0.217718     0.352457   -0.997539    0.274687   -0.169709    0.641426    0.00190374  -0.50392     0.669773     0.247699    0.347845    0.418981   -0.172546  
 -0.0473744   0.319026     -0.196585    0.254577     0.562719     0.350788    0.380003     -0.398622     0.189959     0.268622     0.0241893    0.173792    0.317039     0.178961    -0.323487   -0.352556    0.492192    0.194312    0.294073    0.0602203   -0.29175     0.39043     -1.25922    -0.223728   -0.437183    0.0792898 
  0.761645   -0.19286       0.315709   -0.378952     0.75396      0.476081   -0.546432      0.58008     -0.0502337   -0.640445     0.573389     0.255681    0.195357     0.658798     0.461482    0.201946   -0.13283     0.270355   -0.0390799  -0.599193     0.0889713   0.3511      -0.938217   -0.0765583   0.914294   -0.29924   
 -0.02445    -0.00849279   -0.016573    0.111159    -0.00415816   0.0684104   0.0407427    -0.0131352   -0.0915693   -0.0994614   -0.0526366    0.0441082  -0.0526943   -0.00387196  -0.0283036  -0.0314558  -0.0119761   0.0340012   0.0260331   0.0257049   -0.120757   -0.0190335   -0.0908838  -0.0719244  -0.029144    0.0532838 
  0.206508   -0.0401828     0.436734   -0.301513    -0.432012     0.695895   -0.271605     -0.00435885   0.809112     0.0806364    0.325514    -0.555243   -0.103366     0.2669       0.566095    0.570266    0.0455672   0.168789    0.300486   -0.545535    -0.27319    -0.181918    -0.0423711  -0.564575   -0.641967    0.209337  
 -0.0830038   0.398703      0.22696     0.454094     0.264261    -0.122624    0.691157      0.204706    -0.0262693   -0.35939     -0.134455    -0.0524476  -0.0440568    0.070641     0.023843   -0.0029448   0.0814305   0.476342   -0.0471574   0.0690124    0.383364    0.249768    -0.418678    0.503347    0.180018   -0.683222  
 -0.239016    0.135921      0.656559   -0.721061     0.227525     0.176493   -0.412266      0.447736    -0.0859788   -0.268995     0.00732075  -0.418099    0.121368    -0.037057     0.229642    0.471838    0.539727    0.183455   -0.0879634   0.172468     0.369508    0.240096     0.38447     0.0497075   0.204479   -0.118382  
 -0.0803247  -0.336289      0.19016     0.228034    -0.956278    -0.227407    0.237701      0.30467     -0.349337    -1.03145     -0.686019    -0.0959669   0.093117     0.283863     0.438339   -0.217501   -0.294154    0.0739195  -0.205391    0.127898    -0.160729    0.0337203    0.216653    0.161077   -0.0790188   0.0564322 
 -0.170774   -0.463908     -0.294557    0.280019    -0.654248    -0.202999    0.000677864   0.164669    -0.186744    -0.269125    -0.0889381    0.584904    0.0903312   -0.179196    -0.0960202  -0.0544561   0.0699904   0.0505055  -0.829374    0.193106     0.242574    0.227967     0.357197    0.326252    0.220942    0.500557  
 -0.491401   -0.326327      0.2285     -0.08268      0.276198    -0.153646    0.270174     -0.574646     0.0064064   -0.0521853   -0.270388     0.103212    0.848532     0.13147      0.168786    0.0267742   0.211406    0.478746   -0.277791   -0.209124     0.449285   -0.139593    -0.246344   -0.0929598   0.495092   -0.270192  
  0.124294   -0.118289      0.0625801   0.16788     -0.0375126   -0.490139   -0.560051     -0.0597817   -0.516798    -0.422727     0.217835     0.403712   -0.0397494   -0.556841     0.63067    -0.580681   -0.190556   -0.38482     0.749048    0.432695    -0.110792   -0.0173737   -0.42278    -0.271601   -0.779227   -0.199847  
  0.500981   -0.179541      0.163289   -0.423851     1.13257      0.209802   -0.140411     -0.729169    -0.0125995    0.586252     0.344436    -0.347974    0.257532     0.581098    -0.295339   -0.0753317  -0.250841   -0.343021    0.462564    0.180193     0.132681   -0.65176     -0.103272   -0.219358    0.0204018  -0.33865   INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.406268
INFO: iteration 2, average log likelihood -1.406257
INFO: iteration 3, average log likelihood -1.406247
INFO: iteration 4, average log likelihood -1.406236
INFO: iteration 5, average log likelihood -1.406226
INFO: iteration 6, average log likelihood -1.406217
INFO: iteration 7, average log likelihood -1.406207
INFO: iteration 8, average log likelihood -1.406198
INFO: iteration 9, average log likelihood -1.406189
INFO: iteration 10, average log likelihood -1.406181
INFO: EM with 100000 data points 10 iterations avll -1.406181
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
