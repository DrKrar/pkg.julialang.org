>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.6.6
INFO: Installing JLD v0.6.4
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.0
INFO: Installing Rmath v0.1.3
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.2.0
INFO: Installing StaticArrays v0.0.8
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated
INFO: METADATA is out-of-date â€” you may not have the latest version of GaussianMixtures
INFO: Use `Pkg.update()` to get the latest versions of your packages

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.787
Commit c71f205 (2016-09-26 16:28 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-98-generic #145-Ubuntu SMP Sat Oct 8 20:13:07 UTC 2016 x86_64 x86_64
Memory: 2.939289093017578 GB (695.51953125 MB free)
Uptime: 24225.0 sec
Load Avg:  0.8681640625  0.955078125  1.0185546875
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3504 MHz    1494274 s        879 s     138244 s     517244 s         50 s
#2  3504 MHz     654304 s       6671 s      84528 s    1574380 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.2
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.6.6
 - JLD                           0.6.4
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.0
 - Rmath                         0.1.3
 - SHA                           0.2.1
 - ScikitLearnBase               0.2.0
 - StaticArrays                  0.0.8
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:345
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:378
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:346
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1739
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-4.0327582664579563e6,[9500.58,90499.4],
[3357.75 -6671.98 3542.1; -3133.81 6473.52 -3755.86],

Array{Float64,2}[
[23914.4 -3399.01 -740.279; -3399.01 5430.87 -1796.59; -740.279 -1796.59 10948.9],

[76060.0 3381.73 1165.98; 3381.73 94330.9 1593.2; 1165.98 1593.2 89955.6]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.225128e+03
      1       8.624613e+02      -3.626662e+02 |        7
      2       8.463168e+02      -1.614451e+01 |        0
      3       8.463168e+02       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 846.3168145875816)
INFO: K-means with 272 data points using 3 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.062655
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.837942
INFO: iteration 2, lowerbound -3.727310
INFO: iteration 3, lowerbound -3.593318
INFO: iteration 4, lowerbound -3.419709
INFO: iteration 5, lowerbound -3.222258
INFO: iteration 6, lowerbound -3.026519
INFO: iteration 7, lowerbound -2.855729
INFO: dropping number of Gaussions to 7
INFO: iteration 8, lowerbound -2.716292
INFO: dropping number of Gaussions to 5
INFO: iteration 9, lowerbound -2.585365
INFO: dropping number of Gaussions to 4
INFO: iteration 10, lowerbound -2.480162
INFO: iteration 11, lowerbound -2.412384
INFO: dropping number of Gaussions to 3
INFO: iteration 12, lowerbound -2.367060
INFO: iteration 13, lowerbound -2.332846
INFO: iteration 14, lowerbound -2.313348
INFO: iteration 15, lowerbound -2.307441
INFO: dropping number of Gaussions to 2
INFO: iteration 16, lowerbound -2.302930
INFO: iteration 17, lowerbound -2.299261
INFO: iteration 18, lowerbound -2.299257
INFO: iteration 19, lowerbound -2.299255
INFO: iteration 20, lowerbound -2.299254
INFO: iteration 21, lowerbound -2.299253
INFO: iteration 22, lowerbound -2.299253
INFO: iteration 23, lowerbound -2.299253
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: 47 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Thu 13 Oct 2016 11:13:41 AM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Thu 13 Oct 2016 11:13:43 AM UTC: K-means with 272 data points using 3 iterations
11.3 data points per parameter
,Thu 13 Oct 2016 11:13:45 AM UTC: EM with 272 data points 0 iterations avll -2.062655
5.8 data points per parameter
,Thu 13 Oct 2016 11:13:45 AM UTC: GMM converted to Variational GMM
,Thu 13 Oct 2016 11:13:47 AM UTC: iteration 1, lowerbound -3.837942
,Thu 13 Oct 2016 11:13:47 AM UTC: iteration 2, lowerbound -3.727310
,Thu 13 Oct 2016 11:13:47 AM UTC: iteration 3, lowerbound -3.593318
,Thu 13 Oct 2016 11:13:47 AM UTC: iteration 4, lowerbound -3.419709
,Thu 13 Oct 2016 11:13:47 AM UTC: iteration 5, lowerbound -3.222258
,Thu 13 Oct 2016 11:13:48 AM UTC: iteration 6, lowerbound -3.026519
,Thu 13 Oct 2016 11:13:48 AM UTC: iteration 7, lowerbound -2.855729
,Thu 13 Oct 2016 11:13:48 AM UTC: dropping number of Gaussions to 7
,Thu 13 Oct 2016 11:13:48 AM UTC: iteration 8, lowerbound -2.716292
,Thu 13 Oct 2016 11:13:48 AM UTC: dropping number of Gaussions to 5
,Thu 13 Oct 2016 11:13:48 AM UTC: iteration 9, lowerbound -2.585365
,Thu 13 Oct 2016 11:13:48 AM UTC: dropping number of Gaussions to 4
,Thu 13 Oct 2016 11:13:48 AM UTC: iteration 10, lowerbound -2.480162
,Thu 13 Oct 2016 11:13:48 AM UTC: iteration 11, lowerbound -2.412384
,Thu 13 Oct 2016 11:13:48 AM UTC: dropping number of Gaussions to 3
,Thu 13 Oct 2016 11:13:48 AM UTC: iteration 12, lowerbound -2.367060
,Thu 13 Oct 2016 11:13:48 AM UTC: iteration 13, lowerbound -2.332846
,Thu 13 Oct 2016 11:13:48 AM UTC: iteration 14, lowerbound -2.313348
,Thu 13 Oct 2016 11:13:48 AM UTC: iteration 15, lowerbound -2.307441
,Thu 13 Oct 2016 11:13:48 AM UTC: dropping number of Gaussions to 2
,Thu 13 Oct 2016 11:13:48 AM UTC: iteration 16, lowerbound -2.302930
,Thu 13 Oct 2016 11:13:48 AM UTC: iteration 17, lowerbound -2.299261
,Thu 13 Oct 2016 11:13:48 AM UTC: iteration 18, lowerbound -2.299257
,Thu 13 Oct 2016 11:13:49 AM UTC: iteration 19, lowerbound -2.299255
,Thu 13 Oct 2016 11:13:49 AM UTC: iteration 20, lowerbound -2.299254
,Thu 13 Oct 2016 11:13:49 AM UTC: iteration 21, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:49 AM UTC: iteration 22, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:49 AM UTC: iteration 23, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:49 AM UTC: iteration 24, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:49 AM UTC: iteration 25, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:49 AM UTC: iteration 26, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:49 AM UTC: iteration 27, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:49 AM UTC: iteration 28, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:49 AM UTC: iteration 29, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:49 AM UTC: iteration 30, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:49 AM UTC: iteration 31, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:50 AM UTC: iteration 32, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:50 AM UTC: iteration 33, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:50 AM UTC: iteration 34, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:50 AM UTC: iteration 35, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:50 AM UTC: iteration 36, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:50 AM UTC: iteration 37, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:50 AM UTC: iteration 38, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:50 AM UTC: iteration 39, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:50 AM UTC: iteration 40, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:50 AM UTC: iteration 41, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:50 AM UTC: iteration 42, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:50 AM UTC: iteration 43, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:51 AM UTC: iteration 44, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:51 AM UTC: iteration 45, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:51 AM UTC: iteration 46, lowerbound -2.299253
,Thu 13 Oct 2016 11:13:51 AM UTC: 47 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
Î± = [95.9549,178.045]
Î² = [95.9549,178.045]
m = [2.00023 53.852; 4.2503 79.2869]
Î½ = [97.9549,180.045]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.375876 -0.00895312; 0.0 0.0127487],

[0.184042 -0.00764405; 0.0 0.00858171]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 99999.99999999996
avll from stats: -0.9936700288596971
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9936700288596828
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9936700288596827
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -1.008672900085276
avll from llpg:  -1.0086729000852765
avll direct:     -1.0086729000852765
sum posterior: 100000.0
32Ã—26 Array{Float64,2}:
 -0.176035     0.237246    -0.06399     -0.081482     0.0246297   -0.083122     0.0672458     0.220871    -0.116507     0.112829    -0.0169991    0.0185159     0.111481    -0.114652     0.0653209   -0.228382    -0.107541    -0.182782    -0.11806      0.0119772   -0.118287   -0.0889808     0.0917133   -0.0967686    0.0382683   -0.231386 
  0.00503873   0.0595677   -0.0888196    0.00517005   0.10622      0.104143     0.0849271     0.0669254   -0.136572    -0.0286591    0.193792    -0.0653082    -0.120976     0.0159016   -0.159321    -0.0807089    0.0265067    0.0396861   -0.0241266    0.0136327   -0.028123    0.00529989    0.0195806   -0.0374841   -0.0588536   -0.0482393
 -0.160668    -0.0734543   -0.0558126    0.016043     0.0460837    0.0787542    0.0120262     0.00255081  -0.102419     0.0525315   -0.0221388    0.173782      0.0632753    0.0652496    0.021434     0.0101469    0.0950256   -0.106588     0.0445066    0.0160356   -0.133594   -0.136624      0.120693    -0.0888176    0.0432988   -0.0246369
 -0.0337279   -0.0749931    0.188776    -0.0494186   -0.141637     0.00450393   0.19572      -0.101647    -0.0680386   -0.0728097    0.0275051   -0.00640162    0.00571152   0.0116619    0.097459     0.147123    -0.161842     0.0187345    0.057114    -0.0135157   -0.074143    0.149374      0.102586     0.0189372   -0.0459348   -0.0633709
 -0.0140513   -0.0639428    0.0422397   -0.0697042   -0.0159489    0.0391491    0.0644185    -0.021595     0.00230827  -0.0841978    0.0545064    0.105347      0.0359244    0.0561744    0.200636    -0.0594022   -0.0103707   -0.0442981    0.0725338   -0.00327041   0.119332    0.0226004    -0.152979     0.116018     0.048625     0.0239936
 -0.228285     0.0388856   -0.0973659   -0.133924     0.140962     0.111877     0.0212363    -0.10249      0.0729334    0.00352663   0.0191812    0.00654353    0.0186557   -0.0119482   -0.0414697    0.137894     0.111433     0.202464    -0.0532523   -0.144031    -0.0299876   0.0161588    -0.110637     0.0403916   -0.128862     0.100927 
  0.0126048   -0.0114126   -0.032366     0.0111909   -0.00100399  -0.0654123   -0.123234      0.0961913    0.0243781    0.0121719   -0.103461    -0.0863113    -0.0450286    0.0578817   -0.0764966   -0.113636     0.0509847   -0.196888     0.0747697   -0.0688708    0.103213    0.081004     -0.0632839   -0.0342195   -0.0141594   -0.0133749
 -0.0317003    0.0788368   -0.00186196  -0.0583859   -0.0328955    0.0395885    0.0418484     0.0975377   -0.029432    -0.234864    -0.20024      0.0315306     0.115302     0.0907835    0.0515898   -0.181141     0.0699704    0.0558157   -0.0838169   -0.124838     0.0489861  -0.186585     -0.119717    -0.119949     0.0523412    0.103859 
 -0.0132256    0.136795     0.0165284    0.100078     0.172845     0.0534384    0.0235036    -0.0366929    0.24843     -0.07894     -0.0258769   -0.0290745    -0.0865115    0.116946    -0.0499274   -0.0195506   -0.124182    -0.121547    -0.00134456   0.0644635    0.0199229   0.0164816     0.0850021   -0.0945835   -0.0201759   -0.0612747
 -0.0348604    0.0956765   -0.143433     0.0518271    0.0619503   -0.0763099   -0.0952214     0.0248814    0.0255224    0.0101148    0.0141325    0.0516353     0.0554803   -0.0535307   -0.155064     0.0188699    0.077427     0.0991959    0.0405202   -0.0298666    0.0579203   0.0318594     0.150949    -0.0379859   -0.0196667    0.0210462
 -0.0896955   -0.0277671   -0.0558404    0.0559352   -0.0749571    0.0357042   -0.0912119    -0.0540816    0.0547185    0.0366968   -0.00985703  -0.0393837    -0.156542    -0.117393     0.00341427   0.0635137    0.0257915    4.84456e-5   0.180975     0.0767089   -0.0878008  -0.0125619    -0.323952    -0.172227     0.0187981    0.0390142
 -0.165669    -0.101201     0.156241    -0.1553      -0.0765318   -0.0934896    0.194484     -0.0301784    0.0963283   -0.0334844    0.0272799   -0.0544705    -0.018272     0.129318     0.0131085    0.214697     0.179116     0.0225827    0.128671     0.131125    -0.143912    0.0321775     0.005731    -0.0023285    0.0240212   -0.069725 
  0.0770586   -0.00079354  -0.079904     0.0358961   -0.0306151   -0.0526235   -0.134265     -0.115675     0.149448    -0.150726    -0.0977238    0.0139595     0.220545    -0.00954586  -0.136635     0.126698    -0.102137     0.143029     0.040836    -0.046146     0.0808115  -0.0805807    -0.0104647    0.0521453    0.025453     0.238236 
 -0.0375719    0.0721377   -0.00742692  -0.00402543   0.0716158   -0.0429211   -0.0068621    -0.0426433    0.203899     0.130008    -0.187636     0.141435      0.124034    -0.137834     0.0388076   -0.200974    -0.0459121    0.0521091   -0.0457126   -0.0355344    0.034876    0.0667754     0.0260049    0.130844     0.145993     0.0142706
 -0.189485     0.00453941  -0.110323     0.0218622    0.147054     0.0623423    0.0648852     0.173689     0.0445806    0.15437      0.0739613    0.0170786     0.00830974   0.0630014   -0.0486453   -0.0197689   -0.126213    -0.088536    -0.00670315   0.00813187  -0.0979363  -0.0454638    -0.261432     0.0305232   -0.215756    -0.123538 
 -0.0852398   -0.011113     0.0177879   -0.0136012   -0.040921    -0.0576017   -0.124737      0.0108489   -0.129594    -0.00834933   0.0789961    0.0659974    -0.125575     0.136659     0.0733634   -0.0778402    0.0699048    0.01367     -0.206333    -0.0588564   -0.0447831   0.106364      0.0712967    0.0179213   -0.0986496    0.0648536
  0.0482729   -0.067291     0.0886896    0.167468    -0.0876028    0.00937457  -0.086762      0.236405     0.131563     0.109669    -0.0679374   -0.0130245    -0.0638587    0.0718487    0.0294621   -0.0778567   -0.00045805  -0.10041      0.0386437    0.0767018   -0.0611623  -0.030362     -0.0239191   -0.0476631    0.00725985  -0.0424225
  0.129831     0.0651578   -0.194585    -0.119028    -0.0332226    0.11123      0.116241      0.0155849    0.280803     0.00938617   0.0481442   -0.036824     -0.133878     0.0234743   -0.00342584  -0.106355     0.083121    -0.213471    -0.118938    -0.0191445   -0.223503    0.00716324    0.0774271    0.0907836    0.00900422   0.0774277
  0.042356    -0.0367523    0.135844    -0.130396    -0.0295232   -0.0305256   -0.0364119    -0.0135245    0.109064    -0.124458    -0.12507      0.00254999   -0.147959     0.0309676   -0.0975732   -0.0249561    0.0906929    0.0637191    0.0547771    0.010122    -0.0337184   0.10389       0.0511132   -0.00620264   0.0348756    0.0755833
 -0.15792     -0.179079    -0.0361545    0.0946308   -0.027873     0.110478    -0.1188        0.0696394   -0.0713205   -0.0684202    0.0706072    0.074854      0.0086734   -0.156031    -0.145471    -0.0839932   -0.0544542   -0.137997     0.0688115   -0.0322748    0.313922   -0.164696      0.0744705   -0.00988838   0.0221576   -0.118595 
  0.0876993    0.176203    -0.18302     -0.0441915    0.0694318    0.00660206  -0.0135632    -0.216601     0.189698    -0.0867407    0.0131641   -0.0162018    -0.029958    -0.125084     0.175762    -0.0953644    0.0550962   -0.168878     0.112331    -0.120924    -0.0338425   0.0368864    -0.137547    -0.111369    -0.0618986   -0.274671 
 -0.00927409   0.102017    -0.0603093   -0.0782038   -0.063604    -0.0241238    0.130536     -0.0668633   -0.152832     0.0291594    0.0221034   -0.00872882    0.0782608   -0.0414103   -0.101918    -0.00375169   0.0136407    0.0102059    0.0706155   -0.00083378  -0.0492989  -0.0470388    -0.0715026   -0.0875226    0.0861246    0.0296988
 -0.0992672   -0.175468    -0.072729    -0.111534     0.300984     0.0185397   -0.143391      0.0652371    0.134032     0.151407     0.0159381   -0.0154233     0.0315184   -0.182498     0.16195     -0.0755999   -0.163896     0.0789422    0.0220008   -0.211656    -0.0157273   0.101341     -0.0434428    0.0725533    0.0902266    0.0498145
 -0.106347    -0.0907389   -0.0224928    0.0280842    0.188346    -0.0565057    0.0268307     0.144438    -0.0218975   -0.088364    -0.0102999   -0.0391612     0.034865    -0.0166003   -0.0235167    0.0633657   -0.00346909   0.0226977   -0.0162121    0.0386782    0.019199    0.049028     -0.0269397    0.053537    -0.0203473   -0.10444  
 -0.129593     0.125387    -0.165749     0.105021    -0.107131    -0.0747205    0.0455048    -0.198827    -0.0709184    0.0142753    0.0802558   -0.163598      0.158624     0.0988236   -0.0254405   -0.0986522   -0.0267524    0.109644     0.0868919    0.0657837    0.0451009   0.000398154  -0.0238283    0.100151    -0.115853    -0.0372432
 -0.0517329    0.0707744    0.0201148   -0.150986     0.0940426   -0.0673161    0.0411084     0.09125     -0.168656     0.00991902  -0.0621526   -0.0109588     0.0195917   -0.192651     0.0550539    0.022619     0.256603     0.0129853    0.255282    -0.0601041    0.0445804  -0.0070887    -0.0300778    0.0788748    0.256716     0.0872326
  0.0270091    0.114786    -0.0392264    0.061359     0.13083     -0.0706887   -0.000817724  -0.0994878   -0.0380882   -0.00331235  -0.0397507    0.0296821     0.0352632   -0.0156742   -0.139346    -0.149054     0.0789232   -0.0605776    0.0797369    0.099989    -0.149843    0.172395      0.20894     -0.155823    -0.104303     0.034671 
 -0.111548     0.129667     0.0669137   -0.0591951   -0.012522    -0.0692142    0.197596     -0.0529258    0.140986    -0.0826931   -0.0687516    0.0387076    -0.0834943    0.101864    -0.192755    -0.0503835   -0.0680158   -0.0742956    0.0465569   -0.279376    -0.0134806   0.0616071     0.0256336    0.0930632   -0.132881     0.0617022
 -0.0638275   -0.0978945    0.221103    -0.014835    -0.0711741   -0.176519    -0.0988413     0.0694886   -0.0942188    0.0155386    0.0753246    0.11054      -0.0645787    0.0832597    0.0630927    0.00886658  -0.0551611   -0.0080121   -0.0765491    0.116489    -0.0730665  -0.203362     -0.105763     0.0836323   -0.0236036   -0.147033 
  0.11735      0.0110993   -0.00670286   0.0811625   -0.0299642    0.0631297    0.0124803    -0.0955559   -0.142815     0.129966     0.141208    -0.000750441   0.0200765   -0.0598565    0.0535627    0.0590959    0.0385883   -0.0978742    0.177703    -0.0822151   -0.131491   -0.00831785    0.0721316   -0.114148     0.00614027  -0.126176 
  0.227971    -0.074131     0.0105041   -0.0115698   -0.0450991    0.238948     0.0643873     0.0835762    0.209502    -0.0922809    0.0760438    0.0247413    -0.0185421    0.0938299    0.15403      0.0365111    0.0629362   -0.128879    -0.106172     0.17704      0.0744224   0.0452018     0.00974239   0.0497042   -0.052529    -0.0796339
  0.0315852    0.147266     0.119801    -0.116644    -0.00341723  -0.0542247    0.00264244    0.12783     -0.049431     0.204117    -0.0482247    0.089186      0.0409538   -0.0996367    0.0816589    0.0291633   -0.0886386   -0.00950542   0.0226054    0.00777523   0.0875118  -0.146958      0.00680915   0.215765    -0.195487     0.123814 kind diag, method split
0: avll = -1.4082814445184286
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.408420
INFO: iteration 2, average log likelihood -1.408308
INFO: iteration 3, average log likelihood -1.407879
INFO: iteration 4, average log likelihood -1.402622
INFO: iteration 5, average log likelihood -1.387839
INFO: iteration 6, average log likelihood -1.380520
INFO: iteration 7, average log likelihood -1.378837
INFO: iteration 8, average log likelihood -1.378058
INFO: iteration 9, average log likelihood -1.377479
INFO: iteration 10, average log likelihood -1.376936
INFO: iteration 11, average log likelihood -1.376405
INFO: iteration 12, average log likelihood -1.375892
INFO: iteration 13, average log likelihood -1.375404
INFO: iteration 14, average log likelihood -1.374931
INFO: iteration 15, average log likelihood -1.374468
INFO: iteration 16, average log likelihood -1.374061
INFO: iteration 17, average log likelihood -1.373725
INFO: iteration 18, average log likelihood -1.373446
INFO: iteration 19, average log likelihood -1.373200
INFO: iteration 20, average log likelihood -1.372966
INFO: iteration 21, average log likelihood -1.372735
INFO: iteration 22, average log likelihood -1.372504
INFO: iteration 23, average log likelihood -1.372272
INFO: iteration 24, average log likelihood -1.372037
INFO: iteration 25, average log likelihood -1.371801
INFO: iteration 26, average log likelihood -1.371563
INFO: iteration 27, average log likelihood -1.371318
INFO: iteration 28, average log likelihood -1.371079
INFO: iteration 29, average log likelihood -1.370868
INFO: iteration 30, average log likelihood -1.370703
INFO: iteration 31, average log likelihood -1.370577
INFO: iteration 32, average log likelihood -1.370481
INFO: iteration 33, average log likelihood -1.370409
INFO: iteration 34, average log likelihood -1.370354
INFO: iteration 35, average log likelihood -1.370311
INFO: iteration 36, average log likelihood -1.370277
INFO: iteration 37, average log likelihood -1.370251
INFO: iteration 38, average log likelihood -1.370230
INFO: iteration 39, average log likelihood -1.370212
INFO: iteration 40, average log likelihood -1.370198
INFO: iteration 41, average log likelihood -1.370186
INFO: iteration 42, average log likelihood -1.370177
INFO: iteration 43, average log likelihood -1.370169
INFO: iteration 44, average log likelihood -1.370162
INFO: iteration 45, average log likelihood -1.370157
INFO: iteration 46, average log likelihood -1.370153
INFO: iteration 47, average log likelihood -1.370149
INFO: iteration 48, average log likelihood -1.370147
INFO: iteration 49, average log likelihood -1.370144
INFO: iteration 50, average log likelihood -1.370142
INFO: EM with 100000 data points 50 iterations avll -1.370142
952.4 data points per parameter
1: avll = [-1.40842,-1.40831,-1.40788,-1.40262,-1.38784,-1.38052,-1.37884,-1.37806,-1.37748,-1.37694,-1.37641,-1.37589,-1.3754,-1.37493,-1.37447,-1.37406,-1.37373,-1.37345,-1.3732,-1.37297,-1.37274,-1.3725,-1.37227,-1.37204,-1.3718,-1.37156,-1.37132,-1.37108,-1.37087,-1.3707,-1.37058,-1.37048,-1.37041,-1.37035,-1.37031,-1.37028,-1.37025,-1.37023,-1.37021,-1.3702,-1.37019,-1.37018,-1.37017,-1.37016,-1.37016,-1.37015,-1.37015,-1.37015,-1.37014,-1.37014]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.370306
INFO: iteration 2, average log likelihood -1.370174
INFO: iteration 3, average log likelihood -1.369843
INFO: iteration 4, average log likelihood -1.366500
INFO: iteration 5, average log likelihood -1.353823
INFO: iteration 6, average log likelihood -1.340805
INFO: iteration 7, average log likelihood -1.334799
INFO: iteration 8, average log likelihood -1.331826
INFO: iteration 9, average log likelihood -1.330014
INFO: iteration 10, average log likelihood -1.328584
INFO: iteration 11, average log likelihood -1.327313
INFO: iteration 12, average log likelihood -1.326214
INFO: iteration 13, average log likelihood -1.325370
INFO: iteration 14, average log likelihood -1.324753
INFO: iteration 15, average log likelihood -1.324302
INFO: iteration 16, average log likelihood -1.324001
INFO: iteration 17, average log likelihood -1.323803
INFO: iteration 18, average log likelihood -1.323669
INFO: iteration 19, average log likelihood -1.323584
INFO: iteration 20, average log likelihood -1.323531
INFO: iteration 21, average log likelihood -1.323497
INFO: iteration 22, average log likelihood -1.323474
INFO: iteration 23, average log likelihood -1.323457
INFO: iteration 24, average log likelihood -1.323444
INFO: iteration 25, average log likelihood -1.323433
INFO: iteration 26, average log likelihood -1.323424
INFO: iteration 27, average log likelihood -1.323415
INFO: iteration 28, average log likelihood -1.323405
INFO: iteration 29, average log likelihood -1.323395
INFO: iteration 30, average log likelihood -1.323382
INFO: iteration 31, average log likelihood -1.323366
INFO: iteration 32, average log likelihood -1.323344
INFO: iteration 33, average log likelihood -1.323313
INFO: iteration 34, average log likelihood -1.323262
INFO: iteration 35, average log likelihood -1.323178
INFO: iteration 36, average log likelihood -1.323059
INFO: iteration 37, average log likelihood -1.322921
INFO: iteration 38, average log likelihood -1.322806
INFO: iteration 39, average log likelihood -1.322719
INFO: iteration 40, average log likelihood -1.322658
INFO: iteration 41, average log likelihood -1.322616
INFO: iteration 42, average log likelihood -1.322586
INFO: iteration 43, average log likelihood -1.322564
INFO: iteration 44, average log likelihood -1.322547
INFO: iteration 45, average log likelihood -1.322534
INFO: iteration 46, average log likelihood -1.322523
INFO: iteration 47, average log likelihood -1.322514
INFO: iteration 48, average log likelihood -1.322506
INFO: iteration 49, average log likelihood -1.322499
INFO: iteration 50, average log likelihood -1.322493
INFO: EM with 100000 data points 50 iterations avll -1.322493
473.9 data points per parameter
2: avll = [-1.37031,-1.37017,-1.36984,-1.3665,-1.35382,-1.34081,-1.3348,-1.33183,-1.33001,-1.32858,-1.32731,-1.32621,-1.32537,-1.32475,-1.3243,-1.324,-1.3238,-1.32367,-1.32358,-1.32353,-1.3235,-1.32347,-1.32346,-1.32344,-1.32343,-1.32342,-1.32341,-1.32341,-1.32339,-1.32338,-1.32337,-1.32334,-1.32331,-1.32326,-1.32318,-1.32306,-1.32292,-1.32281,-1.32272,-1.32266,-1.32262,-1.32259,-1.32256,-1.32255,-1.32253,-1.32252,-1.32251,-1.32251,-1.3225,-1.32249]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.322708
INFO: iteration 2, average log likelihood -1.322501
INFO: iteration 3, average log likelihood -1.321920
INFO: iteration 4, average log likelihood -1.316622
INFO: iteration 5, average log likelihood -1.299394
INFO: iteration 6, average log likelihood -1.286480
INFO: iteration 7, average log likelihood -1.280592
INFO: iteration 8, average log likelihood -1.277127
INFO: iteration 9, average log likelihood -1.275075
INFO: iteration 10, average log likelihood -1.273526
INFO: iteration 11, average log likelihood -1.271954
INFO: iteration 12, average log likelihood -1.270204
INFO: iteration 13, average log likelihood -1.268464
INFO: iteration 14, average log likelihood -1.266942
INFO: iteration 15, average log likelihood -1.265839
INFO: iteration 16, average log likelihood -1.265066
INFO: iteration 17, average log likelihood -1.264420
INFO: iteration 18, average log likelihood -1.263746
INFO: iteration 19, average log likelihood -1.262942
INFO: iteration 20, average log likelihood -1.262079
INFO: iteration 21, average log likelihood -1.261433
INFO: iteration 22, average log likelihood -1.261023
INFO: iteration 23, average log likelihood -1.260712
INFO: iteration 24, average log likelihood -1.260365
INFO: iteration 25, average log likelihood -1.259951
INFO: iteration 26, average log likelihood -1.259538
INFO: iteration 27, average log likelihood -1.259221
INFO: iteration 28, average log likelihood -1.259037
INFO: iteration 29, average log likelihood -1.258941
INFO: iteration 30, average log likelihood -1.258893
INFO: iteration 31, average log likelihood -1.258865
INFO: iteration 32, average log likelihood -1.258847
INFO: iteration 33, average log likelihood -1.258833
INFO: iteration 34, average log likelihood -1.258821
INFO: iteration 35, average log likelihood -1.258812
INFO: iteration 36, average log likelihood -1.258803
INFO: iteration 37, average log likelihood -1.258797
INFO: iteration 38, average log likelihood -1.258791
INFO: iteration 39, average log likelihood -1.258786
INFO: iteration 40, average log likelihood -1.258781
INFO: iteration 41, average log likelihood -1.258778
INFO: iteration 42, average log likelihood -1.258774
INFO: iteration 43, average log likelihood -1.258772
INFO: iteration 44, average log likelihood -1.258770
INFO: iteration 45, average log likelihood -1.258768
INFO: iteration 46, average log likelihood -1.258767
INFO: iteration 47, average log likelihood -1.258765
INFO: iteration 48, average log likelihood -1.258764
INFO: iteration 49, average log likelihood -1.258764
INFO: iteration 50, average log likelihood -1.258763
INFO: EM with 100000 data points 50 iterations avll -1.258763
236.4 data points per parameter
3: avll = [-1.32271,-1.3225,-1.32192,-1.31662,-1.29939,-1.28648,-1.28059,-1.27713,-1.27508,-1.27353,-1.27195,-1.2702,-1.26846,-1.26694,-1.26584,-1.26507,-1.26442,-1.26375,-1.26294,-1.26208,-1.26143,-1.26102,-1.26071,-1.26036,-1.25995,-1.25954,-1.25922,-1.25904,-1.25894,-1.25889,-1.25887,-1.25885,-1.25883,-1.25882,-1.25881,-1.2588,-1.2588,-1.25879,-1.25879,-1.25878,-1.25878,-1.25877,-1.25877,-1.25877,-1.25877,-1.25877,-1.25877,-1.25876,-1.25876,-1.25876]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.259035
INFO: iteration 2, average log likelihood -1.258739
INFO: iteration 3, average log likelihood -1.257135
INFO: iteration 4, average log likelihood -1.240998
WARNING: Variances had to be floored 2
INFO: iteration 5, average log likelihood -1.203959
WARNING: Variances had to be floored 1 7
INFO: iteration 6, average log likelihood -1.189502
INFO: iteration 7, average log likelihood -1.188451
WARNING: Variances had to be floored 2 8
INFO: iteration 8, average log likelihood -1.172765
WARNING: Variances had to be floored 1 7
INFO: iteration 9, average log likelihood -1.188702
INFO: iteration 10, average log likelihood -1.187119
WARNING: Variances had to be floored 2 7
INFO: iteration 11, average log likelihood -1.170754
WARNING: Variances had to be floored 1 8
INFO: iteration 12, average log likelihood -1.183604
WARNING: Variances had to be floored 7
INFO: iteration 13, average log likelihood -1.185052
WARNING: Variances had to be floored 2
INFO: iteration 14, average log likelihood -1.174835
WARNING: Variances had to be floored 1 7
INFO: iteration 15, average log likelihood -1.176165
WARNING: Variances had to be floored 8
INFO: iteration 16, average log likelihood -1.178014
WARNING: Variances had to be floored 2 7
INFO: iteration 17, average log likelihood -1.173327
WARNING: Variances had to be floored 1
INFO: iteration 18, average log likelihood -1.182486
WARNING: Variances had to be floored 7
INFO: iteration 19, average log likelihood -1.172777
WARNING: Variances had to be floored 2 8
INFO: iteration 20, average log likelihood -1.168892
WARNING: Variances had to be floored 1 7
INFO: iteration 21, average log likelihood -1.182908
INFO: iteration 22, average log likelihood -1.180690
WARNING: Variances had to be floored 2 7
INFO: iteration 23, average log likelihood -1.164300
WARNING: Variances had to be floored 1 8
INFO: iteration 24, average log likelihood -1.178749
WARNING: Variances had to be floored 7
INFO: iteration 25, average log likelihood -1.181040
WARNING: Variances had to be floored 2
INFO: iteration 26, average log likelihood -1.172001
WARNING: Variances had to be floored 1 7 8
INFO: iteration 27, average log likelihood -1.174061
INFO: iteration 28, average log likelihood -1.185691
WARNING: Variances had to be floored 2
INFO: iteration 29, average log likelihood -1.174973
WARNING: Variances had to be floored 1 7
INFO: iteration 30, average log likelihood -1.175710
WARNING: Variances had to be floored 2
INFO: iteration 31, average log likelihood -1.178201
WARNING: Variances had to be floored 1 7 8
INFO: iteration 32, average log likelihood -1.172978
WARNING: Variances had to be floored 2
INFO: iteration 33, average log likelihood -1.184994
INFO: iteration 34, average log likelihood -1.184440
WARNING: Variances had to be floored 1 7
INFO: iteration 35, average log likelihood -1.167368
WARNING: Variances had to be floored 2
INFO: iteration 36, average log likelihood -1.171021
WARNING: Variances had to be floored 4 7 8
INFO: iteration 37, average log likelihood -1.169195
WARNING: Variances had to be floored 1
INFO: iteration 38, average log likelihood -1.188747
WARNING: Variances had to be floored 2
INFO: iteration 39, average log likelihood -1.181796
WARNING: Variances had to be floored 7
INFO: iteration 40, average log likelihood -1.176716
WARNING: Variances had to be floored 1 8
INFO: iteration 41, average log likelihood -1.173281
WARNING: Variances had to be floored 2 7
INFO: iteration 42, average log likelihood -1.175892
INFO: iteration 43, average log likelihood -1.180746
WARNING: Variances had to be floored 1 7 8
INFO: iteration 44, average log likelihood -1.164582
WARNING: Variances had to be floored 2 4
INFO: iteration 45, average log likelihood -1.177576
INFO: iteration 46, average log likelihood -1.193362
WARNING: Variances had to be floored 1 7
INFO: iteration 47, average log likelihood -1.172074
WARNING: Variances had to be floored 2
INFO: iteration 48, average log likelihood -1.174852
WARNING: Variances had to be floored 7 8
INFO: iteration 49, average log likelihood -1.172992
WARNING: Variances had to be floored 1
INFO: iteration 50, average log likelihood -1.180108
INFO: EM with 100000 data points 50 iterations avll -1.180108
118.1 data points per parameter
4: avll = [-1.25904,-1.25874,-1.25713,-1.241,-1.20396,-1.1895,-1.18845,-1.17277,-1.1887,-1.18712,-1.17075,-1.1836,-1.18505,-1.17483,-1.17617,-1.17801,-1.17333,-1.18249,-1.17278,-1.16889,-1.18291,-1.18069,-1.1643,-1.17875,-1.18104,-1.172,-1.17406,-1.18569,-1.17497,-1.17571,-1.1782,-1.17298,-1.18499,-1.18444,-1.16737,-1.17102,-1.16919,-1.18875,-1.1818,-1.17672,-1.17328,-1.17589,-1.18075,-1.16458,-1.17758,-1.19336,-1.17207,-1.17485,-1.17299,-1.18011]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 3 4
INFO: iteration 1, average log likelihood -1.177911
WARNING: Variances had to be floored 3 4 13 14
INFO: iteration 2, average log likelihood -1.162848
WARNING: Variances had to be floored 1 2 3 4 7 8 32
INFO: iteration 3, average log likelihood -1.162560
WARNING: Variances had to be floored 3 4 13 14 15 16 32
INFO: iteration 4, average log likelihood -1.150980
WARNING: Variances had to be floored 1 2 3 4 32
INFO: iteration 5, average log likelihood -1.126743
WARNING: Variances had to be floored 3 4 7 8 12 13 14 32
INFO: iteration 6, average log likelihood -1.088013
WARNING: Variances had to be floored 1 2 3 4 13 14 15 16 32
INFO: iteration 7, average log likelihood -1.086804
WARNING: Variances had to be floored 3 4 12 13 14 18 28 32
INFO: iteration 8, average log likelihood -1.082939
WARNING: Variances had to be floored 1 2 3 4 7 8 13 14 20 32
INFO: iteration 9, average log likelihood -1.094874
WARNING: Variances had to be floored 3 4 12 13 14 15 16 32
INFO: iteration 10, average log likelihood -1.085165
WARNING: Variances had to be floored 1 2 3 4 7 8 13 14 18 32
INFO: iteration 11, average log likelihood -1.084768
WARNING: Variances had to be floored 3 4 12 13 14 28 32
INFO: iteration 12, average log likelihood -1.086627
WARNING: Variances had to be floored 1 2 3 4 13 14 15 16 32
INFO: iteration 13, average log likelihood -1.086985
WARNING: Variances had to be floored 3 4 7 8 12 13 14 18 20 32
INFO: iteration 14, average log likelihood -1.079949
WARNING: Variances had to be floored 1 2 3 4 13 14 32
INFO: iteration 15, average log likelihood -1.093723
WARNING: Variances had to be floored 3 4 12 13 14 15 16 28 32
INFO: iteration 16, average log likelihood -1.078767
WARNING: Variances had to be floored 1 2 3 4 7 8 14 18 32
INFO: iteration 17, average log likelihood -1.091100
WARNING: Variances had to be floored 3 4 12 13 32
INFO: iteration 18, average log likelihood -1.083088
WARNING: Variances had to be floored 1 2 3 4 13 14 15 16 20 32
INFO: iteration 19, average log likelihood -1.078256
WARNING: Variances had to be floored 3 4 7 8 12 14 18 28 32
INFO: iteration 20, average log likelihood -1.084512
WARNING: Variances had to be floored 1 2 3 4 13 32
INFO: iteration 21, average log likelihood -1.093849
WARNING: Variances had to be floored 3 4 12 13 15 16 32
INFO: iteration 22, average log likelihood -1.078825
WARNING: Variances had to be floored 1 2 3 4 7 8 14 18 32
INFO: iteration 23, average log likelihood -1.081533
WARNING: Variances had to be floored 3 4 12 13 20 28 32
INFO: iteration 24, average log likelihood -1.080425
WARNING: Variances had to be floored 1 2 3 4 13 15 16 32
INFO: iteration 25, average log likelihood -1.092063
WARNING: Variances had to be floored 3 4 7 8 12 18 32
INFO: iteration 26, average log likelihood -1.082617
WARNING: Variances had to be floored 1 2 3 4 13 14 32
INFO: iteration 27, average log likelihood -1.084053
WARNING: Variances had to be floored 3 4 12 13 14 15 16 28 32
INFO: iteration 28, average log likelihood -1.076004
WARNING: Variances had to be floored 1 2 3 4 7 8 18 20 32
INFO: iteration 29, average log likelihood -1.089098
WARNING: Variances had to be floored 3 4 12 13 14 32
INFO: iteration 30, average log likelihood -1.087198
WARNING: Variances had to be floored 1 2 3 4 13 14 15 16 32
INFO: iteration 31, average log likelihood -1.081735
WARNING: Variances had to be floored 3 4 7 8 12 13 18 28 32
INFO: iteration 32, average log likelihood -1.080340
WARNING: Variances had to be floored 1 2 3 4 14 32
INFO: iteration 33, average log likelihood -1.093170
WARNING: Variances had to be floored 3 4 12 13 15 16 20 32
INFO: iteration 34, average log likelihood -1.076027
WARNING: Variances had to be floored 1 2 3 4 7 8 14 18 32
INFO: iteration 35, average log likelihood -1.082968
WARNING: Variances had to be floored 3 4 12 14 28 32
INFO: iteration 36, average log likelihood -1.086898
WARNING: Variances had to be floored 1 2 3 4 13 15 16 32
INFO: iteration 37, average log likelihood -1.086213
WARNING: Variances had to be floored 3 4 7 8 12 14 18 32
INFO: iteration 38, average log likelihood -1.075671
WARNING: Variances had to be floored 1 2 3 4 14 20 32
INFO: iteration 39, average log likelihood -1.084242
WARNING: Variances had to be floored 3 4 12 13 15 16 26 28 32
INFO: iteration 40, average log likelihood -1.074038
WARNING: Variances had to be floored 1 2 3 4 7 8 14 18 32
INFO: iteration 41, average log likelihood -1.089128
WARNING: Variances had to be floored 3 4 12 14 32
INFO: iteration 42, average log likelihood -1.088214
WARNING: Variances had to be floored 1 2 3 4 14 15 16 32
INFO: iteration 43, average log likelihood -1.078416
WARNING: Variances had to be floored 3 4 7 8 12 18 20 28 32
INFO: iteration 44, average log likelihood -1.076629
WARNING: Variances had to be floored 1 2 3 4 14 32
INFO: iteration 45, average log likelihood -1.092087
WARNING: Variances had to be floored 3 4 12 13 15 16 26 32
INFO: iteration 46, average log likelihood -1.071313
WARNING: Variances had to be floored 1 2 3 4 7 8 14 18 32
INFO: iteration 47, average log likelihood -1.081436
WARNING: Variances had to be floored 3 4 12 14 28 32
INFO: iteration 48, average log likelihood -1.085483
WARNING: Variances had to be floored 1 2 3 4 14 15 16 20 32
INFO: iteration 49, average log likelihood -1.084878
WARNING: Variances had to be floored 3 4 7 8 12 13 18 32
INFO: iteration 50, average log likelihood -1.082004
INFO: EM with 100000 data points 50 iterations avll -1.082004
59.0 data points per parameter
5: avll = [-1.17791,-1.16285,-1.16256,-1.15098,-1.12674,-1.08801,-1.0868,-1.08294,-1.09487,-1.08517,-1.08477,-1.08663,-1.08699,-1.07995,-1.09372,-1.07877,-1.0911,-1.08309,-1.07826,-1.08451,-1.09385,-1.07882,-1.08153,-1.08042,-1.09206,-1.08262,-1.08405,-1.076,-1.0891,-1.0872,-1.08174,-1.08034,-1.09317,-1.07603,-1.08297,-1.0869,-1.08621,-1.07567,-1.08424,-1.07404,-1.08913,-1.08821,-1.07842,-1.07663,-1.09209,-1.07131,-1.08144,-1.08548,-1.08488,-1.082]
[-1.40828,-1.40842,-1.40831,-1.40788,-1.40262,-1.38784,-1.38052,-1.37884,-1.37806,-1.37748,-1.37694,-1.37641,-1.37589,-1.3754,-1.37493,-1.37447,-1.37406,-1.37373,-1.37345,-1.3732,-1.37297,-1.37274,-1.3725,-1.37227,-1.37204,-1.3718,-1.37156,-1.37132,-1.37108,-1.37087,-1.3707,-1.37058,-1.37048,-1.37041,-1.37035,-1.37031,-1.37028,-1.37025,-1.37023,-1.37021,-1.3702,-1.37019,-1.37018,-1.37017,-1.37016,-1.37016,-1.37015,-1.37015,-1.37015,-1.37014,-1.37014,-1.37031,-1.37017,-1.36984,-1.3665,-1.35382,-1.34081,-1.3348,-1.33183,-1.33001,-1.32858,-1.32731,-1.32621,-1.32537,-1.32475,-1.3243,-1.324,-1.3238,-1.32367,-1.32358,-1.32353,-1.3235,-1.32347,-1.32346,-1.32344,-1.32343,-1.32342,-1.32341,-1.32341,-1.32339,-1.32338,-1.32337,-1.32334,-1.32331,-1.32326,-1.32318,-1.32306,-1.32292,-1.32281,-1.32272,-1.32266,-1.32262,-1.32259,-1.32256,-1.32255,-1.32253,-1.32252,-1.32251,-1.32251,-1.3225,-1.32249,-1.32271,-1.3225,-1.32192,-1.31662,-1.29939,-1.28648,-1.28059,-1.27713,-1.27508,-1.27353,-1.27195,-1.2702,-1.26846,-1.26694,-1.26584,-1.26507,-1.26442,-1.26375,-1.26294,-1.26208,-1.26143,-1.26102,-1.26071,-1.26036,-1.25995,-1.25954,-1.25922,-1.25904,-1.25894,-1.25889,-1.25887,-1.25885,-1.25883,-1.25882,-1.25881,-1.2588,-1.2588,-1.25879,-1.25879,-1.25878,-1.25878,-1.25877,-1.25877,-1.25877,-1.25877,-1.25877,-1.25877,-1.25876,-1.25876,-1.25876,-1.25904,-1.25874,-1.25713,-1.241,-1.20396,-1.1895,-1.18845,-1.17277,-1.1887,-1.18712,-1.17075,-1.1836,-1.18505,-1.17483,-1.17617,-1.17801,-1.17333,-1.18249,-1.17278,-1.16889,-1.18291,-1.18069,-1.1643,-1.17875,-1.18104,-1.172,-1.17406,-1.18569,-1.17497,-1.17571,-1.1782,-1.17298,-1.18499,-1.18444,-1.16737,-1.17102,-1.16919,-1.18875,-1.1818,-1.17672,-1.17328,-1.17589,-1.18075,-1.16458,-1.17758,-1.19336,-1.17207,-1.17485,-1.17299,-1.18011,-1.17791,-1.16285,-1.16256,-1.15098,-1.12674,-1.08801,-1.0868,-1.08294,-1.09487,-1.08517,-1.08477,-1.08663,-1.08699,-1.07995,-1.09372,-1.07877,-1.0911,-1.08309,-1.07826,-1.08451,-1.09385,-1.07882,-1.08153,-1.08042,-1.09206,-1.08262,-1.08405,-1.076,-1.0891,-1.0872,-1.08174,-1.08034,-1.09317,-1.07603,-1.08297,-1.0869,-1.08621,-1.07567,-1.08424,-1.07404,-1.08913,-1.08821,-1.07842,-1.07663,-1.09209,-1.07131,-1.08144,-1.08548,-1.08488,-1.082]
32Ã—26 Array{Float64,2}:
 -0.00906234  -0.0652501    0.0966876   -0.0430208    0.0103046     0.0567455    0.0590967   -0.012206     -0.400925    -0.0611544    0.230709      0.137237      0.0773909     0.066709      0.208297    -0.0651039   -0.0188989    -0.0639481    1.056       -0.0189731    0.154305    0.0119931   -0.190782     0.00478242    0.0399746    0.0228176 
 -0.0148941   -0.0670107   -0.00224684  -0.105336    -0.0577827     0.0337737    0.0719594   -0.0177523     0.356845    -0.108876    -0.029874      0.0737038    -0.0167012     0.0335849     0.192632    -0.146147    -0.000579423  -0.0293517   -0.919685    -0.0183855    0.0770357   0.00881866   0.0862624    0.190901      0.0404818    0.0217373 
  0.157039    -0.13709     -0.0175558    0.106913    -0.0472887     0.296569     0.0605858    0.0921983     0.145023    -0.0928549    0.135559      0.00596486    0.219946      0.272879      0.147745     0.0466124    0.0611853    -0.0314628   -0.106202     0.17743     -0.138999    0.207079     0.067773    -0.0329843    -0.0496195   -0.0791922 
  0.25216     -0.128248     0.0504651   -0.106962    -0.0367504     0.238388     0.0701755    0.0668433     0.292237    -0.092266     0.0476198     0.0264801    -0.279571     -0.0862274     0.151582     0.0251897    0.0654511    -0.24498     -0.106991     0.177294     0.450911   -0.118729     0.00723939   0.112588     -0.0714389   -0.0749008 
 -0.165898    -0.111942     0.153384    -0.131203    -0.0983017    -0.0949794    0.163627    -0.0276728     0.0896129   -0.0417142    0.0220686    -0.060455     -0.0441736     0.133134      0.0379382    0.218534     0.177283      0.0243047    0.117016     0.146323    -0.0734002   0.0308833    0.00578736  -0.0113016     0.0230084   -0.0794377 
 -0.0216762    0.0565556   -0.0163645    0.082613     0.10563       0.0673508    0.0245016    0.00257542    0.0435689    0.0615333    0.0324225     0.00277093   -0.00786861    0.0341503    -0.0171186    0.0362791   -0.0623838    -0.113502     0.0649364   -0.00505567  -0.0670829  -0.00229231  -0.0249409   -0.0695649    -0.067073    -0.101717  
  0.0459847   -0.0980268    0.102322     0.125539    -0.0863857     0.071395    -0.0833414    0.223993      0.129392     0.227122    -0.0534207    -0.0426458    -0.0795441     0.0719788     0.0415082   -0.214218    -0.0482635    -1.20324     -0.00257089   0.0415615   -0.0386512  -0.054752    -0.151987    -0.0502011    -0.00280998   0.0692875 
  0.0380671   -0.037993     0.0862573    0.205501    -0.0412921    -0.0400543   -0.0814897    0.247596      0.129534     0.093759    -0.100221     -0.000351343  -0.0736091     0.0717986     0.0225556    0.0419359    0.0332943     0.760068     0.0728243    0.121624     0.0540614   0.00986781   0.00022107  -0.0602215     0.0138055   -0.00434234
 -0.0736294    0.167257     0.0641478   -0.0194185   -0.0278705    -0.0191706   -0.432088     0.0550216    -0.150344     0.00523808  -0.00652808    0.0879312    -0.383451      0.124661      0.0732758   -0.0791294    0.0353856    -0.00331698  -0.196502    -0.114013    -0.0540284   0.117212     0.0398561    0.006895     -0.0736305    0.0209989 
 -0.0855753   -0.159766     0.0246768   -0.0119627   -0.0736682    -0.0902206    0.152055    -0.013793     -0.106697    -0.0118156    0.249694      0.00462178    0.164776      0.147953      0.0798019   -0.0740239    0.0722834     0.0873832   -0.135945    -0.0493538   -0.0675053   0.0988961    0.1419       0.0408764    -0.131212     0.0926027 
 -0.145855     0.0965734   -0.150947     0.106622    -0.109192     -0.0602059    0.0461243   -0.196676     -0.0836795    0.0235891    0.0507523    -0.167757      0.155926      0.0844641    -0.0373599   -0.0996651   -0.0362171     0.113108     0.0823073    0.0289144    0.0502427  -0.00258773  -0.0765305    0.0945058    -0.125413    -0.0471187 
  0.0476012   -0.026787     0.117193    -0.127534    -0.0300849    -0.0388758   -0.0420642   -0.00931897    0.111762    -0.121574    -0.0869499    -0.0021959    -0.140635      0.0333689    -0.097472    -0.0245409    0.101643      0.0661267    0.047345     0.0202434   -0.0398222   0.100693     0.0356376   -0.0061832     0.0326277    0.0769443 
 -0.127244     0.119225     0.0528171   -0.0629561   -0.0236405    -0.121619     0.14856     -0.0222812     0.133311    -0.0785765   -0.0784519     0.109799     -0.0576951    -0.0015144    -0.0584575   -0.0063325   -0.0786098     0.0531373    0.03742     -0.266977    -0.019746    0.0747219   -0.00499017   0.089858     -0.0887382   -0.0141016 
 -0.130479     0.154735     0.0701414   -0.0599874    0.000132188   0.0473346    0.230281    -0.0672422     0.136359    -0.0854425   -0.038865     -0.057001     -0.174861      0.186296     -0.267684    -0.0650156   -0.0226648    -0.184319     0.0288809   -0.229704    -0.0187071   0.0561275    0.0237027    0.0867541    -0.183015     0.124376  
  0.127034     0.151334     0.117846    -0.129118     0.00214671   -0.0230751    0.00221092   0.129822     -0.225334     0.292391    -0.082676      0.0514144     0.084044     -0.0976977     0.0780273    0.0521536   -0.0868159     0.0151576   -0.623469     0.0129395    0.0727661  -0.157758     0.18344      0.193949     -0.18659      0.118511  
 -0.087664     0.0781975    0.117166    -0.115741    -0.00812088   -0.0249645   -0.0187603    0.124877      0.0605826    0.229959    -0.0380062     0.128566      0.0641494    -0.0973774     0.0862285    0.0529905   -0.0877573    -0.0137593    0.573334     0.0455005    0.085988   -0.143506    -0.110992     0.239969     -0.199319     0.160243  
 -0.0620688    0.0302924   -0.0392351   -0.0153792   -0.0416528     0.0369179   -0.0188162    0.0367769     0.00801178  -0.0909812   -0.112189      0.012142      0.000378091   0.00233313    0.0271776   -0.0646571    0.0678916     0.0501955    0.0419817   -0.0344855   -0.0183621  -0.128487    -0.218262    -0.168852      0.0342458    0.0783592 
 -0.0442199   -0.0841196    0.247149    -0.0152775   -0.0701837    -0.167371    -0.098499     0.0758096    -0.0890373    0.0489412    0.0401225     0.110782     -0.0541145     0.0907186     0.0632447    0.00946788  -0.0603164     0.00102396  -0.105444     0.105653    -0.0589599  -0.198171    -0.103369     0.0606611    -0.0225242   -0.154037  
 -0.0825253    0.05578     -0.106355    -0.0550977    0.128806      0.107704     0.0743192    0.0164954    -0.0331821   -0.0208649    0.1161       -0.0355922    -0.0511938     0.00718115   -0.125435     0.0294008    0.0599848     0.114238    -0.0150094   -0.0504869   -0.0262282   0.0112015   -0.024441     0.000216905  -0.0640492    0.0192235 
 -0.05299      0.0849321   -0.0578166   -0.113364    -0.0505496    -0.00973779   0.14964     -0.0236982    -0.117419     0.0383807    0.038943      0.0149592     0.05963      -0.0404618    -0.101541    -0.00530129  -0.0114375     0.00643226   0.0803039   -0.00922329  -0.0611676  -0.0420218   -0.107666    -0.0536434     0.0686099    0.0384456 
 -0.00404327   0.0318959    0.0416442    0.0172512    0.00867547   -0.0422414    0.0480116   -0.0890799    -0.0211059   -0.0237969    0.00968229    0.0142345     0.0194974    -0.0147051    -0.0802991    0.019802    -0.0323345     0.0186136    0.069028     0.0332063   -0.0751982   0.140502     0.109324    -0.025187     -0.0726967   -0.0144584 
 -0.106098    -0.205319    -0.0510003   -0.092252     0.321389      0.0472466   -0.158063     0.064094      0.132662     0.147196     0.0173469    -0.0240251     0.0450374    -0.175839      0.159011    -0.066573    -0.177938      0.044019     0.0182092   -0.194705    -0.0216553   0.145624    -0.0362065    0.0949621     0.104026     0.049139  
 -0.0561085    0.0625855    0.0150536   -0.152654     0.0686696    -0.0781369    0.0259148    0.0890726    -0.171129     0.00998919  -0.0591019     0.000578159  -0.0343972    -0.21151       0.0639783    0.0225139    0.224113      0.0338739    0.252628    -0.0353931    0.0144859  -0.00531699  -0.0318485    0.15251       0.255147     0.0862344 
 -0.0439145    0.202414    -0.119993    -0.0741599    0.0600004    -0.0216695    0.0845667    0.0161678     0.00349277   0.0160479   -0.0100635     0.00453865    0.0263493    -0.118659      0.115218    -0.168251     0.00436257   -0.170763     0.0012489   -0.0491424   -0.0641071  -0.0201017   -0.00423962  -0.105335     -0.0202543   -0.227558  
  0.118332     0.0534334   -0.184324    -0.120208    -0.0403298     0.106426     0.13031      0.0141886     0.29951      0.00914071   0.0450096    -0.0364658    -0.133399      0.0242248     0.00954347  -0.102188     0.0689903    -0.211249    -0.126249    -0.0330705   -0.217992   -0.0245921    0.0794519    0.102957      0.0694772    0.0770991 
 -0.0500836    0.110453    -0.161778     0.0550051    0.064761     -0.0794682   -0.100989    -0.0312818     0.0312912    0.0117945   -0.0139993     0.0185679     0.053011     -0.0850274    -0.140348     0.0204402    0.0798627     0.0968445    0.0331783   -0.0409942    0.0577427   0.00494167   0.148029    -0.0324555     0.00111239   0.0374714 
  0.0758341    0.00382253  -0.0800529    0.0797109   -0.0313569    -0.126274    -0.13585     -0.128128      0.172377    -0.153225    -0.0944408     0.0354641     0.213197     -0.000836115  -0.204465     0.127678    -0.102712      0.144551     0.0397219   -0.0383146    0.0830518  -0.0802716   -0.0100697    0.00924589    0.0824804    0.241347  
  0.00595446  -0.0116844   -0.0313473    0.0191615   -8.9116e-5    -0.103786    -0.102845     0.0955367     0.0239678    0.0264878   -0.0770709    -0.0790053    -0.055167      0.0490806    -0.0844622   -0.0856985    0.0217367    -0.184175     0.0747976   -0.0700834    0.0984831   0.103287    -0.0647451   -0.0305741    -0.0111725    0.00255894
 -0.15863     -0.177231    -0.0363116    0.0973044   -0.0397202     0.107629    -0.109086     0.0841731    -0.0838196   -0.0454995    0.0705226     0.0776119     0.0114013    -0.138715     -0.156308    -0.0802512   -0.0493883    -0.139342     0.0715594   -0.0258746    0.318968   -0.167925     0.0718087   -0.0282129     0.0337698   -0.116896  
 -0.159415    -0.0747512   -0.0831672    0.0083371    0.0402276     0.100211     0.00541177  -0.000529842  -0.105637     0.0433888   -0.0230657     0.185857     -0.000681082   0.0715816     0.0348494    0.0487924    0.0928784    -0.0954519    0.0375481    0.00199042  -0.149712   -0.133081     0.12279     -0.152651      0.0406555   -0.0382815 
 -0.139892    -0.0719243    0.0409994    0.0592565    0.192366     -0.0555494    0.0502374    0.143916      0.011228    -0.142926     0.000838951  -0.0131223     0.0294324    -0.0435872    -0.0296758    0.0917922   -0.00034957    0.0239789   -0.0130889   -0.0115147    0.0100944   0.0482018   -0.0320271    0.0708687    -0.0341608   -0.136142  
 -0.0331036    0.0416252   -0.027441    -0.00892462   0.0875167    -0.0418456    0.0110204   -0.0386402     0.211526     0.137776    -0.157018      0.178791      0.116068     -0.155661      0.0344746   -0.183052    -0.0443358     0.0357296    0.00121704  -0.0449987    0.0339651   0.0570875    0.0236054    0.135379      0.13807      0.0149582 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 2 3 4 14 26 32
INFO: iteration 1, average log likelihood -1.081097
WARNING: Variances had to be floored 1 2 3 4 12 14 15 16 26 28 32
INFO: iteration 2, average log likelihood -1.061768
WARNING: Variances had to be floored 1 2 3 4 7 8 14 18 26 32
INFO: iteration 3, average log likelihood -1.065375
WARNING: Variances had to be floored 1 2 3 4 12 14 15 16 20 28 32
INFO: iteration 4, average log likelihood -1.067004
WARNING: Variances had to be floored 1 2 3 4 8 14 26 28 32
INFO: iteration 5, average log likelihood -1.069641
WARNING: Variances had to be floored 1 2 3 4 7 12 14 15 16 18 28 32
INFO: iteration 6, average log likelihood -1.055047
WARNING: Variances had to be floored 1 2 3 4 14 20 26 28 32
INFO: iteration 7, average log likelihood -1.067055
WARNING: Variances had to be floored 1 2 3 4 8 12 14 15 16 28 32
INFO: iteration 8, average log likelihood -1.048935
WARNING: Variances had to be floored 1 2 3 4 7 14 18 24 26 28 32
INFO: iteration 9, average log likelihood -1.047962
WARNING: Variances had to be floored 1 2 3 4 12 14 15 16 20 28 32
INFO: iteration 10, average log likelihood -1.061178
INFO: EM with 100000 data points 10 iterations avll -1.061178
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.058175e+05
      1       6.739825e+05      -2.318350e+05 |       32
      2       6.413741e+05      -3.260841e+04 |       32
      3       6.255674e+05      -1.580674e+04 |       32
      4       6.135712e+05      -1.199615e+04 |       32
      5       6.054263e+05      -8.144921e+03 |       32
      6       6.007136e+05      -4.712717e+03 |       32
      7       5.990902e+05      -1.623396e+03 |       32
      8       5.984620e+05      -6.282346e+02 |       32
      9       5.980713e+05      -3.906493e+02 |       32
     10       5.977830e+05      -2.882839e+02 |       32
     11       5.975230e+05      -2.600521e+02 |       32
     12       5.973049e+05      -2.181007e+02 |       32
     13       5.971418e+05      -1.630928e+02 |       32
     14       5.970210e+05      -1.208109e+02 |       32
     15       5.969410e+05      -7.999646e+01 |       29
     16       5.968966e+05      -4.441782e+01 |       32
     17       5.968611e+05      -3.548094e+01 |       32
     18       5.968310e+05      -3.013836e+01 |       32
     19       5.968145e+05      -1.648553e+01 |       30
     20       5.968033e+05      -1.114769e+01 |       28
     21       5.967948e+05      -8.476380e+00 |       29
     22       5.967881e+05      -6.722254e+00 |       28
     23       5.967816e+05      -6.566166e+00 |       26
     24       5.967737e+05      -7.867945e+00 |       24
     25       5.967647e+05      -9.025188e+00 |       26
     26       5.967511e+05      -1.357972e+01 |       31
     27       5.967348e+05      -1.623547e+01 |       30
     28       5.967063e+05      -2.850794e+01 |       30
     29       5.966644e+05      -4.189218e+01 |       30
     30       5.965933e+05      -7.111965e+01 |       30
     31       5.965120e+05      -8.128228e+01 |       31
     32       5.964368e+05      -7.521244e+01 |       31
     33       5.963867e+05      -5.016752e+01 |       32
     34       5.963476e+05      -3.903267e+01 |       32
     35       5.963064e+05      -4.120246e+01 |       31
     36       5.962648e+05      -4.162999e+01 |       30
     37       5.962176e+05      -4.719308e+01 |       29
     38       5.961618e+05      -5.581301e+01 |       31
     39       5.961092e+05      -5.263256e+01 |       30
     40       5.960558e+05      -5.339352e+01 |       30
     41       5.959863e+05      -6.949100e+01 |       30
     42       5.959145e+05      -7.180941e+01 |       28
     43       5.958444e+05      -7.008168e+01 |       29
     44       5.957912e+05      -5.317086e+01 |       28
     45       5.957567e+05      -3.454215e+01 |       32
     46       5.957336e+05      -2.310722e+01 |       31
     47       5.957158e+05      -1.776566e+01 |       31
     48       5.957014e+05      -1.442729e+01 |       29
     49       5.956887e+05      -1.270593e+01 |       27
     50       5.956778e+05      -1.090687e+01 |       27
K-means terminated without convergence after 50 iterations (objv = 595677.7620483688)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.309945
INFO: iteration 2, average log likelihood -1.283729
INFO: iteration 3, average log likelihood -1.259832
INFO: iteration 4, average log likelihood -1.234283
INFO: iteration 5, average log likelihood -1.195039
WARNING: Variances had to be floored 10 29
INFO: iteration 6, average log likelihood -1.136000
WARNING: Variances had to be floored 4 24 25
INFO: iteration 7, average log likelihood -1.107769
WARNING: Variances had to be floored 1 11 12
INFO: iteration 8, average log likelihood -1.109150
WARNING: Variances had to be floored 2 8 28
INFO: iteration 9, average log likelihood -1.096538
WARNING: Variances had to be floored 4 6 29
INFO: iteration 10, average log likelihood -1.089562
WARNING: Variances had to be floored 19 24 25 30
INFO: iteration 11, average log likelihood -1.092742
WARNING: Variances had to be floored 5
INFO: iteration 12, average log likelihood -1.098048
WARNING: Variances had to be floored 10 11 12 17 28 32
INFO: iteration 13, average log likelihood -1.050810
WARNING: Variances had to be floored 3 24 29
INFO: iteration 14, average log likelihood -1.074380
WARNING: Variances had to be floored 4 6 8 25
INFO: iteration 15, average log likelihood -1.081884
WARNING: Variances had to be floored 2 5 19 30
INFO: iteration 16, average log likelihood -1.084996
WARNING: Variances had to be floored 10 28 29
INFO: iteration 17, average log likelihood -1.072048
WARNING: Variances had to be floored 17 24
INFO: iteration 18, average log likelihood -1.068369
WARNING: Variances had to be floored 1 3 4 8 12 25 32
INFO: iteration 19, average log likelihood -1.042293
WARNING: Variances had to be floored 5 6 10
INFO: iteration 20, average log likelihood -1.098735
WARNING: Variances had to be floored 29
INFO: iteration 21, average log likelihood -1.089773
WARNING: Variances had to be floored 11 19 30
INFO: iteration 22, average log likelihood -1.055094
WARNING: Variances had to be floored 12 17 24 28
INFO: iteration 23, average log likelihood -1.040250
WARNING: Variances had to be floored 2 3 4 5 6 8 10 25 29 32
INFO: iteration 24, average log likelihood -1.037826
WARNING: Variances had to be floored 11
INFO: iteration 25, average log likelihood -1.136622
INFO: iteration 26, average log likelihood -1.106891
WARNING: Variances had to be floored 1 19
INFO: iteration 27, average log likelihood -1.053465
WARNING: Variances had to be floored 4 10 12 17 24 29
INFO: iteration 28, average log likelihood -1.018252
WARNING: Variances had to be floored 3 5 6 8 11 25 28 30 32
INFO: iteration 29, average log likelihood -1.065092
WARNING: Variances had to be floored 2
INFO: iteration 30, average log likelihood -1.131737
INFO: iteration 31, average log likelihood -1.087227
WARNING: Variances had to be floored 1 4 10 29
INFO: iteration 32, average log likelihood -1.033888
WARNING: Variances had to be floored 17 19 24
INFO: iteration 33, average log likelihood -1.060538
WARNING: Variances had to be floored 3 5 6 8 11 12 25 32
INFO: iteration 34, average log likelihood -1.037378
WARNING: Variances had to be floored 28 29
INFO: iteration 35, average log likelihood -1.101456
WARNING: Variances had to be floored 2 4 10 30
INFO: iteration 36, average log likelihood -1.082775
WARNING: Variances had to be floored 17 24
INFO: iteration 37, average log likelihood -1.081523
WARNING: Variances had to be floored 1 11 25
INFO: iteration 38, average log likelihood -1.050553
WARNING: Variances had to be floored 4 5 6 8 12 19 29 32
INFO: iteration 39, average log likelihood -1.034580
WARNING: Variances had to be floored 3 28
INFO: iteration 40, average log likelihood -1.110242
WARNING: Variances had to be floored 10 17
INFO: iteration 41, average log likelihood -1.080899
WARNING: Variances had to be floored 2 11 24 25
INFO: iteration 42, average log likelihood -1.052791
WARNING: Variances had to be floored 8 12 29 30
INFO: iteration 43, average log likelihood -1.048326
WARNING: Variances had to be floored 1 4 5 6 32
INFO: iteration 44, average log likelihood -1.059948
WARNING: Variances had to be floored 3 10 17 19 24
INFO: iteration 45, average log likelihood -1.080080
WARNING: Variances had to be floored 11 12 28
INFO: iteration 46, average log likelihood -1.089285
WARNING: Variances had to be floored 8 25 29
INFO: iteration 47, average log likelihood -1.071631
WARNING: Variances had to be floored 2 4 5 10 11 30
INFO: iteration 48, average log likelihood -1.062158
WARNING: Variances had to be floored 6 17 24 32
INFO: iteration 49, average log likelihood -1.087774
INFO: iteration 50, average log likelihood -1.085694
INFO: EM with 100000 data points 50 iterations avll -1.085694
59.0 data points per parameter
32Ã—26 Array{Float64,2}:
  0.00456974  -0.0117823   -0.03123      0.0170807    0.000966576  -0.107784    -0.104123     0.095635     0.0228606    0.0275416   -0.0781946   -0.0791587   -0.0546152    0.0486109   -0.0860252   -0.0852764     0.0238243   -0.185614     0.0750369   -0.0702284    0.099068    0.101143    -0.0649599   -0.0312994   -0.0124327    0.00692989
  0.114486    -0.0118912    0.13986      0.0861456   -0.0286647     0.0122538   -0.0231465   -0.0557359   -0.127937     0.269753     0.153747     0.0193156    0.0398653   -0.0602667    0.020552     0.143382      0.00946273  -0.119557     0.12216     -0.0377569   -0.352142    0.00380097   0.0489764   -0.191261     0.00228883  -0.148504  
 -0.0445519    0.147643     0.043578     0.0902474    0.165421      0.0582255    0.02008     -0.039855     0.221345    -0.0809168   -0.0304874   -0.0496319   -0.0637004    0.103953    -0.0332609   -0.000184884  -0.140129    -0.127474     0.0126662    0.0640185    0.0117376   0.0135946    0.0992273   -0.0947134   -0.00577978  -0.061132  
 -0.0314752    0.0654525   -0.00374808  -0.0449988   -0.0496724     0.0156109    0.0354227    0.115936    -0.04431     -0.18467     -0.187783     0.0403454    0.0917639    0.0876512    0.0493871   -0.151198      0.0929622    0.0779823   -0.0726435   -0.0964676    0.0336543  -0.218452    -0.118862    -0.131418     0.043119     0.0854379 
  0.240006    -0.139452     0.033767     0.0280924   -0.0363321     0.266736     0.0490389    0.0510155    0.192771    -0.0955995    0.0894717    0.0185468   -0.100412     0.155424     0.166127     0.0299303     0.0598408   -0.1443      -0.0595595    0.131016     0.244902    0.0316496    0.0657325    0.0529615   -0.0524895   -0.0887676 
 -0.0313795    0.121068    -0.0442632   -0.153071    -0.059563     -0.0462516    0.232203    -0.019909    -0.239997     0.0552047    0.0385498    0.0461512    0.10401     -0.0502805   -0.122062    -0.026832     -0.0291489    0.0160683    0.049407    -0.00149227  -0.0507682  -0.0562569   -0.0968892   -0.0420058    0.0631565    0.0444291 
  0.0371999    0.112879    -0.00859895   0.0339565    0.104873     -0.0500355   -0.0613929   -0.102377    -0.00182418  -0.00430606  -0.043342     0.0285348    0.00996987  -0.0462702   -0.151818    -0.108694      0.0740995   -0.044091     0.0767064    0.102439    -0.138308    0.164145     0.133906    -0.130197    -0.0933062    0.0336928 
 -0.00857607   0.0724248    0.1277      -0.113972    -0.00674996   -0.0188228    0.0107269    0.107603    -0.0356642    0.183502    -0.0617926    0.0957453    0.0570435   -0.0898754    0.0933049    0.0350853    -0.0812374    0.00180151   0.0457014    0.0184613    0.0796545  -0.129863     0.0117317    0.21203     -0.155411     0.12215   
 -0.0340654   -0.0794009    0.167983    -0.0504865   -0.137689     -0.00241192   0.183986    -0.0812437   -0.0442347   -0.0751535    0.0466988   -0.0209133    0.0195315    0.0205818    0.070221     0.140452     -0.161844     0.0332214    0.0542813    0.0238185   -0.0604292   0.136469     0.0821627    0.0963144   -0.0452188   -0.0636831 
 -0.104893     0.119895     0.0623735   -0.0541892   -0.0166274    -0.04718      0.19947     -0.0477384    0.0901205   -0.0671666   -0.0408541    0.0254263   -0.127166     0.0868627   -0.220092    -0.00109845   -0.0640108   -0.0415479    0.0658797   -0.240857    -0.0314176   0.0670051    0.0360753    0.0746473   -0.119431     0.104992  
  0.0558826    0.149814    -0.132212     0.00596813   0.032554      0.0416188    0.0504009   -0.156981     0.114336    -0.0654029    0.0141405    0.00198461  -0.025504    -0.0678711    0.147232    -0.0716458     0.0981422   -0.196357     0.0975013   -0.169275    -0.0201154   0.0399965   -0.0698563   -0.0696064   -0.061858    -0.253594  
  0.0486579   -0.026712     0.121702    -0.126803    -0.0301322    -0.0405868   -0.0410669   -0.00749844   0.112847    -0.12205     -0.0859492    0.00313383  -0.142545     0.0342436   -0.0958883   -0.0245019     0.102009     0.0658372    0.0451352    0.0197989   -0.0403118   0.100425     0.0363891   -0.00593613   0.0326729    0.0771775 
 -0.160154    -0.0709236   -0.0815731    0.0101914    0.0449238     0.0965481    0.00158686  -0.00130246  -0.0990765    0.0418756   -0.0222714    0.180942    -0.00411061   0.0770234    0.0370857    0.044818      0.0963046   -0.0968815    0.036291     0.0027989   -0.145364   -0.134415     0.122887    -0.151186     0.0394076   -0.0390105 
 -0.180863     0.243659    -0.0655004   -0.0801095    0.0332403    -0.105808     0.105579     0.237431    -0.12272      0.110963    -0.0369809    0.0186394    0.109837    -0.1195       0.0647204   -0.218532     -0.0951066   -0.155265    -0.127154     0.00167342  -0.11657    -0.091815     0.0950773   -0.0974206    0.0291769   -0.228648  
 -0.101798    -0.203488    -0.051582    -0.0937689    0.315922      0.0505462   -0.158448     0.0637151    0.132926     0.143472     0.0166369   -0.0221732    0.0470817   -0.17511      0.15965     -0.0707198    -0.175496     0.0418038    0.0167431   -0.194888    -0.0196259   0.140999    -0.0338314    0.0982505    0.103031     0.0497407 
  0.0744827    0.00330418  -0.0799261    0.0795908   -0.0315788    -0.125857    -0.136009    -0.12821      0.171433    -0.153044    -0.0944491    0.0370194    0.212164    -0.00161863  -0.203815     0.127118     -0.102226     0.144419     0.0397587   -0.0381635    0.0827103  -0.0804814   -0.0110739    0.00685661   0.0813374    0.240914  
 -0.0580321    0.129382    -0.166711     0.00642801   0.0705075    -0.0957467   -0.108467     0.0082817    0.020115     0.0147919    0.012097     0.0763497    0.0958519   -0.0431352   -0.326174     0.0461188     0.0737883    0.0717419    0.0456323   -0.0961066    0.0616355   0.0374706    0.089224    -0.0299041    0.00172642   0.0163415 
 -0.0753095    0.00632531   0.0430152   -0.0156126   -0.0500843    -0.0547002   -0.143394     0.0221099   -0.12816     -0.00381986   0.11918      0.0451364   -0.115154     0.136468     0.074031    -0.0766578     0.0567744    0.0453729   -0.168084    -0.0826911   -0.0587315   0.10789      0.0837385    0.0238047   -0.10492      0.0566239 
 -0.109762    -0.0231666   -0.10886      0.0844992    0.15777       0.0649516    0.0568744    0.174264     0.0268024    0.136071     0.0201883    0.0540806    0.00575176   0.0560307   -0.0383849   -0.00707271   -0.100833    -0.0872539    0.00203504   0.0110009   -0.0486361  -0.0480744   -0.275749     0.0169457   -0.214183    -0.125184  
 -0.158731    -0.177256    -0.036286     0.0972904   -0.0396001     0.107494    -0.109798     0.0839838   -0.0839766   -0.045591     0.070534     0.0777954    0.0111303   -0.138428    -0.156816    -0.0801276    -0.0494657   -0.139075     0.0716083   -0.0258707    0.318223   -0.168309     0.0717785   -0.0282978    0.0338224   -0.116658  
 -0.0493112    0.0610848    0.0167871   -0.152634     0.0678218    -0.0799506    0.0190757    0.0832395   -0.167255     0.00855994  -0.0587874    0.00224452  -0.0472212   -0.207833     0.0666592    0.0221777     0.225871     0.0321682    0.252545    -0.0348977    0.0147807  -0.00566143  -0.0325693    0.146787     0.252975     0.0818987 
 -0.165903    -0.110742     0.15636     -0.131       -0.0964359    -0.0937713    0.160242    -0.0288629    0.0902428   -0.0421122    0.0249632   -0.0596113   -0.0429927    0.131866     0.0399749    0.217394      0.176681     0.0239256    0.120581     0.147594    -0.0769624   0.0326291    0.00519969  -0.0127675    0.0236905   -0.0808052 
 -0.125392    -0.063167     0.0389783    0.0505891    0.180546     -0.0534681    0.0422643    0.117703     0.0274587   -0.104372    -0.00620473  -0.00388871   0.033653    -0.049612    -0.0249379    0.0707154    -0.00351983   0.022361    -0.0144997   -0.0168234    0.010893    0.0479427   -0.0281616    0.0639898   -0.0206889   -0.12568   
 -0.0867411   -0.00235808  -0.0713438    0.0498706   -0.0243618    -0.0145406   -0.0781751   -0.0797954    0.036374     0.0421907    0.00537846  -0.0340822   -0.100405    -0.116882    -0.0393465    0.0539937     0.034744     0.0302291    0.158289     0.0562289   -0.0490488  -0.0149911   -0.183013    -0.165598     0.0165967    0.0345679 
  0.0403314   -0.0634732    0.0940375    0.167718    -0.0627904     0.00525368  -0.0809017    0.228421     0.125473     0.143889    -0.0793514   -0.0128056   -0.0726419    0.071398     0.032374    -0.0524425    -0.00346594  -0.0542272    0.0414302    0.0847417    0.0130202  -0.0170548   -0.0584129   -0.0603234    0.00725142   0.0237981 
 -0.00120688   0.0559153   -0.0739393   -0.00531634   0.0938981     0.102819     0.111368     0.103064    -0.120654    -0.0415422    0.192101    -0.0627445   -0.0727575    0.0284074   -0.152754    -0.0732701     0.00133435   0.0380897    0.00710922   0.00849161  -0.0203261   0.00574885   0.0183871   -0.0380786   -0.00587891  -0.0473562 
  0.112377     0.0534069   -0.184702    -0.114198    -0.0365377     0.0960157    0.128217     0.0160111    0.285759     0.00586682   0.0466079   -0.0363586   -0.12603      0.0218935   -0.00953232  -0.101384      0.0663726   -0.198278    -0.120289    -0.0261479   -0.205312   -0.0322833    0.0841822    0.0940206    0.0683454    0.0760158 
 -0.178987     0.0690292   -0.143246    -0.132094     0.134132      0.107904     0.0356017   -0.120739     0.0896332   -0.00701943   0.0124112    0.00886406   0.00954064  -0.0405002   -0.0310582    0.140537      0.115806     0.155898    -0.0110392   -0.13279     -0.0430382   0.0163332   -0.103874     0.0278995   -0.115925     0.0648357 
 -0.0252423    0.0372212   -0.0313241    0.00183967   0.080262     -0.0409973    0.0071598   -0.0389893    0.195619     0.129296    -0.15869      0.176033     0.113813    -0.155549     0.0323252   -0.18558      -0.0427438    0.0267782    0.0149642   -0.0299545    0.0330304   0.0536773    0.0150907    0.145717     0.143188     0.0143952 
  0.0288737   -0.0172911   -0.0273001   -0.0898531   -0.0143763     0.0684284    0.066401    -0.0512477    0.0192981   -0.0866861    0.134941     0.073062     0.0334021    0.0323174    0.220197    -0.129292      0.0148689   -0.0805576    0.0291731   -0.00356732   0.112693    0.0279746   -0.0604671    0.0563396    0.0191911   -0.0398232 
 -0.144023     0.0929559   -0.142493     0.0992502   -0.106811     -0.0546293    0.0438969   -0.187304    -0.0786561    0.020438     0.0532234   -0.153988     0.15489      0.0862474   -0.0298771   -0.0965481    -0.0333877    0.111029     0.0688708    0.0318473    0.0481207  -0.0103916   -0.0709361    0.095008    -0.122288    -0.0493282 
 -0.0716175   -0.0816827    0.22299     -0.0280376   -0.0586641    -0.11843     -0.0861452    0.0775203   -0.0732412    0.0442977    0.0397136    0.150735    -0.0810432    0.0988648    0.0771178    0.0139654    -0.0533941    0.0111096   -0.0966035    0.0971692   -0.0578171  -0.20189     -0.107092     0.104049    -0.0154745   -0.165298  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 3 8 12 19 25 29
INFO: iteration 1, average log likelihood -1.033626
WARNING: Variances had to be floored 1 2 3 4 5 6 8 10 11 12 19 25 28 29 32
INFO: iteration 2, average log likelihood -0.984724
WARNING: Variances had to be floored 1 3 8 12 17 19 25 29 30
INFO: iteration 3, average log likelihood -1.012198
WARNING: Variances had to be floored 1 2 3 4 5 6 8 10 11 12 19 24 25 28 29 32
INFO: iteration 4, average log likelihood -0.979534
WARNING: Variances had to be floored 1 3 8 12 19 25 29 30
INFO: iteration 5, average log likelihood -1.023181
WARNING: Variances had to be floored 1 2 3 4 5 6 8 10 11 12 17 19 24 25 28 29 32
INFO: iteration 6, average log likelihood -0.976766
WARNING: Variances had to be floored 1 3 8 12 19 25 29 30
INFO: iteration 7, average log likelihood -1.028766
WARNING: Variances had to be floored 1 2 3 4 5 6 8 10 11 12 19 25 28 29 32
INFO: iteration 8, average log likelihood -0.985926
WARNING: Variances had to be floored 1 3 8 12 17 19 24 25 29 30
INFO: iteration 9, average log likelihood -1.011149
WARNING: Variances had to be floored 1 2 3 4 5 6 8 10 11 12 19 25 28 29 32
INFO: iteration 10, average log likelihood -0.992282
INFO: EM with 100000 data points 10 iterations avll -0.992282
59.0 data points per parameter
32Ã—26 Array{Float64,2}:
 -0.154904     0.119871     0.0543748    0.0834138   0.0672939    -0.165563      0.0943887   -0.0140144    0.108934   -0.0139051   -0.318701    -0.0346555   -0.00782186   0.0904065   -0.0616287   -0.0981107   -0.112284      0.0118541   -0.000997343   0.11852       0.0244639    0.0130542    0.19139       0.12684     -0.105996   -0.0881269  
  0.0370221   -0.155342    -0.165885    -0.0302191  -0.0934514     0.0168815     0.0370383   -0.11904      0.0995201   0.0072311    0.185075    -0.0192813   -0.0275478   -0.0770049    0.00984908   0.0419496    0.0165488     0.0863302    0.203538      0.00215903   -0.0302349   -0.0481838    0.0330889    -0.0956428    0.0520982  -0.00235649 
  0.0607163   -0.0522377    0.0917522   -0.0159287  -0.0786362     0.109278     -0.0614849   -0.0600957   -0.0119996   0.0299415    0.0845913   -0.0522836   -0.0320162   -0.0346244    0.225211    -0.144624    -0.0455296    -0.0131362    0.0241943     0.124138     -0.09843      0.0253072    0.028893     -0.0345813   -0.0582878   0.109118   
 -0.219138     0.104274    -0.134595     0.0385718  -0.0382558     0.000782333   0.0661444   -0.0822356   -0.0690417  -0.0520248    0.165218    -0.0682168    0.00274328  -0.0303608   -0.0116616   -0.0855863    0.105878      0.0606521   -0.118645      0.0209961     0.12585      0.197881     0.161918      0.0857314    0.0543389   0.0924283  
 -0.0126125   -0.0115821   -0.00603263   0.0704457  -0.0715615     0.0517295     0.120429     0.150348    -0.110726   -0.0512882   -0.102993     0.0812156    0.0817462    0.175826    -0.0674373    0.0924513   -0.202298     -0.0264142   -0.0277369     0.0385621    -0.0161988   -0.119612    -0.206462     -0.172064    -0.0140733  -0.0155297  
 -0.109446    -0.0054384    0.0309552    0.276304   -0.0541807    -0.105715     -0.0376511   -0.0220586   -0.132122    0.00799944   0.0322082    0.0927524   -0.0944159   -0.0265825   -0.0462063   -0.0710489   -0.00809471   -0.0478997    0.0770039    -0.0566393     0.146296     0.0752068   -0.0188954    -0.0160044   -0.1569      0.0184037  
 -0.00478129   0.0714742   -0.0513237    0.0606619  -0.0758114    -0.0441049    -0.0662881   -0.0323591   -0.0419187   0.111036    -0.156391     0.0615997   -0.0906012    0.065109    -0.125552     0.0547273   -0.129531     -0.191128     0.0879399     0.10217      -0.0246742   -0.0233616   -0.0315405    -0.0347924   -0.133257   -0.165254   
  0.0580858    0.0607991    0.0497773    0.152381    0.0585263    -0.0851167    -0.0494051   -0.051362    -0.054379    0.00680305  -0.109952    -0.103095     0.0817435   -0.0936016    0.0407323   -0.151628     0.0285943    -0.091786    -0.070508     -0.000729815  -0.00505808  -0.155117     0.00893289   -0.134769     0.0410856  -0.00263837 
  0.00522954   0.0372773   -0.0127339    0.0900538   0.0646815    -0.0322354     0.0635104    0.0363108   -0.130083   -0.0462682   -0.0666353    0.197858    -0.0548783   -0.105526    -0.00543477   0.0909599   -0.131343      0.00542641   0.183219     -0.0858092    -0.0570043   -0.235516     0.0315143     0.176682    -0.0939528   0.167931   
 -0.208503     0.0380474    0.207699    -0.168453   -0.151368     -0.00154715    0.155163     0.0659511   -0.0603229   0.0536941    0.123997    -0.231823     0.0699817    0.0634699    0.0606638    0.216284     0.0890922    -0.0234759    0.0100352    -0.0941589     0.169604    -0.0248803    0.0978944    -0.233626     0.0306515  -0.0902977  
 -0.0152427   -0.113261     0.0270247    0.0707989  -0.00738048   -0.0179179     0.0390796   -0.146065    -0.179401   -0.0997093    0.0630103   -0.0499273    0.0629993   -0.0731345   -0.118078     0.0129855   -0.0593256     0.153717    -0.0181284     0.180013     -0.167462     0.0286954    0.0368374     0.0435622   -0.0961915  -0.0386234  
 -0.0122646    0.138484    -0.0226045   -0.119084   -0.0951804     0.0451965     0.069046    -0.13364     -0.191675    0.10734     -0.144438     0.0651151   -0.230551    -0.00311035  -0.273479    -0.00492157  -0.0344068    -0.0622964    0.201662     -0.0681063    -0.141271     0.0369948   -0.00439931    0.00867646   0.0396259  -0.189749   
 -0.101387     0.182574     0.031625    -0.0179288  -0.0971308     0.0593347     0.00137505   0.0232459    0.0834717  -0.00159883   0.0623564    0.00964618  -0.109363     0.0307703   -0.148846     0.0582899   -0.00809527   -0.143061    -0.0135046     0.0505872     0.0142064   -0.0103348    0.0350488    -0.0958153    0.093421    0.116469   
 -0.0849279    0.0855301   -0.0568079    0.0687416  -0.00912766    0.139598      0.0681394   -0.0552686    0.0454795   0.080541     0.126189    -0.0434837    0.0944421    0.0653264   -0.0604857   -0.275899     0.0210976    -0.133247    -0.0387494     0.232546     -0.0341068   -0.0424335    0.0257775     0.060438    -0.0164097  -0.00672987 
 -0.190387     0.11388      0.150355    -0.0409559   0.106632      0.00348322    0.107275    -0.117613    -0.106105    0.00934725   0.00549652   0.0102256   -0.178414    -0.179671    -0.196192    -0.0560462    0.000200557  -0.0224656   -0.134465     -0.0439363    -0.0578551   -0.149963     0.0755675    -0.135059     0.176412    0.10875    
 -0.170486     0.0257545    0.130965     0.119294   -0.111866     -0.0271434     0.0205008    0.00176288  -0.072328    0.0329506    0.0483249    0.228018     0.190396    -0.00402968  -0.217502    -0.00931173   0.152957     -0.118461    -0.0401229    -0.0148198     0.0821018   -0.17136      0.0543235     0.0568518   -0.0752414  -0.0336409  
  0.0230602    0.0154061    0.0276783   -0.239067   -0.013076      0.0778354    -0.0234723    0.045007    -0.0782261   0.0814558   -0.108049    -0.0160026   -0.0138744    0.19209     -0.0180099   -0.0842322   -0.15968      -0.00758465  -0.0307381    -0.10462       0.248076    -0.0800731   -0.0584507     0.0733604    0.0794711  -0.0256637  
 -0.00170522  -0.0814164   -0.220806    -0.108276    0.115556     -0.018296     -0.0177613    0.0731862   -0.184376    0.0870005    0.0709321    0.0739415    0.0425379   -0.00647286   0.162543     0.0126947   -0.0970872     0.00912403  -0.0362265    -0.00281298   -0.0340149    0.00821635   0.000380608  -0.134087     0.114096   -0.118427   
  0.133752     0.126171     0.0607133    0.0487471  -0.000212644   0.066489      0.11746     -0.0888272   -0.0707559   0.0406982    0.0437648   -0.02712     -0.00551062   0.117682    -0.0292268    0.00337242   0.0208243     0.0606708   -0.0730059     0.0316684     0.0381576   -0.0701244   -0.155072      0.117682    -0.0558477  -0.0698289  
  0.136086     0.00849704   0.0182021   -0.0272621   0.106937     -0.0696061     0.0016877   -0.0917675   -0.169814    0.052411    -0.129437    -0.15947     -0.0464807   -0.0262703   -0.0227287    0.0745454    0.0219359    -0.113132     0.124034     -0.0333816    -0.19339      0.209795     0.025698     -0.0128534   -0.212282   -0.184529   
  0.0350424    0.0148879    0.107477     0.102596   -0.0751535    -0.108062     -0.0101174    0.251779    -0.0487225   0.0714682   -0.0485414   -0.0704802   -0.0721202    0.0204079    0.0742001    0.0721092    0.026347      0.0255566   -0.046982      0.115854     -0.0190993    0.0325665   -0.0169182     0.0392031    0.0318488  -0.0685506  
  0.0391621    0.0909566   -0.0245057    0.0416671  -0.0922757    -0.105907      0.0470378   -0.00920201  -0.013078   -0.214687    -0.178421    -0.0865274    0.048989    -0.18378     -0.0371117    0.0570368   -0.0920039     0.0290946   -0.156182     -0.0840659    -0.0593447   -0.0347151   -0.0570573     0.113672    -0.0533551   0.101362   
  0.26341      0.0359801   -0.0654095   -0.0391996   0.103736     -0.0606936    -0.227624    -0.0449266    0.0645995   0.0322275   -0.132382    -0.146636    -0.013206    -0.180214     0.0193359    0.0850905    0.0510107     0.0437838   -0.250002     -0.0167861    -0.00538649  -0.106065    -0.0488982    -0.156378    -0.0744014   0.000928145
  0.0436635   -0.106239     0.127991    -0.0329588   0.0574433    -0.203106     -0.0619446    0.087005     0.0703797  -0.0659583    0.106716     0.0228342   -0.0528305    0.0854998    0.0533062   -0.0768806   -0.144433     -0.058494     0.155321     -0.168433      0.0832111    0.0292988    0.114436     -0.108414    -0.101991    0.17455    
 -0.0666365   -0.0761823   -0.0357325   -0.0140899   0.0600755     0.102113     -0.0697158   -0.0281212   -0.0514194   0.0681825   -0.108315    -0.0371485   -0.107205    -0.151269     0.0833235    0.060977     0.073556      0.114455    -0.0271425    -0.0125701    -0.117242    -0.0919362    0.0575302    -0.103134    -0.0177337   0.0704291  
  0.00590581   0.0372809   -0.152071    -0.0144594  -0.0463633    -0.0240067    -0.0729582   -0.0641334   -0.0360094   0.0440968   -0.0399741    0.092788    -0.0413683   -0.0438054    0.178336    -0.074471    -0.0210884     0.0398705    0.0150993     0.251272      0.128053     0.0484487   -0.112629     -0.198133     0.0470554   0.228355   
  0.0782923   -0.108691    -0.265064     0.0499223  -0.122566      0.00798232   -0.00476013  -0.0678444   -0.158694    0.00989683   0.0446046    0.0656363   -0.113335    -0.0936616   -0.0245824   -0.112231     0.158623      0.197857     0.0666837    -0.0759084     0.164734    -0.206207    -0.0173799     0.0373469    0.125375   -0.09943    
 -0.0198451    0.034799    -0.115371     0.0382343  -0.142783      0.0299339    -0.117088     0.0522803    0.0495155  -0.0130376   -0.0380429    0.146557    -0.0665263    0.0284156    0.0701227    0.0663774    0.00807484    0.0861264    0.0397896    -0.101026      0.101402    -0.209058    -0.0900088    -0.0896653   -0.0442184   0.179379   
  0.137503     0.0996664    0.186151     0.116581    0.0709922     0.131416     -0.155641    -0.0697431    0.025881   -0.035516    -0.0111059   -0.0160447    0.0802362    0.0421581   -0.180745    -0.00974919  -0.0675278     0.150503     0.0214916     0.0666485    -0.00074933  -0.116099    -0.0575775    -0.0957679   -0.0642909   0.0506305  
  0.167569     0.0574461    0.0661384   -0.100881   -0.107749      0.0281909    -0.0200384    0.0574994    0.0416699   0.129742     0.0234474    0.00497733   0.0247653   -0.0141831    0.029846     0.0927279   -0.027056     -0.0610052   -0.0631588     0.0214175     0.027179    -0.0894745    0.0453033    -0.0315145    0.139871   -0.0697556  
  0.0483716   -0.0413646    0.031529     0.0934229  -0.117814      0.236804      0.0336697    0.204371     0.0830162  -0.00242142  -0.120771    -0.039924     0.0983827    0.0308441   -0.0107719    0.0319708    0.104667      0.0757529    0.0303537     0.20845      -0.0746956    0.024223    -0.0201794     0.0482203    0.010921   -0.15966    
  0.0353622    0.00880083  -0.0511621    0.0222183  -0.0522101     0.032667      0.0405808    0.0239655   -0.195118    0.134004     0.109023     0.124812    -0.0265344   -0.104839    -0.137599     0.145246     0.111707     -0.0270282    0.0819293     0.0321626     0.0596887   -0.115741    -0.112612      0.0981278    0.0142248  -0.0801515  kind full, method split
0: avll = -1.431108439163401
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.431128
INFO: iteration 2, average log likelihood -1.431049
INFO: iteration 3, average log likelihood -1.430986
INFO: iteration 4, average log likelihood -1.430913
INFO: iteration 5, average log likelihood -1.430825
INFO: iteration 6, average log likelihood -1.430725
INFO: iteration 7, average log likelihood -1.430612
INFO: iteration 8, average log likelihood -1.430477
INFO: iteration 9, average log likelihood -1.430288
INFO: iteration 10, average log likelihood -1.429980
INFO: iteration 11, average log likelihood -1.429456
INFO: iteration 12, average log likelihood -1.428667
INFO: iteration 13, average log likelihood -1.427730
INFO: iteration 14, average log likelihood -1.426926
INFO: iteration 15, average log likelihood -1.426424
INFO: iteration 16, average log likelihood -1.426178
INFO: iteration 17, average log likelihood -1.426071
INFO: iteration 18, average log likelihood -1.426026
INFO: iteration 19, average log likelihood -1.426007
INFO: iteration 20, average log likelihood -1.425999
INFO: iteration 21, average log likelihood -1.425995
INFO: iteration 22, average log likelihood -1.425993
INFO: iteration 23, average log likelihood -1.425992
INFO: iteration 24, average log likelihood -1.425991
INFO: iteration 25, average log likelihood -1.425991
INFO: iteration 26, average log likelihood -1.425990
INFO: iteration 27, average log likelihood -1.425990
INFO: iteration 28, average log likelihood -1.425989
INFO: iteration 29, average log likelihood -1.425989
INFO: iteration 30, average log likelihood -1.425989
INFO: iteration 31, average log likelihood -1.425989
INFO: iteration 32, average log likelihood -1.425988
INFO: iteration 33, average log likelihood -1.425988
INFO: iteration 34, average log likelihood -1.425988
INFO: iteration 35, average log likelihood -1.425988
INFO: iteration 36, average log likelihood -1.425988
INFO: iteration 37, average log likelihood -1.425988
INFO: iteration 38, average log likelihood -1.425987
INFO: iteration 39, average log likelihood -1.425987
INFO: iteration 40, average log likelihood -1.425987
INFO: iteration 41, average log likelihood -1.425987
INFO: iteration 42, average log likelihood -1.425987
INFO: iteration 43, average log likelihood -1.425987
INFO: iteration 44, average log likelihood -1.425987
INFO: iteration 45, average log likelihood -1.425987
INFO: iteration 46, average log likelihood -1.425987
INFO: iteration 47, average log likelihood -1.425987
INFO: iteration 48, average log likelihood -1.425987
INFO: iteration 49, average log likelihood -1.425987
INFO: iteration 50, average log likelihood -1.425987
INFO: EM with 100000 data points 50 iterations avll -1.425987
952.4 data points per parameter
1: avll = [-1.43113,-1.43105,-1.43099,-1.43091,-1.43083,-1.43072,-1.43061,-1.43048,-1.43029,-1.42998,-1.42946,-1.42867,-1.42773,-1.42693,-1.42642,-1.42618,-1.42607,-1.42603,-1.42601,-1.426,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.426002
INFO: iteration 2, average log likelihood -1.425935
INFO: iteration 3, average log likelihood -1.425878
INFO: iteration 4, average log likelihood -1.425810
INFO: iteration 5, average log likelihood -1.425729
INFO: iteration 6, average log likelihood -1.425640
INFO: iteration 7, average log likelihood -1.425550
INFO: iteration 8, average log likelihood -1.425470
INFO: iteration 9, average log likelihood -1.425403
INFO: iteration 10, average log likelihood -1.425349
INFO: iteration 11, average log likelihood -1.425305
INFO: iteration 12, average log likelihood -1.425268
INFO: iteration 13, average log likelihood -1.425236
INFO: iteration 14, average log likelihood -1.425208
INFO: iteration 15, average log likelihood -1.425183
INFO: iteration 16, average log likelihood -1.425160
INFO: iteration 17, average log likelihood -1.425140
INFO: iteration 18, average log likelihood -1.425121
INFO: iteration 19, average log likelihood -1.425104
INFO: iteration 20, average log likelihood -1.425088
INFO: iteration 21, average log likelihood -1.425072
INFO: iteration 22, average log likelihood -1.425058
INFO: iteration 23, average log likelihood -1.425044
INFO: iteration 24, average log likelihood -1.425031
INFO: iteration 25, average log likelihood -1.425018
INFO: iteration 26, average log likelihood -1.425006
INFO: iteration 27, average log likelihood -1.424995
INFO: iteration 28, average log likelihood -1.424985
INFO: iteration 29, average log likelihood -1.424975
INFO: iteration 30, average log likelihood -1.424966
INFO: iteration 31, average log likelihood -1.424958
INFO: iteration 32, average log likelihood -1.424950
INFO: iteration 33, average log likelihood -1.424942
INFO: iteration 34, average log likelihood -1.424936
INFO: iteration 35, average log likelihood -1.424929
INFO: iteration 36, average log likelihood -1.424924
INFO: iteration 37, average log likelihood -1.424918
INFO: iteration 38, average log likelihood -1.424913
INFO: iteration 39, average log likelihood -1.424909
INFO: iteration 40, average log likelihood -1.424904
INFO: iteration 41, average log likelihood -1.424900
INFO: iteration 42, average log likelihood -1.424897
INFO: iteration 43, average log likelihood -1.424893
INFO: iteration 44, average log likelihood -1.424890
INFO: iteration 45, average log likelihood -1.424887
INFO: iteration 46, average log likelihood -1.424884
INFO: iteration 47, average log likelihood -1.424881
INFO: iteration 48, average log likelihood -1.424879
INFO: iteration 49, average log likelihood -1.424876
INFO: iteration 50, average log likelihood -1.424874
INFO: EM with 100000 data points 50 iterations avll -1.424874
473.9 data points per parameter
2: avll = [-1.426,-1.42594,-1.42588,-1.42581,-1.42573,-1.42564,-1.42555,-1.42547,-1.4254,-1.42535,-1.4253,-1.42527,-1.42524,-1.42521,-1.42518,-1.42516,-1.42514,-1.42512,-1.4251,-1.42509,-1.42507,-1.42506,-1.42504,-1.42503,-1.42502,-1.42501,-1.425,-1.42498,-1.42498,-1.42497,-1.42496,-1.42495,-1.42494,-1.42494,-1.42493,-1.42492,-1.42492,-1.42491,-1.42491,-1.4249,-1.4249,-1.4249,-1.42489,-1.42489,-1.42489,-1.42488,-1.42488,-1.42488,-1.42488,-1.42487]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.424885
INFO: iteration 2, average log likelihood -1.424838
INFO: iteration 3, average log likelihood -1.424802
INFO: iteration 4, average log likelihood -1.424761
INFO: iteration 5, average log likelihood -1.424710
INFO: iteration 6, average log likelihood -1.424645
INFO: iteration 7, average log likelihood -1.424567
INFO: iteration 8, average log likelihood -1.424476
INFO: iteration 9, average log likelihood -1.424377
INFO: iteration 10, average log likelihood -1.424276
INFO: iteration 11, average log likelihood -1.424177
INFO: iteration 12, average log likelihood -1.424087
INFO: iteration 13, average log likelihood -1.424006
INFO: iteration 14, average log likelihood -1.423937
INFO: iteration 15, average log likelihood -1.423878
INFO: iteration 16, average log likelihood -1.423828
INFO: iteration 17, average log likelihood -1.423787
INFO: iteration 18, average log likelihood -1.423752
INFO: iteration 19, average log likelihood -1.423723
INFO: iteration 20, average log likelihood -1.423698
INFO: iteration 21, average log likelihood -1.423676
INFO: iteration 22, average log likelihood -1.423657
INFO: iteration 23, average log likelihood -1.423640
INFO: iteration 24, average log likelihood -1.423626
INFO: iteration 25, average log likelihood -1.423613
INFO: iteration 26, average log likelihood -1.423601
INFO: iteration 27, average log likelihood -1.423590
INFO: iteration 28, average log likelihood -1.423581
INFO: iteration 29, average log likelihood -1.423572
INFO: iteration 30, average log likelihood -1.423564
INFO: iteration 31, average log likelihood -1.423557
INFO: iteration 32, average log likelihood -1.423550
INFO: iteration 33, average log likelihood -1.423543
INFO: iteration 34, average log likelihood -1.423537
INFO: iteration 35, average log likelihood -1.423531
INFO: iteration 36, average log likelihood -1.423525
INFO: iteration 37, average log likelihood -1.423520
INFO: iteration 38, average log likelihood -1.423515
INFO: iteration 39, average log likelihood -1.423509
INFO: iteration 40, average log likelihood -1.423504
INFO: iteration 41, average log likelihood -1.423499
INFO: iteration 42, average log likelihood -1.423494
INFO: iteration 43, average log likelihood -1.423490
INFO: iteration 44, average log likelihood -1.423485
INFO: iteration 45, average log likelihood -1.423480
INFO: iteration 46, average log likelihood -1.423475
INFO: iteration 47, average log likelihood -1.423470
INFO: iteration 48, average log likelihood -1.423466
INFO: iteration 49, average log likelihood -1.423461
INFO: iteration 50, average log likelihood -1.423456
INFO: EM with 100000 data points 50 iterations avll -1.423456
236.4 data points per parameter
3: avll = [-1.42488,-1.42484,-1.4248,-1.42476,-1.42471,-1.42465,-1.42457,-1.42448,-1.42438,-1.42428,-1.42418,-1.42409,-1.42401,-1.42394,-1.42388,-1.42383,-1.42379,-1.42375,-1.42372,-1.4237,-1.42368,-1.42366,-1.42364,-1.42363,-1.42361,-1.4236,-1.42359,-1.42358,-1.42357,-1.42356,-1.42356,-1.42355,-1.42354,-1.42354,-1.42353,-1.42353,-1.42352,-1.42351,-1.42351,-1.4235,-1.4235,-1.42349,-1.42349,-1.42348,-1.42348,-1.42348,-1.42347,-1.42347,-1.42346,-1.42346]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.423461
INFO: iteration 2, average log likelihood -1.423398
INFO: iteration 3, average log likelihood -1.423342
INFO: iteration 4, average log likelihood -1.423281
INFO: iteration 5, average log likelihood -1.423209
INFO: iteration 6, average log likelihood -1.423124
INFO: iteration 7, average log likelihood -1.423027
INFO: iteration 8, average log likelihood -1.422920
INFO: iteration 9, average log likelihood -1.422809
INFO: iteration 10, average log likelihood -1.422699
INFO: iteration 11, average log likelihood -1.422594
INFO: iteration 12, average log likelihood -1.422497
INFO: iteration 13, average log likelihood -1.422409
INFO: iteration 14, average log likelihood -1.422330
INFO: iteration 15, average log likelihood -1.422261
INFO: iteration 16, average log likelihood -1.422201
INFO: iteration 17, average log likelihood -1.422148
INFO: iteration 18, average log likelihood -1.422102
INFO: iteration 19, average log likelihood -1.422061
INFO: iteration 20, average log likelihood -1.422024
INFO: iteration 21, average log likelihood -1.421991
INFO: iteration 22, average log likelihood -1.421961
INFO: iteration 23, average log likelihood -1.421933
INFO: iteration 24, average log likelihood -1.421906
INFO: iteration 25, average log likelihood -1.421881
INFO: iteration 26, average log likelihood -1.421857
INFO: iteration 27, average log likelihood -1.421834
INFO: iteration 28, average log likelihood -1.421812
INFO: iteration 29, average log likelihood -1.421790
INFO: iteration 30, average log likelihood -1.421769
INFO: iteration 31, average log likelihood -1.421749
INFO: iteration 32, average log likelihood -1.421729
INFO: iteration 33, average log likelihood -1.421710
INFO: iteration 34, average log likelihood -1.421691
INFO: iteration 35, average log likelihood -1.421673
INFO: iteration 36, average log likelihood -1.421655
INFO: iteration 37, average log likelihood -1.421638
INFO: iteration 38, average log likelihood -1.421621
INFO: iteration 39, average log likelihood -1.421605
INFO: iteration 40, average log likelihood -1.421589
INFO: iteration 41, average log likelihood -1.421574
INFO: iteration 42, average log likelihood -1.421560
INFO: iteration 43, average log likelihood -1.421546
INFO: iteration 44, average log likelihood -1.421533
INFO: iteration 45, average log likelihood -1.421520
INFO: iteration 46, average log likelihood -1.421508
INFO: iteration 47, average log likelihood -1.421496
INFO: iteration 48, average log likelihood -1.421485
INFO: iteration 49, average log likelihood -1.421474
INFO: iteration 50, average log likelihood -1.421464
INFO: EM with 100000 data points 50 iterations avll -1.421464
118.1 data points per parameter
4: avll = [-1.42346,-1.4234,-1.42334,-1.42328,-1.42321,-1.42312,-1.42303,-1.42292,-1.42281,-1.4227,-1.42259,-1.4225,-1.42241,-1.42233,-1.42226,-1.4222,-1.42215,-1.4221,-1.42206,-1.42202,-1.42199,-1.42196,-1.42193,-1.42191,-1.42188,-1.42186,-1.42183,-1.42181,-1.42179,-1.42177,-1.42175,-1.42173,-1.42171,-1.42169,-1.42167,-1.42165,-1.42164,-1.42162,-1.4216,-1.42159,-1.42157,-1.42156,-1.42155,-1.42153,-1.42152,-1.42151,-1.4215,-1.42148,-1.42147,-1.42146]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.421462
INFO: iteration 2, average log likelihood -1.421397
INFO: iteration 3, average log likelihood -1.421336
INFO: iteration 4, average log likelihood -1.421265
INFO: iteration 5, average log likelihood -1.421179
INFO: iteration 6, average log likelihood -1.421073
INFO: iteration 7, average log likelihood -1.420946
INFO: iteration 8, average log likelihood -1.420799
INFO: iteration 9, average log likelihood -1.420638
INFO: iteration 10, average log likelihood -1.420471
INFO: iteration 11, average log likelihood -1.420307
INFO: iteration 12, average log likelihood -1.420154
INFO: iteration 13, average log likelihood -1.420017
INFO: iteration 14, average log likelihood -1.419898
INFO: iteration 15, average log likelihood -1.419797
INFO: iteration 16, average log likelihood -1.419710
INFO: iteration 17, average log likelihood -1.419635
INFO: iteration 18, average log likelihood -1.419571
INFO: iteration 19, average log likelihood -1.419514
INFO: iteration 20, average log likelihood -1.419463
INFO: iteration 21, average log likelihood -1.419416
INFO: iteration 22, average log likelihood -1.419374
INFO: iteration 23, average log likelihood -1.419334
INFO: iteration 24, average log likelihood -1.419298
INFO: iteration 25, average log likelihood -1.419263
INFO: iteration 26, average log likelihood -1.419230
INFO: iteration 27, average log likelihood -1.419199
INFO: iteration 28, average log likelihood -1.419170
INFO: iteration 29, average log likelihood -1.419142
INFO: iteration 30, average log likelihood -1.419115
INFO: iteration 31, average log likelihood -1.419090
INFO: iteration 32, average log likelihood -1.419065
INFO: iteration 33, average log likelihood -1.419042
INFO: iteration 34, average log likelihood -1.419020
INFO: iteration 35, average log likelihood -1.418998
INFO: iteration 36, average log likelihood -1.418978
INFO: iteration 37, average log likelihood -1.418958
INFO: iteration 38, average log likelihood -1.418939
INFO: iteration 39, average log likelihood -1.418921
INFO: iteration 40, average log likelihood -1.418904
INFO: iteration 41, average log likelihood -1.418887
INFO: iteration 42, average log likelihood -1.418871
INFO: iteration 43, average log likelihood -1.418856
INFO: iteration 44, average log likelihood -1.418841
INFO: iteration 45, average log likelihood -1.418827
INFO: iteration 46, average log likelihood -1.418814
INFO: iteration 47, average log likelihood -1.418801
INFO: iteration 48, average log likelihood -1.418788
INFO: iteration 49, average log likelihood -1.418776
INFO: iteration 50, average log likelihood -1.418764
INFO: EM with 100000 data points 50 iterations avll -1.418764
59.0 data points per parameter
5: avll = [-1.42146,-1.4214,-1.42134,-1.42127,-1.42118,-1.42107,-1.42095,-1.4208,-1.42064,-1.42047,-1.42031,-1.42015,-1.42002,-1.4199,-1.4198,-1.41971,-1.41964,-1.41957,-1.41951,-1.41946,-1.41942,-1.41937,-1.41933,-1.4193,-1.41926,-1.41923,-1.4192,-1.41917,-1.41914,-1.41912,-1.41909,-1.41907,-1.41904,-1.41902,-1.419,-1.41898,-1.41896,-1.41894,-1.41892,-1.4189,-1.41889,-1.41887,-1.41886,-1.41884,-1.41883,-1.41881,-1.4188,-1.41879,-1.41878,-1.41876]
[-1.43111,-1.43113,-1.43105,-1.43099,-1.43091,-1.43083,-1.43072,-1.43061,-1.43048,-1.43029,-1.42998,-1.42946,-1.42867,-1.42773,-1.42693,-1.42642,-1.42618,-1.42607,-1.42603,-1.42601,-1.426,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.42599,-1.426,-1.42594,-1.42588,-1.42581,-1.42573,-1.42564,-1.42555,-1.42547,-1.4254,-1.42535,-1.4253,-1.42527,-1.42524,-1.42521,-1.42518,-1.42516,-1.42514,-1.42512,-1.4251,-1.42509,-1.42507,-1.42506,-1.42504,-1.42503,-1.42502,-1.42501,-1.425,-1.42498,-1.42498,-1.42497,-1.42496,-1.42495,-1.42494,-1.42494,-1.42493,-1.42492,-1.42492,-1.42491,-1.42491,-1.4249,-1.4249,-1.4249,-1.42489,-1.42489,-1.42489,-1.42488,-1.42488,-1.42488,-1.42488,-1.42487,-1.42488,-1.42484,-1.4248,-1.42476,-1.42471,-1.42465,-1.42457,-1.42448,-1.42438,-1.42428,-1.42418,-1.42409,-1.42401,-1.42394,-1.42388,-1.42383,-1.42379,-1.42375,-1.42372,-1.4237,-1.42368,-1.42366,-1.42364,-1.42363,-1.42361,-1.4236,-1.42359,-1.42358,-1.42357,-1.42356,-1.42356,-1.42355,-1.42354,-1.42354,-1.42353,-1.42353,-1.42352,-1.42351,-1.42351,-1.4235,-1.4235,-1.42349,-1.42349,-1.42348,-1.42348,-1.42348,-1.42347,-1.42347,-1.42346,-1.42346,-1.42346,-1.4234,-1.42334,-1.42328,-1.42321,-1.42312,-1.42303,-1.42292,-1.42281,-1.4227,-1.42259,-1.4225,-1.42241,-1.42233,-1.42226,-1.4222,-1.42215,-1.4221,-1.42206,-1.42202,-1.42199,-1.42196,-1.42193,-1.42191,-1.42188,-1.42186,-1.42183,-1.42181,-1.42179,-1.42177,-1.42175,-1.42173,-1.42171,-1.42169,-1.42167,-1.42165,-1.42164,-1.42162,-1.4216,-1.42159,-1.42157,-1.42156,-1.42155,-1.42153,-1.42152,-1.42151,-1.4215,-1.42148,-1.42147,-1.42146,-1.42146,-1.4214,-1.42134,-1.42127,-1.42118,-1.42107,-1.42095,-1.4208,-1.42064,-1.42047,-1.42031,-1.42015,-1.42002,-1.4199,-1.4198,-1.41971,-1.41964,-1.41957,-1.41951,-1.41946,-1.41942,-1.41937,-1.41933,-1.4193,-1.41926,-1.41923,-1.4192,-1.41917,-1.41914,-1.41912,-1.41909,-1.41907,-1.41904,-1.41902,-1.419,-1.41898,-1.41896,-1.41894,-1.41892,-1.4189,-1.41889,-1.41887,-1.41886,-1.41884,-1.41883,-1.41881,-1.4188,-1.41879,-1.41878,-1.41876]
32Ã—26 Array{Float64,2}:
  0.325254   -0.367072    -0.0641683  -0.302001    -0.816917    0.16635     -0.0702427     0.28758      -0.220859    -0.137062   -0.177082   -0.115664    -0.429298     0.324941     -0.494902    0.0449814  -0.10683     -0.20815    -0.0173274   0.105403     0.39713    -0.326043    -0.658271    0.596926   -0.0512739   -0.765226 
  0.727813   -0.0217701    0.0849082  -0.0846654    0.22558     0.0702606    0.129357     -0.201789     -0.468709     0.098077   -0.200165   -0.415457     0.120193     0.342241     -0.288187   -0.155225   -0.58271      0.335293    0.0920808  -0.109575     0.340271   -0.190189    -0.032445   -0.272836   -0.0129589   -0.892222 
 -0.138142   -0.431709     0.079029   -0.400054    -0.162257    0.229209     0.423243     -0.087053     -0.14808      0.320838   -0.616986    0.305007    -0.0253942    0.395223     -0.466602   -0.223318    0.0419071   -0.341027   -0.400032    0.271123    -0.29035    -0.0463957   -0.0953583  -0.417458   -0.0681586    0.0686283
  0.073775    0.0946982    0.451317    0.505894     0.0648107   0.419405     0.280723     -0.157363     -0.223864    -0.136826   -0.35934    -0.298433    -0.621529     0.000703176  -0.437674    0.0717663  -0.584321    -0.536767   -0.119942   -0.00527918  -0.350755    0.567966     0.329929    0.298308    0.129854    -0.0501342
 -0.181231    0.0666787    0.710009    0.516663    -0.123253   -0.605384     0.530459     -0.127775      0.391315    -0.977115   -0.148743   -0.118131     0.124827     0.756471     -0.462366   -0.341045   -0.0647154   -0.425744    0.451407   -0.913247    -0.083182   -0.221707     0.100663    0.112152   -0.347991    -0.094492 
 -0.479138    0.479733    -0.112351    0.500645     0.134483   -0.369609    -0.398561      0.562149      0.277837    -0.274434   -0.169035   -0.790033     0.0444894    0.433108     -0.101207   -0.317901   -0.504148     0.108499    0.0733303  -0.0436769    0.310515   -0.385239     0.376515    0.500624   -0.153066     0.102546 
  0.179103   -0.0344889   -0.579667    0.284638     0.0992026  -0.468407    -0.191552      0.275891      0.0901834    0.303941   -0.012255   -0.348182     0.168808     0.389117     -0.0798065   0.499653   -0.160127     0.0156797   0.152221   -0.096422    -0.350834   -0.0318231   -0.145847    0.220267   -0.127784     0.16853  
 -0.262205    0.0921029   -0.22436     0.0439778   -0.251523    0.0163395   -0.641209      0.259182      0.0915155   -0.169801   -0.0403293  -0.17153     -0.0736415   -0.128876     -0.0543099   0.208952    0.177912    -0.111044   -0.249569    0.529432     0.135451   -0.172245    -0.215339    0.136896   -0.199631     0.747356 
  0.0371405   0.110455     0.257964   -0.154624    -0.125951    0.0888847   -0.115261     -0.177988     -0.407382    -0.0351033  -0.0443212   0.0971726   -0.0363379   -0.0249376    -0.0635618  -0.0487466  -0.0135407    0.0429313  -0.0909997   0.0151042    0.133524   -0.037118    -0.0254914  -0.0376342  -0.0930124   -0.0325783
 -0.0136372   0.0229511   -0.149122    0.0459637    0.0172863  -0.0740055    0.185812      0.0280807     0.262927    -0.0149707   0.088833    0.0278627   -0.00142277  -0.050712     -0.0467783  -0.103406   -0.122952    -0.0476574   0.0756057   0.0533049   -0.0842068   0.00480981   0.10545    -0.0923942   0.0906668   -0.0889731
 -0.0749557  -0.320498     0.156059    0.00590619   0.0182355   0.473881     0.15057       0.40735      -0.178318     0.126739   -0.311275    0.360652     0.0569037   -0.0175005     0.368033    0.154716    0.664685     0.231572    0.372768   -0.183299    -0.30719     0.262159    -0.196835   -0.101406    0.214032    -0.063036 
 -0.531436    0.0397161    0.711304    0.12403     -0.0133721  -0.0117732    0.197539      0.298032     -0.0334022   -0.0163598   0.146075    0.084862     0.221591    -0.190236      0.204391    0.290576    0.366985    -0.0300876   0.116556   -0.151157     0.808201   -0.0934082   -0.18617     0.374546    0.166708    -0.282189 
 -0.101482    0.118904     0.0270838   0.0340559    0.0595742   0.0455829   -0.154485     -0.63458       0.42851      0.115853    0.572449    0.461587     0.212032    -0.140649      0.196857   -0.237998   -0.125979     0.0475858  -0.060071    0.0867804   -0.0320112  -0.479623     0.611201   -0.401664   -0.482818     0.372786 
 -0.255252   -0.217197    -0.460661    0.145236    -0.0471442  -0.590586    -0.141747     -0.224988      0.00915039   0.383316   -0.233794    0.200178     0.176659    -0.255976     -0.514757    0.212076    0.796165     0.626423   -0.0127869  -0.123761    -0.284937   -0.32772      0.536414   -0.216442    0.0618588   -0.0214634
  0.354307    0.0281568   -0.285931   -0.463451    -0.184491   -0.176965    -0.167049     -0.0341794     0.249259     0.0143312  -0.0796331  -0.191203     0.562684    -0.423432      0.991528   -0.323609    0.259314     0.164519    0.102368    0.100647     0.302284   -0.140791    -0.201302   -0.680537   -0.0402226    0.230334 
  0.0655994  -0.286702     0.365655    0.58207      0.364077   -0.0733138   -0.000328336  -0.11327       0.295374     0.0794946   0.319694    0.00394501  -0.0762799   -0.239204      0.831637    0.647602    0.0785229   -0.144489    0.314584   -0.0896621   -0.43776     0.378651    -0.0586392  -0.330436   -0.261315     0.702858 
 -0.393029   -0.398779     0.260162   -0.107411    -0.645567   -0.809341     0.47832       0.291173     -0.599804    -0.415052    0.459798    0.298743    -0.146594     0.366785     -0.226237    0.70763    -0.264287    -0.623694    0.552872    0.147204     0.258781    0.523594    -0.265333    0.250744   -0.514514     0.0522934
 -0.390319    0.11099      0.279171    0.14214      0.12019    -0.1409       0.270421     -0.0769475    -0.297282    -0.234391    0.451228    0.27611     -0.758291    -0.393235     -0.642133    0.481928   -0.390921     0.226163   -0.0941218  -0.19336      0.166554   -0.154087     0.218619    0.346681    0.146171    -0.276145 
  0.213672   -0.265433     0.567558   -0.156721    -0.0809484   0.241665     0.584638     -0.447243     -0.252363     0.146931    0.289637    0.496734    -0.153122    -0.169226      0.224025   -0.199785   -0.0453       0.0814166   0.213617   -0.262203     0.280774    0.169902     0.0568981  -0.501962    0.296427    -0.657533 
  0.316988    0.424761     0.362484   -0.120762    -0.326373    0.366476     0.450077     -0.239823     -0.221558    -0.293366    0.134994    1.24933      0.156502    -0.350957     -0.0530236  -0.296974    0.29546     -0.105196   -0.157967   -0.132716    -0.16932     0.240164     0.295248   -0.479789   -0.232875     0.453912 
  0.0976319  -0.255796    -0.150675    0.625412    -0.203371   -0.696517     0.32533      -0.237208     -0.226735     0.963164   -0.0883074  -0.285944     0.200835     0.0937505     0.0203962   0.053691   -0.0432509   -0.941916    0.169592    0.603665     0.576121   -0.340888     0.491226   -0.237938   -0.0441774    0.133981 
 -0.107523   -0.376376    -0.627788    0.161211     0.402667   -0.517611    -0.0311993    -0.000527793   0.191877     0.492851   -0.15224    -1.00461      0.0924283    0.561418     -0.208755    0.191506   -0.319001     0.0910411   0.245846    0.00868695  -0.0435952  -0.264782    -0.346396   -0.0322436   0.436764    -0.418994 
 -0.22785     0.04596     -0.727846   -0.0404669    0.101456    0.0272453    0.725428      0.362583      0.0666407   -0.110334    0.0911006  -0.0923228   -0.0795831   -0.478455      0.0349281  -0.222271   -0.334991    -0.0623861   0.453959   -0.0917595   -0.312967    0.422692     0.129741   -0.273097    0.634244    -0.349547 
  0.298406    0.0723913   -0.441168    0.257817     0.712311    0.314916     0.207518     -0.415137     -0.0439807    0.762594    0.0317077  -0.407651    -0.131155    -0.362466      0.145764   -0.204372    0.210713     0.491926   -0.123143    0.134391    -0.196071    0.133505     0.101436   -0.540601    0.552831    -0.0850349
  0.0918225   0.556443    -0.188831   -0.659843     0.0124829   0.117297    -0.437917     -0.147206     -0.565496    -0.0600382   0.146186   -0.185571    -0.141327     0.0179515    -0.191421   -0.07747    -0.367042    -0.0776618  -0.693847    0.392019     0.035899   -0.11054     -0.631512    0.249776    0.0977013    0.218315 
 -0.30399     0.529146    -0.0556459  -0.662045     0.170245   -0.225627    -0.227693      0.0688605     0.286907    -0.888104    0.323317    0.41082      0.385803     0.229628     -0.218821   -0.410011    0.644923     0.563641   -0.322166   -0.359675    -0.14497    -0.322084    -0.72199     0.1968     -0.377426    -0.137578 
  0.213714    0.35785      0.308288   -0.155757    -0.347981    0.351164    -0.444963     -0.0512731    -0.40308      0.117433   -0.293961    0.189769    -0.107282     0.255836     -0.415735   -0.514275   -0.181645    -0.0570328  -0.618794    0.278456     0.323574   -0.388134     0.292456    0.0052063  -0.32129     -0.120135 
  0.434737    0.689238    -0.103426    0.0278044    0.147058    0.694256    -1.02785      -0.0210184     0.217545     0.0371189  -0.596732   -0.243113    -0.122432    -0.0296799     0.101028   -0.500461   -0.00134384   0.46947    -0.422628   -0.273912    -0.212302   -0.191289     0.37129     0.0842412   0.167653    -0.0787559
 -0.48917    -0.0845941   -0.29343    -0.19791     -0.612224   -0.00341639  -0.726129      0.958905     -0.136065    -0.0472933  -0.739735   -0.149509    -0.344319     0.427001     -0.148689   -0.128281    0.135173    -0.396907    0.0180209   0.276189    -0.299037   -0.085492    -0.19488     0.324896   -0.0733004    0.376693 
  0.513983   -0.0124781    0.0704012   0.532232    -0.284161    0.790985    -0.364944      0.290141      0.097311     0.0861498  -0.547876    0.0553308    0.318243     0.40507       0.309857   -0.0178908   0.197372    -0.375225    0.373739    0.266549    -0.558523    0.135367    -0.0150812   0.32436    -0.848426     0.314222 
 -0.446487    0.00628465  -0.657057    0.173941    -0.569998   -0.22301     -0.491776      0.00596445    0.63037     -0.21115     0.756829    0.475357    -0.0431858   -0.490504      0.255001    0.382457    0.367495     0.250416    0.290332    0.201745    -0.0246496  -0.210945     0.0113387   0.386981   -0.00773171   0.515506 
 -0.0493157   0.0275111    0.24001     0.0453423    0.510099    0.125262     0.10543       0.00160105   -0.193583    -0.152759   -0.0318246  -0.309611     0.126348    -0.29386       0.237549    0.197806    0.00575984   0.190767   -0.0641486  -0.0894256    0.0289197   0.127033    -0.235867    0.0579249   0.0335747    0.249837 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.418753
INFO: iteration 2, average log likelihood -1.418742
INFO: iteration 3, average log likelihood -1.418731
INFO: iteration 4, average log likelihood -1.418720
INFO: iteration 5, average log likelihood -1.418710
INFO: iteration 6, average log likelihood -1.418700
INFO: iteration 7, average log likelihood -1.418691
INFO: iteration 8, average log likelihood -1.418681
INFO: iteration 9, average log likelihood -1.418672
INFO: iteration 10, average log likelihood -1.418663
INFO: EM with 100000 data points 10 iterations avll -1.418663
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.206501e+05
      1       7.184334e+05      -2.022167e+05 |       32
      2       7.023838e+05      -1.604961e+04 |       32
      3       6.968546e+05      -5.529201e+03 |       32
      4       6.941976e+05      -2.656936e+03 |       32
      5       6.925189e+05      -1.678694e+03 |       32
      6       6.913629e+05      -1.156043e+03 |       32
      7       6.904846e+05      -8.782619e+02 |       32
      8       6.897426e+05      -7.420094e+02 |       32
      9       6.891445e+05      -5.981140e+02 |       32
     10       6.886677e+05      -4.768280e+02 |       32
     11       6.882654e+05      -4.022861e+02 |       32
     12       6.879441e+05      -3.212806e+02 |       32
     13       6.876694e+05      -2.747101e+02 |       32
     14       6.874120e+05      -2.574122e+02 |       32
     15       6.871801e+05      -2.319427e+02 |       32
     16       6.869712e+05      -2.088771e+02 |       32
     17       6.867884e+05      -1.827360e+02 |       32
     18       6.866075e+05      -1.809607e+02 |       32
     19       6.864421e+05      -1.653339e+02 |       32
     20       6.862912e+05      -1.509707e+02 |       32
     21       6.861697e+05      -1.214938e+02 |       32
     22       6.860641e+05      -1.056131e+02 |       32
     23       6.859694e+05      -9.467633e+01 |       32
     24       6.858854e+05      -8.399810e+01 |       32
     25       6.858093e+05      -7.606542e+01 |       32
     26       6.857380e+05      -7.130774e+01 |       32
     27       6.856744e+05      -6.359868e+01 |       32
     28       6.856145e+05      -5.990019e+01 |       32
     29       6.855501e+05      -6.443240e+01 |       32
     30       6.854840e+05      -6.603962e+01 |       32
     31       6.854176e+05      -6.644030e+01 |       32
     32       6.853466e+05      -7.098101e+01 |       32
     33       6.852801e+05      -6.654342e+01 |       32
     34       6.852136e+05      -6.651486e+01 |       32
     35       6.851591e+05      -5.451489e+01 |       32
     36       6.851064e+05      -5.270235e+01 |       32
     37       6.850602e+05      -4.616607e+01 |       32
     38       6.850232e+05      -3.693691e+01 |       32
     39       6.849911e+05      -3.210300e+01 |       32
     40       6.849621e+05      -2.902963e+01 |       32
     41       6.849306e+05      -3.150199e+01 |       32
     42       6.848976e+05      -3.298851e+01 |       32
     43       6.848669e+05      -3.076510e+01 |       32
     44       6.848376e+05      -2.923384e+01 |       32
     45       6.848124e+05      -2.525543e+01 |       32
     46       6.847869e+05      -2.549436e+01 |       32
     47       6.847593e+05      -2.760486e+01 |       32
     48       6.847375e+05      -2.180281e+01 |       32
     49       6.847203e+05      -1.713733e+01 |       32
     50       6.847016e+05      -1.869677e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 684701.6362105042)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.430369
INFO: iteration 2, average log likelihood -1.425383
INFO: iteration 3, average log likelihood -1.424091
INFO: iteration 4, average log likelihood -1.423213
INFO: iteration 5, average log likelihood -1.422308
INFO: iteration 6, average log likelihood -1.421397
INFO: iteration 7, average log likelihood -1.420670
INFO: iteration 8, average log likelihood -1.420211
INFO: iteration 9, average log likelihood -1.419949
INFO: iteration 10, average log likelihood -1.419790
INFO: iteration 11, average log likelihood -1.419680
INFO: iteration 12, average log likelihood -1.419595
INFO: iteration 13, average log likelihood -1.419526
INFO: iteration 14, average log likelihood -1.419466
INFO: iteration 15, average log likelihood -1.419413
INFO: iteration 16, average log likelihood -1.419366
INFO: iteration 17, average log likelihood -1.419324
INFO: iteration 18, average log likelihood -1.419285
INFO: iteration 19, average log likelihood -1.419249
INFO: iteration 20, average log likelihood -1.419216
INFO: iteration 21, average log likelihood -1.419185
INFO: iteration 22, average log likelihood -1.419156
INFO: iteration 23, average log likelihood -1.419128
INFO: iteration 24, average log likelihood -1.419103
INFO: iteration 25, average log likelihood -1.419078
INFO: iteration 26, average log likelihood -1.419054
INFO: iteration 27, average log likelihood -1.419032
INFO: iteration 28, average log likelihood -1.419010
INFO: iteration 29, average log likelihood -1.418989
INFO: iteration 30, average log likelihood -1.418969
INFO: iteration 31, average log likelihood -1.418950
INFO: iteration 32, average log likelihood -1.418931
INFO: iteration 33, average log likelihood -1.418913
INFO: iteration 34, average log likelihood -1.418896
INFO: iteration 35, average log likelihood -1.418879
INFO: iteration 36, average log likelihood -1.418862
INFO: iteration 37, average log likelihood -1.418846
INFO: iteration 38, average log likelihood -1.418831
INFO: iteration 39, average log likelihood -1.418815
INFO: iteration 40, average log likelihood -1.418800
INFO: iteration 41, average log likelihood -1.418786
INFO: iteration 42, average log likelihood -1.418771
INFO: iteration 43, average log likelihood -1.418757
INFO: iteration 44, average log likelihood -1.418744
INFO: iteration 45, average log likelihood -1.418730
INFO: iteration 46, average log likelihood -1.418717
INFO: iteration 47, average log likelihood -1.418704
INFO: iteration 48, average log likelihood -1.418691
INFO: iteration 49, average log likelihood -1.418679
INFO: iteration 50, average log likelihood -1.418666
INFO: EM with 100000 data points 50 iterations avll -1.418666
59.0 data points per parameter
32Ã—26 Array{Float64,2}:
  0.000376382   0.0519867   0.0383071   0.132482     0.0630706   -0.0993908   -0.0514395  -0.0753946    0.0194682   -0.0421268   0.107702    -0.0881202   -0.224154      0.177222     -0.441734    -0.0152742  -0.330176    -0.0360899   -0.156561    0.144679    -0.0545341   -0.180306     0.199288    0.134494   -0.0183149   -0.0705397
 -0.173861     -0.29524    -0.0189442   0.454279    -0.196429    -0.527164     0.104597   -0.18716     -0.173302     0.738105    0.0121187   -0.148089     0.220254      0.175083      0.038322    -0.124845    0.13061     -0.767177     0.169973    0.488807     0.613689    -0.395587     0.355191   -0.266721   -0.246274     0.11435  
  0.309446     -0.222815   -0.108278    0.469759    -0.192363    -0.0274289   -0.287897    0.0493337    0.116396     0.380876   -0.285684     0.00956777   0.495641      0.258425      0.0545201    0.207019    0.729758    -0.0416244    0.246388   -0.0474806   -0.0521927   -0.215365    -0.0319977   0.389583   -0.27955     -0.0832031
 -0.520708     -0.134614    0.538506   -0.107627     0.53393      0.204503     0.16832     0.468181    -0.139477     0.282217   -0.227341    -0.695543     0.30872       0.0617131     0.369678     0.165868   -0.133159    -0.380976    -0.0959646  -0.0342771    0.469925     0.0934803   -0.940089    0.464873    0.521219    -0.547276 
 -0.284493      0.186858    0.672211    0.508219    -0.0346265   -0.456301     0.172208    0.202261     0.436083    -0.814149   -0.0735246   -0.368034     0.255143      0.540574     -0.127934    -0.318487   -0.207688    -0.178906     0.34306    -0.621244     0.206997    -0.240906     0.330446    0.132312   -0.3279      -0.0613737
 -0.2158        0.254274    0.293642    0.0534673   -0.106858    -0.372674     0.366828   -0.150002    -0.297501    -0.15556     0.790224     0.446992    -0.310804     -0.271688     -0.273298     0.467002   -0.519279    -0.0336223    0.101565   -0.106506     0.519442    -0.0850872    0.0107616   0.395901   -0.0582269   -0.596825 
 -0.0210285     0.473331   -0.439124   -0.39442     -0.2818       0.00359146  -0.305351   -0.00910313  -0.157708     0.110572   -0.417917     0.00843533   0.223099      0.0677507    -0.70798     -0.76238     0.0879334    0.456111    -0.723697   -0.162042     0.151347    -0.619219     0.173737   -0.153105    0.322591    -0.671777 
  0.139974     -0.0217862   0.0905847   0.410419    -0.123194     0.262725     0.612425   -0.168328    -0.225559    -0.0827265  -0.3317      -0.562795    -0.753021     -0.488431     -0.217158     0.248861   -0.986389    -0.483032    -0.166077    0.0521696   -0.125401     0.464091     0.644528    0.0469773   0.409337     0.187674 
  0.141438      0.118273   -0.332933   -0.110088     0.0732723   -0.138507     0.0647878  -0.148237     0.120272     0.205169    0.0244187   -0.00436632   0.387568     -0.289446      0.441057    -0.118098    0.0660302    0.120615     0.140251   -0.0490549   -0.00204709  -0.0708015    0.200658   -0.445996    0.00170414   0.110758 
  0.775546     -0.239677    0.180164   -0.102205    -0.031376     0.256601    -0.017806   -0.169908    -0.713146     0.292981   -0.448735    -0.576843    -0.162687      0.314193     -0.169553    -0.0500613  -0.43436      0.396653     0.26398    -0.0867719    0.448086    -0.169063     0.046742   -0.213182    0.0538991   -1.08702  
 -0.355994     -0.498647    0.283642    0.157152     0.148046    -0.0041927    0.87735     0.0687886   -0.148684     0.158181   -0.226868     0.255457    -0.183282     -0.0448103    -0.330042    -0.0981103   0.131845    -0.18642      0.328417   -0.345406    -0.178596     0.230124     0.276319   -0.367848    0.582245    -0.564588 
 -0.359223      0.512246   -0.045215   -0.644376     0.161211    -0.178971    -0.334211   -0.0729258    0.223254    -0.835375    0.41413      0.499695     0.360494      0.402899     -0.231473    -0.243553    0.529969     0.337543    -0.215606   -0.515267    -0.415074    -0.188215    -0.706596    0.316128   -0.540339     0.045267 
  0.133629     -0.60121    -0.275768    0.0133972   -0.28914      0.773587    -0.127721    0.719283     0.0141488    0.084024   -0.325735     0.330307    -0.0833303     0.197606      0.522939    -0.0469565   0.248499     0.20438      0.287166    0.0673167   -0.798783     0.154382    -0.21472    -0.210068    0.0227061    0.530959 
  0.157165     -0.222536    0.335366    0.578123     0.103131    -0.14109      0.0965244   0.0433625    0.158905     0.124544    0.24009     -0.0390857   -0.130954     -0.0169057     0.668514     0.797253   -0.00850584  -0.530432     0.574493    0.0402827   -0.4839       0.528611    -0.164415    0.0375642  -0.498427     0.591919 
 -0.132531      0.175703   -1.0042      0.507565     0.310608    -0.577854    -0.180533    0.460533     0.307517     0.32294    -0.151732    -0.812001     0.161932      0.541369     -0.153851     0.0485855  -0.567188    -0.00487631   0.328325   -0.0417857   -0.342768    -0.209336     0.128055    0.251794    0.0883464    0.0286948
 -0.125466     -0.0457583   0.459139   -0.240193    -0.176269     0.379205     0.245328   -0.0618069   -0.0909872   -0.138995    0.210693     0.559486    -0.143582     -0.293801      0.214795    -0.227096    0.0505339    0.0967718    0.0721206  -0.031478     0.333826    -0.00121482   0.0348108  -0.18666     0.0588704   -0.300863 
 -0.778795      0.0717419   0.183197    0.274445     0.137205    -0.348303     0.0698701   0.35084     -0.365178    -0.180979    0.159131     0.0372961   -0.30116      -0.443104     -0.273371     0.73126     0.667209     0.375773     0.240528   -0.255971     0.182732     0.0593415    0.130064    0.265037    0.17343      0.319333 
 -0.428767      0.595585    0.550279    0.1895      -0.273769     0.522275     0.128274    0.0956762   -0.0695688   -0.478993    0.321291     1.07537      0.305468     -0.431092      0.293012    -0.605194    0.392228    -0.543017    -0.560648    0.0528549    0.125507     0.550403     0.115906   -0.196828    0.0762503    0.79325  
 -0.0660363    -0.0178337   0.0852476   0.332972     0.848361     0.198       -0.0704163  -0.617121    -0.00310369   0.642751    0.183934    -0.0770692    0.0479393    -0.245879      0.245643    -0.121434    0.34753      0.481535    -0.279608   -0.0712686   -0.312144    -0.0175096    0.379827   -0.784378    0.0841211    0.319695 
  0.396102      0.102837    0.613629    0.00303866  -0.241194     0.0901387   -0.468075   -0.0555035   -0.315189    -0.15506    -0.4732       0.330411    -0.0882496     0.34923      -0.204193     0.176669    0.240137     0.243798    -0.664399    0.265285     0.124565    -0.358829     0.160243    0.121352   -0.642463     0.667622 
  0.240261      0.67783     0.182837    0.270849    -0.16944      0.803465    -0.667898   -0.126943     0.152318     0.0297639  -0.608432    -0.108505     0.10884       0.156406      0.182695    -0.722432   -0.211944    -0.318809    -0.043673    0.139566    -0.23531      0.00777675   0.633141    0.146288   -0.711647     0.0542674
  0.0427188     0.328435    0.201346   -0.126985    -0.0489078    0.333902    -0.246358    0.0937116   -0.173461    -0.374873    0.0866963   -0.0279313   -0.243296     -0.288935      0.138794     0.159189   -0.179587     0.303177    -0.131184   -0.138741    -0.0112902    0.1425      -0.192156    0.172491    0.0623424    0.0104174
  0.0357915    -0.191777    0.150382   -0.104795    -0.664079    -0.0278495   -0.104283    0.343896    -0.289502    -0.301283   -0.210456     0.0189509   -0.576871      0.63554      -0.808378     0.0899911  -0.290663    -0.689416     0.0124854   0.163945     0.158735    -0.00722649  -0.378512    0.625384   -0.277158    -0.270594 
  0.240071      0.225886   -0.288352   -0.730565     0.0676832    0.24694     -0.329531   -0.53839     -0.689213     0.553065    0.0775661    0.101786    -0.091273     -0.000958427  -0.390274    -0.089752   -0.153232    -0.118097    -0.653995    0.559298     0.0483525   -0.113971    -0.485783    0.0806379   0.172187     0.223206 
  0.0885066    -0.960633   -0.46587    -0.200139     0.00343397  -0.582641     0.312408   -0.0293766    0.0544415    0.274167    0.307949    -0.342038     0.0785143     0.423641     -0.27458      0.871539    0.132628     0.484627     0.251592    0.185223     0.237704    -0.314137    -0.521823   -0.290104    0.298745    -0.0715692
  0.0762434     0.164569   -0.215108   -0.286426     0.0128086   -0.392104    -0.241128   -0.0723654    0.289788    -0.203075    0.0574027   -0.319078     0.661796     -0.420324      0.922359    -0.460438    0.317179     0.577938     0.11854     0.142747     0.606524    -0.453039    -0.145686   -0.163691   -0.163973     0.254192 
 -0.656224     -0.176377   -0.591024    0.138593    -0.461022    -0.39013     -0.143473   -0.390986     0.682554    -0.0712408   0.566303     0.548495    -0.000677727  -0.655914      0.0379632    0.324439    0.421858     0.199149     0.205963    0.0830671   -0.323228    -0.480501     0.462391   -0.109249   -0.0485033    0.563042 
 -0.228676      0.321022   -0.408928   -0.146076    -0.257997     0.07685     -0.815602    0.407314     0.101727    -0.238969    0.0566406   -0.337333    -0.077265     -0.194892      0.164175     0.110241   -0.0398834   -0.10991     -0.278383    0.519199     0.152981    -0.17699     -0.387856    0.174683   -0.0977483    0.569759 
  0.0251065     0.123002    0.0374259  -0.059332    -0.0231843    0.125951     0.0219268   0.211904    -0.347228    -0.105744   -0.186134    -0.124884    -0.0540586     0.0429606    -0.00271883   0.0491369  -0.0411204   -0.00255546   0.0320003  -0.107631    -0.0811634    0.217522    -0.481926    0.102649    0.12357     -0.140762 
 -0.256449     -0.374311   -0.27324    -0.267983    -0.309272    -0.139933     0.224633    0.0823073    0.00440155   0.295289   -0.535182     0.212662     0.0840996     0.148864     -0.232084    -0.020719    0.183564    -0.239984    -0.128665    0.333347    -0.351819     0.176015     0.0144869  -0.377944   -0.140588     0.195417 
  0.41318       0.513355   -0.568517    0.257799     0.815833     0.381824    -0.0784752  -0.0669809    0.405012     0.312813    0.00608575  -0.58065     -0.46149      -0.44989      -0.0910803   -0.202092   -0.117824     0.507586     0.0770377  -0.00820327  -0.302339     0.160102     0.023311    0.0399497   0.71032     -0.247309 
  0.911178      0.0802807   0.133049   -0.0881726    0.0690765    0.107457     0.750234   -0.717086    -0.120321    -0.0202422   0.153348     0.537043     0.325148     -0.0965418     0.0619577   -0.215995   -0.0581413   -0.0414389    0.197547   -0.182825    -0.206047     0.342771     0.0949866  -0.749535   -0.150395    -0.133467 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.418654
INFO: iteration 2, average log likelihood -1.418642
INFO: iteration 3, average log likelihood -1.418631
INFO: iteration 4, average log likelihood -1.418619
INFO: iteration 5, average log likelihood -1.418608
INFO: iteration 6, average log likelihood -1.418597
INFO: iteration 7, average log likelihood -1.418586
INFO: iteration 8, average log likelihood -1.418576
INFO: iteration 9, average log likelihood -1.418565
INFO: iteration 10, average log likelihood -1.418555
INFO: EM with 100000 data points 10 iterations avll -1.418555
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
