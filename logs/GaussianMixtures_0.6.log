>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.7.0
INFO: Installing JLD v0.6.6
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.1
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.2.0
INFO: Installing StaticArrays v0.1.0
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1233
Commit 42df52a (2016-11-11 19:03 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-101-generic #148-Ubuntu SMP Thu Oct 20 22:08:32 UTC 2016 x86_64 x86_64
Memory: 2.939281463623047 GB (671.15625 MB free)
Uptime: 24437.0 sec
Load Avg:  1.0341796875  0.990234375  1.029296875
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3499 MHz    1470583 s       6415 s     157588 s     561712 s         53 s
#2  3499 MHz     682109 s         85 s      80734 s    1595660 s          1 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.3
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.7.0
 - JLD                           0.6.6
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.1
 - Rmath                         0.1.4
 - SHA                           0.2.1
 - ScikitLearnBase               0.2.0
 - StaticArrays                  0.1.0
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-1.1368847986946348e6,[37396.8,62603.2],
[-6109.66 -20932.0 -30429.1; 6425.62 20231.4 30714.1],

Array{Float64,2}[
[33395.5 -992.069 3421.39; -992.069 39668.4 6753.73; 3421.39 6753.73 44083.9],

[66679.3 1192.93 -3803.38; 1192.93 60491.0 -6660.95; -3803.38 -6660.95 55420.9]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.948791e+03
      1       1.156133e+03      -7.926575e+02 |        6
      2       9.901460e+02      -1.659870e+02 |        4
      3       9.238751e+02      -6.627092e+01 |        2
      4       9.046360e+02      -1.923905e+01 |        0
      5       9.046360e+02       0.000000e+00 |        0
K-means converged with 5 iterations (objv = 904.6360291040346)
INFO: K-means with 272 data points using 5 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.073809
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.859617
INFO: iteration 2, lowerbound -3.737622
INFO: iteration 3, lowerbound -3.582911
INFO: iteration 4, lowerbound -3.376470
INFO: iteration 5, lowerbound -3.142081
INFO: iteration 6, lowerbound -2.916885
INFO: dropping number of Gaussions to 7
INFO: iteration 7, lowerbound -2.719292
INFO: iteration 8, lowerbound -2.577184
INFO: dropping number of Gaussions to 6
INFO: iteration 9, lowerbound -2.498158
INFO: dropping number of Gaussions to 3
INFO: iteration 10, lowerbound -2.425673
INFO: iteration 11, lowerbound -2.370720
INFO: iteration 12, lowerbound -2.338085
INFO: iteration 13, lowerbound -2.316202
INFO: iteration 14, lowerbound -2.307491
INFO: dropping number of Gaussions to 2
INFO: iteration 15, lowerbound -2.302973
INFO: iteration 16, lowerbound -2.299262
INFO: iteration 17, lowerbound -2.299257
INFO: iteration 18, lowerbound -2.299255
INFO: iteration 19, lowerbound -2.299254
INFO: iteration 20, lowerbound -2.299253
INFO: iteration 21, lowerbound -2.299253
INFO: iteration 22, lowerbound -2.299253
INFO: iteration 23, lowerbound -2.299253
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Sun 13 Nov 2016 12:17:14 PM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Sun 13 Nov 2016 12:17:16 PM UTC: K-means with 272 data points using 5 iterations
11.3 data points per parameter
,Sun 13 Nov 2016 12:17:17 PM UTC: EM with 272 data points 0 iterations avll -2.073809
5.8 data points per parameter
,Sun 13 Nov 2016 12:17:18 PM UTC: GMM converted to Variational GMM
,Sun 13 Nov 2016 12:17:19 PM UTC: iteration 1, lowerbound -3.859617
,Sun 13 Nov 2016 12:17:19 PM UTC: iteration 2, lowerbound -3.737622
,Sun 13 Nov 2016 12:17:20 PM UTC: iteration 3, lowerbound -3.582911
,Sun 13 Nov 2016 12:17:20 PM UTC: iteration 4, lowerbound -3.376470
,Sun 13 Nov 2016 12:17:20 PM UTC: iteration 5, lowerbound -3.142081
,Sun 13 Nov 2016 12:17:20 PM UTC: iteration 6, lowerbound -2.916885
,Sun 13 Nov 2016 12:17:20 PM UTC: dropping number of Gaussions to 7
,Sun 13 Nov 2016 12:17:20 PM UTC: iteration 7, lowerbound -2.719292
,Sun 13 Nov 2016 12:17:20 PM UTC: iteration 8, lowerbound -2.577184
,Sun 13 Nov 2016 12:17:20 PM UTC: dropping number of Gaussions to 6
,Sun 13 Nov 2016 12:17:20 PM UTC: iteration 9, lowerbound -2.498158
,Sun 13 Nov 2016 12:17:20 PM UTC: dropping number of Gaussions to 3
,Sun 13 Nov 2016 12:17:20 PM UTC: iteration 10, lowerbound -2.425673
,Sun 13 Nov 2016 12:17:20 PM UTC: iteration 11, lowerbound -2.370720
,Sun 13 Nov 2016 12:17:20 PM UTC: iteration 12, lowerbound -2.338085
,Sun 13 Nov 2016 12:17:20 PM UTC: iteration 13, lowerbound -2.316202
,Sun 13 Nov 2016 12:17:21 PM UTC: iteration 14, lowerbound -2.307491
,Sun 13 Nov 2016 12:17:21 PM UTC: dropping number of Gaussions to 2
,Sun 13 Nov 2016 12:17:21 PM UTC: iteration 15, lowerbound -2.302973
,Sun 13 Nov 2016 12:17:21 PM UTC: iteration 16, lowerbound -2.299262
,Sun 13 Nov 2016 12:17:21 PM UTC: iteration 17, lowerbound -2.299257
,Sun 13 Nov 2016 12:17:21 PM UTC: iteration 18, lowerbound -2.299255
,Sun 13 Nov 2016 12:17:21 PM UTC: iteration 19, lowerbound -2.299254
,Sun 13 Nov 2016 12:17:21 PM UTC: iteration 20, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:21 PM UTC: iteration 21, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:21 PM UTC: iteration 22, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:21 PM UTC: iteration 23, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:21 PM UTC: iteration 24, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:21 PM UTC: iteration 25, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:21 PM UTC: iteration 26, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:21 PM UTC: iteration 27, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:22 PM UTC: iteration 28, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:22 PM UTC: iteration 29, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:22 PM UTC: iteration 30, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:22 PM UTC: iteration 31, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:22 PM UTC: iteration 32, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:22 PM UTC: iteration 33, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:22 PM UTC: iteration 34, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:22 PM UTC: iteration 35, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:22 PM UTC: iteration 36, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:22 PM UTC: iteration 37, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:22 PM UTC: iteration 38, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:22 PM UTC: iteration 39, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:22 PM UTC: iteration 40, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:22 PM UTC: iteration 41, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:22 PM UTC: iteration 42, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:22 PM UTC: iteration 43, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:22 PM UTC: iteration 44, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:23 PM UTC: iteration 45, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:23 PM UTC: iteration 46, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:23 PM UTC: iteration 47, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:23 PM UTC: iteration 48, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:23 PM UTC: iteration 49, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:23 PM UTC: iteration 50, lowerbound -2.299253
,Sun 13 Nov 2016 12:17:23 PM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.9549,178.045]
β = [95.9549,178.045]
m = [2.00023 53.852; 4.2503 79.2869]
ν = [97.9549,180.045]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.375876 -0.00895312; 0.0 0.0127487],

[0.184042 -0.00764405; 0.0 0.00858171]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.00000000004
avll from stats: -0.9968824921122509
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.996882492112251
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.996882492112251
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 99999.99999999999
avll from stats: -0.9722444517336037
avll from llpg:  -0.9722444517336037
avll direct:     -0.9722444517336037
sum posterior: 100000.0
32×26 Array{Float64,2}:
 -0.0878379   0.103697     0.0144947    0.0743257    -0.0416432    0.0219548    0.0287299    0.0322658    0.173127     0.120256     0.0427856    0.0545236   -0.152566     0.0338747    0.156905    -0.00322904  -0.0630352  -0.191289    -0.0131841    0.0309177    -0.101901      0.135548      0.0148351   -0.129266     0.0259034   -0.131407  
  0.0273407   0.00515893   0.0573073    0.000588042  -0.161876     0.0259124    0.0362971   -0.0578531   -0.0784091   -0.00506709  -0.0869691    0.0177625   -0.0760645   -0.116876     0.154785    -0.12677     -0.0640525   0.154158     0.0199707   -0.0710663     0.101309      0.0903611     0.113227     0.0110434    0.106691    -0.0540154 
 -0.140933    0.0803279   -0.234263     0.0662347    -0.00313386   0.163487     0.0105169   -0.124094     0.0571102   -0.0194323    0.0235929    0.0601191   -0.110436    -0.128242    -0.0419862   -0.0731303   -0.0651024  -0.0843759   -0.251778     0.0741149     0.0311942    -0.106006     -0.134874     0.0563413    0.0485629    0.0503081 
  0.0965577   0.115525     0.0167458    0.16678       0.0839574   -0.0451086   -0.0216307   -0.187895     0.111464    -0.154928    -0.077317     0.138746     0.16024      0.0485902    0.0411164    0.165484    -0.0693652  -0.0356877    0.168422     0.00939527   -0.109617     -0.0064007     0.0233779   -0.0452897   -0.0030241   -0.0562465 
  0.0158792   0.185085     0.102238    -0.235326      0.135837    -0.0942477   -0.00919194  -0.027868     0.0174252   -0.135887    -0.146338     0.029791    -0.126715     0.0168327    0.179041    -0.0425786    0.0914661   0.0640482   -0.191424    -0.0273549    -0.263452      0.000303041  -0.0375918    0.180554     0.0823413    0.0368345 
 -0.1073      0.0310387   -0.0426048    0.0182072    -0.0585749    0.15285     -0.0668597   -0.00244222   0.0143833   -0.0730415    0.114094    -0.0508471   -0.192185     0.0465479    0.0665877   -0.0338395    0.0992858  -0.181917     0.00253548   0.0626131     0.0684545    -0.0848824     0.00315531   0.0309355   -0.0260405   -0.0704602 
 -0.0173005   0.0413635   -0.164557    -0.0140185    -0.174277    -0.1294      -0.00243998   0.0887101    0.103269     0.0406442    0.139845     0.0653025    0.0389476   -0.00442786  -0.00264501  -0.0775414   -0.0624363   0.011242     0.243945    -0.2499        0.118319     -0.134674     -0.0204048   -0.00570701   0.153104    -0.0642969 
  0.118553    0.183204    -0.117306     0.0417637    -0.0408271   -0.0589566   -0.0377513    0.0553819   -0.117978     0.053227    -0.107556    -0.0315494    0.155752    -0.106351     0.0648834   -0.0152498    0.112586   -0.0977779    0.100868    -0.198106     -0.0384989    -0.0501058     0.0788615   -0.00554783  -0.0677559    0.0450846 
  0.122357    0.0980878    0.137221     0.0383958     0.151961    -0.0754198    0.0769084    0.0822653    0.0132061   -0.115753    -0.0708718   -0.0637709   -0.0305074   -0.0747778   -0.161705    -0.0721356    0.0859355   0.0869043   -0.023546    -0.144333      0.0175512    -0.0360022    -0.0715725    0.221918    -0.145736    -0.233913  
 -0.029368    0.0930814   -0.0280371   -0.0653015    -0.0736142   -0.0761617    0.0153547   -0.231259    -0.0776919    0.0746293   -0.0353256    0.0546317    0.108936    -0.0163132    0.00709907  -0.120836     0.094831    0.0785257    0.123159     0.0137185    -0.123557     -0.0769757     0.172579     0.145661    -0.00903422  -0.0224193 
 -0.100146   -0.133064     0.00283334   0.146275     -0.00396281  -0.041237     0.114583    -0.193737    -0.00825557  -0.0785706    0.111955    -0.0706213   -0.109421     0.00406474  -0.148021     0.00852877   0.0897028  -0.0776929   -0.0668965    0.0316515    -0.0563978     0.0282009    -0.100964     0.0319811    0.151072     0.0129846 
  0.0868625  -0.0900207    0.0730794    0.0237565    -0.0792168   -0.222222    -0.041223    -0.00538893  -0.0777357   -0.068669     0.10708      0.224604     0.0913553   -0.126367    -0.0269152    0.0367023    0.14014     0.00450729  -0.00903336  -0.0865136    -0.134978     -0.0305045    -0.0533547    0.0270768    0.0457018    0.110227  
  0.085302    0.023491     0.133988     0.0489865    -0.200733     0.101467    -0.0587792    0.0520931    0.147415    -0.0200541   -0.0310202    0.0276853   -0.0739658   -0.0986908    0.0185565   -0.0408595   -0.0440132  -0.028249     0.0247455   -0.051668      0.000634779  -0.0568322     0.15921      0.132892    -0.107201     0.013103  
  0.178872    0.0232167   -0.0627881   -0.0935929     0.0814246    0.0770042    0.121117     0.0200488    0.161625     0.0294131   -0.0968327   -0.00777739  -0.0143469    0.0351396    0.0582025   -0.030503     0.0475639   0.0150539    0.190202     0.082506      0.081754     -0.0270226     0.0461067   -0.188413    -0.149818     0.0326986 
  0.114886   -0.0260245   -0.0649993    0.0718831    -0.139106     0.0748272    0.0476979    0.0639472   -0.0900746   -0.0605141    0.0713181   -0.0325476   -0.138824     0.0148394    0.143104    -0.0409522    0.0585769  -0.00444635   0.0226515   -0.210637     -0.020785      0.0561008    -0.165844    -0.170979    -0.081851     0.016951  
  0.0199356   0.0614585    0.0581676   -0.0406506    -0.0667002   -0.151414    -0.0707288    0.243314    -0.00869409   0.115848    -0.10032     -0.0482052    0.157638     0.0620845    0.0287879    0.185651    -0.0873931  -0.0358739   -0.0697743    0.140828      0.0829303     0.0914625    -0.00243757  -0.0011484    0.0867861   -0.0328983 
  0.0182875  -0.164651    -0.0626289   -0.0575137     0.171876    -0.0771522    0.0781561   -0.0644084    0.261527    -0.269338     0.115346    -0.117651     0.0453123   -0.135587     0.0179013   -0.0190278   -0.0379932   0.153342     0.0523564    0.0362606     0.11332      -0.0697842     0.0561231    0.0828117    0.146967     0.0318636 
 -0.0863647  -0.157226    -0.0657746    0.0659266    -0.119616    -0.0690489    0.114581    -0.0394203   -0.0316087   -0.0287931   -0.00269951   0.0916846   -0.00589954  -0.0161736   -0.0792666    0.128598     0.238184   -0.110009     0.13573      0.0723263     0.0563174     0.0194972    -0.124846    -0.0676872   -0.141942    -0.134219  
 -0.105948   -0.0988547    0.0467071   -0.0929156    -0.144367    -0.00105059  -0.0477781   -0.0035263   -0.105921     0.0767263   -0.0291449    0.228154     0.020812     0.0368368   -0.0793581   -0.112201    -0.0119969   0.0765813   -0.134272     0.0481932     0.171699     -0.10815       0.0578317   -0.189249     0.0640541    0.00767927
  0.0729353   0.0389917   -0.370084     0.111974      0.1272       0.0218042   -0.0209723    0.063642     0.161908     0.0881962   -0.0607973   -0.033123    -0.0786329    0.0284328   -0.0235914   -0.0562678    0.0485166   0.00665018   0.0146965    0.174127     -0.138744     -0.0173312    -0.0510079    0.0441703   -0.0407218   -0.0546696 
  0.0144341  -0.022219    -0.0346964    0.0033954    -0.0544435    0.0153297    0.119179     0.0965249    0.0788589    0.0892887    0.00244821   0.248711     0.0204695   -0.0380257   -0.0343422   -0.0103505   -0.017557   -0.0722702   -0.0742441   -0.0366879     0.025175     -0.111328      0.0242595   -0.0483387   -0.214503    -0.0919219 
 -0.0504818  -0.0807553   -0.0215763    0.0287418     0.0293443    0.0347887   -0.316133     0.00545403  -0.036645     0.0691307   -0.108238    -0.0468569    0.103129     0.133084     0.148793     0.0800772   -0.0135366   0.00654664  -0.0662501   -0.0780133     0.0484678    -0.041394     -0.0618983    0.0633823    0.11405     -0.172892  
  0.0713279  -0.0993337    0.0127819    0.0316232    -0.139707    -0.0786756   -0.114302     0.0341212    0.0402188    0.217205     0.0679083   -0.00627121  -0.0315053   -0.0749225   -0.0536788    0.069291     0.167911    0.0803508   -0.0825045   -0.0069285     0.111997      0.0297253     0.0353474    0.180378    -0.0752394    0.0289935 
  0.0371411   0.010575    -0.0932485    0.0657607    -0.197138     0.13335      0.20316     -0.0997123   -0.0391155   -0.214678     0.0451758    0.0787546   -0.0896964   -0.168335     0.160303    -0.0291949   -0.0695861  -0.106041    -0.0662221   -0.000292461  -0.0317105    -0.00117442    0.128226     0.0312304    0.0217216    0.232132  
 -0.0306082  -0.081535    -0.0607206   -0.0683845     0.0979856   -0.066379    -0.0860994    0.0751148   -0.0619762   -0.151678     0.0113721   -0.158636     0.0668964    0.14601     -0.0303675   -0.0828788   -0.0276238   0.142838    -0.144452    -0.00530251    0.0192845    -0.228662      0.0859376    0.0769769    0.0770743   -0.0607065 
 -0.0311503  -0.0912354   -0.120291    -0.0117246     0.172195     0.0150066   -0.072644    -0.0489341    0.0162554   -0.076835    -0.141387     0.0733733   -0.0355968   -0.042655    -0.0335721   -0.0321769    0.0671247  -0.0171385   -0.0905434   -0.00539165    0.0147575    -0.0503987    -0.0862302    0.0880893    0.0139764   -0.0108689 
  0.0125054  -0.183628    -0.138684     0.0504494    -0.139042    -0.0200992   -0.00646333   0.0675899   -0.167297     0.0859653    0.0448525   -0.0264964   -0.0762544    0.144603    -0.0301397   -0.0190863   -0.0274689  -0.0695626   -0.0684809   -0.100529     -0.179582      0.0117586     0.0807877   -0.0873466    0.11465     -0.0272515 
 -0.0624661   0.155186    -0.0107379    0.0983717     0.0789661   -0.147996    -0.0530351    0.121426     0.0313658   -0.147381     0.117571    -0.135224    -0.0243713    0.0219173   -0.0870793    0.12135     -0.0465412   0.134987    -0.0555918   -0.111845     -0.113647     -0.0706009    -0.0147293    0.193994     0.0424384   -0.0537143 
 -0.113961    0.080514    -0.126348     0.0430564     0.278911     0.143454     0.200361     0.00366994   0.138841     0.00497223   0.037613    -0.0524423   -0.122532     0.0913389    0.0830823   -0.0666175    0.0368896  -0.0344087   -0.113481    -0.108078      0.0207711    -0.218205      0.0452445   -0.0120205   -0.0139422    0.113616  
  0.031668   -0.0499443    0.117557     0.0156318    -0.149543    -0.00048537   0.0195948   -0.101357     0.03422      0.0162879   -0.225177     0.152602    -0.106801    -0.0194447    0.161944     0.141476    -0.101011    0.0270835   -0.0673213   -0.0798737     0.0785542     0.158052     -0.0498221   -0.0187724   -0.0692263    0.0867328 
  0.0867954  -0.100849     0.0201291   -0.141182     -0.154698     0.130184     0.0283085    0.0255856    0.156949    -0.0242774   -0.108593    -0.0160332    0.0364586   -0.075409     0.0542482   -0.0597073   -0.171261   -0.0277947    0.0527211   -0.235448      0.0876258    -0.00567021    0.164573    -0.244224     0.086786     0.0735578 
  0.108807    0.0720331   -0.00126865  -0.13819      -0.0871997   -0.0508811   -0.211553     0.170383    -0.0786901    0.0289321    0.172061     0.137044     0.0595251    0.0313773   -0.206867     0.0819484   -0.0113166   0.0564731   -0.00365014  -0.0623952    -0.0772883    -0.00435125    0.0165495    0.11366      0.14808     -0.00692377kind diag, method split
0: avll = -1.3902698003223217
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.390430
INFO: iteration 2, average log likelihood -1.390275
INFO: iteration 3, average log likelihood -1.388917
INFO: iteration 4, average log likelihood -1.378287
INFO: iteration 5, average log likelihood -1.359489
INFO: iteration 6, average log likelihood -1.353594
INFO: iteration 7, average log likelihood -1.352502
INFO: iteration 8, average log likelihood -1.351996
INFO: iteration 9, average log likelihood -1.351702
INFO: iteration 10, average log likelihood -1.351520
INFO: iteration 11, average log likelihood -1.351408
INFO: iteration 12, average log likelihood -1.351328
INFO: iteration 13, average log likelihood -1.351242
INFO: iteration 14, average log likelihood -1.351095
INFO: iteration 15, average log likelihood -1.350846
INFO: iteration 16, average log likelihood -1.350548
INFO: iteration 17, average log likelihood -1.350295
INFO: iteration 18, average log likelihood -1.350109
INFO: iteration 19, average log likelihood -1.349964
INFO: iteration 20, average log likelihood -1.349828
INFO: iteration 21, average log likelihood -1.349681
INFO: iteration 22, average log likelihood -1.349521
INFO: iteration 23, average log likelihood -1.349347
INFO: iteration 24, average log likelihood -1.349177
INFO: iteration 25, average log likelihood -1.349034
INFO: iteration 26, average log likelihood -1.348919
INFO: iteration 27, average log likelihood -1.348821
INFO: iteration 28, average log likelihood -1.348732
INFO: iteration 29, average log likelihood -1.348652
INFO: iteration 30, average log likelihood -1.348582
INFO: iteration 31, average log likelihood -1.348523
INFO: iteration 32, average log likelihood -1.348476
INFO: iteration 33, average log likelihood -1.348439
INFO: iteration 34, average log likelihood -1.348412
INFO: iteration 35, average log likelihood -1.348393
INFO: iteration 36, average log likelihood -1.348380
INFO: iteration 37, average log likelihood -1.348371
INFO: iteration 38, average log likelihood -1.348364
INFO: iteration 39, average log likelihood -1.348360
INFO: iteration 40, average log likelihood -1.348357
INFO: iteration 41, average log likelihood -1.348354
INFO: iteration 42, average log likelihood -1.348352
INFO: iteration 43, average log likelihood -1.348351
INFO: iteration 44, average log likelihood -1.348350
INFO: iteration 45, average log likelihood -1.348349
INFO: iteration 46, average log likelihood -1.348349
INFO: iteration 47, average log likelihood -1.348348
INFO: iteration 48, average log likelihood -1.348348
INFO: iteration 49, average log likelihood -1.348348
INFO: iteration 50, average log likelihood -1.348347
INFO: EM with 100000 data points 50 iterations avll -1.348347
952.4 data points per parameter
1: avll = [-1.39043,-1.39028,-1.38892,-1.37829,-1.35949,-1.35359,-1.3525,-1.352,-1.3517,-1.35152,-1.35141,-1.35133,-1.35124,-1.3511,-1.35085,-1.35055,-1.3503,-1.35011,-1.34996,-1.34983,-1.34968,-1.34952,-1.34935,-1.34918,-1.34903,-1.34892,-1.34882,-1.34873,-1.34865,-1.34858,-1.34852,-1.34848,-1.34844,-1.34841,-1.34839,-1.34838,-1.34837,-1.34836,-1.34836,-1.34836,-1.34835,-1.34835,-1.34835,-1.34835,-1.34835,-1.34835,-1.34835,-1.34835,-1.34835,-1.34835]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.348516
INFO: iteration 2, average log likelihood -1.348366
INFO: iteration 3, average log likelihood -1.347900
INFO: iteration 4, average log likelihood -1.344494
INFO: iteration 5, average log likelihood -1.334067
INFO: iteration 6, average log likelihood -1.321902
INFO: iteration 7, average log likelihood -1.316431
INFO: iteration 8, average log likelihood -1.314333
INFO: iteration 9, average log likelihood -1.313136
INFO: iteration 10, average log likelihood -1.312274
INFO: iteration 11, average log likelihood -1.311530
INFO: iteration 12, average log likelihood -1.310799
INFO: iteration 13, average log likelihood -1.310063
INFO: iteration 14, average log likelihood -1.309340
INFO: iteration 15, average log likelihood -1.308658
INFO: iteration 16, average log likelihood -1.308023
INFO: iteration 17, average log likelihood -1.307394
INFO: iteration 18, average log likelihood -1.306806
INFO: iteration 19, average log likelihood -1.306426
INFO: iteration 20, average log likelihood -1.306118
INFO: iteration 21, average log likelihood -1.305812
INFO: iteration 22, average log likelihood -1.305508
INFO: iteration 23, average log likelihood -1.305232
INFO: iteration 24, average log likelihood -1.304974
INFO: iteration 25, average log likelihood -1.304726
INFO: iteration 26, average log likelihood -1.304467
INFO: iteration 27, average log likelihood -1.304185
INFO: iteration 28, average log likelihood -1.303942
INFO: iteration 29, average log likelihood -1.303782
INFO: iteration 30, average log likelihood -1.303681
INFO: iteration 31, average log likelihood -1.303606
INFO: iteration 32, average log likelihood -1.303539
INFO: iteration 33, average log likelihood -1.303465
INFO: iteration 34, average log likelihood -1.303363
INFO: iteration 35, average log likelihood -1.303222
INFO: iteration 36, average log likelihood -1.303115
INFO: iteration 37, average log likelihood -1.303067
INFO: iteration 38, average log likelihood -1.303046
INFO: iteration 39, average log likelihood -1.303034
INFO: iteration 40, average log likelihood -1.303026
INFO: iteration 41, average log likelihood -1.303021
INFO: iteration 42, average log likelihood -1.303017
INFO: iteration 43, average log likelihood -1.303015
INFO: iteration 44, average log likelihood -1.303013
INFO: iteration 45, average log likelihood -1.303011
INFO: iteration 46, average log likelihood -1.303010
INFO: iteration 47, average log likelihood -1.303009
INFO: iteration 48, average log likelihood -1.303008
INFO: iteration 49, average log likelihood -1.303008
INFO: iteration 50, average log likelihood -1.303007
INFO: EM with 100000 data points 50 iterations avll -1.303007
473.9 data points per parameter
2: avll = [-1.34852,-1.34837,-1.3479,-1.34449,-1.33407,-1.3219,-1.31643,-1.31433,-1.31314,-1.31227,-1.31153,-1.3108,-1.31006,-1.30934,-1.30866,-1.30802,-1.30739,-1.30681,-1.30643,-1.30612,-1.30581,-1.30551,-1.30523,-1.30497,-1.30473,-1.30447,-1.30419,-1.30394,-1.30378,-1.30368,-1.30361,-1.30354,-1.30346,-1.30336,-1.30322,-1.30312,-1.30307,-1.30305,-1.30303,-1.30303,-1.30302,-1.30302,-1.30301,-1.30301,-1.30301,-1.30301,-1.30301,-1.30301,-1.30301,-1.30301]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.303221
INFO: iteration 2, average log likelihood -1.303025
INFO: iteration 3, average log likelihood -1.302236
INFO: iteration 4, average log likelihood -1.294935
INFO: iteration 5, average log likelihood -1.276114
INFO: iteration 6, average log likelihood -1.259863
INFO: iteration 7, average log likelihood -1.251151
INFO: iteration 8, average log likelihood -1.246648
INFO: iteration 9, average log likelihood -1.243907
INFO: iteration 10, average log likelihood -1.242097
INFO: iteration 11, average log likelihood -1.240968
INFO: iteration 12, average log likelihood -1.240277
INFO: iteration 13, average log likelihood -1.239827
INFO: iteration 14, average log likelihood -1.239512
INFO: iteration 15, average log likelihood -1.239292
INFO: iteration 16, average log likelihood -1.239147
INFO: iteration 17, average log likelihood -1.239053
INFO: iteration 18, average log likelihood -1.238989
INFO: iteration 19, average log likelihood -1.238940
INFO: iteration 20, average log likelihood -1.238898
INFO: iteration 21, average log likelihood -1.238858
INFO: iteration 22, average log likelihood -1.238818
INFO: iteration 23, average log likelihood -1.238776
INFO: iteration 24, average log likelihood -1.238727
INFO: iteration 25, average log likelihood -1.238664
INFO: iteration 26, average log likelihood -1.238576
INFO: iteration 27, average log likelihood -1.238432
INFO: iteration 28, average log likelihood -1.238167
INFO: iteration 29, average log likelihood -1.237638
INFO: iteration 30, average log likelihood -1.236575
WARNING: Variances had to be floored 1
INFO: iteration 31, average log likelihood -1.234680
INFO: iteration 32, average log likelihood -1.250044
INFO: iteration 33, average log likelihood -1.243479
INFO: iteration 34, average log likelihood -1.240414
INFO: iteration 35, average log likelihood -1.239028
INFO: iteration 36, average log likelihood -1.238451
INFO: iteration 37, average log likelihood -1.237997
INFO: iteration 38, average log likelihood -1.237289
INFO: iteration 39, average log likelihood -1.236000
WARNING: Variances had to be floored 1
INFO: iteration 40, average log likelihood -1.233750
INFO: iteration 41, average log likelihood -1.249468
INFO: iteration 42, average log likelihood -1.243214
INFO: iteration 43, average log likelihood -1.240249
INFO: iteration 44, average log likelihood -1.238505
INFO: iteration 45, average log likelihood -1.237264
INFO: iteration 46, average log likelihood -1.235687
WARNING: Variances had to be floored 1
INFO: iteration 47, average log likelihood -1.233335
INFO: iteration 48, average log likelihood -1.249187
INFO: iteration 49, average log likelihood -1.243026
INFO: iteration 50, average log likelihood -1.240145
INFO: EM with 100000 data points 50 iterations avll -1.240145
236.4 data points per parameter
3: avll = [-1.30322,-1.30302,-1.30224,-1.29493,-1.27611,-1.25986,-1.25115,-1.24665,-1.24391,-1.2421,-1.24097,-1.24028,-1.23983,-1.23951,-1.23929,-1.23915,-1.23905,-1.23899,-1.23894,-1.2389,-1.23886,-1.23882,-1.23878,-1.23873,-1.23866,-1.23858,-1.23843,-1.23817,-1.23764,-1.23658,-1.23468,-1.25004,-1.24348,-1.24041,-1.23903,-1.23845,-1.238,-1.23729,-1.236,-1.23375,-1.24947,-1.24321,-1.24025,-1.2385,-1.23726,-1.23569,-1.23333,-1.24919,-1.24303,-1.24015]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.238662
INFO: iteration 2, average log likelihood -1.237013
INFO: iteration 3, average log likelihood -1.234416
WARNING: Variances had to be floored 1 2
INFO: iteration 4, average log likelihood -1.222385
INFO: iteration 5, average log likelihood -1.203252
WARNING: Variances had to be floored 14
INFO: iteration 6, average log likelihood -1.177166
WARNING: Variances had to be floored 1 2
INFO: iteration 7, average log likelihood -1.171866
INFO: iteration 8, average log likelihood -1.164760
WARNING: Variances had to be floored 14
INFO: iteration 9, average log likelihood -1.155274
WARNING: Variances had to be floored 1 2
INFO: iteration 10, average log likelihood -1.159146
INFO: iteration 11, average log likelihood -1.157401
WARNING: Variances had to be floored 14
INFO: iteration 12, average log likelihood -1.150718
WARNING: Variances had to be floored 1 2
INFO: iteration 13, average log likelihood -1.156414
INFO: iteration 14, average log likelihood -1.155222
WARNING: Variances had to be floored 14
INFO: iteration 15, average log likelihood -1.149503
WARNING: Variances had to be floored 1 2
INFO: iteration 16, average log likelihood -1.155881
INFO: iteration 17, average log likelihood -1.154769
WARNING: Variances had to be floored 14
INFO: iteration 18, average log likelihood -1.149208
WARNING: Variances had to be floored 1 2
INFO: iteration 19, average log likelihood -1.155712
INFO: iteration 20, average log likelihood -1.154661
WARNING: Variances had to be floored 14
INFO: iteration 21, average log likelihood -1.149163
WARNING: Variances had to be floored 1 2
INFO: iteration 22, average log likelihood -1.155711
INFO: iteration 23, average log likelihood -1.154627
WARNING: Variances had to be floored 14
INFO: iteration 24, average log likelihood -1.149167
WARNING: Variances had to be floored 1 2
INFO: iteration 25, average log likelihood -1.155736
INFO: iteration 26, average log likelihood -1.154605
WARNING: Variances had to be floored 14
INFO: iteration 27, average log likelihood -1.149182
WARNING: Variances had to be floored 1 2
INFO: iteration 28, average log likelihood -1.155770
INFO: iteration 29, average log likelihood -1.154588
WARNING: Variances had to be floored 14
INFO: iteration 30, average log likelihood -1.149195
WARNING: Variances had to be floored 1 2
INFO: iteration 31, average log likelihood -1.155798
INFO: iteration 32, average log likelihood -1.154572
WARNING: Variances had to be floored 14
INFO: iteration 33, average log likelihood -1.149203
WARNING: Variances had to be floored 1 2
INFO: iteration 34, average log likelihood -1.155819
INFO: iteration 35, average log likelihood -1.154557
WARNING: Variances had to be floored 14
INFO: iteration 36, average log likelihood -1.149207
WARNING: Variances had to be floored 1 2
INFO: iteration 37, average log likelihood -1.155835
INFO: iteration 38, average log likelihood -1.154543
WARNING: Variances had to be floored 14
INFO: iteration 39, average log likelihood -1.149208
WARNING: Variances had to be floored 1 2
INFO: iteration 40, average log likelihood -1.155846
INFO: iteration 41, average log likelihood -1.154530
WARNING: Variances had to be floored 14
INFO: iteration 42, average log likelihood -1.149207
WARNING: Variances had to be floored 1 2
INFO: iteration 43, average log likelihood -1.155851
INFO: iteration 44, average log likelihood -1.154519
WARNING: Variances had to be floored 14
INFO: iteration 45, average log likelihood -1.149205
WARNING: Variances had to be floored 1 2
INFO: iteration 46, average log likelihood -1.155850
INFO: iteration 47, average log likelihood -1.154511
WARNING: Variances had to be floored 14
INFO: iteration 48, average log likelihood -1.149201
WARNING: Variances had to be floored 1 2
INFO: iteration 49, average log likelihood -1.155844
INFO: iteration 50, average log likelihood -1.154505
INFO: EM with 100000 data points 50 iterations avll -1.154505
118.1 data points per parameter
4: avll = [-1.23866,-1.23701,-1.23442,-1.22239,-1.20325,-1.17717,-1.17187,-1.16476,-1.15527,-1.15915,-1.1574,-1.15072,-1.15641,-1.15522,-1.1495,-1.15588,-1.15477,-1.14921,-1.15571,-1.15466,-1.14916,-1.15571,-1.15463,-1.14917,-1.15574,-1.15461,-1.14918,-1.15577,-1.15459,-1.1492,-1.1558,-1.15457,-1.1492,-1.15582,-1.15456,-1.14921,-1.15584,-1.15454,-1.14921,-1.15585,-1.15453,-1.14921,-1.15585,-1.15452,-1.1492,-1.15585,-1.15451,-1.1492,-1.15584,-1.15451]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 27 28
INFO: iteration 1, average log likelihood -1.149544
WARNING: Variances had to be floored 1 2 3 4 27 28
INFO: iteration 2, average log likelihood -1.146318
WARNING: Variances had to be floored 27 28
INFO: iteration 3, average log likelihood -1.147389
WARNING: Variances had to be floored 1 2 3 4 27 28
INFO: iteration 4, average log likelihood -1.123433
WARNING: Variances had to be floored 12 18 19 27 28
INFO: iteration 5, average log likelihood -1.068165
WARNING: Variances had to be floored 1 2 3 4 20 22 27 28 29
INFO: iteration 6, average log likelihood -1.055960
WARNING: Variances had to be floored 11 18 19 27 28
INFO: iteration 7, average log likelihood -1.060818
WARNING: Variances had to be floored 1 2 3 4 12 20 27 28
INFO: iteration 8, average log likelihood -1.054856
WARNING: Variances had to be floored 18 19 22 27 28 29
INFO: iteration 9, average log likelihood -1.044567
WARNING: Variances had to be floored 1 2 3 4 20 27 28
INFO: iteration 10, average log likelihood -1.059975
WARNING: Variances had to be floored 11 12 18 19 24 27 28
INFO: iteration 11, average log likelihood -1.041076
WARNING: Variances had to be floored 1 2 3 4 20 22 27 28 29
INFO: iteration 12, average log likelihood -1.060004
WARNING: Variances had to be floored 18 19 27 28
INFO: iteration 13, average log likelihood -1.067815
WARNING: Variances had to be floored 1 2 3 4 12 20 27 28
INFO: iteration 14, average log likelihood -1.050010
WARNING: Variances had to be floored 18 19 22 27 28 29
INFO: iteration 15, average log likelihood -1.041692
WARNING: Variances had to be floored 1 2 3 4 11 20 27 28
INFO: iteration 16, average log likelihood -1.058237
WARNING: Variances had to be floored 12 18 19 27 28
INFO: iteration 17, average log likelihood -1.046831
WARNING: Variances had to be floored 1 2 3 4 20 22 24 27 28 29
INFO: iteration 18, average log likelihood -1.045026
WARNING: Variances had to be floored 18 19 27 28
INFO: iteration 19, average log likelihood -1.062572
WARNING: Variances had to be floored 1 2 3 4 11 12 20 27 28
INFO: iteration 20, average log likelihood -1.042054
WARNING: Variances had to be floored 18 19 22 27 28 29
INFO: iteration 21, average log likelihood -1.038217
WARNING: Variances had to be floored 1 2 3 4 20 27 28
INFO: iteration 22, average log likelihood -1.058182
WARNING: Variances had to be floored 12 18 19 27 28
INFO: iteration 23, average log likelihood -1.034676
WARNING: Variances had to be floored 1 2 3 4 11 20 22 24 27 28 29
INFO: iteration 24, average log likelihood -1.034104
WARNING: Variances had to be floored 18 19 27 28
INFO: iteration 25, average log likelihood -1.061197
WARNING: Variances had to be floored 1 2 3 4 12 20 27 28
INFO: iteration 26, average log likelihood -1.041041
WARNING: Variances had to be floored 11 18 19 22 27 28 29
INFO: iteration 27, average log likelihood -1.031952
WARNING: Variances had to be floored 1 2 3 4 20 27 28
INFO: iteration 28, average log likelihood -1.054613
WARNING: Variances had to be floored 12 18 19 24 27 28
INFO: iteration 29, average log likelihood -1.032946
WARNING: Variances had to be floored 1 2 3 4 11 20 22 27 28 29
INFO: iteration 30, average log likelihood -1.044012
WARNING: Variances had to be floored 18 19 27 28
INFO: iteration 31, average log likelihood -1.054339
WARNING: Variances had to be floored 1 2 3 4 12 20 27 28
INFO: iteration 32, average log likelihood -1.034246
WARNING: Variances had to be floored 1 2 3 4 11 18 19 22 24 27 28 29
INFO: iteration 33, average log likelihood -1.019757
WARNING: Variances had to be floored 1 2 3 4 20 27 28
INFO: iteration 34, average log likelihood -1.056995
WARNING: Variances had to be floored 1 2 3 4 12 18 19 27 28
INFO: iteration 35, average log likelihood -1.024026
WARNING: Variances had to be floored 1 2 3 4 11 20 22 27 28 29
INFO: iteration 36, average log likelihood -1.025285
WARNING: Variances had to be floored 1 2 3 4 18 19 24 27 28
INFO: iteration 37, average log likelihood -1.036715
WARNING: Variances had to be floored 1 2 3 4 12 20 27 28
INFO: iteration 38, average log likelihood -1.037882
WARNING: Variances had to be floored 1 2 3 4 11 18 19 22 27 28 29
INFO: iteration 39, average log likelihood -1.020696
WARNING: Variances had to be floored 1 2 3 4 20 27 28
INFO: iteration 40, average log likelihood -1.045899
WARNING: Variances had to be floored 1 2 3 4 12 18 19 24 27 28
INFO: iteration 41, average log likelihood -1.018829
WARNING: Variances had to be floored 1 2 3 4 11 20 22 27 28 29
INFO: iteration 42, average log likelihood -1.034919
WARNING: Variances had to be floored 1 2 3 4 18 19 27 28
INFO: iteration 43, average log likelihood -1.041241
WARNING: Variances had to be floored 1 2 3 4 12 20 27 28
INFO: iteration 44, average log likelihood -1.028122
WARNING: Variances had to be floored 1 2 3 4 11 18 19 22 24 27 28 29
INFO: iteration 45, average log likelihood -1.015808
WARNING: Variances had to be floored 1 2 3 4 20 27 28
INFO: iteration 46, average log likelihood -1.055714
WARNING: Variances had to be floored 1 2 3 4 12 18 19 27 28
INFO: iteration 47, average log likelihood -1.023548
WARNING: Variances had to be floored 1 2 3 4 11 20 22 27 28 29
INFO: iteration 48, average log likelihood -1.025162
WARNING: Variances had to be floored 1 2 3 4 18 19 24 27 28
INFO: iteration 49, average log likelihood -1.036684
WARNING: Variances had to be floored 1 2 3 4 12 20 27 28
INFO: iteration 50, average log likelihood -1.037870
INFO: EM with 100000 data points 50 iterations avll -1.037870
59.0 data points per parameter
5: avll = [-1.14954,-1.14632,-1.14739,-1.12343,-1.06817,-1.05596,-1.06082,-1.05486,-1.04457,-1.05997,-1.04108,-1.06,-1.06782,-1.05001,-1.04169,-1.05824,-1.04683,-1.04503,-1.06257,-1.04205,-1.03822,-1.05818,-1.03468,-1.0341,-1.0612,-1.04104,-1.03195,-1.05461,-1.03295,-1.04401,-1.05434,-1.03425,-1.01976,-1.057,-1.02403,-1.02528,-1.03671,-1.03788,-1.0207,-1.0459,-1.01883,-1.03492,-1.04124,-1.02812,-1.01581,-1.05571,-1.02355,-1.02516,-1.03668,-1.03787]
[-1.39027,-1.39043,-1.39028,-1.38892,-1.37829,-1.35949,-1.35359,-1.3525,-1.352,-1.3517,-1.35152,-1.35141,-1.35133,-1.35124,-1.3511,-1.35085,-1.35055,-1.3503,-1.35011,-1.34996,-1.34983,-1.34968,-1.34952,-1.34935,-1.34918,-1.34903,-1.34892,-1.34882,-1.34873,-1.34865,-1.34858,-1.34852,-1.34848,-1.34844,-1.34841,-1.34839,-1.34838,-1.34837,-1.34836,-1.34836,-1.34836,-1.34835,-1.34835,-1.34835,-1.34835,-1.34835,-1.34835,-1.34835,-1.34835,-1.34835,-1.34835,-1.34852,-1.34837,-1.3479,-1.34449,-1.33407,-1.3219,-1.31643,-1.31433,-1.31314,-1.31227,-1.31153,-1.3108,-1.31006,-1.30934,-1.30866,-1.30802,-1.30739,-1.30681,-1.30643,-1.30612,-1.30581,-1.30551,-1.30523,-1.30497,-1.30473,-1.30447,-1.30419,-1.30394,-1.30378,-1.30368,-1.30361,-1.30354,-1.30346,-1.30336,-1.30322,-1.30312,-1.30307,-1.30305,-1.30303,-1.30303,-1.30302,-1.30302,-1.30301,-1.30301,-1.30301,-1.30301,-1.30301,-1.30301,-1.30301,-1.30301,-1.30322,-1.30302,-1.30224,-1.29493,-1.27611,-1.25986,-1.25115,-1.24665,-1.24391,-1.2421,-1.24097,-1.24028,-1.23983,-1.23951,-1.23929,-1.23915,-1.23905,-1.23899,-1.23894,-1.2389,-1.23886,-1.23882,-1.23878,-1.23873,-1.23866,-1.23858,-1.23843,-1.23817,-1.23764,-1.23658,-1.23468,-1.25004,-1.24348,-1.24041,-1.23903,-1.23845,-1.238,-1.23729,-1.236,-1.23375,-1.24947,-1.24321,-1.24025,-1.2385,-1.23726,-1.23569,-1.23333,-1.24919,-1.24303,-1.24015,-1.23866,-1.23701,-1.23442,-1.22239,-1.20325,-1.17717,-1.17187,-1.16476,-1.15527,-1.15915,-1.1574,-1.15072,-1.15641,-1.15522,-1.1495,-1.15588,-1.15477,-1.14921,-1.15571,-1.15466,-1.14916,-1.15571,-1.15463,-1.14917,-1.15574,-1.15461,-1.14918,-1.15577,-1.15459,-1.1492,-1.1558,-1.15457,-1.1492,-1.15582,-1.15456,-1.14921,-1.15584,-1.15454,-1.14921,-1.15585,-1.15453,-1.14921,-1.15585,-1.15452,-1.1492,-1.15585,-1.15451,-1.1492,-1.15584,-1.15451,-1.14954,-1.14632,-1.14739,-1.12343,-1.06817,-1.05596,-1.06082,-1.05486,-1.04457,-1.05997,-1.04108,-1.06,-1.06782,-1.05001,-1.04169,-1.05824,-1.04683,-1.04503,-1.06257,-1.04205,-1.03822,-1.05818,-1.03468,-1.0341,-1.0612,-1.04104,-1.03195,-1.05461,-1.03295,-1.04401,-1.05434,-1.03425,-1.01976,-1.057,-1.02403,-1.02528,-1.03671,-1.03788,-1.0207,-1.0459,-1.01883,-1.03492,-1.04124,-1.02812,-1.01581,-1.05571,-1.02355,-1.02516,-1.03668,-1.03787]
32×26 Array{Float64,2}:
 -0.139571    -1.61712     -0.698484     -0.095238    -0.227479    0.0231597   -0.872629     0.135844    -0.0374229    0.757707     0.0404467   -0.0647183   -0.278026   -0.105526    0.135248    -0.0307887   -0.0848284   -0.104881    -0.0661299    0.0061992   -0.0326669   -0.0207719   -0.136231    -0.117549      0.0373841   -0.0556238  
 -0.160138    -1.3879       0.643752      0.0362065   -0.178709    0.210658     0.412755    -0.215185    -0.0392191   -0.800903     0.0444006    0.198314    -0.0645232  -0.224949    0.282734    -0.0283154   -0.0447561   -0.105291    -0.0659062   -0.00925768  -0.0183707   -0.00892691   0.291446     0.186736      0.0187524    0.408983   
  0.0122273    1.38057     -0.404158      0.116019    -0.208621    0.161979    -0.609474    -0.0711547   -0.0388047   -0.415382     0.0493585    0.113512    -0.063247   -0.0841423   0.150776    -0.0292406   -0.093085    -0.103156    -0.065716     0.00637231  -0.0476896   -0.00266985   0.198169     0.0371332     0.013629     0.228239   
  0.468724     1.11066     -0.000502074  -0.0711143   -0.140333    0.0265548    2.23176     -0.0899335   -0.038893     0.24469      0.0546656    0.104448     0.158907   -0.285375   -0.0723207   -0.031673    -0.133396    -0.104767    -0.0650899    0.00379911  -0.0487362    0.00155312   0.112609    -0.183663      0.015417     0.383421   
 -0.0422742   -0.0740409   -0.0273705     0.00138813   0.068414   -0.0111217   -0.195644     0.0248134   -0.0405428   -0.0562774   -0.0531167   -0.093241     0.0504685   0.129192    0.0944693    0.0140826   -0.0168764    0.0600569   -0.119496    -0.043621     0.035946    -0.126828    -0.00433554   0.0761498     0.0809349   -0.134637   
 -0.151869     0.0930856   -0.249467      0.0769018    0.0025216   0.160752    -0.00135714  -0.0921021    0.0606899   -0.0166061    0.0222829    0.0749287   -0.0957864  -0.158219   -0.0296416   -0.0668787   -0.0246657   -0.101406    -0.222661     0.0698999    0.0028995   -0.110156    -0.12892      0.0563987     0.077334     0.0379122  
  0.0479776    0.0410453    0.0522358    -0.044791    -0.0660378  -0.151281    -0.072192     0.226385    -0.0234176    0.0841463   -0.0956232   -0.0243994    0.146463    0.0566759   0.0219046    0.185021    -0.0636859   -0.051044    -0.00747289   0.135227     0.0826672    0.106785    -0.0160507   -0.000281857   0.0808944   -0.0369292  
  0.149684    -0.0259502   -0.0729619     0.0666384   -0.137349    0.0767015    0.00790359   0.0498268   -0.0889545   -0.0488954    0.0640632   -0.0238521   -0.138191   -0.028163    0.143974    -0.0519561    0.0563058   -0.00380911   0.019909    -0.208731    -0.0196297    0.0436415   -0.167018    -0.163003     -0.0798632    0.0187404  
 -0.128162     0.0149036   -0.0250768     0.0281787   -0.098416    0.152515    -0.0978553   -0.00876138  -0.0756693   -0.0560923    0.152118    -0.0580978   -0.203439    0.0466037   0.0435457   -0.0482226    0.097541    -0.161796    -0.0221338    0.0406616    0.00482372  -0.0908859   -0.0104068    0.0374286    -0.0255706   -0.0695543  
  0.00138855   0.163857    -0.0528252     0.076024    -0.0364947  -0.00638355  -0.00864077   0.0639546    0.006046     0.0944868   -0.0262025    0.0321593   -0.0275512  -0.030325    0.119244    -0.026151     0.00564064  -0.14658      0.0173684   -0.0751385   -0.0720498    0.051792     0.0376308   -0.0874222     0.00369581  -0.0588372  
 -0.0278345    0.105544    -0.00893199   -0.0595378   -0.152829   -0.0804358   -0.020183    -0.229478    -0.0705331    0.0683945   -0.0344752    0.0528067    0.0929875  -0.0170545   0.00764707  -0.139088     0.143071     0.0799249    0.109158     0.0203161   -0.121116    -0.0812323    0.165997     0.156355     -0.0331113   -0.0197363  
 -0.0958317   -0.137485     0.0207473     0.140829     0.0252122  -0.0397018    0.152823    -0.189758     0.0377788   -0.0765177    0.155732    -0.0841464   -0.0947852   0.0037846  -0.123623     0.00846875   0.082364    -0.0779207   -0.0673566    0.0263603   -0.0525264    0.00989151  -0.0735722    0.0511154     0.171212     0.00931122 
 -0.0687808    0.160683    -0.0113424     0.0932083    0.0848743  -0.137078    -0.0181983    0.131515     0.0254603   -0.181215     0.158646    -0.136557    -0.0634359   0.019478   -0.0836101    0.0932939   -0.0202931    0.155758    -0.0634999   -0.103002    -0.102002    -0.0618583   -0.0168218    0.18403       0.0253288   -0.0716826  
  0.0490413    0.00966062   0.0291318    -0.00440426  -0.111934    0.039888     0.0387859    0.0744787    0.109362     0.0300121   -0.00451358   0.123231    -0.0213984  -0.058938    0.0236482   -0.0209407   -0.0216774   -0.0369156   -0.0190411   -0.0500358    0.00849619  -0.0917693    0.0923653    0.0484566    -0.156412    -0.0332387  
  0.0879148   -0.0915563    0.0545273     0.0237761   -0.0687591  -0.17479     -0.0624245   -0.00828221  -0.0884521   -0.0676717    0.106325     0.237115     0.113656   -0.122052   -0.0358384    0.0374954    0.154569    -0.00102953  -0.00527846  -0.079499    -0.144255    -0.0397208   -0.0496262    0.00428712    0.0660975    0.112976   
  0.0965278   -0.101343     0.0147765    -0.122221    -0.152769    0.144389     0.0193606    0.0474253    0.150179     0.0064003   -0.10878      0.0109695    0.0581767  -0.0800558   0.0573079   -0.0408269   -0.175937    -0.0637044    0.0982084   -0.230838     0.067192    -0.00578741   0.157403    -0.262587      0.110611     0.0750751  
 -0.0185394   -0.104596    -0.144575     -0.0304457    0.172607    0.0174115   -0.0756082   -0.0468823    0.0154708   -0.0754607   -0.157599     0.0736181   -0.035338   -0.0425715  -0.0437797   -0.0273463    0.0720327   -0.0106975   -0.0842149   -0.0153695    0.0327094   -0.0509762   -0.117348     0.0927028     0.00430123  -0.0164456  
  0.057185     0.120166     0.0223222     0.169793     0.0937328  -0.0432754    0.00404341  -0.18943      0.111529    -0.152184    -0.0581427    0.139694     0.157147    0.0491839   0.041448     0.162513    -0.0595406   -0.0409783    0.166905     0.00960823  -0.122112    -0.00704904   0.0257119   -0.0436016     0.00495315  -0.0571406  
 -0.0166369    0.040272    -0.161489     -0.0248729   -0.17326    -0.11933      0.0149482    0.0958794    0.102684     0.0215363    0.143987     0.0736842    0.0266073  -0.0067377  -0.0102673   -0.0930779   -0.0595954    0.0380981    0.210778    -0.242683     0.109339    -0.125937    -0.0212485   -0.00578984    0.149653    -0.0696177  
  0.0160285    0.185105     0.0828666    -0.234676     0.130444   -0.09576      0.0048486   -0.0206686    0.0226567   -0.134113    -0.150631     0.00323677  -0.124452    0.0228871   0.192298    -0.0664898    0.0912531    0.0249502   -0.194211    -0.0284867   -0.264355     0.0169216   -0.0388087    0.182518      0.0822909    0.0374274  
  0.169478    -0.00906422  -0.0614451    -0.057416     0.0572019   0.0776875    0.147681     0.0130916    0.152285     0.0357594   -0.0981481   -0.0101909   -0.0130566   0.0351995   0.079209    -0.0384932    0.041192     0.0118527    0.190284     0.100595     0.0695403   -0.0157772    0.0455232   -0.222085     -0.154126     0.0426996  
  0.00442027  -0.178909    -0.139003      0.043112    -0.132022   -0.0216444   -0.00467887   0.0757358   -0.145172     0.0870214    0.0336503   -0.0260249   -0.0523023   0.142607   -0.00424761  -0.0139357   -0.0265814   -0.065188    -0.0648761   -0.0918706   -0.183674    -0.00589823   0.0895164   -0.0798324     0.112898    -0.019368   
  0.0074212   -0.113106     0.0281188     0.0408833   -0.13926    -0.0361548    0.0782087   -0.071234     0.00894227  -0.00531926  -0.105687     0.124649    -0.0390053  -0.0313784   0.0357375    0.143022     0.0657271   -0.0430888    0.0408331    0.00445901   0.0587966    0.105587    -0.0916145   -0.0543433    -0.111762    -0.0468256  
 -0.107393     0.0775362   -0.138681      0.041044     0.25513     0.133015     0.228981     0.0326194    0.118395    -0.00158161   0.0385525   -0.0468191   -0.110501    0.0759941   0.071248    -0.0408569    0.0340889   -0.0399474   -0.0994809   -0.125991     0.0230089   -0.221142     0.0493      -0.0244411    -0.0219817    0.104453   
  0.0303248    0.00999568  -0.0113093     0.0127036   -0.157401    0.0316481    0.00885088  -0.0507349   -0.192906    -0.0122786   -0.0687744    0.0311233   -0.0726122  -0.68751     0.166128    -0.187314     0.0443257    0.152517     0.0279997   -0.0718228    0.0746909    0.0889199    0.0833545   -0.00805396    0.198088    -0.156008   
  0.0274182    0.0298368    0.0918422    -0.00533153  -0.144112    0.0197162    0.0612142   -0.0630305    0.134419    -0.0186883   -0.0882353    0.00132608  -0.0657123   0.45351     0.132994    -0.0842846   -0.115308     0.153713     0.00618006  -0.0684189    0.101852     0.0917814    0.12698      0.00378995    0.0491749    0.0611754  
  0.115005    -0.143097     0.0290128    -0.116142     0.0738271  -0.0338746   -0.2116       0.152444    -0.0506984   -0.00252132   0.154374     0.32021      0.0651441   0.16483    -0.196369     0.0837565   -0.136094     0.053667    -0.0176016   -0.112373    -0.0519988    0.00294746  -0.49846     -0.0367789     0.148392     0.000878119
  0.10161      0.225903     0.0215927    -0.143632    -0.262239   -0.0332543   -0.211803     0.175444    -0.103906     0.0967723    0.2133      -0.0342541    0.0592592   0.0470326  -0.216051     0.0877997    0.0605114    0.0538754    0.0028236    0.00204254  -0.046311    -0.00745946   0.548478     0.233677      0.149371    -0.0314424  
 -0.0771138   -0.105909     0.0455488    -0.0975432   -0.114401    0.00985221  -0.043853    -0.00170255  -0.0678567    0.0776536   -0.0294723    0.218998     0.0235968   0.0223774  -0.0458905   -0.12526     -0.0294995    0.0668224   -0.116062     0.0427746    0.167187    -0.106336     0.0382989   -0.169632      0.0630954    0.00115772 
  0.106923    -0.102584     0.016546      0.0274885   -0.166459   -0.0704261   -0.116279     0.0441463    0.0273328    0.226865     0.09067     -0.00465161  -0.0323666  -0.0720987  -0.0589094    0.0860121    0.161289     0.0823915   -0.105697     0.0114164    0.111668     0.0466566    0.0365234    0.189632     -0.0810794    0.0382143  
  0.122462     0.109906     0.137523      0.0304166    0.152495   -0.0948475    0.0795183    0.0168179    0.00356828  -0.0876389   -0.0712528   -0.129852    -0.0189119  -0.1111     -0.160097    -0.075401     0.0642695    0.0894688   -0.0281133   -0.122987     0.0274371   -0.0424817   -0.0682295    0.265351     -0.148835    -0.233365   
  0.0704684   -0.0652471   -0.222171      0.027011     0.129618   -0.0255672    0.0209922    0.0137947    0.201414    -0.100571     0.018257    -0.074241    -0.0358553  -0.0219691   0.00469104  -0.048641     0.00551012   0.0759264    0.0144683    0.103171    -0.0238902   -0.0257223   -0.0112416    0.0620461     0.0653749   -0.0126854  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 2 3 4 11 18 19 22 27 28 29
INFO: iteration 1, average log likelihood -1.020686
WARNING: Variances had to be floored 1 2 3 4 11 12 18 19 20 22 27 28 29
INFO: iteration 2, average log likelihood -1.001357
WARNING: Variances had to be floored 1 2 3 4 11 18 19 22 24 27 28 29
INFO: iteration 3, average log likelihood -1.016165
WARNING: Variances had to be floored 1 2 3 4 11 12 18 19 20 22 27 28 29
INFO: iteration 4, average log likelihood -1.005042
WARNING: Variances had to be floored 1 2 3 4 11 18 19 22 27 28 29
INFO: iteration 5, average log likelihood -1.016796
WARNING: Variances had to be floored 1 2 3 4 11 12 18 19 20 22 24 27 28 29
INFO: iteration 6, average log likelihood -1.000599
WARNING: Variances had to be floored 1 2 3 4 11 18 19 22 27 28 29
INFO: iteration 7, average log likelihood -1.020613
WARNING: Variances had to be floored 1 2 3 4 11 12 18 19 20 22 27 28 29
INFO: iteration 8, average log likelihood -1.001240
WARNING: Variances had to be floored 1 2 3 4 11 18 19 22 24 27 28 29
INFO: iteration 9, average log likelihood -1.016164
WARNING: Variances had to be floored 1 2 3 4 11 12 18 19 20 22 27 28 29
INFO: iteration 10, average log likelihood -1.005037
INFO: EM with 100000 data points 10 iterations avll -1.005037
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.501208e+05
      1       6.538143e+05      -1.963066e+05 |       32
      2       6.245909e+05      -2.922333e+04 |       32
      3       6.102979e+05      -1.429302e+04 |       32
      4       6.019546e+05      -8.343307e+03 |       32
      5       5.958977e+05      -6.056893e+03 |       32
      6       5.918925e+05      -4.005200e+03 |       32
      7       5.892859e+05      -2.606629e+03 |       32
      8       5.877296e+05      -1.556310e+03 |       32
      9       5.868079e+05      -9.217048e+02 |       32
     10       5.864041e+05      -4.037484e+02 |       32
     11       5.861962e+05      -2.079353e+02 |       32
     12       5.861050e+05      -9.121833e+01 |       32
     13       5.860584e+05      -4.653446e+01 |       32
     14       5.860326e+05      -2.582255e+01 |       32
     15       5.860133e+05      -1.933603e+01 |       30
     16       5.860014e+05      -1.190058e+01 |       30
     17       5.859925e+05      -8.914391e+00 |       27
     18       5.859855e+05      -6.973598e+00 |       28
     19       5.859805e+05      -5.016669e+00 |       27
     20       5.859762e+05      -4.282987e+00 |       28
     21       5.859726e+05      -3.607402e+00 |       20
     22       5.859701e+05      -2.495029e+00 |       22
     23       5.859683e+05      -1.823608e+00 |       20
     24       5.859667e+05      -1.545663e+00 |       19
     25       5.859653e+05      -1.436551e+00 |       18
     26       5.859630e+05      -2.297661e+00 |       15
     27       5.859615e+05      -1.452713e+00 |       14
     28       5.859603e+05      -1.219278e+00 |       14
     29       5.859588e+05      -1.493442e+00 |       12
     30       5.859549e+05      -3.912353e+00 |       15
     31       5.859466e+05      -8.260294e+00 |       21
     32       5.859394e+05      -7.271173e+00 |       18
     33       5.859312e+05      -8.154193e+00 |       24
     34       5.859248e+05      -6.402815e+00 |       19
     35       5.859207e+05      -4.090092e+00 |       20
     36       5.859169e+05      -3.852022e+00 |       24
     37       5.859115e+05      -5.346727e+00 |       21
     38       5.859045e+05      -7.009095e+00 |       22
     39       5.858956e+05      -8.896092e+00 |       22
     40       5.858830e+05      -1.266726e+01 |       22
     41       5.858683e+05      -1.467643e+01 |       23
     42       5.858494e+05      -1.889338e+01 |       22
     43       5.858164e+05      -3.301960e+01 |       23
     44       5.857870e+05      -2.939488e+01 |       24
     45       5.857587e+05      -2.824869e+01 |       29
     46       5.857305e+05      -2.817998e+01 |       26
     47       5.857010e+05      -2.955285e+01 |       28
     48       5.856637e+05      -3.727334e+01 |       29
     49       5.856141e+05      -4.960942e+01 |       29
     50       5.855547e+05      -5.940245e+01 |       29
K-means terminated without convergence after 50 iterations (objv = 585554.7005999405)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.292648
INFO: iteration 2, average log likelihood -1.258701
INFO: iteration 3, average log likelihood -1.226385
INFO: iteration 4, average log likelihood -1.192518
INFO: iteration 5, average log likelihood -1.151348
WARNING: Variances had to be floored 5 10
INFO: iteration 6, average log likelihood -1.101276
WARNING: Variances had to be floored 27
INFO: iteration 7, average log likelihood -1.100698
WARNING: Variances had to be floored 3 7 12 20
INFO: iteration 8, average log likelihood -1.056551
WARNING: Variances had to be floored 10 19
INFO: iteration 9, average log likelihood -1.065356
WARNING: Variances had to be floored 5 9 22
INFO: iteration 10, average log likelihood -1.054697
WARNING: Variances had to be floored 27
INFO: iteration 11, average log likelihood -1.067556
WARNING: Variances had to be floored 10
INFO: iteration 12, average log likelihood -1.046416
WARNING: Variances had to be floored 3 5 7 20
INFO: iteration 13, average log likelihood -1.039745
WARNING: Variances had to be floored 6 9
INFO: iteration 14, average log likelihood -1.051719
WARNING: Variances had to be floored 10 22 27
INFO: iteration 15, average log likelihood -1.041532
WARNING: Variances had to be floored 5 15
INFO: iteration 16, average log likelihood -1.054867
WARNING: Variances had to be floored 3 19
INFO: iteration 17, average log likelihood -1.050300
WARNING: Variances had to be floored 6 7 10 20
INFO: iteration 18, average log likelihood -1.035376
WARNING: Variances had to be floored 5 9 27
INFO: iteration 19, average log likelihood -1.054763
WARNING: Variances had to be floored 15
INFO: iteration 20, average log likelihood -1.052230
WARNING: Variances had to be floored 3 10 22
INFO: iteration 21, average log likelihood -1.032204
WARNING: Variances had to be floored 6 7 27
INFO: iteration 22, average log likelihood -1.050957
WARNING: Variances had to be floored 5 9 15 20
INFO: iteration 23, average log likelihood -1.050936
INFO: iteration 24, average log likelihood -1.083858
WARNING: Variances had to be floored 3 10 19 27
INFO: iteration 25, average log likelihood -1.021599
WARNING: Variances had to be floored 5 6 15
INFO: iteration 26, average log likelihood -1.045550
WARNING: Variances had to be floored 7 20 22
INFO: iteration 27, average log likelihood -1.068177
WARNING: Variances had to be floored 9
INFO: iteration 28, average log likelihood -1.064285
WARNING: Variances had to be floored 10 19
INFO: iteration 29, average log likelihood -1.033055
WARNING: Variances had to be floored 3 5 6 15
INFO: iteration 30, average log likelihood -1.036576
INFO: iteration 31, average log likelihood -1.068677
WARNING: Variances had to be floored 7 9 10 19 20 22
INFO: iteration 32, average log likelihood -1.019643
WARNING: Variances had to be floored 5
INFO: iteration 33, average log likelihood -1.071990
WARNING: Variances had to be floored 3 6 15
INFO: iteration 34, average log likelihood -1.041340
WARNING: Variances had to be floored 10 24
INFO: iteration 35, average log likelihood -1.047880
WARNING: Variances had to be floored 5 9 19 20
INFO: iteration 36, average log likelihood -1.039507
WARNING: Variances had to be floored 7
INFO: iteration 37, average log likelihood -1.046792
WARNING: Variances had to be floored 3 6 10 15 22 24
INFO: iteration 38, average log likelihood -1.011097
INFO: iteration 39, average log likelihood -1.085988
WARNING: Variances had to be floored 5 9 19 20
INFO: iteration 40, average log likelihood -1.031944
WARNING: Variances had to be floored 24
INFO: iteration 41, average log likelihood -1.049558
WARNING: Variances had to be floored 3 7 10 15
INFO: iteration 42, average log likelihood -1.021156
WARNING: Variances had to be floored 6 22
INFO: iteration 43, average log likelihood -1.054008
WARNING: Variances had to be floored 5 9 19 20 24
INFO: iteration 44, average log likelihood -1.031517
INFO: iteration 45, average log likelihood -1.074545
WARNING: Variances had to be floored 3 7 10 15
INFO: iteration 46, average log likelihood -1.020038
WARNING: Variances had to be floored 6 24
INFO: iteration 47, average log likelihood -1.047746
WARNING: Variances had to be floored 5 9 19 20 22
INFO: iteration 48, average log likelihood -1.034353
INFO: iteration 49, average log likelihood -1.068651
WARNING: Variances had to be floored 3 7 10 15 24
INFO: iteration 50, average log likelihood -1.017002
INFO: EM with 100000 data points 50 iterations avll -1.017002
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.00105282   0.111991   -0.0424242   -0.128435    -0.0361179   -0.111202     0.00315286   0.0391313     0.0666892   -0.0559908   -0.00109446   0.0378484   -0.0463514   0.00973428   0.0926206    -0.081273     0.014745    0.0372612     0.0118542   -0.144851     -0.0680853   -0.0583136   -0.0292715   0.084393     0.117886    -0.0167516 
  0.159103    -0.0240164  -0.0627247   -0.0570695    0.0555739    0.0725333    0.133904     0.0151998     0.137462     0.0392595   -0.0939838   -0.010487    -0.0139021   0.047244     0.0865849    -0.0394868    0.0368404   0.009825      0.165472     0.103327      0.0333941   -0.0146192    0.0420746  -0.223707    -0.127727     0.0405635 
 -0.0291488    0.104334   -0.0105198   -0.0608829   -0.155223    -0.0796085   -0.0173938   -0.231233     -0.0715611    0.0666178   -0.0339982    0.052651     0.0936208  -0.0149437    0.0116404    -0.136116     0.143363    0.0778092     0.108561     0.0212206    -0.121238    -0.0828299    0.168526    0.160793    -0.0323898   -0.0196787 
  0.0960134   -0.10199     0.0144505   -0.121927    -0.155897     0.140366     0.019867     0.0508578     0.156569     0.00685851  -0.107884     0.00978308   0.055246   -0.0813672    0.0567009    -0.0410267   -0.181573   -0.0617635     0.0990788   -0.230655      0.0718047   -0.00280278   0.156183   -0.264066     0.107356     0.0737933 
  0.101518     0.035595    0.0324797   -0.108385    -0.0999098   -0.0338151   -0.184155     0.132491     -0.0577049    0.0440126    0.152469     0.139707     0.0593876   0.0984343   -0.189294      0.0918542   -0.0446981   0.0603665    -0.0141274   -0.0621223    -0.0352125    0.0154319    0.0102382   0.0939397    0.131558    -0.0232567 
 -0.0793011   -0.109209    0.0465706   -0.0972475   -0.121047     0.00788486  -0.0437154    0.000354651  -0.0691486    0.0764571   -0.0293794    0.215719     0.0183454   0.0315679   -0.0527228    -0.126938    -0.0315051   0.067373     -0.115334     0.0410289     0.165923    -0.104822     0.0416512  -0.174546     0.064065     0.0028437 
  0.0297936   -0.151403   -0.0527758   -0.061088     0.151423    -0.0686739    0.0736783   -0.0340852     0.274634    -0.298529     0.109425    -0.107442     0.0357802  -0.10833      0.0272688    -0.0295898   -0.0414381   0.152254      0.0541427    0.026889      0.106675    -0.0361738    0.0588423   0.0882404    0.149641     0.0337082 
  0.0887162   -0.0903765   0.0560497    0.0238839   -0.0649283   -0.178566    -0.060682    -0.00921612   -0.0935644   -0.0685387    0.106639     0.235851     0.116742   -0.123226    -0.0350095     0.0383727    0.149941   -0.000717371  -0.00684318  -0.0835631    -0.147929    -0.0412436   -0.0519151   0.00242507   0.0671118    0.115397  
  0.00742247  -0.161003   -0.112923    -0.0709684   -0.131859    -0.0244134   -0.0575485    0.00869572   -0.248229     0.0428191   -0.0251562   -0.0316246   -0.0434283   0.0918893   -0.000879069  -0.00386935  -0.0111752  -0.0863334    -0.0699526   -0.136769     -0.143751    -0.00727537   0.150573   -0.12755      0.114548    -0.0137226 
  0.00548192  -0.0402262  -0.0796623    0.0251037   -0.192709     0.128157     0.111245    -0.0829649    -0.0395719   -0.210221     0.048372     0.108028    -0.0661119  -0.167913     0.15478      -0.0293992   -0.0837038  -0.105431     -0.0661221    0.000371455  -0.036509    -0.00604942   0.151504    0.0154956    0.019341     0.261853  
  0.0856449    0.0262443   0.103881     0.0323393   -0.199387     0.064234    -0.0587561    0.057994      0.149616    -0.0205877   -0.0229395    0.0288286   -0.0526596  -0.101528     0.0543408    -0.031425    -0.0216372  -0.0325653     0.0358639   -0.0592557     0.00782731  -0.0552687    0.158749    0.136072    -0.118323     0.0162315 
  0.0988027    0.0287352  -0.372701     0.109136     0.113404     0.0184515   -0.0211791    0.0551871     0.122282     0.0883366   -0.0613057   -0.0274288   -0.10596     0.0555246   -0.0235524    -0.0667685    0.042975    0.00413541   -0.0232883    0.17162      -0.145047    -0.0126057   -0.0768831   0.0362978   -0.0220092   -0.0519873 
  0.106103    -0.101207    0.0168845    0.0300312   -0.169393    -0.0691333   -0.11551      0.0407896     0.0282411    0.233924     0.0904545   -0.00294508  -0.033905   -0.0710805   -0.0558542     0.0878482    0.161443    0.0816517    -0.108689     0.0104156     0.11164      0.0471281    0.0365641   0.187323    -0.083491     0.0372418 
 -0.024647    -0.138613   -0.0400152    0.058516    -0.123569    -0.0657315    0.124593    -0.0488545    -0.0329565   -0.0275933   -0.00365931   0.100454    -0.0095343  -0.0297006   -0.0741967     0.134511     0.189417   -0.116287      0.119857     0.0630882     0.047478     0.052775    -0.10658    -0.079512    -0.144281    -0.116316  
 -0.0868       0.102845    0.0104081    0.0749173   -0.0298793    0.0339299    0.0147102    0.0688649     0.105446     0.119676     0.0445779    0.0693465   -0.194829    0.0381297    0.158615     -0.0176778   -0.0707073  -0.189235     -0.0467592    0.0289026    -0.0914       0.131487     0.0199903  -0.123035     0.0400193   -0.13149   
 -0.0497632   -0.0562535  -0.0582968   -0.0276841    0.126604    -0.0171761   -0.028413     0.0451669    -0.0341223   -0.116829     0.00663279  -0.132328    -0.0162955   0.120925     0.0105554    -0.0673586   -0.0382056   0.118489     -0.143638    -0.0224281     0.0199941   -0.21741      0.0641076   0.0770033    0.0587907   -0.0541825 
 -0.148295     0.0911977  -0.240362     0.0773336   -0.00208982   0.161947     0.00174844  -0.0936651     0.0610797   -0.0124207    0.0235066    0.0772679   -0.092177   -0.157568    -0.029234     -0.0647493   -0.0292541  -0.0954845    -0.223408     0.0673098     0.00175076  -0.102206    -0.127006    0.0567679    0.0667738    0.0426747 
 -0.021459    -0.0991982  -0.138488    -0.0287713    0.169813     0.0174749   -0.077472    -0.0557221     0.0181661   -0.077836    -0.151341     0.0752271   -0.0356526  -0.0358723   -0.0393894    -0.0225099    0.0694451  -0.0140985    -0.0763366   -0.0106356     0.0239701   -0.0482745   -0.115247    0.0893147    0.00388939  -0.0186433 
  0.0381235   -0.105209    0.0797453    0.0237199   -0.160064     0.0131934    0.0126148   -0.0651819     0.026299     0.04413     -0.253767     0.112602    -0.0697323  -0.0227915    0.281044      0.13649     -0.115563    0.0677552    -0.0899646   -0.093294      0.0276742    0.137999    -0.0134323  -0.0677417   -0.0324639    0.0797409 
 -0.0989441   -0.14576     0.00356402   0.237032     0.0273488   -0.0365973    0.201281    -0.164471      0.114838    -0.0500128    0.180021    -0.0671099   -0.111983    0.0379837   -0.166325      0.00839162   0.0720526  -0.0814199    -0.0679389    0.0674492    -0.0772995    0.0176599   -0.125146    0.111571     0.159421     0.00162301
 -0.0486181   -0.0846516   0.018016     0.0189003    0.0262833   -0.0235264   -0.308708    -0.0822966    -0.08168     -0.4622      -0.0835959    0.0312077    0.159735    0.139787     0.291817      0.13693      0.0564011  -0.0909875    -0.0902735   -0.0521585     0.0722749   -0.0819214   -0.0253435   0.0515159    0.0887301   -0.172902  
 -0.10458      0.07123    -0.133518     0.0377429    0.24194      0.135443     0.217023     0.0414851     0.124084     0.00504422   0.0259102   -0.0294342   -0.137784    0.0888935    0.0933917    -0.0368972    0.0200035  -0.0704002    -0.107237    -0.145653      0.00298887  -0.187559     0.0416107  -0.0383981   -0.00214858   0.116906  
 -0.0461426   -0.0641324  -0.0485234    0.0418904    0.0524166    0.123072    -0.312514     0.182887      0.059937     0.933133    -0.132226    -0.129806    -0.0498087   0.121212    -0.0116711     0.00245437  -0.0591997   0.130152     -0.0616188   -0.125375      0.0112556    0.0322239   -0.119184    0.0903114    0.0996215   -0.18186   
  0.0639527    0.126748    0.026995     0.173294     0.0940698   -0.045325     0.0115818   -0.189501      0.113585    -0.153511    -0.063339     0.140584     0.162399    0.0488557    0.0422416     0.168385    -0.0596995  -0.0398023     0.172135     0.0114217    -0.127588    -0.00633156   0.0267446  -0.045361     0.00571105  -0.0577469 
 -0.068544     0.158354   -0.0131194    0.0925518    0.0854391   -0.143157    -0.0267721    0.134032      0.0256635   -0.183032     0.16299     -0.136385    -0.063487    0.0218246   -0.0866828     0.0929913   -0.0182088   0.156998     -0.0634873   -0.105207     -0.102297    -0.0592805   -0.0165211   0.184586     0.0294486   -0.0753788 
  0.1079       0.219966   -0.117886     0.0668631   -0.0538058   -0.0528913   -0.0352133    0.0535626    -0.118073     0.0563027   -0.108082    -0.0232704    0.172445   -0.110247     0.0738113    -0.0328238    0.0914352  -0.0977922     0.0927749   -0.198333     -0.048073    -0.0439904    0.0595237  -0.0424012   -0.0415765    0.036458  
  0.149481    -0.0236357  -0.0713027    0.0627546   -0.1352       0.076781     0.00999256   0.0458695    -0.0882742   -0.0496891    0.0615006   -0.0220752   -0.138856   -0.0288691    0.145442     -0.0515622    0.0551817  -0.00539153    0.017986    -0.206379     -0.0199931    0.0446822   -0.164989   -0.156995    -0.0775298    0.0186008 
 -0.129318     0.0144619  -0.0254242    0.0246564   -0.0986439    0.154086    -0.0990749   -0.00420542   -0.0758085   -0.0556362    0.152537    -0.056072    -0.201477    0.0465036    0.0479652    -0.0490442    0.0972991  -0.16193      -0.0195861    0.0400882     0.0064468   -0.0888562   -0.0105657   0.0334106   -0.0256567   -0.0700129 
  0.0279365   -0.0145564  -0.0388985   -0.028242    -0.0552156    0.0153316    0.136715     0.0959613     0.073957     0.093232     0.00707059   0.229206    -0.0038853  -0.0312507    0.00802506   -0.0012741   -0.0159273  -0.05886      -0.077422    -0.0447619     0.00357939  -0.101483     0.0175592  -0.0471916   -0.198287    -0.108979  
  0.0289276    0.0207392   0.0413948    0.00360525  -0.150903     0.0253097    0.0359046   -0.0570923    -0.0277039   -0.0151591   -0.0799779    0.0163771   -0.0692109  -0.106538     0.149698     -0.134336    -0.0366842   0.153453      0.0168004   -0.06993       0.0874166    0.0904532    0.105457   -0.00179058   0.122975    -0.0444685 
  0.122684     0.110521    0.137567     0.0299317    0.152583    -0.0944123    0.0798251    0.0163372     0.00340417  -0.0875516   -0.0712787   -0.129263    -0.0201197  -0.112659    -0.160056     -0.0748213    0.0635254   0.0894157    -0.0284533   -0.122441      0.0276781   -0.0422426   -0.0683545   0.264696    -0.14902     -0.233382  
  0.0488951    0.0413278   0.0525701   -0.0450676   -0.0659327   -0.150959    -0.0722823    0.227359     -0.0229621    0.0853799   -0.0961032   -0.0250501    0.145776    0.0566694    0.0209116     0.185362    -0.0649219  -0.0536975    -0.00712297   0.136305      0.082747     0.106637    -0.0149287   0.00127254   0.0806606   -0.0367665 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 6
INFO: iteration 1, average log likelihood -1.066566
WARNING: Variances had to be floored 5 6 9 19 20
INFO: iteration 2, average log likelihood -1.012648
WARNING: Variances had to be floored 5 6 10 15 22 24
INFO: iteration 3, average log likelihood -0.999568
WARNING: Variances had to be floored 3 6 7 9 19 20
INFO: iteration 4, average log likelihood -1.031588
WARNING: Variances had to be floored 5 6 22
INFO: iteration 5, average log likelihood -1.020776
WARNING: Variances had to be floored 5 6 9 10 15 19 20 24
INFO: iteration 6, average log likelihood -0.984823
WARNING: Variances had to be floored 6 22
INFO: iteration 7, average log likelihood -1.042677
WARNING: Variances had to be floored 3 5 6 7 9 10 19 20
INFO: iteration 8, average log likelihood -0.990231
WARNING: Variances had to be floored 6 15 22 24
INFO: iteration 9, average log likelihood -1.027683
WARNING: Variances had to be floored 5 6 9 19 20
INFO: iteration 10, average log likelihood -1.030426
INFO: EM with 100000 data points 10 iterations avll -1.030426
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0846876   -0.0229628    0.0650566   -0.100569     -0.0575747    0.0696127    0.13848     -0.116167    0.0656151   -0.0615988    0.121584     0.131993      0.1161       0.171967     -0.00380985   0.0199229    0.0102478    0.187511   -0.00635683  -0.00838973   0.173313     0.0948756    0.0566105     0.101738   -0.0822498   -0.0389322 
 -0.0837263   -0.0123805    0.0139469    0.0390003     0.114641    -0.107234    -0.00436484   0.110022    0.0211488    0.00353543   0.128035     0.0297272    -0.0616765    0.000825177  -0.00329299   0.195925     0.0259589    0.0852933   0.162126     0.168545     0.03019     -0.135074     0.0360021     0.193386    0.270046    -0.101692  
  0.14206      0.21011      0.084027     0.136409     -0.0896565   -0.232206     0.00023735  -0.292406   -0.192861     0.0503958   -0.0425589   -0.125282     -0.0703854    0.165922     -0.0899017    0.0208368    0.0839806    0.135327   -0.045253    -0.193606    -0.151265    -0.0598649   -0.0380673    -0.116194    0.0244524   -0.0202164 
 -0.123624     0.0968659    0.173266     0.0165373    -0.0065376   -0.119891    -0.0433569   -0.212656   -0.040947     0.180515    -0.0112873    0.159201      0.157399     0.16396       0.131848     0.163506    -0.182838     0.149063    0.0706044    0.0354361   -0.0529271    0.24309     -0.0226749     0.136605    0.0400917   -0.0209186 
  0.0279053    0.0362328    0.0132488    0.172542     -0.0672255   -0.172262    -0.00476162   0.101939   -0.03974     -0.105521    -0.0320636   -0.00379384    0.0648495   -0.0404992    -0.0478594   -0.113601    -0.160817    -0.150708    0.00874917   0.240653     0.123343    -0.0844442   -0.108842      0.118769   -0.0429886   -0.193876  
 -0.184123    -0.0296068   -0.128756     0.0774849    -0.0157708   -0.0338713    0.0710897   -0.208142   -0.0180384    0.0228012   -0.144519    -0.0700104    -0.108481    -0.00959158    0.179145    -0.0834197    0.0683782    0.12783     0.0461447    0.0478058    0.212814    -0.0779462   -0.0816364    -0.0554861   0.0674843   -0.0356898 
  0.0341863   -0.1138      -0.184626     0.0802516    -0.100361    -0.0132278    0.0349761    0.0367663  -0.0484666    0.0273873    0.13095     -0.11987       0.0493042    0.0699869    -0.139563     0.0761403   -0.186961     0.0611299   0.162133     0.0675234   -0.0439342   -0.0723559   -0.00117632   -0.0694453   0.0780226   -0.117374  
 -0.0652519   -0.0202223    0.00200359   0.0438695    -0.0382971    0.112508    -0.0938694    0.0136564  -0.0720447   -0.204116     0.0357245    0.0140131    -0.17496     -0.0476851    -0.0266869   -0.13662      0.0456737    0.0538456  -0.0324944    0.0415971   -0.0595816    0.053971     0.0755843    -0.0174601  -0.078114    -0.0188666 
 -0.00702422  -0.0587814   -0.0102656   -0.0257479     0.00697521   0.0623644    0.0955918   -0.0735818  -0.00882215   0.0755979    0.0871936   -0.0475048     0.096829     0.0138327     0.0281151   -0.031582    -0.105826     0.047796    0.037851    -0.102403    -0.0109232    0.0186275   -0.0340395    -0.110431    0.0987062    0.0012638 
  0.00753579   0.235029    -0.130215     0.0449035     0.0467167   -0.158188     0.152601     0.0307585  -0.0580869   -0.0668918    0.0566293    0.0951293     0.0588869   -0.0749638     0.0117655   -0.0742308   -0.116011    -0.0134329  -0.00546322   0.0482702   -0.166304    -0.0532148    0.0708596     0.0232615  -0.125291    -0.0245888 
  0.153322     0.0886217    0.130308     0.0947996     0.122158    -0.0204396    0.112023    -0.0369055   0.0391029    0.0394323   -0.163672     0.0953885    -0.0246684    0.158412      0.0492561   -0.0197696   -0.0661141    0.0609934   0.194255     0.190125     0.0299315    0.0488514    0.120682     -0.0799145   0.199883    -0.029641  
  0.20355      0.124506     0.185813    -0.0699725     0.0152656   -0.0975051    0.131444     0.0608558   0.186624    -0.0435023    0.00799357  -0.05038      -0.0114743   -0.13155       0.0362647    0.0245473    0.0805977    0.25179     0.0551072    0.0470445    0.113429     0.0428851   -0.0267912    -0.0350656  -0.0115716    0.0196865 
 -0.1346       0.0454847   -0.275351     0.019914     -0.12147     -0.200578     0.185493     0.0701765   0.141505     0.0670277   -0.0759209   -0.0791134    -0.0645069   -0.177449      0.072603     0.0714359   -0.063622     0.199074    0.0431763   -0.12554     -0.0707496   -0.0411543    0.160799     -0.0259917  -0.00644694   0.0797899 
  0.151693     0.0278178   -0.0157073    0.0820648    -0.0946693    0.016382    -0.0239854    0.0396509  -0.0168425   -0.136408     0.0413587   -0.112889     -0.0872185   -0.169456     -0.00933893  -0.0907555    0.106663    -0.0461329  -0.1036       0.00131932  -0.0179732    0.0322938    0.0335234     0.0147725  -0.071783     0.00979644
  0.0978625    0.0456773    0.0697477   -0.0679502    -0.114043     0.12776     -0.0982948    0.053391   -0.0943437   -0.0873174   -0.0776358   -0.0298618    -0.102052    -0.151758      0.0117309   -0.112787     0.0581976   -0.0154013  -0.0543415    0.036541    -0.0755364    0.153665     0.042223     -0.0417249   0.0374098   -0.019887  
  0.0153248   -0.0201817    0.123569    -0.0448726    -0.00568088   0.0117563   -0.124801     0.0254138  -0.048288     0.141223    -0.131761    -0.114659     -0.0479325    0.102375     -0.0187484    0.0373138    0.056257    -0.0769954  -0.0582491    0.0149511   -0.0698151    0.0148888   -0.0363272     0.0464171  -0.102751    -0.266327  
  0.05659      0.111734     0.238373    -0.150193     -0.19392     -0.109153    -0.128871     0.0575628   0.0023441   -0.0458507   -0.0557842   -0.0994935     0.0248805   -0.0954285    -0.0284308   -0.0541889   -0.122925     0.05208     0.0113693    0.0716594   -0.109246     0.167738     0.0829307    -0.0253929   0.01959     -0.0481792 
  0.00606288  -0.130809     0.00542723   0.0118465    -0.322265     0.142259    -0.060305    -0.0858838  -0.117682    -0.0540609   -0.171568     0.14409      -0.210212     0.0712191    -0.072764    -0.0139019   -0.0044023   -0.0854478  -0.134485     0.101472     0.173194    -0.0543345    0.11579       0.068936   -0.0763036    0.198458  
 -0.0770074    0.0918378    0.123668    -0.000843479  -0.0235818    0.156981    -0.0725933    0.117492   -0.13703     -0.0750619    0.0194888   -0.000130288  -0.106644     0.119436      0.0419064    0.068737     0.00167856   0.0721408   0.0648001   -0.0715087   -0.0439004   -0.0327533   -0.177345     -0.0423179   0.145493    -0.0466139 
 -0.111295    -0.15648      0.0991861    0.1349        0.0424352   -0.0777455   -0.0800546   -0.0702451  -0.0469641    0.024549    -0.0555864    0.16033       0.0957909    0.019858      0.182443    -0.171727    -0.216771    -0.0334258   0.0952455   -0.0340606   -0.0815614   -0.0229214    0.000787153   0.123149   -0.108581     0.0769832 
  0.0547202   -0.163454    -0.0197754   -0.01754      -0.210723    -0.144805     0.00457146   0.142626    0.0829766    0.00482624   0.0883286   -0.0407425     0.0548128    6.62322e-5   -0.0704567   -0.159483     0.0682844    0.0234471  -0.277928     0.132217     0.102641    -0.121163     0.180521     -0.0451743   0.00743958   0.00298761
  0.140365    -0.0473934   -0.0358237   -0.127319     -0.150953    -0.0247545    0.0156659    0.0481661   0.0401739    0.0558533    0.0346519    0.0250335     0.0128644    0.0774593     0.087346     0.207338    -0.14872     -0.057707   -0.157109    -0.0140475   -0.267003     0.0241679   -0.0311822    -0.128956    0.188494     0.0905295 
  0.106457     0.00236386  -0.0640561   -0.123523      0.120573    -0.113276     0.0456209    0.0214616  -0.0423723   -0.101011     0.0565145   -0.129771      0.169289     0.149563      0.10822     -0.153414    -0.0665945   -0.0836667   0.0263656   -0.0211646   -0.139125    -0.00091454  -0.157383      0.0466056   0.242183    -0.0822057 
  0.00713275   0.136858     0.25614      0.200707      0.0699517    0.17301      0.0283089    0.116396    0.00789753  -0.00204936  -0.0405709   -0.0207184     0.114192     0.0674493     0.0882091   -0.0217531    0.0634453   -0.0797932   0.0467882   -0.190661     0.0567262   -0.100331    -0.0640384     0.128293    0.0433134    0.0136647 
 -0.0937958    0.157258     0.0367969    0.125808      0.113498     0.145334    -0.0886009    0.0499459   0.0543993    0.0336124   -0.0661953   -0.00358732   -0.043902    -0.0749647     0.102879    -0.0511532   -0.089495     0.105484   -0.00956869  -0.138313     0.00347972  -0.0175024   -0.0114601     0.0833617   0.0322729    0.0753904 
 -0.0374313   -0.0163941    0.108309    -0.128998      0.00125231   0.0787211   -0.00136481  -0.148125   -0.141292     0.152047    -0.160868    -0.114829      0.0503086   -0.0685619    -0.0166299    0.0655448   -0.100278     0.0459097   0.0174573   -0.0808508   -0.100846     0.00609137  -0.0959562     0.0255644   0.0374229    0.112758  
  0.0195817    0.112097     0.0208272   -0.0440561     0.0665005    0.0504281    0.0204245    0.0433756   0.0558399    0.0106946    0.017281    -0.0701327    -0.0687846   -0.0207029    -0.131573     1.83616e-5  -0.00408687   0.0251707   0.0896247   -0.211508     0.0563958   -0.112655    -0.0694789    -0.0217974  -0.143127     0.0490552 
  0.0578541   -0.122192     0.0247013    0.0186619    -0.0318242   -0.0124286    0.23958      0.0230845   0.137837     0.101883     0.12197      0.226259      0.0665649    0.0455477     0.0539279    0.0255806    0.011813     0.0390329  -0.210985     0.118605     0.0596763   -0.0309072   -0.10524      -0.0452765  -0.0456271   -0.00841114
  0.0174217   -0.11528      0.0554226    0.0590126     0.0890686   -0.0874001    0.158344     0.0782859  -0.103968     0.100646    -0.0457588   -0.242155     -0.117702    -0.149984      0.117845    -0.0633725   -0.0337272   -0.0642676   0.311048    -0.0242416   -0.148807     0.0116196    0.0301673     0.0470213   0.145508     0.197233  
  0.0261999   -0.0477194   -0.00629256  -0.0379117    -0.0667668   -0.00877738   0.0340353   -0.227324   -0.0473095   -0.13144     -0.0502539    0.0312678     0.0401773    0.0133949    -0.0114618    0.0867039   -0.0634366    0.0446815   0.0539975   -0.155998     0.0831136    0.0608171    0.0630043    -0.0535202  -0.0311839    0.0258469 
 -0.177736    -0.120347     0.121714    -0.0851109     0.0880576    0.111139     0.179084    -0.184217    0.0436462    0.153784     0.0710119    0.0323971    -0.061745     0.0333613     0.0343908    0.0830604   -0.117781     0.199408   -0.0211082   -0.191724     0.0126301   -0.00590531   0.043461     -0.0140586   0.0258545    0.0170621 
 -0.0224797   -0.0819552    0.0324656   -0.0573798     0.212547    -0.157613     0.052097     0.064164    0.0598854   -0.0326039   -0.0865689    0.0400874     0.00635511   0.0657999    -0.128835    -0.0705258    0.169102    -0.0738172  -0.0735343   -0.0189616   -0.00959228  -0.102976    -0.104955      0.0302962  -0.00324129   0.176108  kind full, method split
0: avll = -1.4239901368194445
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.424010
INFO: iteration 2, average log likelihood -1.423949
INFO: iteration 3, average log likelihood -1.423908
INFO: iteration 4, average log likelihood -1.423862
INFO: iteration 5, average log likelihood -1.423810
INFO: iteration 6, average log likelihood -1.423747
INFO: iteration 7, average log likelihood -1.423665
INFO: iteration 8, average log likelihood -1.423541
INFO: iteration 9, average log likelihood -1.423315
INFO: iteration 10, average log likelihood -1.422866
INFO: iteration 11, average log likelihood -1.422049
INFO: iteration 12, average log likelihood -1.420896
INFO: iteration 13, average log likelihood -1.419794
INFO: iteration 14, average log likelihood -1.419109
INFO: iteration 15, average log likelihood -1.418802
INFO: iteration 16, average log likelihood -1.418684
INFO: iteration 17, average log likelihood -1.418640
INFO: iteration 18, average log likelihood -1.418623
INFO: iteration 19, average log likelihood -1.418616
INFO: iteration 20, average log likelihood -1.418612
INFO: iteration 21, average log likelihood -1.418611
INFO: iteration 22, average log likelihood -1.418609
INFO: iteration 23, average log likelihood -1.418608
INFO: iteration 24, average log likelihood -1.418608
INFO: iteration 25, average log likelihood -1.418607
INFO: iteration 26, average log likelihood -1.418607
INFO: iteration 27, average log likelihood -1.418606
INFO: iteration 28, average log likelihood -1.418606
INFO: iteration 29, average log likelihood -1.418605
INFO: iteration 30, average log likelihood -1.418605
INFO: iteration 31, average log likelihood -1.418604
INFO: iteration 32, average log likelihood -1.418604
INFO: iteration 33, average log likelihood -1.418604
INFO: iteration 34, average log likelihood -1.418604
INFO: iteration 35, average log likelihood -1.418603
INFO: iteration 36, average log likelihood -1.418603
INFO: iteration 37, average log likelihood -1.418603
INFO: iteration 38, average log likelihood -1.418603
INFO: iteration 39, average log likelihood -1.418603
INFO: iteration 40, average log likelihood -1.418603
INFO: iteration 41, average log likelihood -1.418603
INFO: iteration 42, average log likelihood -1.418602
INFO: iteration 43, average log likelihood -1.418602
INFO: iteration 44, average log likelihood -1.418602
INFO: iteration 45, average log likelihood -1.418602
INFO: iteration 46, average log likelihood -1.418602
INFO: iteration 47, average log likelihood -1.418602
INFO: iteration 48, average log likelihood -1.418602
INFO: iteration 49, average log likelihood -1.418602
INFO: iteration 50, average log likelihood -1.418602
INFO: EM with 100000 data points 50 iterations avll -1.418602
952.4 data points per parameter
1: avll = [-1.42401,-1.42395,-1.42391,-1.42386,-1.42381,-1.42375,-1.42366,-1.42354,-1.42331,-1.42287,-1.42205,-1.4209,-1.41979,-1.41911,-1.4188,-1.41868,-1.41864,-1.41862,-1.41862,-1.41861,-1.41861,-1.41861,-1.41861,-1.41861,-1.41861,-1.41861,-1.41861,-1.41861,-1.41861,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.418618
INFO: iteration 2, average log likelihood -1.418550
INFO: iteration 3, average log likelihood -1.418492
INFO: iteration 4, average log likelihood -1.418423
INFO: iteration 5, average log likelihood -1.418338
INFO: iteration 6, average log likelihood -1.418240
INFO: iteration 7, average log likelihood -1.418135
INFO: iteration 8, average log likelihood -1.418035
INFO: iteration 9, average log likelihood -1.417948
INFO: iteration 10, average log likelihood -1.417877
INFO: iteration 11, average log likelihood -1.417821
INFO: iteration 12, average log likelihood -1.417776
INFO: iteration 13, average log likelihood -1.417740
INFO: iteration 14, average log likelihood -1.417712
INFO: iteration 15, average log likelihood -1.417691
INFO: iteration 16, average log likelihood -1.417674
INFO: iteration 17, average log likelihood -1.417661
INFO: iteration 18, average log likelihood -1.417650
INFO: iteration 19, average log likelihood -1.417641
INFO: iteration 20, average log likelihood -1.417633
INFO: iteration 21, average log likelihood -1.417625
INFO: iteration 22, average log likelihood -1.417617
INFO: iteration 23, average log likelihood -1.417610
INFO: iteration 24, average log likelihood -1.417603
INFO: iteration 25, average log likelihood -1.417596
INFO: iteration 26, average log likelihood -1.417588
INFO: iteration 27, average log likelihood -1.417581
INFO: iteration 28, average log likelihood -1.417574
INFO: iteration 29, average log likelihood -1.417566
INFO: iteration 30, average log likelihood -1.417559
INFO: iteration 31, average log likelihood -1.417551
INFO: iteration 32, average log likelihood -1.417544
INFO: iteration 33, average log likelihood -1.417537
INFO: iteration 34, average log likelihood -1.417529
INFO: iteration 35, average log likelihood -1.417522
INFO: iteration 36, average log likelihood -1.417515
INFO: iteration 37, average log likelihood -1.417508
INFO: iteration 38, average log likelihood -1.417501
INFO: iteration 39, average log likelihood -1.417495
INFO: iteration 40, average log likelihood -1.417488
INFO: iteration 41, average log likelihood -1.417482
INFO: iteration 42, average log likelihood -1.417476
INFO: iteration 43, average log likelihood -1.417471
INFO: iteration 44, average log likelihood -1.417466
INFO: iteration 45, average log likelihood -1.417461
INFO: iteration 46, average log likelihood -1.417456
INFO: iteration 47, average log likelihood -1.417452
INFO: iteration 48, average log likelihood -1.417448
INFO: iteration 49, average log likelihood -1.417444
INFO: iteration 50, average log likelihood -1.417440
INFO: EM with 100000 data points 50 iterations avll -1.417440
473.9 data points per parameter
2: avll = [-1.41862,-1.41855,-1.41849,-1.41842,-1.41834,-1.41824,-1.41813,-1.41804,-1.41795,-1.41788,-1.41782,-1.41778,-1.41774,-1.41771,-1.41769,-1.41767,-1.41766,-1.41765,-1.41764,-1.41763,-1.41762,-1.41762,-1.41761,-1.4176,-1.4176,-1.41759,-1.41758,-1.41757,-1.41757,-1.41756,-1.41755,-1.41754,-1.41754,-1.41753,-1.41752,-1.41751,-1.41751,-1.4175,-1.41749,-1.41749,-1.41748,-1.41748,-1.41747,-1.41747,-1.41746,-1.41746,-1.41745,-1.41745,-1.41744,-1.41744]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.417448
INFO: iteration 2, average log likelihood -1.417385
INFO: iteration 3, average log likelihood -1.417327
INFO: iteration 4, average log likelihood -1.417260
INFO: iteration 5, average log likelihood -1.417177
INFO: iteration 6, average log likelihood -1.417075
INFO: iteration 7, average log likelihood -1.416959
INFO: iteration 8, average log likelihood -1.416837
INFO: iteration 9, average log likelihood -1.416718
INFO: iteration 10, average log likelihood -1.416613
INFO: iteration 11, average log likelihood -1.416526
INFO: iteration 12, average log likelihood -1.416458
INFO: iteration 13, average log likelihood -1.416404
INFO: iteration 14, average log likelihood -1.416363
INFO: iteration 15, average log likelihood -1.416329
INFO: iteration 16, average log likelihood -1.416301
INFO: iteration 17, average log likelihood -1.416277
INFO: iteration 18, average log likelihood -1.416256
INFO: iteration 19, average log likelihood -1.416236
INFO: iteration 20, average log likelihood -1.416217
INFO: iteration 21, average log likelihood -1.416199
INFO: iteration 22, average log likelihood -1.416182
INFO: iteration 23, average log likelihood -1.416166
INFO: iteration 24, average log likelihood -1.416151
INFO: iteration 25, average log likelihood -1.416136
INFO: iteration 26, average log likelihood -1.416123
INFO: iteration 27, average log likelihood -1.416110
INFO: iteration 28, average log likelihood -1.416097
INFO: iteration 29, average log likelihood -1.416085
INFO: iteration 30, average log likelihood -1.416074
INFO: iteration 31, average log likelihood -1.416063
INFO: iteration 32, average log likelihood -1.416053
INFO: iteration 33, average log likelihood -1.416044
INFO: iteration 34, average log likelihood -1.416035
INFO: iteration 35, average log likelihood -1.416026
INFO: iteration 36, average log likelihood -1.416018
INFO: iteration 37, average log likelihood -1.416010
INFO: iteration 38, average log likelihood -1.416003
INFO: iteration 39, average log likelihood -1.415996
INFO: iteration 40, average log likelihood -1.415989
INFO: iteration 41, average log likelihood -1.415982
INFO: iteration 42, average log likelihood -1.415976
INFO: iteration 43, average log likelihood -1.415970
INFO: iteration 44, average log likelihood -1.415964
INFO: iteration 45, average log likelihood -1.415959
INFO: iteration 46, average log likelihood -1.415953
INFO: iteration 47, average log likelihood -1.415948
INFO: iteration 48, average log likelihood -1.415942
INFO: iteration 49, average log likelihood -1.415937
INFO: iteration 50, average log likelihood -1.415932
INFO: EM with 100000 data points 50 iterations avll -1.415932
236.4 data points per parameter
3: avll = [-1.41745,-1.41738,-1.41733,-1.41726,-1.41718,-1.41708,-1.41696,-1.41684,-1.41672,-1.41661,-1.41653,-1.41646,-1.4164,-1.41636,-1.41633,-1.4163,-1.41628,-1.41626,-1.41624,-1.41622,-1.4162,-1.41618,-1.41617,-1.41615,-1.41614,-1.41612,-1.41611,-1.4161,-1.41609,-1.41607,-1.41606,-1.41605,-1.41604,-1.41603,-1.41603,-1.41602,-1.41601,-1.416,-1.416,-1.41599,-1.41598,-1.41598,-1.41597,-1.41596,-1.41596,-1.41595,-1.41595,-1.41594,-1.41594,-1.41593]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.415936
INFO: iteration 2, average log likelihood -1.415879
INFO: iteration 3, average log likelihood -1.415828
INFO: iteration 4, average log likelihood -1.415770
INFO: iteration 5, average log likelihood -1.415698
INFO: iteration 6, average log likelihood -1.415611
INFO: iteration 7, average log likelihood -1.415506
INFO: iteration 8, average log likelihood -1.415387
INFO: iteration 9, average log likelihood -1.415259
INFO: iteration 10, average log likelihood -1.415130
INFO: iteration 11, average log likelihood -1.415005
INFO: iteration 12, average log likelihood -1.414891
INFO: iteration 13, average log likelihood -1.414790
INFO: iteration 14, average log likelihood -1.414702
INFO: iteration 15, average log likelihood -1.414628
INFO: iteration 16, average log likelihood -1.414566
INFO: iteration 17, average log likelihood -1.414515
INFO: iteration 18, average log likelihood -1.414472
INFO: iteration 19, average log likelihood -1.414436
INFO: iteration 20, average log likelihood -1.414405
INFO: iteration 21, average log likelihood -1.414378
INFO: iteration 22, average log likelihood -1.414354
INFO: iteration 23, average log likelihood -1.414332
INFO: iteration 24, average log likelihood -1.414313
INFO: iteration 25, average log likelihood -1.414294
INFO: iteration 26, average log likelihood -1.414276
INFO: iteration 27, average log likelihood -1.414260
INFO: iteration 28, average log likelihood -1.414244
INFO: iteration 29, average log likelihood -1.414229
INFO: iteration 30, average log likelihood -1.414214
INFO: iteration 31, average log likelihood -1.414200
INFO: iteration 32, average log likelihood -1.414186
INFO: iteration 33, average log likelihood -1.414173
INFO: iteration 34, average log likelihood -1.414161
INFO: iteration 35, average log likelihood -1.414148
INFO: iteration 36, average log likelihood -1.414136
INFO: iteration 37, average log likelihood -1.414125
INFO: iteration 38, average log likelihood -1.414114
INFO: iteration 39, average log likelihood -1.414103
INFO: iteration 40, average log likelihood -1.414093
INFO: iteration 41, average log likelihood -1.414083
INFO: iteration 42, average log likelihood -1.414074
INFO: iteration 43, average log likelihood -1.414065
INFO: iteration 44, average log likelihood -1.414056
INFO: iteration 45, average log likelihood -1.414047
INFO: iteration 46, average log likelihood -1.414039
INFO: iteration 47, average log likelihood -1.414031
INFO: iteration 48, average log likelihood -1.414023
INFO: iteration 49, average log likelihood -1.414015
INFO: iteration 50, average log likelihood -1.414008
INFO: EM with 100000 data points 50 iterations avll -1.414008
118.1 data points per parameter
4: avll = [-1.41594,-1.41588,-1.41583,-1.41577,-1.4157,-1.41561,-1.41551,-1.41539,-1.41526,-1.41513,-1.41501,-1.41489,-1.41479,-1.4147,-1.41463,-1.41457,-1.41451,-1.41447,-1.41444,-1.4144,-1.41438,-1.41435,-1.41433,-1.41431,-1.41429,-1.41428,-1.41426,-1.41424,-1.41423,-1.41421,-1.4142,-1.41419,-1.41417,-1.41416,-1.41415,-1.41414,-1.41413,-1.41411,-1.4141,-1.41409,-1.41408,-1.41407,-1.41406,-1.41406,-1.41405,-1.41404,-1.41403,-1.41402,-1.41402,-1.41401]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.414009
INFO: iteration 2, average log likelihood -1.413943
INFO: iteration 3, average log likelihood -1.413881
INFO: iteration 4, average log likelihood -1.413807
INFO: iteration 5, average log likelihood -1.413715
INFO: iteration 6, average log likelihood -1.413601
INFO: iteration 7, average log likelihood -1.413465
INFO: iteration 8, average log likelihood -1.413311
INFO: iteration 9, average log likelihood -1.413151
INFO: iteration 10, average log likelihood -1.412991
INFO: iteration 11, average log likelihood -1.412839
INFO: iteration 12, average log likelihood -1.412700
INFO: iteration 13, average log likelihood -1.412574
INFO: iteration 14, average log likelihood -1.412462
INFO: iteration 15, average log likelihood -1.412363
INFO: iteration 16, average log likelihood -1.412275
INFO: iteration 17, average log likelihood -1.412198
INFO: iteration 18, average log likelihood -1.412129
INFO: iteration 19, average log likelihood -1.412069
INFO: iteration 20, average log likelihood -1.412016
INFO: iteration 21, average log likelihood -1.411968
INFO: iteration 22, average log likelihood -1.411925
INFO: iteration 23, average log likelihood -1.411886
INFO: iteration 24, average log likelihood -1.411850
INFO: iteration 25, average log likelihood -1.411817
INFO: iteration 26, average log likelihood -1.411786
INFO: iteration 27, average log likelihood -1.411757
INFO: iteration 28, average log likelihood -1.411729
INFO: iteration 29, average log likelihood -1.411703
INFO: iteration 30, average log likelihood -1.411678
INFO: iteration 31, average log likelihood -1.411654
INFO: iteration 32, average log likelihood -1.411631
INFO: iteration 33, average log likelihood -1.411609
INFO: iteration 34, average log likelihood -1.411587
INFO: iteration 35, average log likelihood -1.411567
INFO: iteration 36, average log likelihood -1.411547
INFO: iteration 37, average log likelihood -1.411527
INFO: iteration 38, average log likelihood -1.411508
INFO: iteration 39, average log likelihood -1.411489
INFO: iteration 40, average log likelihood -1.411471
INFO: iteration 41, average log likelihood -1.411454
INFO: iteration 42, average log likelihood -1.411437
INFO: iteration 43, average log likelihood -1.411420
INFO: iteration 44, average log likelihood -1.411404
INFO: iteration 45, average log likelihood -1.411388
INFO: iteration 46, average log likelihood -1.411373
INFO: iteration 47, average log likelihood -1.411358
INFO: iteration 48, average log likelihood -1.411343
INFO: iteration 49, average log likelihood -1.411329
INFO: iteration 50, average log likelihood -1.411315
INFO: EM with 100000 data points 50 iterations avll -1.411315
59.0 data points per parameter
5: avll = [-1.41401,-1.41394,-1.41388,-1.41381,-1.41372,-1.4136,-1.41346,-1.41331,-1.41315,-1.41299,-1.41284,-1.4127,-1.41257,-1.41246,-1.41236,-1.41227,-1.4122,-1.41213,-1.41207,-1.41202,-1.41197,-1.41192,-1.41189,-1.41185,-1.41182,-1.41179,-1.41176,-1.41173,-1.4117,-1.41168,-1.41165,-1.41163,-1.41161,-1.41159,-1.41157,-1.41155,-1.41153,-1.41151,-1.41149,-1.41147,-1.41145,-1.41144,-1.41142,-1.4114,-1.41139,-1.41137,-1.41136,-1.41134,-1.41133,-1.41132]
[-1.42399,-1.42401,-1.42395,-1.42391,-1.42386,-1.42381,-1.42375,-1.42366,-1.42354,-1.42331,-1.42287,-1.42205,-1.4209,-1.41979,-1.41911,-1.4188,-1.41868,-1.41864,-1.41862,-1.41862,-1.41861,-1.41861,-1.41861,-1.41861,-1.41861,-1.41861,-1.41861,-1.41861,-1.41861,-1.41861,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.4186,-1.41862,-1.41855,-1.41849,-1.41842,-1.41834,-1.41824,-1.41813,-1.41804,-1.41795,-1.41788,-1.41782,-1.41778,-1.41774,-1.41771,-1.41769,-1.41767,-1.41766,-1.41765,-1.41764,-1.41763,-1.41762,-1.41762,-1.41761,-1.4176,-1.4176,-1.41759,-1.41758,-1.41757,-1.41757,-1.41756,-1.41755,-1.41754,-1.41754,-1.41753,-1.41752,-1.41751,-1.41751,-1.4175,-1.41749,-1.41749,-1.41748,-1.41748,-1.41747,-1.41747,-1.41746,-1.41746,-1.41745,-1.41745,-1.41744,-1.41744,-1.41745,-1.41738,-1.41733,-1.41726,-1.41718,-1.41708,-1.41696,-1.41684,-1.41672,-1.41661,-1.41653,-1.41646,-1.4164,-1.41636,-1.41633,-1.4163,-1.41628,-1.41626,-1.41624,-1.41622,-1.4162,-1.41618,-1.41617,-1.41615,-1.41614,-1.41612,-1.41611,-1.4161,-1.41609,-1.41607,-1.41606,-1.41605,-1.41604,-1.41603,-1.41603,-1.41602,-1.41601,-1.416,-1.416,-1.41599,-1.41598,-1.41598,-1.41597,-1.41596,-1.41596,-1.41595,-1.41595,-1.41594,-1.41594,-1.41593,-1.41594,-1.41588,-1.41583,-1.41577,-1.4157,-1.41561,-1.41551,-1.41539,-1.41526,-1.41513,-1.41501,-1.41489,-1.41479,-1.4147,-1.41463,-1.41457,-1.41451,-1.41447,-1.41444,-1.4144,-1.41438,-1.41435,-1.41433,-1.41431,-1.41429,-1.41428,-1.41426,-1.41424,-1.41423,-1.41421,-1.4142,-1.41419,-1.41417,-1.41416,-1.41415,-1.41414,-1.41413,-1.41411,-1.4141,-1.41409,-1.41408,-1.41407,-1.41406,-1.41406,-1.41405,-1.41404,-1.41403,-1.41402,-1.41402,-1.41401,-1.41401,-1.41394,-1.41388,-1.41381,-1.41372,-1.4136,-1.41346,-1.41331,-1.41315,-1.41299,-1.41284,-1.4127,-1.41257,-1.41246,-1.41236,-1.41227,-1.4122,-1.41213,-1.41207,-1.41202,-1.41197,-1.41192,-1.41189,-1.41185,-1.41182,-1.41179,-1.41176,-1.41173,-1.4117,-1.41168,-1.41165,-1.41163,-1.41161,-1.41159,-1.41157,-1.41155,-1.41153,-1.41151,-1.41149,-1.41147,-1.41145,-1.41144,-1.41142,-1.4114,-1.41139,-1.41137,-1.41136,-1.41134,-1.41133,-1.41132]
32×26 Array{Float64,2}:
  0.490822     0.428382   -0.435995    0.329677     0.15127    -0.347685     0.249872    -0.64112     -0.0374083    0.331498    0.0566438    0.125763    0.256166   -0.277909    -0.231831   -0.0621305   0.598306    -0.0981748   -0.400303     0.225644   -0.317948     0.0380328    0.279454     0.0735219   -0.334021    -0.140224 
  0.187793     0.10142    -0.337144    0.142069     0.181903    0.157621     0.286389     0.47552      0.0831724   -0.0473116  -0.00141622   0.543002    0.421277   -0.0385938    0.0476706  -0.150589    0.627062    -0.370965    -0.656714     0.228137    0.218999    -0.0577229    0.866061     0.0770916    0.225182    -0.181841 
 -0.0746515    0.629313   -0.0662088   0.321973    -0.101237   -0.421766     0.120767     0.413022    -0.516505    -0.209998    0.191367    -0.358655    0.0708545  -0.69401      0.219364   -0.0157814   0.025035     0.591143    -0.0162243    0.0364413   0.448631    -0.649225     0.148613     0.185284     0.13965     -0.340269 
 -0.0802269    0.162897   -0.667848    0.00922392   0.040813   -0.432251    -0.139116    -0.0166881    0.444109    -0.0694042  -0.379424    -0.179482    0.0206491  -0.019567    -0.0255663  -0.871585   -0.00436132   0.349555    -0.270758    -0.173011   -0.219457    -0.551313     0.235994    -0.48097     -0.359971     0.198838 
  0.200682     0.108565   -0.0869022   0.00118758  -0.0434999  -0.183147     0.0859806    0.00208478   0.153289     0.0606443  -0.309186    -0.0804895   0.0599285  -0.0376686    0.0593115  -0.0934957   0.148355    -0.0430358   -0.121292     0.112032   -0.0883562    0.0536977   -0.0183449    0.142777     0.0859599    0.187683 
 -0.139816    -0.0630391   0.0400489   0.163203    -0.0388785  -0.0406717   -0.0120756   -0.0564232   -0.168921    -0.0122181   0.241336     0.0253151  -0.0706819  -0.122491    -0.0325802   0.0921604  -0.161359     0.198104     0.174007     0.0791653  -0.00297698  -0.030167     0.00282607  -0.0961515    0.00480192  -0.208148 
  0.438034     0.135826    0.520217   -0.146375    -0.415136   -0.0565773   -0.0130585   -0.345015     0.145016    -0.173286    0.0615471   -0.0970887  -0.62885    -0.0179773   -0.0037876   0.630401    0.0246746   -0.0278848    0.422663    -0.167212   -0.379446     0.409162    -0.0585344   -0.0287243    0.262225    -0.266339 
  0.277921     0.0149882   0.0590947   0.168987    -0.0362236  -0.0369589   -0.0503914   -0.162086     0.386577     0.403204    0.099442    -0.112532    0.543555    0.469495     0.143511    0.569745   -0.0710686   -0.123376     0.338079    -0.0956824  -0.168789    -0.291496     0.424971    -0.1106       0.320337    -0.508622 
 -0.0829349   -0.0761259  -0.274535   -0.583787     0.017881    0.137292    -0.335564    -0.674469    -0.0427963    0.321492   -0.202177    -0.223138   -0.0341876   0.00267584  -0.0221012   0.0124704  -0.179727    -0.188311     0.00641469  -0.155735    0.243506     0.339799    -0.258183    -0.047469    -0.309454     0.377913 
 -0.262787    -0.915461    0.16937    -0.346816     0.0848556  -0.683365    -0.0841936   -0.491083     0.231142     0.0152466  -0.187673    -0.351183   -0.346632    0.430263     0.249138   -0.190519   -0.0706078   -0.769582     0.317081    -0.387374    0.821145    -0.184879     0.471125    -0.0369312   -0.348807     0.382561 
 -0.246384     0.262002    0.0984706  -0.205212     0.16821    -0.209672    -0.0184804    0.0478484   -0.00552782  -0.192859   -0.509594     0.379859    0.291329   -0.27035      0.467502   -0.395689   -0.0579777   -0.282792    -0.648321     0.643613    0.160343     0.25299     -0.638528     0.294907     0.429335     0.346943 
 -0.0709048   -0.411594   -0.143746    0.0858221    0.355431   -0.0998673   -0.399742    -0.322456    -0.0839769    0.0973244  -0.243616     0.0248818   0.273205    0.164611     0.0777745   0.125622   -0.0426621   -0.127617     1.05232      0.724027    0.0107755   -0.0387741   -0.57433     -0.274618     0.585109     0.0166632
 -0.0640509   -0.305717    0.34281    -0.0845156   -0.0320926  -0.157557    -0.0747943    0.515191    -0.074352    -0.32655    -0.00784815   0.152715   -0.312071    0.347424    -0.402789   -0.258441   -0.279385     0.263046     0.0527281   -0.169792    0.0180077   -0.0829447   -0.0808102   -0.0294471    0.195585    -0.0196533
 -0.314999    -0.578809    0.325893   -0.127479    -0.38898     0.509343     0.043802     0.538377    -0.0147083   -0.333257   -0.288912     0.339378   -0.0659828   0.398663     0.339278   -0.0940571  -0.400363    -0.314044    -0.116123    -0.0570041   0.280539     0.236817    -0.0212167    0.127077     0.112028     0.376915 
 -0.278937     0.57322     0.32223    -0.12764      0.0295911   0.548344     0.145441     0.0196399   -0.135402    -0.135352    0.228436     0.102949    0.0843799  -0.0202937    0.266626    0.0719957   0.0554294    0.107845    -0.171874    -0.644949    0.217999    -0.307173    -0.118552     0.0304418   -0.165014     0.198832 
  0.41992      0.416311    0.563461   -0.055317     0.456551    0.0164644    0.43143      0.45882     -0.0145434   -0.174485    0.381677    -0.305314    0.38561     0.79708      0.107329   -0.160952   -0.048648    -0.246471    -0.350604    -0.234663   -0.278848    -0.255276     0.0515294   -0.00141252  -0.144756     0.374556 
  0.401375     0.180125   -0.447982   -0.177028    -0.0325161  -0.197696    -0.0505024   -0.39153      0.0162486    0.332798   -0.181837     0.0961006  -0.0302359  -0.412634     0.0121877   0.177436    0.605871    -0.124035    -0.266191     0.217006    0.0656927    0.262424     0.289583     0.170813    -0.0451299   -0.106708 
 -0.281736    -0.118763   -0.168166    0.0261975   -0.225549   -0.276367    -0.433009    -0.354139     0.173473    -0.533005   -0.340302     0.121542   -0.773547   -0.322186    -0.117119    0.341175    0.469922     0.156558     1.0485       0.222464    0.207947     0.185546     0.10625     -0.142647     0.460902    -0.296534 
 -0.15229      0.255733   -0.0640667  -0.107305     0.346198    0.390852     0.554006    -0.130139    -0.637001     0.716902    0.404962     0.175138   -0.0329801  -0.494355    -0.113419    0.463503    0.180189     0.445676    -0.0291687    0.581344    0.0228102    0.124894    -0.532504     0.54832      0.41912     -0.0988914
 -0.0325052    0.111732   -0.230765   -0.658029     0.363045    0.178305     0.00687671  -0.0753192    0.211807     0.314649    0.140753     0.263611    0.0113284   0.458381    -0.37644     0.644287    0.540034     0.323709     0.0109486   -0.504263   -0.304298     0.338906    -0.189056     0.499309     0.247612    -0.0281276
 -0.128692     0.0787723  -0.0900688  -0.0861816    0.0491015   0.427493     0.327671     0.0368152    0.162935    -0.373988   -0.572804     0.698448   -0.376617    0.548395     0.214161   -0.321067    0.190901    -0.318865     0.557811     0.439706   -0.479119    -0.142239    -0.244951     0.231914     0.11487      0.700645 
  0.116567     0.317101   -0.116094   -0.0708732   -0.682552    0.0968205    0.351872     0.0873173    0.285691    -0.538462   -0.493688    -0.0408689   0.524194    0.382564     0.160098    0.597033    0.837811    -0.574988     0.202659    -0.33823     0.411329    -0.296937    -0.0234045    0.198159     0.559399     0.688972 
 -0.65519      0.362352   -0.290094   -0.145493     0.325782   -0.251423     0.065837    -0.60748     -0.0618679   -0.294648    0.733303    -0.591228    0.599869    0.351338     0.198734   -0.281238    0.133998    -0.00674541   0.462114    -0.229432    0.217088    -0.429047    -0.214914    -0.331854    -0.037325    -0.0169639
  0.0501936   -0.0498001   0.507898    0.0022629    0.311522    0.281022     0.116839     0.699067    -0.0995384    0.145144    0.136626     0.0503439   0.271424    0.220039     0.105287   -0.0963865  -0.438791    -0.106273    -0.196361    -0.240897    0.0487577   -0.232652     0.087282     0.037878     0.160664     0.157055 
  0.0340308   -0.334757    0.45006     1.06391     -0.148299    0.0954959   -0.0476778    0.401708     0.23973     -0.404009   -0.408741     0.0784471  -0.0187661  -0.0692839    0.211558   -0.125773   -0.673073     0.348811     0.534714    -0.271141   -0.499898    -0.221047     0.00751587  -0.128323    -0.276069    -0.577891 
 -0.317731    -0.604976    0.502864   -0.173598    -0.757424    0.871422    -0.509922     0.143239     0.0488258   -0.459457   -0.385959    -0.0832195  -0.581021    1.07509     -0.209841   -0.350473   -0.570263     0.0690517    0.315652    -0.787301   -0.0805525    0.126385     0.165653    -0.595298    -0.130434     0.0224299
 -0.109216     0.115793   -0.339127    0.400157    -0.294195   -0.139792    -0.349846    -0.310126     0.0825833   -0.443776    0.175774    -0.202761   -0.260676   -0.591422    -0.0978281   0.238402   -0.0962046    0.590029    -0.662689    -0.393988    0.362235     0.291548     0.69939     -0.191725    -1.14152     -0.685126 
  0.00283639  -0.1592      0.368652   -0.714691    -0.786049    0.0289893    0.0612403    0.395866    -0.22166      0.163899    0.270721    -0.923328    0.0172774  -0.125114    -0.109418    0.548586   -0.401404     0.684069     0.177694    -0.408215    0.531226     0.00388995   0.0878249   -0.219756     0.331121     0.069854 
  0.280039    -0.451984    0.47472     0.838715    -0.501427   -0.148638     0.335898    -0.119571     0.0608583    0.128498    0.0967214   -0.259862   -0.19181    -0.506753     0.0602111  -0.551772   -0.256508    -0.566556    -0.31983      0.0359271   0.0156654    0.219397     0.158279    -0.239433    -0.0425609    0.154409 
 -0.122955    -0.267496    0.146087    0.0267349   -0.418261   -0.570425    -0.0273744    0.406455    -0.179138    -0.261163   -0.0778379   -0.0452335  -0.380651   -0.00417895  -0.57831    -0.270664   -0.542041     0.366456    -0.0181464    0.401164   -0.209377     0.405937    -0.665168     0.283119     0.0227089    0.196943 
  0.0377892   -0.382305    0.258807   -0.161112     0.205927   -0.00296882  -0.287717    -0.833455    -0.111524     0.454196    0.301955    -0.2452     -0.674064   -0.29156      0.133844    0.0476682  -0.936451     0.446842     0.453065     0.0670595  -0.501637     0.134895    -0.385736    -0.0690254   -0.475353    -0.278517 
  0.102803    -0.332877    0.122228    0.160284     0.591924    0.0594674   -0.0233628    0.283253    -0.356074     0.772864    0.197205     0.0644401   0.105864   -0.163633    -0.0265467  -0.484033   -0.704578     0.127405    -0.52569      0.265023   -0.486278     0.35413     -0.00618454  -0.121565    -0.345605    -0.226194 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.411302
INFO: iteration 2, average log likelihood -1.411289
INFO: iteration 3, average log likelihood -1.411276
INFO: iteration 4, average log likelihood -1.411263
INFO: iteration 5, average log likelihood -1.411251
INFO: iteration 6, average log likelihood -1.411239
INFO: iteration 7, average log likelihood -1.411228
INFO: iteration 8, average log likelihood -1.411216
INFO: iteration 9, average log likelihood -1.411205
INFO: iteration 10, average log likelihood -1.411195
INFO: EM with 100000 data points 10 iterations avll -1.411195
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.251965e+05
      1       7.060763e+05      -2.191202e+05 |       32
      2       6.928070e+05      -1.326931e+04 |       32
      3       6.879371e+05      -4.869931e+03 |       32
      4       6.850226e+05      -2.914507e+03 |       32
      5       6.830207e+05      -2.001930e+03 |       32
      6       6.816940e+05      -1.326704e+03 |       32
      7       6.808535e+05      -8.404793e+02 |       32
      8       6.802269e+05      -6.265639e+02 |       32
      9       6.797008e+05      -5.260792e+02 |       32
     10       6.792535e+05      -4.473041e+02 |       32
     11       6.788923e+05      -3.612208e+02 |       32
     12       6.786149e+05      -2.773777e+02 |       32
     13       6.783813e+05      -2.336114e+02 |       32
     14       6.781572e+05      -2.241024e+02 |       32
     15       6.779499e+05      -2.073496e+02 |       32
     16       6.777702e+05      -1.796321e+02 |       32
     17       6.775938e+05      -1.764689e+02 |       32
     18       6.774177e+05      -1.760296e+02 |       32
     19       6.772562e+05      -1.615492e+02 |       32
     20       6.771091e+05      -1.471355e+02 |       32
     21       6.769690e+05      -1.401126e+02 |       32
     22       6.768381e+05      -1.308776e+02 |       32
     23       6.767161e+05      -1.219749e+02 |       32
     24       6.766099e+05      -1.062002e+02 |       32
     25       6.765116e+05      -9.825664e+01 |       32
     26       6.764289e+05      -8.269276e+01 |       32
     27       6.763526e+05      -7.638965e+01 |       32
     28       6.762785e+05      -7.401045e+01 |       32
     29       6.761985e+05      -8.007515e+01 |       32
     30       6.761157e+05      -8.278344e+01 |       32
     31       6.760254e+05      -9.025751e+01 |       32
     32       6.759405e+05      -8.493843e+01 |       32
     33       6.758546e+05      -8.589513e+01 |       32
     34       6.757742e+05      -8.035788e+01 |       32
     35       6.756979e+05      -7.629997e+01 |       32
     36       6.756226e+05      -7.536451e+01 |       32
     37       6.755506e+05      -7.199090e+01 |       32
     38       6.754805e+05      -7.010803e+01 |       32
     39       6.754223e+05      -5.822035e+01 |       32
     40       6.753613e+05      -6.096570e+01 |       32
     41       6.753106e+05      -5.066546e+01 |       32
     42       6.752659e+05      -4.473520e+01 |       32
     43       6.752184e+05      -4.747565e+01 |       32
     44       6.751685e+05      -4.995024e+01 |       32
     45       6.751244e+05      -4.411099e+01 |       32
     46       6.750871e+05      -3.724902e+01 |       32
     47       6.750547e+05      -3.244384e+01 |       32
     48       6.750244e+05      -3.030911e+01 |       32
     49       6.750000e+05      -2.439070e+01 |       32
     50       6.749778e+05      -2.212340e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 674977.8389919862)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.423203
INFO: iteration 2, average log likelihood -1.418133
INFO: iteration 3, average log likelihood -1.416723
INFO: iteration 4, average log likelihood -1.415630
INFO: iteration 5, average log likelihood -1.414494
INFO: iteration 6, average log likelihood -1.413510
INFO: iteration 7, average log likelihood -1.412869
INFO: iteration 8, average log likelihood -1.412510
INFO: iteration 9, average log likelihood -1.412298
INFO: iteration 10, average log likelihood -1.412158
INFO: iteration 11, average log likelihood -1.412053
INFO: iteration 12, average log likelihood -1.411970
INFO: iteration 13, average log likelihood -1.411900
INFO: iteration 14, average log likelihood -1.411840
INFO: iteration 15, average log likelihood -1.411787
INFO: iteration 16, average log likelihood -1.411740
INFO: iteration 17, average log likelihood -1.411696
INFO: iteration 18, average log likelihood -1.411657
INFO: iteration 19, average log likelihood -1.411621
INFO: iteration 20, average log likelihood -1.411587
INFO: iteration 21, average log likelihood -1.411555
INFO: iteration 22, average log likelihood -1.411526
INFO: iteration 23, average log likelihood -1.411498
INFO: iteration 24, average log likelihood -1.411472
INFO: iteration 25, average log likelihood -1.411448
INFO: iteration 26, average log likelihood -1.411425
INFO: iteration 27, average log likelihood -1.411402
INFO: iteration 28, average log likelihood -1.411381
INFO: iteration 29, average log likelihood -1.411361
INFO: iteration 30, average log likelihood -1.411341
INFO: iteration 31, average log likelihood -1.411323
INFO: iteration 32, average log likelihood -1.411304
INFO: iteration 33, average log likelihood -1.411287
INFO: iteration 34, average log likelihood -1.411270
INFO: iteration 35, average log likelihood -1.411253
INFO: iteration 36, average log likelihood -1.411237
INFO: iteration 37, average log likelihood -1.411221
INFO: iteration 38, average log likelihood -1.411205
INFO: iteration 39, average log likelihood -1.411190
INFO: iteration 40, average log likelihood -1.411174
INFO: iteration 41, average log likelihood -1.411159
INFO: iteration 42, average log likelihood -1.411144
INFO: iteration 43, average log likelihood -1.411130
INFO: iteration 44, average log likelihood -1.411115
INFO: iteration 45, average log likelihood -1.411100
INFO: iteration 46, average log likelihood -1.411086
INFO: iteration 47, average log likelihood -1.411072
INFO: iteration 48, average log likelihood -1.411058
INFO: iteration 49, average log likelihood -1.411044
INFO: iteration 50, average log likelihood -1.411031
INFO: EM with 100000 data points 50 iterations avll -1.411031
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0549876    0.334829     0.00351837  -0.30724     0.177854     0.445515     0.142786   -0.343781     0.188795    0.395851    0.135925    0.342477     -0.0128256   0.359849    -0.157873     0.843838     0.608685    0.164002     0.249624   -0.439389    -0.384436    0.228396    0.00805484   0.403165    0.231338     -0.245952 
  0.0918632    0.359673     0.0713245    0.581373   -0.372499    -0.0713156    0.434614    0.170887    -0.0880747  -0.60939     0.562344    0.152231     -0.381164   -0.556326    -0.0444173    0.03119      0.0976075   0.191974    -0.776894   -0.408759     0.0225952   0.24878     0.658861     0.0836016  -0.667417     -0.26074  
 -0.149813    -0.027685     0.124951    -0.0595292  -0.0159828    0.0758386   -0.081534    0.00387419  -0.148078   -0.0037884   0.0832831   0.0395825    -0.120907    0.0357496   -0.0333655   -0.0159163   -0.142093    0.155021     0.0541662  -0.0922991    0.0634305   0.0539381  -0.13028     -0.0470747   0.000315195   0.0260625
 -0.163118    -0.237213    -0.295044     0.35209     0.313762     0.150031     0.124638   -0.0395358   -0.0196009   0.382354   -0.0197667   0.497925     -0.0624491  -0.301088     0.0182115   -0.316747    -0.46901     0.0840006    0.0512113   0.523007    -0.600914    0.134999    0.127698    -0.271982   -0.248975     -0.289933 
 -0.688869     0.54864     -0.583053    -0.201359    0.230553    -0.472358    -0.0354549  -0.5701      -0.436443   -0.46981     0.568203   -0.629264      0.28675     0.542076     0.0304476   -0.105098     0.726763    0.0888758    0.415437   -0.200115     0.174161   -0.222754   -0.337921    -0.0239203  -0.409619     -0.287049 
  0.687706     0.0471056   -0.589856     0.223703   -0.0559051   -0.0309686    0.484595   -0.473496     0.0191863   0.334197    0.511379    0.134908      0.535054   -0.196211    -0.135856    -0.135373     1.03738    -0.207222    -0.58459     0.424437     0.273526    0.020053    0.918479    -0.227192    0.034543     -0.20959  
 -0.213757    -0.237063     0.594786     0.291774   -0.156145    -0.251027    -0.117541    0.513692     0.470296   -0.249734   -0.232765    0.185462     -0.367566    0.107694    -0.0459579    0.0583153   -0.886985    0.0175603    0.262598   -0.195446    -0.440364    0.0684718  -0.581262     0.425693   -0.0141683    -0.104129 
  0.0559926    0.0957176   -0.129781    -0.102644    0.00549486  -0.0938993    0.193737   -0.112877    -0.51534     0.627337    0.40995    -0.0406153    -0.204473   -0.911438    -0.151293     0.300451     0.0322343   0.419728    -0.160989    0.423857     0.102271    0.165319   -0.192712     0.395409    0.20744      -0.285849 
 -0.0918309   -0.271898    -0.169533    -0.891204    0.559174    -0.574423    -0.408204    0.268364    -0.105483   -0.237409    0.109478   -0.0952383    -0.0141399   0.355194    -0.428171    -0.229366     0.12943     0.484704    -0.147332   -0.0838472    0.152715   -0.0035579  -0.708114     0.109397    0.508969      0.703979 
 -0.0440737    0.00654701   0.389428    -0.743127   -0.727419     0.0827663    0.073048    0.11215     -0.216277   -0.0324182   0.269908   -0.830827     -0.222898   -0.0469518    0.0106681    0.55425     -0.408521    0.66391      0.410623   -0.420121     0.326466    0.0278384  -0.195431    -0.213361    0.246717      0.153192 
 -0.0203932    0.670572     0.384023    -0.179173    0.218836     0.281236     0.66731     0.502074    -0.533523    0.0995869   0.428287    0.216609      0.34502     0.214069     0.0748448    0.0838795   -0.229678    0.30425     -0.419064    0.234663    -0.135151   -0.159058   -0.422695     0.51447     0.227617      0.0313214
  0.0311954   -0.13709     -0.0279792    0.625358   -0.350984    -0.59759      0.0542174   0.295362    -0.219876    0.107602    0.0410838  -0.73773       0.270391   -0.231342    -0.0888885   -0.462882    -0.49976     0.0948989   -0.349852    0.347265     0.194886   -0.118731   -0.444461    -0.236118   -0.0716776     0.0113915
 -0.222282     0.595522    -0.716692    -0.153055    0.573811    -0.214783     0.301146    0.282753     0.0243659   0.434904    0.0244064  -0.000399868   0.482141    0.130228     0.0801724   -0.483562     0.344746    0.100518    -0.605335   -0.00636926  -0.142081   -0.390303    0.508054     0.0416225  -0.276201      0.353618 
 -0.0708811   -0.0602152    0.147564     0.172936    0.0103791    0.129018     0.0155456   0.139926     0.293814   -0.236098   -0.100095   -0.00380378    0.051875    0.456164     0.109654    -0.111867    -0.361749   -0.0744218    0.342632   -0.0514317   -0.256155   -0.235409   -0.0259063   -0.19152     0.0100924     0.101653 
 -0.351558    -0.919105     0.44746     -0.189412   -0.814527     0.949111    -0.0133398   0.4945      -0.46178    -0.657517   -0.411463    0.144062     -1.13672     0.560579    -0.136666    -0.547801    -0.462612    0.17741      0.472542   -0.262895     0.053636    0.204892    0.181266    -0.258026   -0.129269      0.250416 
  0.261728    -0.761124     0.577387     0.109905    0.39524      0.250227     0.0818883   0.433871    -0.271084    0.519697    0.136726   -0.0399682     0.0155783   0.0843638    0.140527    -0.544241    -0.581147   -0.211718    -0.458791    0.32823     -0.217212    0.46535     0.0920249    0.175924    0.00579194    0.145843 
  0.144454     0.245316    -0.232447    -0.242324   -0.374723    -0.103498     0.269664    0.319469     0.110802   -0.0914695  -0.153283   -0.049378      0.694347    0.196247     0.102123     0.461572     0.705452   -0.559799    -0.164146   -0.100433     0.617616   -0.0417816   0.167992     0.151792    0.811798      0.679258 
  0.0302836    0.33542      0.627085    -0.102923    0.603779     0.0681369    0.195097    0.0662416   -0.0721822   0.15992     0.643691   -0.300726      0.253924   -0.00258886   0.157618    -0.290212    -0.318996   -0.251786    -0.0995546  -0.942338     0.173535   -0.603837    0.110139    -0.448994   -0.306377     -0.0284276
 -0.13828      0.16542      0.0857159    0.0346228  -0.195344     0.449933     0.289207    0.0400345    0.332439   -0.637796   -0.398293    0.473652     -0.094313    0.56058      0.273133    -0.191804     0.346338   -0.56779      0.215344    0.162898    -0.0912752  -0.193635   -0.083212     0.107045    0.258299      0.726983 
  0.0684772    0.163348    -0.0692061   -0.0717449  -0.0364177   -0.222478     0.20231     0.0138629    0.261074   -0.112219   -0.172092   -0.21901       0.109032   -0.0299752    0.205962     0.180037     0.263213   -0.153034     0.126659    0.109886     0.241312   -0.0767273   0.0270304    0.429861    0.326921      0.186212 
  0.505958    -0.12246      0.145694     0.147758   -0.0671168   -0.146379     0.0606443   0.0265305    0.0704957   0.124203    0.0112584  -0.0177328     0.203005    0.178635    -0.091739     0.270163     0.0225574  -0.0792554    0.100924   -0.0344089   -0.154763   -0.125204    0.554738    -0.130939    0.0810752    -0.458755 
 -0.244038    -0.178971     0.435157    -0.0888652  -0.437464     0.38831     -0.0872395   0.313929     0.316695   -0.106839    0.016048    0.000917082   0.410075    0.631043     0.148876    -0.00439351  -0.271942   -0.114538    -0.518854   -0.82677      0.195644    0.0093993   0.402956    -0.197135   -0.113355      0.362351 
  0.158892     0.602075     0.121006     0.45731     0.495417    -0.120527    -0.192219   -0.235406     0.238032   -0.0687811  -0.652382    0.786349      0.571909   -0.347455     0.485592    -0.323502     0.302995   -0.637962    -0.826344    0.25835     -0.296505    0.171388   -0.24163      0.0654127  -0.132631     -0.405627 
  0.560726     0.263271    -0.121779    -0.0480619  -0.0878393   -0.305458     0.0829674  -0.326298     0.0458747   0.147992   -0.242196   -0.1114       -0.19959    -0.267522    -0.153622    -0.33129      0.235787    0.109873    -0.27946     0.0131813   -0.462832    0.191108   -0.269716    -0.018185   -0.106933      0.312515 
  0.420513     0.0527      -0.567091    -0.15312    -0.22451     -0.31156     -0.122016    0.106754     0.243565   -0.0889709  -1.21707     0.214172     -0.396055   -0.116698    -0.168083    -0.184652     0.438172    0.103839     0.134701    0.342235    -0.371441    0.249553    0.168226     0.409254   -0.151015      0.0400906
 -0.0833031    0.449804     0.109145     0.606808   -0.135214    -0.102898    -0.020124    0.441243    -0.269715   -0.457466   -0.0906554  -0.129549      0.074377   -0.344444     0.302847    -0.179576    -0.0791672   0.600607     0.297683   -0.105997     0.218692   -0.851197    0.196402    -0.219178    0.216342     -0.445528 
 -0.227213    -0.00874859  -0.478846    -0.103423   -0.133253    -0.27293     -0.386012   -0.2709       0.153351   -0.175641   -0.185724   -0.433329     -0.187184   -0.701508     0.0758099    0.250322     0.27514     0.24541     -0.213619   -0.275425     0.982246   -0.0533817   0.778587     0.104901   -0.445423     -0.43555  
  0.15053     -0.234648     0.00611465   0.183142   -0.0509716   -0.159216    -0.617058   -0.531666    -0.15292     0.0779434   0.142809   -0.332159     -0.297728   -0.207935    -0.257761    -0.0291246   -0.763646    0.882294    -0.0499531  -0.292133    -0.416719    0.314223    0.02181     -0.406785   -0.852098     -0.612249 
 -0.037291    -0.260147     0.202713     0.259912   -0.106099    -0.211968    -0.500089   -0.455173     0.364785   -0.183407    0.0854206  -0.118531     -0.348557    0.0347533   -0.00130877   0.302291     0.0295159   0.0589419    1.19102     0.425994    -0.172576    0.158941   -0.221573    -0.311172    0.5865       -0.323366 
 -0.434616    -0.435551     0.0556174   -0.163148   -0.100383     0.00315186  -0.0421343  -0.12615     -0.605964   -0.464606   -0.413343    0.992532     -0.339413   -0.191404     0.0356444    0.0948309    0.268549    0.00263492   0.307394    0.611083     0.621633    0.404712   -0.331498     0.115079    0.701879      0.0329269
 -0.235991    -0.00381659  -0.310387    -0.301157    0.325403     0.272485    -0.0511729  -0.704918    -0.15431     0.484828   -0.158997   -0.16566       0.276389   -0.0756017    0.294819     0.223048    -0.161022   -0.428077     0.27052     0.296638     0.111123    0.0155415  -0.645436     0.0372298   0.0369559     0.392345 
 -0.00354146  -0.917832     0.287511     0.0206     -0.0423375   -0.576469    -0.0839344  -0.6692       0.248039    0.11082    -0.24881    -0.299694     -0.413084    0.229043    -0.0952917   -0.397012    -0.26757    -1.03448      0.291543   -0.133512     0.365718    0.0244887   0.222964    -0.38803    -0.330416      0.473899 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.411017
INFO: iteration 2, average log likelihood -1.411004
INFO: iteration 3, average log likelihood -1.410991
INFO: iteration 4, average log likelihood -1.410979
INFO: iteration 5, average log likelihood -1.410966
INFO: iteration 6, average log likelihood -1.410954
INFO: iteration 7, average log likelihood -1.410942
INFO: iteration 8, average log likelihood -1.410931
INFO: iteration 9, average log likelihood -1.410919
INFO: iteration 10, average log likelihood -1.410908
INFO: EM with 100000 data points 10 iterations avll -1.410908
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
