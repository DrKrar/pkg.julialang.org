>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.6.6
INFO: Installing JLD v0.6.5
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.0
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.2.0
INFO: Installing StaticArrays v0.0.11
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1178
Commit 421f079 (2016-11-02 19:28 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-100-generic #147-Ubuntu SMP Tue Oct 18 16:48:51 UTC 2016 x86_64 x86_64
Memory: 2.939289093017578 GB (670.8359375 MB free)
Uptime: 23064.0 sec
Load Avg:  0.9619140625  0.9638671875  1.0185546875
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3500 MHz    1477586 s       2272 s     136620 s     440806 s         57 s
#2  3500 MHz     606800 s       5005 s      75252 s    1548629 s          1 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.3
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.6.6
 - JLD                           0.6.5
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.0
 - Rmath                         0.1.4
 - SHA                           0.2.1
 - ScikitLearnBase               0.2.0
 - StaticArrays                  0.0.11
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-1.5000599315073339e6,[41174.1,58825.9],
[-3784.49 -7490.56 -35170.7; 3881.54 7002.2 35174.4],

Array{Float64,2}[
[40210.9 -511.644 -765.681; -511.644 48349.6 -2235.45; -765.681 -2235.45 55799.7],

[59324.5 485.835 1054.72; 485.835 51884.6 2327.22; 1054.72 2327.22 44731.1]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.167781e+03
      1       9.213688e+02      -2.464121e+02 |        6
      2       8.952759e+02      -2.609298e+01 |        2
      3       8.879839e+02      -7.291999e+00 |        0
      4       8.879839e+02       0.000000e+00 |        0
K-means converged with 4 iterations (objv = 887.9838687712409)
INFO: K-means with 272 data points using 4 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.076245
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.801202
INFO: iteration 2, lowerbound -3.666900
INFO: iteration 3, lowerbound -3.531513
INFO: iteration 4, lowerbound -3.386114
INFO: iteration 5, lowerbound -3.242220
INFO: iteration 6, lowerbound -3.110256
INFO: dropping number of Gaussions to 7
INFO: iteration 7, lowerbound -2.991834
INFO: iteration 8, lowerbound -2.903284
INFO: dropping number of Gaussions to 5
INFO: iteration 9, lowerbound -2.831651
INFO: dropping number of Gaussions to 4
INFO: iteration 10, lowerbound -2.776073
INFO: iteration 11, lowerbound -2.742762
INFO: dropping number of Gaussions to 3
INFO: iteration 12, lowerbound -2.718645
INFO: iteration 13, lowerbound -2.689661
INFO: iteration 14, lowerbound -2.658428
INFO: iteration 15, lowerbound -2.621577
INFO: iteration 16, lowerbound -2.580419
INFO: iteration 17, lowerbound -2.537094
INFO: iteration 18, lowerbound -2.494154
INFO: iteration 19, lowerbound -2.453739
INFO: iteration 20, lowerbound -2.416798
INFO: iteration 21, lowerbound -2.383077
INFO: iteration 22, lowerbound -2.352340
INFO: iteration 23, lowerbound -2.326519
INFO: iteration 24, lowerbound -2.310495
INFO: iteration 25, lowerbound -2.308118
INFO: dropping number of Gaussions to 2
INFO: iteration 26, lowerbound -2.302915
INFO: iteration 27, lowerbound -2.299259
INFO: iteration 28, lowerbound -2.299256
INFO: iteration 29, lowerbound -2.299254
INFO: iteration 30, lowerbound -2.299254
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Thu 03 Nov 2016 10:53:46 AM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Thu 03 Nov 2016 10:53:48 AM UTC: K-means with 272 data points using 4 iterations
11.3 data points per parameter
,Thu 03 Nov 2016 10:53:49 AM UTC: EM with 272 data points 0 iterations avll -2.076245
5.8 data points per parameter
,Thu 03 Nov 2016 10:53:50 AM UTC: GMM converted to Variational GMM
,Thu 03 Nov 2016 10:53:52 AM UTC: iteration 1, lowerbound -3.801202
,Thu 03 Nov 2016 10:53:52 AM UTC: iteration 2, lowerbound -3.666900
,Thu 03 Nov 2016 10:53:52 AM UTC: iteration 3, lowerbound -3.531513
,Thu 03 Nov 2016 10:53:52 AM UTC: iteration 4, lowerbound -3.386114
,Thu 03 Nov 2016 10:53:52 AM UTC: iteration 5, lowerbound -3.242220
,Thu 03 Nov 2016 10:53:52 AM UTC: iteration 6, lowerbound -3.110256
,Thu 03 Nov 2016 10:53:52 AM UTC: dropping number of Gaussions to 7
,Thu 03 Nov 2016 10:53:52 AM UTC: iteration 7, lowerbound -2.991834
,Thu 03 Nov 2016 10:53:52 AM UTC: iteration 8, lowerbound -2.903284
,Thu 03 Nov 2016 10:53:52 AM UTC: dropping number of Gaussions to 5
,Thu 03 Nov 2016 10:53:52 AM UTC: iteration 9, lowerbound -2.831651
,Thu 03 Nov 2016 10:53:53 AM UTC: dropping number of Gaussions to 4
,Thu 03 Nov 2016 10:53:53 AM UTC: iteration 10, lowerbound -2.776073
,Thu 03 Nov 2016 10:53:53 AM UTC: iteration 11, lowerbound -2.742762
,Thu 03 Nov 2016 10:53:53 AM UTC: dropping number of Gaussions to 3
,Thu 03 Nov 2016 10:53:53 AM UTC: iteration 12, lowerbound -2.718645
,Thu 03 Nov 2016 10:53:53 AM UTC: iteration 13, lowerbound -2.689661
,Thu 03 Nov 2016 10:53:53 AM UTC: iteration 14, lowerbound -2.658428
,Thu 03 Nov 2016 10:53:53 AM UTC: iteration 15, lowerbound -2.621577
,Thu 03 Nov 2016 10:53:53 AM UTC: iteration 16, lowerbound -2.580419
,Thu 03 Nov 2016 10:53:53 AM UTC: iteration 17, lowerbound -2.537094
,Thu 03 Nov 2016 10:53:53 AM UTC: iteration 18, lowerbound -2.494154
,Thu 03 Nov 2016 10:53:53 AM UTC: iteration 19, lowerbound -2.453739
,Thu 03 Nov 2016 10:53:53 AM UTC: iteration 20, lowerbound -2.416798
,Thu 03 Nov 2016 10:53:53 AM UTC: iteration 21, lowerbound -2.383077
,Thu 03 Nov 2016 10:53:53 AM UTC: iteration 22, lowerbound -2.352340
,Thu 03 Nov 2016 10:53:54 AM UTC: iteration 23, lowerbound -2.326519
,Thu 03 Nov 2016 10:53:54 AM UTC: iteration 24, lowerbound -2.310495
,Thu 03 Nov 2016 10:53:54 AM UTC: iteration 25, lowerbound -2.308118
,Thu 03 Nov 2016 10:53:54 AM UTC: dropping number of Gaussions to 2
,Thu 03 Nov 2016 10:53:54 AM UTC: iteration 26, lowerbound -2.302915
,Thu 03 Nov 2016 10:53:54 AM UTC: iteration 27, lowerbound -2.299259
,Thu 03 Nov 2016 10:53:54 AM UTC: iteration 28, lowerbound -2.299256
,Thu 03 Nov 2016 10:53:54 AM UTC: iteration 29, lowerbound -2.299254
,Thu 03 Nov 2016 10:53:54 AM UTC: iteration 30, lowerbound -2.299254
,Thu 03 Nov 2016 10:53:54 AM UTC: iteration 31, lowerbound -2.299253
,Thu 03 Nov 2016 10:53:54 AM UTC: iteration 32, lowerbound -2.299253
,Thu 03 Nov 2016 10:53:54 AM UTC: iteration 33, lowerbound -2.299253
,Thu 03 Nov 2016 10:53:54 AM UTC: iteration 34, lowerbound -2.299253
,Thu 03 Nov 2016 10:53:54 AM UTC: iteration 35, lowerbound -2.299253
,Thu 03 Nov 2016 10:53:54 AM UTC: iteration 36, lowerbound -2.299253
,Thu 03 Nov 2016 10:53:54 AM UTC: iteration 37, lowerbound -2.299253
,Thu 03 Nov 2016 10:53:55 AM UTC: iteration 38, lowerbound -2.299253
,Thu 03 Nov 2016 10:53:55 AM UTC: iteration 39, lowerbound -2.299253
,Thu 03 Nov 2016 10:53:55 AM UTC: iteration 40, lowerbound -2.299253
,Thu 03 Nov 2016 10:53:55 AM UTC: iteration 41, lowerbound -2.299253
,Thu 03 Nov 2016 10:53:55 AM UTC: iteration 42, lowerbound -2.299253
,Thu 03 Nov 2016 10:53:55 AM UTC: iteration 43, lowerbound -2.299253
,Thu 03 Nov 2016 10:53:55 AM UTC: iteration 44, lowerbound -2.299253
,Thu 03 Nov 2016 10:53:55 AM UTC: iteration 45, lowerbound -2.299253
,Thu 03 Nov 2016 10:53:55 AM UTC: iteration 46, lowerbound -2.299253
,Thu 03 Nov 2016 10:53:55 AM UTC: iteration 47, lowerbound -2.299253
,Thu 03 Nov 2016 10:53:55 AM UTC: iteration 48, lowerbound -2.299253
,Thu 03 Nov 2016 10:53:55 AM UTC: iteration 49, lowerbound -2.299253
,Thu 03 Nov 2016 10:53:55 AM UTC: iteration 50, lowerbound -2.299253
,Thu 03 Nov 2016 10:53:55 AM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.9549,178.045]
β = [95.9549,178.045]
m = [2.00023 53.852; 4.2503 79.2869]
ν = [97.9549,180.045]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.375876 -0.00895312; 0.0 0.0127487],

[0.184042 -0.00764405; 0.0 0.00858171]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.00000000012
avll from stats: -0.992535502676742
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9925355026767428
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9925355026767428
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
WARNING: is is deprecated, use === instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in is(::Array{Float64,2}, ::Vararg{Array{Float64,2},N}) at ./deprecated.jl:30
 in _rcopy!(::Array{Float64,2}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/PDMats/src/utils.jl:10
 in unwhiten!(::Array{Float64,2}, ::PDMats.PDMat{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/PDMats/src/pdmat.jl:60
 in rand(::Distributions.MvNormal{Float64,PDMats.PDMat{Float64,Array{Float64,2}},Array{Float64,1}}, ::Int64) at /home/vagrant/.julia/v0.6/Distributions/src/multivariates.jl:18
 in rand(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:60
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.00000000001
avll from stats: -0.9658355687534533
avll from llpg:  -0.9658355687534531
avll direct:     -0.9658355687534532
sum posterior: 100000.0
32×26 Array{Float64,2}:
 -0.100732    -0.0262215    -0.0328633   -0.0751171    0.0165482   0.0987164   -0.0327435   -0.0870688   -0.103119     -0.135238     0.13066       0.0975824    0.124        -0.119242     -0.0325455    0.212197     0.200245     -0.124325    -0.101009    -0.072512    -0.0835531   -0.0351445   0.0138741    0.048044     0.0590501    0.0240235 
  0.0413691   -0.115495     -0.126868     0.215724     0.033181    0.0248007    0.121267    -0.0126793    0.215879      0.0733884   -0.0217324     0.112744     0.0312907    -0.25512       0.00605494  -0.0275077   -0.120359      0.126733     0.0267303   -0.00301364  -0.0866459    0.162689   -0.0463808    0.0631757   -0.172264    -0.0573632 
 -0.185192    -0.297497      0.0797444    0.142857     0.100096   -0.082759    -0.0653398   -0.0257943    0.129866     -0.0319813   -0.227922     -0.0534349    0.247568      0.0678631     0.193077     0.0201444    0.0198389     0.0592606   -0.128773     0.132132     0.0135623   -0.0360524  -0.129061    -0.00167645  -0.248642    -0.0231925 
 -0.0449779   -0.112429      0.0165664   -0.119599     0.079048   -0.127158     0.0634965    0.135254     0.0412804     0.0239892    0.057288     -0.130416     0.101633      0.0486189    -0.0521431   -0.0608385   -0.0286346     0.180329     0.00522008  -0.0427872    0.0785502   -0.0837589  -0.247285    -0.10688      0.0503324   -0.0525644 
 -0.0440179   -0.00205257   -0.00980907  -0.207576    -0.116529   -0.0623216    0.0265679   -0.0931003   -0.0363009     0.0526305   -0.0323843    -0.021141    -0.0310945    -0.0850175    -0.0288072    0.0115099   -0.0214055    -0.0011075   -0.146055    -0.130151    -0.00460957  -0.146773   -0.111417     0.185211    -0.0976766    0.00366562
 -0.0206706   -0.13685      -0.0332231    0.0685781   -0.0754818   0.00195701  -0.142915     0.081128     0.0421386     0.0208655   -0.00292115   -0.002998    -0.0656054    -0.0122171     0.0848587   -0.184812     0.0325323     0.154288    -0.0578858   -0.0373694    0.0323219    0.06295    -0.125485     0.126054     0.0087536    0.0234698 
  0.138076     0.000914765  -0.143186     0.0262462    0.0645851   0.0897268   -0.0418932    0.0738954   -0.0646348     0.096286     0.0617535    -0.0724896   -0.113072      0.243852      0.102994    -0.0165894   -0.0484805     0.237253     0.135988    -0.0712727   -0.181314     0.127056   -0.123613     0.0896795    0.0761944   -0.0432674 
 -0.0171984   -0.0585665    -0.0989472   -0.0861621    0.0746594  -0.113863    -0.00617464   0.126974     0.131347     -0.197389     0.000817605  -0.0446033   -0.0565708     0.0841688     0.0359492    0.0387802    0.0044551    -0.0323931   -0.0103475   -0.0687406   -0.121595    -0.0857225  -0.0866934   -0.101526    -0.0192217    0.0623309 
  0.207699     0.116082     -0.00836051  -0.155305    -0.166517    0.206364     0.128056    -0.0478202    0.000256444   0.11896      0.00830817   -0.0614284    0.00600483    0.0142        0.116976    -0.0421493   -0.118473     -0.0271457   -0.0266362   -0.0166655    0.0939292    0.0272504  -0.031533    -0.130671    -0.159812     0.0349735 
  0.0507746   -0.124729      0.0574839    0.00367248   0.113607   -0.06122      0.0749825    0.0410401    0.0559619     0.0696776   -0.105151     -0.0543629    0.0157703    -0.00346406   -0.0687378    0.0427869   -0.144382      0.0780816    0.183531    -0.0466976    0.0644624   -0.0180704  -0.0609465   -0.0497111    0.133847     0.0843967 
 -0.167615    -0.125661      0.116248    -0.0592751   -0.0128325  -0.016632     0.0671489    0.0496261    0.0643968     0.0557512    0.00559929   -0.12585     -0.0951978     0.081787     -0.00844351   0.0438637    0.142361     -0.0509818    0.172648     0.00571511  -0.0599106   -0.113828   -0.238615    -0.169486    -0.00284834  -0.0812384 
 -0.0124226    0.116211      0.184445    -0.0654256   -0.245917   -0.174693     0.054422     0.057556     0.116352     -0.0756657   -0.180898      0.0131904   -0.107193     -0.0274746    -0.140507     0.0811118    0.103557      0.0155018   -0.0467826    0.10322      0.110165     0.0433262  -0.21226      0.00982015  -0.01694      0.0307133 
  0.0282268    0.0736535     0.117864     0.0418308   -0.146776   -0.0766226   -0.021795     0.0997259    0.175233      0.0280472   -0.157925      0.0149045    0.131171     -0.0964248     0.0330583    0.0337904   -0.0735124     0.135879    -0.0242323   -0.00923893   0.134396    -0.0953369  -0.012137    -0.0629701   -0.128192     0.0270493 
 -0.0436231   -0.0215929    -0.147138    -0.135674     0.129695    0.145141     0.135479     0.0893681    0.0675495    -0.133715     0.0466515     0.00550588   0.0475227    -0.0130861    -0.0487232   -0.0483742    0.0720145     0.0214814   -0.0139171    0.0114329    0.0699217   -0.123715   -0.0126106   -0.109401    -0.0428317   -0.137519  
 -0.0406909    0.0574448    -0.011926    -0.0544029   -0.0125468  -0.0455696   -0.248148    -0.0897439    0.142522      0.115364     0.0041852     0.120407    -0.160194     -0.0984647    -0.0189272   -0.064571     0.161558      0.00459572   0.0199959   -0.0177127    0.049121     0.0266871  -0.0588011   -0.00818549  -0.110191     0.0340119 
 -0.0859253    0.0500216    -0.156597    -0.107744     0.0249854  -0.0760752    0.176664    -0.0234071    0.0241262    -0.0838612   -0.017302      0.0850216   -0.0235886    -0.153136     -0.0447718   -0.0165601    0.064992      0.0477008   -0.138921     0.13834      0.198458     0.136575   -0.0301069    0.14801      0.0613802   -0.0896844 
  0.111637    -0.083705     -0.0902518    0.136514     0.0278946  -0.15067     -0.0715847   -0.0929766   -0.10812       0.109121    -0.0232232     0.0666327    0.206106      0.038382      0.00959822  -0.260001    -0.209712     -0.0139654    0.0201548    0.0337547    0.132857    -0.0957925   0.0266976    0.0774314   -0.0279891   -0.0168249 
 -0.0474989   -0.167803      0.0624336   -0.0829846   -0.0779189   0.0490125    0.00933851   0.107646     0.00229863    0.0865016   -0.00329549    0.185782     0.142369      0.0483103    -0.0168901    0.0694394   -0.136909     -0.0707666    0.0941071    0.118253    -0.0146773    0.0309509  -0.114021     0.0100054   -0.0178275   -0.0352331 
 -0.0428444   -0.000571122  -0.0750427    0.0846895    0.0341837  -0.0707797   -0.040176    -0.031442    -0.0517141    -0.00345992   0.020045      0.117611     0.000769239   0.0217802    -0.236942     0.0450945   -0.0900775     0.07534     -0.0240565   -0.131527     0.0912181    0.0304883  -0.384603    -0.0389947   -0.0793473    0.156472  
  0.105351     0.0243881    -0.031383     0.0422018    0.0754684  -0.00117387  -0.086604     0.0981239   -0.0725052     0.043232     0.0258193    -0.0255788    0.0388654     0.0820261    -0.0235415    0.123562     0.089496      0.126608     0.0171334    0.200163    -0.0803087    0.11756     0.0432738   -0.0649052   -0.0859909    0.208287  
 -0.190758    -0.0306731    -0.0933047    0.260593     0.144593   -0.0494023    5.00003e-5   0.163385    -0.0461973     0.0935597    0.0438765     0.120756    -0.0582911    -0.212161     -0.0119958   -0.0761745   -0.0827477     0.102022    -0.0951175   -0.0143662   -0.0501783   -0.0407744  -0.0489566    0.0676956   -0.0678964    0.0449064 
 -0.163364    -0.106533     -0.184363    -0.0465867    0.0500091   0.0229974   -0.125333     0.248785    -0.0648736     0.12552     -0.0627116    -0.0585043   -0.0096981    -0.126133      0.200164    -0.0134008    0.279672      0.0569321    0.00761562  -0.0284808   -0.0345186   -0.0336959  -0.154775    -0.126284     0.0378892    0.034823  
  0.098969     0.0925389    -0.146038    -0.0210388    0.0669032  -0.138318     0.147091     0.0972582    0.0006675    -0.026603    -0.0106822     0.0453583   -0.126333     -0.0964719     0.151314    -0.152021     0.055919     -0.196988     0.0666045   -0.0778839   -0.0671278   -0.0119879   0.0374679   -0.128746     0.0178053    0.0477651 
  0.186092     0.0858347    -0.0829415   -0.0287005    0.0624797  -0.121487     0.0205909   -0.0490236   -0.0185879    -0.0554985   -0.180679     -0.0378926    0.058775     -0.0906472    -0.102293     0.131659     0.000525832  -0.0506097    0.0718786   -0.149574     0.0330748    0.0288225  -0.0248258    0.0108502    0.0629849    0.024205  
 -0.0804872   -0.0950399    -0.112561    -0.00305285   0.0787964  -0.0769152   -0.126148     0.160692    -0.0426236     0.0605519   -0.0301071    -0.0271855    0.0772429    -0.0167619     0.145109     0.0680227    0.0408646     0.0906025    0.0708519   -0.0825372   -0.0603269   -0.124302    0.0614595    0.0267354   -0.143731    -0.0551743 
  0.0592514    0.00774264   -0.0332255    0.0528488    0.0456519   0.0928782   -0.1454       0.24496      0.0404932    -0.0868979   -0.0121466    -0.0700807    0.0669239     0.000164362  -0.1816      -0.0211157   -0.0811213    -0.0389476    0.0954305   -0.0217299   -0.0151008    0.0635024  -0.106672     0.145865    -0.0873118    0.00970855
  0.0134253   -0.0753565    -0.0328303    0.0918597   -0.0539045  -0.0353668   -0.175732     0.00593797  -0.0727542     0.0232828   -0.171534     -0.128424    -0.089696      0.0270595    -0.252284     0.0385212   -0.0439724    -0.124177    -0.0924738   -0.0951853    0.0465221    0.076103    0.235355    -0.00992408  -0.27059     -0.12593   
  0.0298165    0.0226983    -0.00394257  -0.0701379   -0.141792   -0.224501     0.0270526   -0.113549     0.00341558    0.0825233    0.083143     -0.0955098    0.0962239     0.0776125     0.242223    -0.103199    -0.000639381   0.00213438   0.260691     0.0194451    0.133706     0.0135306  -0.0497573   -0.0055525    0.153716     0.11419   
  0.0869261   -0.108448      0.0575071    0.123703     0.043847   -0.0584474    0.169355     0.0291191   -0.106823     -0.0199077    0.0622927     0.231735     0.303672     -0.0938568     0.0803607   -0.00719102  -0.0798745     0.0402394    0.0520989    0.095782    -0.160512     0.158816   -0.0559175   -0.0528996    0.00265581  -0.0925187 
 -0.00105763   0.159873     -0.0536169   -0.216543    -0.0695053   0.0485015    0.0730174   -0.133767    -0.0119102     0.0169314   -0.0336584    -0.0848848    0.0203771    -0.0824261     0.125503    -0.00426968  -0.0859792    -0.0542602    0.176909     0.131015    -0.0944978    0.103308   -0.158155    -0.0794992   -0.0851683    0.0519996 
  0.135634     0.00953365   -0.0825421   -0.118782     0.0143385   0.0982817    0.0942525    0.277927    -0.0702015     0.0209113    0.0632633     0.0730997    0.133889     -0.0735361    -0.043842    -0.116202    -0.00399628    0.076985    -0.15         0.0425121   -0.14399     -0.0455065  -0.0212739    0.0877086   -0.0196181    0.0460617 
 -0.0355607    0.186366     -0.0576955   -0.0684106   -0.0979106   0.01496     -0.0814111    0.00216259  -0.118782      0.0422843   -0.230544     -0.0843786    0.0422083    -0.0829997    -0.075323    -0.070268     0.0388587    -0.0837931   -0.0147382    0.0595233    0.0263371    0.0563493  -0.00203123  -0.0583695    0.114752    -0.0852955 kind diag, method split
0: avll = -1.4450561905604675
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.445167
INFO: iteration 2, average log likelihood -1.445068
INFO: iteration 3, average log likelihood -1.444302
INFO: iteration 4, average log likelihood -1.434399
INFO: iteration 5, average log likelihood -1.414020
INFO: iteration 6, average log likelihood -1.407346
INFO: iteration 7, average log likelihood -1.405755
INFO: iteration 8, average log likelihood -1.404789
INFO: iteration 9, average log likelihood -1.404012
INFO: iteration 10, average log likelihood -1.403328
INFO: iteration 11, average log likelihood -1.402822
INFO: iteration 12, average log likelihood -1.402488
INFO: iteration 13, average log likelihood -1.402250
INFO: iteration 14, average log likelihood -1.402085
INFO: iteration 15, average log likelihood -1.401969
INFO: iteration 16, average log likelihood -1.401887
INFO: iteration 17, average log likelihood -1.401827
INFO: iteration 18, average log likelihood -1.401782
INFO: iteration 19, average log likelihood -1.401752
INFO: iteration 20, average log likelihood -1.401732
INFO: iteration 21, average log likelihood -1.401719
INFO: iteration 22, average log likelihood -1.401711
INFO: iteration 23, average log likelihood -1.401706
INFO: iteration 24, average log likelihood -1.401703
INFO: iteration 25, average log likelihood -1.401701
INFO: iteration 26, average log likelihood -1.401700
INFO: iteration 27, average log likelihood -1.401699
INFO: iteration 28, average log likelihood -1.401698
INFO: iteration 29, average log likelihood -1.401698
INFO: iteration 30, average log likelihood -1.401698
INFO: iteration 31, average log likelihood -1.401697
INFO: iteration 32, average log likelihood -1.401697
INFO: iteration 33, average log likelihood -1.401697
INFO: iteration 34, average log likelihood -1.401697
INFO: iteration 35, average log likelihood -1.401697
INFO: iteration 36, average log likelihood -1.401697
INFO: iteration 37, average log likelihood -1.401697
INFO: iteration 38, average log likelihood -1.401697
INFO: iteration 39, average log likelihood -1.401697
INFO: iteration 40, average log likelihood -1.401697
INFO: iteration 41, average log likelihood -1.401697
INFO: iteration 42, average log likelihood -1.401697
INFO: iteration 43, average log likelihood -1.401697
INFO: iteration 44, average log likelihood -1.401697
INFO: iteration 45, average log likelihood -1.401697
INFO: iteration 46, average log likelihood -1.401697
INFO: iteration 47, average log likelihood -1.401697
INFO: iteration 48, average log likelihood -1.401697
INFO: iteration 49, average log likelihood -1.401697
INFO: iteration 50, average log likelihood -1.401697
INFO: EM with 100000 data points 50 iterations avll -1.401697
952.4 data points per parameter
1: avll = [-1.44517,-1.44507,-1.4443,-1.4344,-1.41402,-1.40735,-1.40576,-1.40479,-1.40401,-1.40333,-1.40282,-1.40249,-1.40225,-1.40208,-1.40197,-1.40189,-1.40183,-1.40178,-1.40175,-1.40173,-1.40172,-1.40171,-1.40171,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.401812
INFO: iteration 2, average log likelihood -1.401718
INFO: iteration 3, average log likelihood -1.401280
INFO: iteration 4, average log likelihood -1.396093
INFO: iteration 5, average log likelihood -1.378017
INFO: iteration 6, average log likelihood -1.365761
INFO: iteration 7, average log likelihood -1.362299
INFO: iteration 8, average log likelihood -1.360863
INFO: iteration 9, average log likelihood -1.359882
INFO: iteration 10, average log likelihood -1.359057
INFO: iteration 11, average log likelihood -1.358239
INFO: iteration 12, average log likelihood -1.357283
INFO: iteration 13, average log likelihood -1.356274
INFO: iteration 14, average log likelihood -1.355576
INFO: iteration 15, average log likelihood -1.355183
INFO: iteration 16, average log likelihood -1.354962
INFO: iteration 17, average log likelihood -1.354794
INFO: iteration 18, average log likelihood -1.354623
INFO: iteration 19, average log likelihood -1.354485
INFO: iteration 20, average log likelihood -1.354403
INFO: iteration 21, average log likelihood -1.354358
INFO: iteration 22, average log likelihood -1.354330
INFO: iteration 23, average log likelihood -1.354310
INFO: iteration 24, average log likelihood -1.354295
INFO: iteration 25, average log likelihood -1.354282
INFO: iteration 26, average log likelihood -1.354272
INFO: iteration 27, average log likelihood -1.354262
INFO: iteration 28, average log likelihood -1.354254
INFO: iteration 29, average log likelihood -1.354246
INFO: iteration 30, average log likelihood -1.354238
INFO: iteration 31, average log likelihood -1.354231
INFO: iteration 32, average log likelihood -1.354225
INFO: iteration 33, average log likelihood -1.354219
INFO: iteration 34, average log likelihood -1.354213
INFO: iteration 35, average log likelihood -1.354207
INFO: iteration 36, average log likelihood -1.354201
INFO: iteration 37, average log likelihood -1.354195
INFO: iteration 38, average log likelihood -1.354189
INFO: iteration 39, average log likelihood -1.354183
INFO: iteration 40, average log likelihood -1.354177
INFO: iteration 41, average log likelihood -1.354172
INFO: iteration 42, average log likelihood -1.354166
INFO: iteration 43, average log likelihood -1.354160
INFO: iteration 44, average log likelihood -1.354154
INFO: iteration 45, average log likelihood -1.354148
INFO: iteration 46, average log likelihood -1.354142
INFO: iteration 47, average log likelihood -1.354136
INFO: iteration 48, average log likelihood -1.354130
INFO: iteration 49, average log likelihood -1.354124
INFO: iteration 50, average log likelihood -1.354118
INFO: EM with 100000 data points 50 iterations avll -1.354118
473.9 data points per parameter
2: avll = [-1.40181,-1.40172,-1.40128,-1.39609,-1.37802,-1.36576,-1.3623,-1.36086,-1.35988,-1.35906,-1.35824,-1.35728,-1.35627,-1.35558,-1.35518,-1.35496,-1.35479,-1.35462,-1.35448,-1.3544,-1.35436,-1.35433,-1.35431,-1.35429,-1.35428,-1.35427,-1.35426,-1.35425,-1.35425,-1.35424,-1.35423,-1.35422,-1.35422,-1.35421,-1.35421,-1.3542,-1.35419,-1.35419,-1.35418,-1.35418,-1.35417,-1.35417,-1.35416,-1.35415,-1.35415,-1.35414,-1.35414,-1.35413,-1.35412,-1.35412]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.354315
INFO: iteration 2, average log likelihood -1.354117
INFO: iteration 3, average log likelihood -1.353434
INFO: iteration 4, average log likelihood -1.347211
INFO: iteration 5, average log likelihood -1.330204
INFO: iteration 6, average log likelihood -1.314876
INFO: iteration 7, average log likelihood -1.307490
INFO: iteration 8, average log likelihood -1.303601
INFO: iteration 9, average log likelihood -1.301245
INFO: iteration 10, average log likelihood -1.300042
INFO: iteration 11, average log likelihood -1.299174
INFO: iteration 12, average log likelihood -1.298339
INFO: iteration 13, average log likelihood -1.297502
INFO: iteration 14, average log likelihood -1.296646
INFO: iteration 15, average log likelihood -1.295804
INFO: iteration 16, average log likelihood -1.295122
INFO: iteration 17, average log likelihood -1.294868
INFO: iteration 18, average log likelihood -1.294771
INFO: iteration 19, average log likelihood -1.294659
INFO: iteration 20, average log likelihood -1.294531
INFO: iteration 21, average log likelihood -1.294387
INFO: iteration 22, average log likelihood -1.294238
INFO: iteration 23, average log likelihood -1.294094
INFO: iteration 24, average log likelihood -1.293965
INFO: iteration 25, average log likelihood -1.293852
INFO: iteration 26, average log likelihood -1.293745
INFO: iteration 27, average log likelihood -1.293632
INFO: iteration 28, average log likelihood -1.293496
INFO: iteration 29, average log likelihood -1.293316
INFO: iteration 30, average log likelihood -1.293070
INFO: iteration 31, average log likelihood -1.292758
INFO: iteration 32, average log likelihood -1.292420
INFO: iteration 33, average log likelihood -1.292093
INFO: iteration 34, average log likelihood -1.291792
INFO: iteration 35, average log likelihood -1.291515
INFO: iteration 36, average log likelihood -1.291276
INFO: iteration 37, average log likelihood -1.291104
INFO: iteration 38, average log likelihood -1.290999
INFO: iteration 39, average log likelihood -1.290932
INFO: iteration 40, average log likelihood -1.290884
INFO: iteration 41, average log likelihood -1.290846
INFO: iteration 42, average log likelihood -1.290811
INFO: iteration 43, average log likelihood -1.290776
INFO: iteration 44, average log likelihood -1.290738
INFO: iteration 45, average log likelihood -1.290696
INFO: iteration 46, average log likelihood -1.290653
INFO: iteration 47, average log likelihood -1.290613
INFO: iteration 48, average log likelihood -1.290575
INFO: iteration 49, average log likelihood -1.290543
INFO: iteration 50, average log likelihood -1.290515
INFO: EM with 100000 data points 50 iterations avll -1.290515
236.4 data points per parameter
3: avll = [-1.35431,-1.35412,-1.35343,-1.34721,-1.3302,-1.31488,-1.30749,-1.3036,-1.30125,-1.30004,-1.29917,-1.29834,-1.2975,-1.29665,-1.2958,-1.29512,-1.29487,-1.29477,-1.29466,-1.29453,-1.29439,-1.29424,-1.29409,-1.29397,-1.29385,-1.29375,-1.29363,-1.2935,-1.29332,-1.29307,-1.29276,-1.29242,-1.29209,-1.29179,-1.29151,-1.29128,-1.2911,-1.291,-1.29093,-1.29088,-1.29085,-1.29081,-1.29078,-1.29074,-1.2907,-1.29065,-1.29061,-1.29057,-1.29054,-1.29052]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.290776
INFO: iteration 2, average log likelihood -1.290439
INFO: iteration 3, average log likelihood -1.289400
WARNING: Variances had to be floored 3
INFO: iteration 4, average log likelihood -1.281304
WARNING: Variances had to be floored 3
INFO: iteration 5, average log likelihood -1.265331
INFO: iteration 6, average log likelihood -1.241395
WARNING: Variances had to be floored 3
INFO: iteration 7, average log likelihood -1.223825
INFO: iteration 8, average log likelihood -1.217323
WARNING: Variances had to be floored 3 7
INFO: iteration 9, average log likelihood -1.207326
INFO: iteration 10, average log likelihood -1.217028
WARNING: Variances had to be floored 3 4
INFO: iteration 11, average log likelihood -1.205496
INFO: iteration 12, average log likelihood -1.213610
WARNING: Variances had to be floored 3
INFO: iteration 13, average log likelihood -1.207222
WARNING: Variances had to be floored 7
INFO: iteration 14, average log likelihood -1.203382
WARNING: Variances had to be floored 3
INFO: iteration 15, average log likelihood -1.209860
WARNING: Variances had to be floored 4
INFO: iteration 16, average log likelihood -1.205018
WARNING: Variances had to be floored 3
INFO: iteration 17, average log likelihood -1.210897
WARNING: Variances had to be floored 3
INFO: iteration 18, average log likelihood -1.205592
WARNING: Variances had to be floored 7
INFO: iteration 19, average log likelihood -1.200727
WARNING: Variances had to be floored 3 4
INFO: iteration 20, average log likelihood -1.207960
INFO: iteration 21, average log likelihood -1.215553
WARNING: Variances had to be floored 3
INFO: iteration 22, average log likelihood -1.208783
INFO: iteration 23, average log likelihood -1.204246
WARNING: Variances had to be floored 3 7
INFO: iteration 24, average log likelihood -1.197996
WARNING: Variances had to be floored 4
INFO: iteration 25, average log likelihood -1.210196
WARNING: Variances had to be floored 3
INFO: iteration 26, average log likelihood -1.213845
INFO: iteration 27, average log likelihood -1.207448
WARNING: Variances had to be floored 3
INFO: iteration 28, average log likelihood -1.199601
WARNING: Variances had to be floored 4 7
INFO: iteration 29, average log likelihood -1.198871
WARNING: Variances had to be floored 3
INFO: iteration 30, average log likelihood -1.219228
INFO: iteration 31, average log likelihood -1.210365
WARNING: Variances had to be floored 3
INFO: iteration 32, average log likelihood -1.201470
WARNING: Variances had to be floored 4
INFO: iteration 33, average log likelihood -1.200301
WARNING: Variances had to be floored 3 7
INFO: iteration 34, average log likelihood -1.207787
WARNING: Variances had to be floored 3
INFO: iteration 35, average log likelihood -1.215662
INFO: iteration 36, average log likelihood -1.206716
WARNING: Variances had to be floored 3 4
INFO: iteration 37, average log likelihood -1.199545
INFO: iteration 38, average log likelihood -1.210679
WARNING: Variances had to be floored 3 7
INFO: iteration 39, average log likelihood -1.204821
INFO: iteration 40, average log likelihood -1.212637
WARNING: Variances had to be floored 3
INFO: iteration 41, average log likelihood -1.201713
WARNING: Variances had to be floored 4
INFO: iteration 42, average log likelihood -1.199280
WARNING: Variances had to be floored 3
INFO: iteration 43, average log likelihood -1.206499
WARNING: Variances had to be floored 3 7
INFO: iteration 44, average log likelihood -1.201504
INFO: iteration 45, average log likelihood -1.209687
WARNING: Variances had to be floored 3 4
INFO: iteration 46, average log likelihood -1.199878
INFO: iteration 47, average log likelihood -1.209980
WARNING: Variances had to be floored 3
INFO: iteration 48, average log likelihood -1.204223
WARNING: Variances had to be floored 7
INFO: iteration 49, average log likelihood -1.200172
WARNING: Variances had to be floored 3
INFO: iteration 50, average log likelihood -1.207052
INFO: EM with 100000 data points 50 iterations avll -1.207052
118.1 data points per parameter
4: avll = [-1.29078,-1.29044,-1.2894,-1.2813,-1.26533,-1.2414,-1.22382,-1.21732,-1.20733,-1.21703,-1.2055,-1.21361,-1.20722,-1.20338,-1.20986,-1.20502,-1.2109,-1.20559,-1.20073,-1.20796,-1.21555,-1.20878,-1.20425,-1.198,-1.2102,-1.21385,-1.20745,-1.1996,-1.19887,-1.21923,-1.21036,-1.20147,-1.2003,-1.20779,-1.21566,-1.20672,-1.19955,-1.21068,-1.20482,-1.21264,-1.20171,-1.19928,-1.2065,-1.2015,-1.20969,-1.19988,-1.20998,-1.20422,-1.20017,-1.20705]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 7 8
INFO: iteration 1, average log likelihood -1.202427
WARNING: Variances had to be floored 5 6 7 8
INFO: iteration 2, average log likelihood -1.196711
WARNING: Variances had to be floored 5 7 8
INFO: iteration 3, average log likelihood -1.196229
WARNING: Variances had to be floored 6 7 8 13
INFO: iteration 4, average log likelihood -1.182414
WARNING: Variances had to be floored 5 6 7 8 14 16
INFO: iteration 5, average log likelihood -1.149817
WARNING: Variances had to be floored 3 4 5 6 7 8 30
INFO: iteration 6, average log likelihood -1.124623
WARNING: Variances had to be floored 5 6 7 8 16 23 24
INFO: iteration 7, average log likelihood -1.107611
WARNING: Variances had to be floored 4 5 6 7 8 10 13 14 26 27
INFO: iteration 8, average log likelihood -1.091067
WARNING: Variances had to be floored 3 5 6 7 8 16 30
INFO: iteration 9, average log likelihood -1.128849
WARNING: Variances had to be floored 4 5 6 7 8 23 24
INFO: iteration 10, average log likelihood -1.103960
WARNING: Variances had to be floored 3 5 6 7 8 14 16 26 27
INFO: iteration 11, average log likelihood -1.098121
WARNING: Variances had to be floored 4 5 6 7 8 10 13 30
INFO: iteration 12, average log likelihood -1.100109
WARNING: Variances had to be floored 3 5 6 7 8 16 23 24
INFO: iteration 13, average log likelihood -1.110995
WARNING: Variances had to be floored 4 5 6 7 8 14 26 27
INFO: iteration 14, average log likelihood -1.101125
WARNING: Variances had to be floored 3 5 6 7 8 10 13 16 30
INFO: iteration 15, average log likelihood -1.104752
WARNING: Variances had to be floored 4 5 6 7 8 23 24
INFO: iteration 16, average log likelihood -1.110206
WARNING: Variances had to be floored 3 5 6 7 8 14 16 26 27
INFO: iteration 17, average log likelihood -1.102143
WARNING: Variances had to be floored 4 5 6 7 8 10 13 30
INFO: iteration 18, average log likelihood -1.104803
WARNING: Variances had to be floored 3 5 6 7 8 16 23 24
INFO: iteration 19, average log likelihood -1.111129
WARNING: Variances had to be floored 4 5 6 7 8 14 26 27
INFO: iteration 20, average log likelihood -1.101310
WARNING: Variances had to be floored 3 5 6 7 8 10 13 16 30
INFO: iteration 21, average log likelihood -1.105266
WARNING: Variances had to be floored 4 5 6 7 8 23 24
INFO: iteration 22, average log likelihood -1.110314
WARNING: Variances had to be floored 3 5 6 7 8 14 16 26 27
INFO: iteration 23, average log likelihood -1.102332
WARNING: Variances had to be floored 4 5 6 7 8 10 13 30
INFO: iteration 24, average log likelihood -1.105096
WARNING: Variances had to be floored 3 5 6 7 8 16 23 24
INFO: iteration 25, average log likelihood -1.111213
WARNING: Variances had to be floored 4 5 6 7 8 14 26 27
INFO: iteration 26, average log likelihood -1.101428
WARNING: Variances had to be floored 3 5 6 7 8 10 13 16 30
INFO: iteration 27, average log likelihood -1.105468
WARNING: Variances had to be floored 4 5 6 7 8 23 24
INFO: iteration 28, average log likelihood -1.110360
WARNING: Variances had to be floored 3 5 6 7 8 14 16 26 27
INFO: iteration 29, average log likelihood -1.102431
WARNING: Variances had to be floored 4 5 6 7 8 10 13 30
INFO: iteration 30, average log likelihood -1.105223
WARNING: Variances had to be floored 3 5 6 7 8 16 23 24
INFO: iteration 31, average log likelihood -1.111243
WARNING: Variances had to be floored 4 5 6 7 8 14 26 27
INFO: iteration 32, average log likelihood -1.101486
WARNING: Variances had to be floored 3 5 6 7 8 10 13 16 30
INFO: iteration 33, average log likelihood -1.105564
WARNING: Variances had to be floored 4 5 6 7 8 23 24
INFO: iteration 34, average log likelihood -1.110374
WARNING: Variances had to be floored 3 5 6 7 8 14 16 26 27
INFO: iteration 35, average log likelihood -1.102479
WARNING: Variances had to be floored 4 5 6 7 8 10 13 30
INFO: iteration 36, average log likelihood -1.105290
WARNING: Variances had to be floored 3 5 6 7 8 16 23 24
INFO: iteration 37, average log likelihood -1.111249
WARNING: Variances had to be floored 4 5 6 7 8 14 26 27
INFO: iteration 38, average log likelihood -1.101514
WARNING: Variances had to be floored 3 5 6 7 8 10 13 16 30
INFO: iteration 39, average log likelihood -1.105620
WARNING: Variances had to be floored 4 5 6 7 8 23 24
INFO: iteration 40, average log likelihood -1.110372
WARNING: Variances had to be floored 3 5 6 7 8 14 16 26 27
INFO: iteration 41, average log likelihood -1.102501
WARNING: Variances had to be floored 4 5 6 7 8 10 13 30
INFO: iteration 42, average log likelihood -1.105334
WARNING: Variances had to be floored 3 5 6 7 8 16 23 24
INFO: iteration 43, average log likelihood -1.111242
WARNING: Variances had to be floored 4 5 6 7 8 14 26 27
INFO: iteration 44, average log likelihood -1.101525
WARNING: Variances had to be floored 3 5 6 7 8 10 13 16 30
INFO: iteration 45, average log likelihood -1.105663
WARNING: Variances had to be floored 4 5 6 7 8 23 24
INFO: iteration 46, average log likelihood -1.110357
WARNING: Variances had to be floored 3 5 6 7 8 14 16 26 27
INFO: iteration 47, average log likelihood -1.102506
WARNING: Variances had to be floored 4 5 6 7 8 10 13 30
INFO: iteration 48, average log likelihood -1.105378
WARNING: Variances had to be floored 3 5 6 7 8 16 23 24
INFO: iteration 49, average log likelihood -1.111215
WARNING: Variances had to be floored 4 5 6 7 8 14 26 27
INFO: iteration 50, average log likelihood -1.101516
INFO: EM with 100000 data points 50 iterations avll -1.101516
59.0 data points per parameter
5: avll = [-1.20243,-1.19671,-1.19623,-1.18241,-1.14982,-1.12462,-1.10761,-1.09107,-1.12885,-1.10396,-1.09812,-1.10011,-1.111,-1.10113,-1.10475,-1.11021,-1.10214,-1.1048,-1.11113,-1.10131,-1.10527,-1.11031,-1.10233,-1.1051,-1.11121,-1.10143,-1.10547,-1.11036,-1.10243,-1.10522,-1.11124,-1.10149,-1.10556,-1.11037,-1.10248,-1.10529,-1.11125,-1.10151,-1.10562,-1.11037,-1.1025,-1.10533,-1.11124,-1.10153,-1.10566,-1.11036,-1.10251,-1.10538,-1.11122,-1.10152]
[-1.44506,-1.44517,-1.44507,-1.4443,-1.4344,-1.41402,-1.40735,-1.40576,-1.40479,-1.40401,-1.40333,-1.40282,-1.40249,-1.40225,-1.40208,-1.40197,-1.40189,-1.40183,-1.40178,-1.40175,-1.40173,-1.40172,-1.40171,-1.40171,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.4017,-1.40181,-1.40172,-1.40128,-1.39609,-1.37802,-1.36576,-1.3623,-1.36086,-1.35988,-1.35906,-1.35824,-1.35728,-1.35627,-1.35558,-1.35518,-1.35496,-1.35479,-1.35462,-1.35448,-1.3544,-1.35436,-1.35433,-1.35431,-1.35429,-1.35428,-1.35427,-1.35426,-1.35425,-1.35425,-1.35424,-1.35423,-1.35422,-1.35422,-1.35421,-1.35421,-1.3542,-1.35419,-1.35419,-1.35418,-1.35418,-1.35417,-1.35417,-1.35416,-1.35415,-1.35415,-1.35414,-1.35414,-1.35413,-1.35412,-1.35412,-1.35431,-1.35412,-1.35343,-1.34721,-1.3302,-1.31488,-1.30749,-1.3036,-1.30125,-1.30004,-1.29917,-1.29834,-1.2975,-1.29665,-1.2958,-1.29512,-1.29487,-1.29477,-1.29466,-1.29453,-1.29439,-1.29424,-1.29409,-1.29397,-1.29385,-1.29375,-1.29363,-1.2935,-1.29332,-1.29307,-1.29276,-1.29242,-1.29209,-1.29179,-1.29151,-1.29128,-1.2911,-1.291,-1.29093,-1.29088,-1.29085,-1.29081,-1.29078,-1.29074,-1.2907,-1.29065,-1.29061,-1.29057,-1.29054,-1.29052,-1.29078,-1.29044,-1.2894,-1.2813,-1.26533,-1.2414,-1.22382,-1.21732,-1.20733,-1.21703,-1.2055,-1.21361,-1.20722,-1.20338,-1.20986,-1.20502,-1.2109,-1.20559,-1.20073,-1.20796,-1.21555,-1.20878,-1.20425,-1.198,-1.2102,-1.21385,-1.20745,-1.1996,-1.19887,-1.21923,-1.21036,-1.20147,-1.2003,-1.20779,-1.21566,-1.20672,-1.19955,-1.21068,-1.20482,-1.21264,-1.20171,-1.19928,-1.2065,-1.2015,-1.20969,-1.19988,-1.20998,-1.20422,-1.20017,-1.20705,-1.20243,-1.19671,-1.19623,-1.18241,-1.14982,-1.12462,-1.10761,-1.09107,-1.12885,-1.10396,-1.09812,-1.10011,-1.111,-1.10113,-1.10475,-1.11021,-1.10214,-1.1048,-1.11113,-1.10131,-1.10527,-1.11031,-1.10233,-1.1051,-1.11121,-1.10143,-1.10547,-1.11036,-1.10243,-1.10522,-1.11124,-1.10149,-1.10556,-1.11037,-1.10248,-1.10529,-1.11125,-1.10151,-1.10562,-1.11037,-1.1025,-1.10533,-1.11124,-1.10153,-1.10566,-1.11036,-1.10251,-1.10538,-1.11122,-1.10152]
32×26 Array{Float64,2}:
 -0.0251671    -0.0190278    -0.153563   -0.0338936    0.0593572   -0.0542171    0.00213043    0.165705    -0.0331511     0.0424593   -0.0338385    0.0190755   -0.0576881    -0.141526     0.171658   -0.0721621    0.166916    -0.0726958     0.0390577   -0.0516323   -0.0557721    -0.0373883   -0.0680945   -0.119214     0.0144204    0.0288606 
  0.104215     -0.00290614   -0.0623457  -0.0204022    0.00814668  -0.00859535  -0.0219591     0.0715335   -0.0614686     0.00339301  -0.0955613   -0.00362124   0.0398704    -0.0226212   -0.127826    0.0147632   -0.0392861   -0.0611098    -0.0528054   -0.0700631   -0.0319255     0.00562093   0.0401094    0.0294186   -0.076755    -0.0173733 
 -0.0483169    -0.00288816    0.0512197  -0.204536    -0.114483    -0.0635128    0.0263605    -0.090173    -0.0593921     0.0238389   -0.0329134   -0.0414883   -0.0359148    -0.0393605   -0.0735452   0.00673272  -0.027932     0.00961053   -0.143357    -0.125149    -0.00349177   -0.157938    -0.138722     0.190271    -0.102953    -0.0135721 
  0.206783      0.0952531    -0.0644424  -0.150549    -0.167602     0.206056     0.127711     -0.0374513    0.000513248   0.134536     0.00125533  -0.0751651   -0.00568018   -0.00887426   0.131914   -0.0436476   -0.10632     -0.0342858    -0.0152265   -0.0472519    0.0934768     0.0270281   -0.0235941   -0.132792    -0.163322     0.0327266 
 -0.35676      -0.157472      0.139521   -0.0428055   -0.10737     -0.0360566    0.0833415    -0.173231     0.0666901     0.056108    -0.0494766   -0.0981636   -0.0440401     0.0424186   -0.0458956   0.017686     0.0222011   -0.0379581     0.16623     -0.0179684   -0.0185161    -0.102237    -0.152199    -0.132975    -0.0889475   -0.0515688 
  0.660643     -0.0952693    -0.105558    0.0119003    0.392599    -0.0522583    0.0363797     0.390369     0.0323099     0.0812092   -0.137035    -0.0793439    0.0120458     0.0350283   -0.0538875   0.02261     -0.0619038    0.112357      0.159246    -0.030719     0.0775637     0.0732758   -0.100669    -0.0497557    0.470875     0.121914  
  0.148836     -0.0851456    -0.0819168   0.0722475    0.0301919   -0.196757     0.0654161    -0.0946604   -0.107605      0.111075    -0.113857    -0.15144      0.178379      0.0362751   -0.0162661  -0.643302    -0.591016    -0.0238809    -0.00210331   0.0322653    0.0981621     0.00788406   0.0215042    0.0301111   -0.0272358    0.0155141 
  0.0744222    -0.0808604    -0.0736964   0.19901      0.0284873   -0.0947917   -0.16502      -0.0907889   -0.0956833     0.132707     0.0627561    0.30782      0.244114      0.0162128    0.098925   -0.0263076    0.186448    -0.00404915    0.0282835    0.0339952    0.188109     -0.183594     0.0186377    0.094169    -0.0270395   -0.0616977 
  0.0309305     0.0260239     0.107044    0.0458039   -0.139604    -0.0816978   -0.0235882     0.0981562    0.153406     -0.0101822   -0.168277     0.0148696    0.091734     -0.0937441    0.0463123   0.0167678   -0.0725152    0.134647     -0.0260528   -0.0114761    0.108936     -0.0968053   -0.004953    -0.0613711   -0.134942     0.0274663 
 -0.0707611    -0.0964978    -0.140718   -0.00343865   0.0669732   -0.0671657   -0.105929      0.160279    -0.0507156     0.0578023   -0.0245156   -0.0320993    0.0821017    -0.0161298    0.135043    0.0768894    0.0192311    0.099491      0.0755519   -0.0731122   -0.0651266    -0.12696      0.0445288    0.00482197  -0.134882    -0.0193235 
  0.0633846     0.000549925  -0.041187    0.0417374    0.0539882    0.0693617   -0.146619      0.244928     0.0466554    -0.110374    -0.0216417   -0.0763278    0.0650529     0.0035894   -0.185004   -0.0233998   -0.092219    -0.0733415     0.0942439   -0.0844438   -0.0153874     0.0638281   -0.121289     0.15335     -0.12403     -0.00684051
 -0.00497243   -0.0456289    -0.149032    0.0372476    0.0877403    0.0761795    0.144271      0.0471086    0.12443      -0.0317471    0.0132697    0.0700079    0.0884093    -0.123904    -0.0304592  -0.0552945   -0.0112336    0.0721744     0.0143343    0.026412    -0.00259023    0.0392523   -0.0566082   -0.00333087  -0.132676    -0.103844  
 -0.0383444    -0.160889      0.12518    -0.114033    -0.14695      0.0292563    0.0400638     0.101284     0.00743908    0.0711724   -0.0431491    0.217588     0.126769      0.0442049   -0.0310344   0.0785028   -0.130986    -0.00732756    0.118675     0.129163    -0.000413713   0.0524113   -0.112523     0.00630586  -0.0333629   -0.0378367 
  0.0243765     0.0222485     0.0115472  -0.0632519   -0.137579    -0.23332      0.0299962    -0.0939673    0.00320816    0.0791872    0.0449514   -0.080202     0.0973633     0.0805384    0.22394    -0.094157     0.00446416  -0.0794321     0.260242    -0.0215601    0.145084      0.0287287   -0.00091255   0.00010733   0.15208      0.106162  
 -0.0398599     0.0700262    -0.0109464  -0.0484779   -0.00697798  -0.0694731   -0.258067     -0.0959679    0.153464      0.133881     0.00924184   0.114513    -0.215248     -0.104231    -0.027483   -0.0801225    0.140425     0.00455434    0.0385864   -0.0283728    0.046957      0.0324192   -0.0567591   -0.0132941   -0.116219     0.0570108 
 -0.0805384     0.0315915    -0.0768054  -0.106925     0.0235938   -0.0918372    0.144562     -0.0307175    0.0537483    -0.142626     0.0117216    0.0772748   -0.000331145  -0.152278    -0.0479581  -0.0429239    0.108197     0.0533925    -0.183573     0.150138     0.20062       0.131488    -0.0324257    0.139411     0.0683366   -0.0769877 
  0.087458     -0.00440231   -0.104154   -0.0615641   -0.3295       0.19418     -0.159853      0.0791364    0.0491696     0.0585877   -0.00393923  -0.236902    -0.154507     -0.0223912    0.0861814  -1.33552      0.0133318    0.150496     -0.143291    -0.101358     0.0298996     0.0505868   -0.13033      0.157642    -0.0474443   -0.110831  
 -0.238353      0.0250557    -0.152091    0.188905    -0.00103357  -0.313637    -0.131892      0.284807     0.096563      0.051911    -0.254004     0.235652     0.0730864    -0.0259799    0.0854496   0.645448     0.0353284    0.168704     -0.0814577   -0.00319279   0.0312456     0.0387845   -0.13527      0.179198     0.0995315    0.0507628 
  0.0128121    -0.457308     -0.0204468   0.155135     0.324919     0.306837    -0.0299599    -0.121126    -0.0182485    -0.081059     0.348993     0.16724     -0.00764527    0.0301445    0.0867347  -0.860781     0.0596562    0.180583      0.07374      0.174937    -0.00922329    0.0997989   -0.155182     0.169428    -0.0671202    0.0232581 
  0.143247     -0.238407      0.38984     0.102101    -0.0370634    0.151202    -0.26941       0.126155     0.00914939   -0.134807    -0.0316701   -0.227562    -0.115874      0.00248182   0.0775336   1.33734      0.0304193    0.136809     -0.0713194   -0.103287     0.0259625     0.0847272   -0.111148    -0.00712386  -0.00950789   0.0476252 
 -0.102502     -0.0323312    -0.0342563  -0.095649     0.0187253    0.098853    -0.032602     -0.0712433   -0.125684     -0.135181     0.126938     0.0910128    0.147364     -0.123406    -0.0476392   0.235286     0.201484    -0.116599     -0.117583    -0.0576965   -0.101979     -0.0450346    0.0296867    0.0475356    0.0633623    0.0015768 
 -0.0133301    -0.0506291    -0.104802   -0.0820667    0.0676814   -0.101284    -0.0104157     0.131797     0.103637     -0.193842    -0.00556677  -0.0366961   -0.058434      0.0826029    0.0265968   0.0135703   -0.0348023   -0.0313813    -0.0126365   -0.0607447   -0.0708268    -0.100451    -0.0921577   -0.0873037   -0.0294422    0.0619708 
  0.000497334   0.163093     -0.0099982  -0.247379    -0.0868717    0.0433852    0.0714609    -0.143658     0.0136565     0.0204493   -0.0475243   -0.104252     0.0389919    -0.0597177    0.1373      0.00509566  -0.0857726   -0.0274058     0.174861     0.0846806   -0.0549495     0.105579    -0.16079     -0.107592    -0.0746313    0.0518213 
  0.120719     -0.00164731   -0.182625    0.0487001    0.0728473    0.0613835   -0.0507531     0.0462738   -0.069978      0.085358     0.0759352   -0.0780227   -0.096678      0.25855      0.155317   -0.0329112   -0.0469021    0.207952      0.130882    -0.0531194   -0.196042      0.132204    -0.118968     0.0911757    0.06208     -0.0563827 
 -0.0446561    -0.116675     -0.014298   -0.119232     0.0968852   -0.16522      0.0599028     0.131973    -0.00260287    0.030126     0.0580442   -0.130751     0.104589      0.0658007   -0.0616143  -0.0625428   -0.0264804    0.163836      0.00312724  -0.0380111    0.0767907    -0.105425    -0.241188    -0.100233     0.0550399    0.0113673 
 -0.0460025     0.0237201    -0.0535546   0.100925     0.033637    -0.0748495   -0.0426593    -0.0278569   -0.0495414     0.0100605    0.0151514    0.120677     0.00142633   -0.0130629   -0.239472    0.0418616   -0.102123     0.0825636    -0.0123294   -0.131725     0.0916235     0.0290953   -0.385119    -0.0907577   -0.069363     0.203458  
 -0.181141     -0.270795      0.0190584   0.138008     0.0976784   -0.0602276   -0.0722243    -0.0394902    0.12453      -0.0200785   -0.227382    -0.0626569    0.249865      0.0509112    0.156204    0.0197832    0.0304897    0.0584287    -0.0944609    0.136239     0.014536     -0.0441793   -0.142397     0.0107412   -0.243166    -0.0254036 
 -0.00699361    0.139635      0.0490544  -0.0712963   -0.166609    -0.0689264    0.000366629  -0.00429438  -0.00738517   -0.00946795  -0.208723    -0.0257155   -0.0281467    -0.0612249   -0.106225   -0.016264     0.0709164   -0.0411654    -0.0365568    0.0773786    0.0654716     0.0532804   -0.114125    -0.0168468    0.0517304   -0.0359585 
  0.0365217    -0.100715      0.0536424   0.090027     0.0367207   -0.0548874    0.1691        0.02972     -0.108396     -0.00225682   0.0627105    0.217842     0.299507     -0.068932     0.0854044  -0.00263605  -0.0771506    0.037976      0.0552459    0.135857    -0.158135      0.154134    -0.0570923   -0.0455205   -0.0069196   -0.0928685 
 -0.212014     -0.0306698    -0.0882468   0.24006      0.187384    -0.0413915   -0.00372137    0.163506    -0.0539227     0.127775     0.0432596    0.120156    -0.0538971    -0.198444    -0.020689   -0.0759146   -0.0881927    0.113713     -0.096948    -0.0191017   -0.0472579    -0.0983002   -0.0455356    0.0697719   -0.0704867    0.0428153 
  0.0364585     0.0257355    -0.0451528  -0.0195776   -0.424592     0.0232112   -0.0914202    -0.0395814   -0.0304437     0.0417709    0.0934294   -0.0182666    0.0618926     0.0270063   -0.0171747   0.121654     0.0527895    0.000872102   0.014301     0.160108    -0.048518     -0.0177085   -0.0571007   -0.00302633   0.0334177    0.198998  
  0.167009     -0.000187347  -0.0171418   0.0484291    0.274719    -0.029533    -0.0567029     0.20851     -0.0787184     0.0469385   -0.0230629   -0.017151     0.0383121     0.00570268  -0.0364881   0.118327     0.0881363    0.133109      0.0166846    0.213212    -0.100246      0.136717     0.0594076   -0.118058    -0.132327     0.190468  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 3 5 6 7 8 10 13 16 30
INFO: iteration 1, average log likelihood -1.105718
WARNING: Variances had to be floored 3 4 5 6 7 8 10 13 14 16 23 24 30
INFO: iteration 2, average log likelihood -1.067088
WARNING: Variances had to be floored 3 5 6 7 8 10 13 16 26 27 30
INFO: iteration 3, average log likelihood -1.084716
WARNING: Variances had to be floored 3 4 5 6 7 8 10 13 14 16 23 24 30
INFO: iteration 4, average log likelihood -1.078823
WARNING: Variances had to be floored 3 5 6 7 8 10 13 16 30
INFO: iteration 5, average log likelihood -1.093145
WARNING: Variances had to be floored 3 4 5 6 7 8 10 13 14 16 23 24 26 27 30
INFO: iteration 6, average log likelihood -1.058240
WARNING: Variances had to be floored 3 5 6 7 8 10 13 16 30
INFO: iteration 7, average log likelihood -1.105150
WARNING: Variances had to be floored 3 4 5 6 7 8 10 13 14 16 23 24 30
INFO: iteration 8, average log likelihood -1.066626
WARNING: Variances had to be floored 3 5 6 7 8 10 13 16 26 27 30
INFO: iteration 9, average log likelihood -1.084478
WARNING: Variances had to be floored 3 4 5 6 7 8 10 13 14 16 23 24 30
INFO: iteration 10, average log likelihood -1.078665
INFO: EM with 100000 data points 10 iterations avll -1.078665
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.529093e+05
      1       7.290002e+05      -2.239091e+05 |       32
      2       6.954497e+05      -3.355051e+04 |       32
      3       6.769289e+05      -1.852085e+04 |       32
      4       6.650498e+05      -1.187909e+04 |       32
      5       6.577868e+05      -7.262969e+03 |       32
      6       6.533041e+05      -4.482719e+03 |       32
      7       6.507903e+05      -2.513820e+03 |       32
      8       6.495565e+05      -1.233797e+03 |       32
      9       6.488645e+05      -6.919671e+02 |       32
     10       6.485638e+05      -3.007395e+02 |       32
     11       6.484521e+05      -1.116424e+02 |       32
     12       6.484032e+05      -4.895419e+01 |       31
     13       6.483637e+05      -3.950242e+01 |       32
     14       6.483205e+05      -4.319198e+01 |       32
     15       6.482615e+05      -5.893487e+01 |       31
     16       6.481865e+05      -7.502245e+01 |       32
     17       6.480603e+05      -1.262099e+02 |       31
     18       6.479075e+05      -1.527961e+02 |       32
     19       6.477272e+05      -1.802964e+02 |       32
     20       6.475966e+05      -1.306424e+02 |       32
     21       6.474867e+05      -1.098431e+02 |       32
     22       6.474037e+05      -8.301309e+01 |       32
     23       6.473388e+05      -6.488873e+01 |       31
     24       6.472993e+05      -3.949085e+01 |       32
     25       6.472693e+05      -3.002822e+01 |       32
     26       6.472516e+05      -1.773587e+01 |       31
     27       6.472347e+05      -1.689264e+01 |       32
     28       6.472209e+05      -1.374305e+01 |       29
     29       6.472123e+05      -8.630089e+00 |       30
     30       6.472015e+05      -1.077548e+01 |       30
     31       6.471934e+05      -8.107297e+00 |       28
     32       6.471851e+05      -8.259153e+00 |       23
     33       6.471789e+05      -6.292140e+00 |       26
     34       6.471681e+05      -1.072000e+01 |       31
     35       6.471543e+05      -1.379720e+01 |       30
     36       6.471402e+05      -1.415414e+01 |       29
     37       6.471230e+05      -1.723082e+01 |       28
     38       6.470974e+05      -2.559727e+01 |       30
     39       6.470500e+05      -4.733526e+01 |       31
     40       6.469576e+05      -9.240539e+01 |       32
     41       6.468276e+05      -1.300652e+02 |       31
     42       6.466417e+05      -1.858154e+02 |       32
     43       6.463087e+05      -3.329963e+02 |       32
     44       6.458278e+05      -4.808926e+02 |       32
     45       6.453664e+05      -4.614183e+02 |       32
     46       6.451154e+05      -2.510446e+02 |       31
     47       6.450222e+05      -9.320594e+01 |       29
     48       6.449924e+05      -2.976918e+01 |       31
     49       6.449797e+05      -1.266742e+01 |       26
     50       6.449757e+05      -4.010601e+00 |       19
K-means terminated without convergence after 50 iterations (objv = 644975.7321695308)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.347757
INFO: iteration 2, average log likelihood -1.311649
INFO: iteration 3, average log likelihood -1.276472
INFO: iteration 4, average log likelihood -1.241019
INFO: iteration 5, average log likelihood -1.196767
WARNING: Variances had to be floored 8 20 31
INFO: iteration 6, average log likelihood -1.146957
WARNING: Variances had to be floored 4 9 27
INFO: iteration 7, average log likelihood -1.145754
WARNING: Variances had to be floored 1 14 17 23 28
INFO: iteration 8, average log likelihood -1.144325
INFO: iteration 9, average log likelihood -1.149477
WARNING: Variances had to be floored 3 4 8 27
INFO: iteration 10, average log likelihood -1.095390
WARNING: Variances had to be floored 9 16
INFO: iteration 11, average log likelihood -1.133471
WARNING: Variances had to be floored 1 14 23 28
INFO: iteration 12, average log likelihood -1.115008
WARNING: Variances had to be floored 8 17 20 27
INFO: iteration 13, average log likelihood -1.115123
WARNING: Variances had to be floored 4 6 9
INFO: iteration 14, average log likelihood -1.126783
WARNING: Variances had to be floored 1 23 31
INFO: iteration 15, average log likelihood -1.124658
WARNING: Variances had to be floored 28
INFO: iteration 16, average log likelihood -1.115827
WARNING: Variances had to be floored 3 8 14 16 27
INFO: iteration 17, average log likelihood -1.079038
WARNING: Variances had to be floored 4 9
INFO: iteration 18, average log likelihood -1.142517
WARNING: Variances had to be floored 1 2 17 20 23
INFO: iteration 19, average log likelihood -1.119540
WARNING: Variances had to be floored 6 8 27 28
INFO: iteration 20, average log likelihood -1.111840
WARNING: Variances had to be floored 3 4 9 14
INFO: iteration 21, average log likelihood -1.117045
WARNING: Variances had to be floored 16 31
INFO: iteration 22, average log likelihood -1.132630
WARNING: Variances had to be floored 1 20 23
INFO: iteration 23, average log likelihood -1.107063
WARNING: Variances had to be floored 2 4 8 14 17 27 28
INFO: iteration 24, average log likelihood -1.082644
WARNING: Variances had to be floored 3 6 9
INFO: iteration 25, average log likelihood -1.132000
WARNING: Variances had to be floored 1 16 23
INFO: iteration 26, average log likelihood -1.133668
WARNING: Variances had to be floored 8
INFO: iteration 27, average log likelihood -1.136021
WARNING: Variances had to be floored 4 9 14 17 20 27 28 31
INFO: iteration 28, average log likelihood -1.084252
WARNING: Variances had to be floored 1 3 23
INFO: iteration 29, average log likelihood -1.139798
WARNING: Variances had to be floored 6 16
INFO: iteration 30, average log likelihood -1.139583
WARNING: Variances had to be floored 4 8
INFO: iteration 31, average log likelihood -1.124034
WARNING: Variances had to be floored 2 17 27
INFO: iteration 32, average log likelihood -1.112710
WARNING: Variances had to be floored 1 9 20 23
INFO: iteration 33, average log likelihood -1.095605
WARNING: Variances had to be floored 3 4 8 14 16 31
INFO: iteration 34, average log likelihood -1.109232
INFO: iteration 35, average log likelihood -1.154145
WARNING: Variances had to be floored 2 6 20 27
INFO: iteration 36, average log likelihood -1.092373
WARNING: Variances had to be floored 1 4 8 9 17 23
INFO: iteration 37, average log likelihood -1.107302
INFO: iteration 38, average log likelihood -1.161591
WARNING: Variances had to be floored 3 14 16 27 28 31
INFO: iteration 39, average log likelihood -1.079131
WARNING: Variances had to be floored 1 2 4 8 9 20 23
INFO: iteration 40, average log likelihood -1.118743
INFO: iteration 41, average log likelihood -1.179569
WARNING: Variances had to be floored 17 27
INFO: iteration 42, average log likelihood -1.112761
WARNING: Variances had to be floored 1 4 14 20
INFO: iteration 43, average log likelihood -1.105762
WARNING: Variances had to be floored 9 16 23
INFO: iteration 44, average log likelihood -1.093326
WARNING: Variances had to be floored 2 3 4 6 17 27 28
INFO: iteration 45, average log likelihood -1.090257
WARNING: Variances had to be floored 1 20 31
INFO: iteration 46, average log likelihood -1.142832
WARNING: Variances had to be floored 8
INFO: iteration 47, average log likelihood -1.131995
WARNING: Variances had to be floored 9 23 27
INFO: iteration 48, average log likelihood -1.106566
WARNING: Variances had to be floored 4 14 16
INFO: iteration 49, average log likelihood -1.119547
WARNING: Variances had to be floored 1 8 17 20
INFO: iteration 50, average log likelihood -1.101546
INFO: EM with 100000 data points 50 iterations avll -1.101546
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0985052   -0.158732     -0.018267     0.0435069   0.0851874   -0.0669699     0.0656493   0.000509813   0.0352928     0.0778375   -0.124258    -0.0456641    0.0591       0.00453247  -0.0575955   -0.0304491   -0.143565      0.0596431    0.151439    -0.0337752     0.0916185   -0.00815277  -0.0564354    -0.047021     0.116299      0.0656153 
  0.194034     0.0855194    -0.0750244   -0.036116    0.059955    -0.116398      0.0147113  -0.0510947    -0.0201593    -0.111088    -0.199449    -0.0386586    0.0694562   -0.060848    -0.0969014    0.108495    -0.00191524   -0.0575689    0.0671232   -0.149562      0.0332231   -0.0123035   -0.000644358   0.0180861    0.0644582     0.0221787 
  0.20689      0.0942278    -0.067594    -0.152123   -0.167556     0.206245      0.128646   -0.0376739     0.000367871   0.134995     0.00137246  -0.0763134   -0.00579476  -0.00676231   0.130279    -0.0422892   -0.106336     -0.0345729   -0.0139726   -0.0475232     0.0934802    0.0265294   -0.0253276    -0.132701    -0.163725      0.0335957 
  0.0279726    0.0214438     0.00375862  -0.0574478  -0.132445    -0.230454      0.0384036  -0.0892887    -6.34514e-5    0.0956067    0.0794927   -0.0825279    0.0999578    0.075431     0.235134    -0.0945219    0.0108547    -0.0868848    0.255965    -0.00698561    0.148292     0.021147     0.0106042     0.00372334   0.134715      0.103798  
 -0.0667155    0.00834773   -0.0259834   -0.160685   -0.0466375   -0.0787516     0.0840791  -0.0633725    -0.00372982   -0.0613877   -0.00387086   0.0181193   -0.0100339   -0.0931268   -0.059488    -0.0149291    0.0418551     0.0453456   -0.151708     0.00633258    0.0929423   -0.0197679   -0.0874201     0.1658      -0.0191653    -0.0470616 
 -0.0492802   -0.0118607    -0.0873963    0.0675816   0.109266     0.0216051     0.0410297   0.213321     -0.0653339     0.0808211    0.0526993    0.108018     0.0306554   -0.138991    -0.0358099   -0.0858187   -0.0597459     0.0885395   -0.117892     0.00716984   -0.0819152   -0.0799967   -0.0355762     0.070544    -0.0520901     0.0454534 
 -0.0409367    0.0696305    -0.012874    -0.0518976  -0.0038614   -0.0676424    -0.258917   -0.0939108     0.141038      0.125802     0.0104186    0.114336    -0.200345    -0.104857    -0.0238279   -0.0780129    0.14353       0.00419926   0.0280185   -0.0220812     0.0505703    0.0375344   -0.0549565    -0.00593239  -0.103543      0.0539672 
  0.00546096   0.168605     -0.0161477   -0.244728   -0.0734921    0.0431041     0.0727601  -0.145999     -0.00212066    0.0190766   -0.0421718   -0.0947792    0.0441598   -0.0749132    0.135634     0.00781442  -0.0789871    -0.0540222    0.176919     0.0836835    -0.0480311    0.104095    -0.162134     -0.101432    -0.0754173     0.0525232 
 -0.196235    -0.127863      0.14098     -0.0873465  -0.0110515   -0.0187516     0.0801267  -0.0168479     0.0472897     0.0566011    0.00384894  -0.123502    -0.0918579    0.0932226   -0.0123929    0.046336     0.137736     -0.0607623    0.16626      0.00505286   -0.0712448   -0.113425    -0.237408     -0.161924    -0.000676397  -0.080211  
  0.0266642   -0.100928      0.0545491    0.0953897   0.0460448   -0.0533807     0.17005     0.0335985    -0.107335     -0.00537999   0.0625465    0.215956     0.291064    -0.0696084    0.0821939   -0.00628184  -0.0767882     0.0407985    0.0448241    0.133125     -0.155967     0.143808    -0.0584514    -0.0417963   -0.00877957   -0.0898416 
 -0.0463868   -0.116496     -0.0140333   -0.118263    0.0996747   -0.162347      0.0596102   0.131336     -0.00374244    0.0296331    0.0581501   -0.129631     0.103824     0.065828    -0.0594534   -0.0644798   -0.0259875     0.165559     0.00329237  -0.0383286     0.0766629   -0.109651    -0.240345     -0.0985355    0.0555632     0.0145649 
 -0.104085    -0.0325552    -0.0343285   -0.0962735   0.0186806    0.0991131    -0.032646   -0.0734942    -0.124956     -0.136914     0.128813     0.0914318    0.150908    -0.121706    -0.0461035    0.236089     0.202105     -0.116371    -0.117152    -0.0534492    -0.102102    -0.0453079    0.0286122     0.0488234    0.0616906     0.00594884
  0.063714     0.000327275  -0.0410034    0.041891    0.0508821    0.0651985    -0.1487      0.244243      0.0454626    -0.10546     -0.0191768   -0.0748252    0.0637766    0.00433798  -0.18188     -0.0243507   -0.0891632    -0.074625     0.0953311   -0.0930913    -0.0154386    0.0640033   -0.117189      0.154787    -0.124478     -0.00462044
  0.00847978   0.153347     -0.0864913   -0.0246604  -0.0367273   -0.0363531    -0.126916   -0.0105774    -0.0706548     0.0564165   -0.22429     -0.0550916    0.0684162   -0.0887795   -0.0396295   -0.0778291    0.0649242    -0.0477563   -0.0862074   -0.000640173   0.0217679    0.0701662   -0.00411943    0.00869765   0.0240424    -0.0472163 
 -0.0106955   -0.140907     -0.00816868   0.0907012  -0.0346265    0.0640207    -0.147069    0.106553      0.0403337    -0.0139833   -0.00348478  -0.0126314   -0.048492    -0.00684609   0.0846285   -0.11681      0.032883      0.159048    -0.065284    -0.0148437     0.0207812    0.0636343   -0.133066      0.13178      0.00055975   -0.00295103
  0.124874    -0.107433     -0.0762651    0.156138    0.0311225   -0.187251     -0.0880634  -0.0776325    -0.0973642     0.112109    -0.00827551   0.130958     0.226976     0.0141163    0.0590947   -0.512546    -0.371265     -0.017693    -0.00134188   0.0335819     0.164891    -0.116677     0.0685807     0.0845699   -0.0381248    -0.0126674 
 -0.0308325    0.181271     -0.0562766   -0.0716     -0.0965373    0.0107179    -0.0767211  -0.0472777    -0.11315       0.04199     -0.229082    -0.0753334    0.0424371   -0.098164    -0.079874    -0.0845355    0.0386686    -0.0853435   -0.0573254    0.0535607     0.0263954    0.0626351   -0.0240955    -0.0478299    0.113297     -0.0855811 
 -0.0497936   -0.0246086    -0.165765    -0.121874    0.154141     0.127205      0.158554    0.0833813     0.0377774    -0.108555     0.0403627    0.0169392    0.0644746   -0.0124401   -0.0186546   -0.0569149    0.0827991     0.0193764   -0.00536549   0.0364871     0.0747491   -0.0959648   -0.014375     -0.063214    -0.105258     -0.151506  
  0.0977923    0.0785095    -0.116909    -0.0197956   0.0720409   -0.13374       0.154625    0.0892024     0.0104173    -0.0206656    0.01376      0.0942839   -0.117344    -0.0999555    0.169541    -0.142728     0.0614755    -0.193896     0.0575891   -0.0820347    -0.0890237   -0.0554757    0.0336961    -0.124972     0.0140625     0.0497225 
  0.109033     0.000220245  -0.173413     0.0327712   0.0611652    0.068634     -0.0451764   0.0593153    -0.0446835     0.0853565    0.0701797   -0.0755763   -0.0979517    0.266049     0.161516    -0.034816    -0.0477418     0.229323     0.137118    -0.0415144    -0.188208     0.133984    -0.123573      0.0900051    0.0650689    -0.0475296 
 -0.0609567   -0.085459     -0.109127     0.0433544  -0.00204366  -0.00851035   -0.160447    0.0912964    -0.102947      0.0942048   -0.132741    -0.0721527   -0.0195551   -0.0765178   -0.0325886   -0.01652      0.104186     -0.0486838   -0.0378611   -0.05725       0.00975804   0.0211202    0.0177914    -0.0591278   -0.103576     -0.0561913 
  0.0326729    0.0272529     0.103999     0.0469371  -0.13961     -0.0827849    -0.0233673   0.0972604     0.154429     -0.0102177   -0.169779     0.0158746    0.0961404   -0.0936867    0.0460798    0.016504    -0.0706272     0.134058    -0.0263033   -0.0105393     0.110069    -0.0980764   -0.00759271   -0.0594254   -0.134946      0.0278184 
 -0.0448409    0.0210562    -0.0597936    0.110681    0.0340837   -0.0753611    -0.0435617  -0.0248195    -0.0463358     0.00542475   0.0182719    0.117928     0.00229106  -0.00620884  -0.243523     0.0463141   -0.104199      0.0826254   -0.00959484  -0.130427      0.0903378    0.0255862   -0.378036     -0.0937189   -0.0839666     0.219671  
 -0.0664329   -0.132273     -0.055428    -0.054355   -0.00960657  -0.0120123    -0.0465218   0.136209     -0.0206439     0.0721608   -0.0220748    0.0751734    0.112497     0.0168157    0.0623136    0.0769146   -0.0571967     0.0305603    0.0789956    0.0146448    -0.0482299   -0.0491843   -0.0339826     0.0136603   -0.0847564    -0.0342961 
  0.0371121   -0.06955      -0.128998     0.234564    0.0325901    0.00546988    0.135812   -0.0119473     0.207746      0.0636704   -0.0134164    0.128038     0.0952476   -0.250779    -0.0291673   -0.0731444   -0.121535      0.136131     0.0439073   -0.00578439   -0.087499     0.181862    -0.0972614     0.0763997   -0.179013     -0.0572053 
  0.0669229    0.0833405    -0.0260691    0.0345917  -1.09547     -0.000138528  -0.0814741   0.0466569    -0.0677953     0.0416679    0.103552    -0.0333823    0.0398018    0.0640796    0.00847539   0.119876     0.0587178     0.0832288    0.0203713    0.19329      -0.0933938    0.061365    -0.00425828   -0.0273675   -0.0335367     0.201653  
 -0.181266    -0.287702      0.0211923    0.144669    0.0999531   -0.0651941    -0.0783964  -0.0457656     0.128064     -0.015233    -0.225893    -0.0630205    0.260734     0.059959     0.16753      0.0185678    0.0372637     0.0589514   -0.0972351    0.13845       0.0120859   -0.0463332   -0.146286      0.0108089   -0.240157     -0.0239037 
  0.0957183    0.00509314   -0.0318189   -0.0543954   0.0656967    0.0882925     0.0438023   0.273034     -0.0781159     0.0301094   -0.00903182   0.00515874   0.107021    -0.0425819   -0.00922284  -0.0289812    0.000185514   0.0697343   -0.206781     0.043682     -0.103636    -0.0455793   -0.0348982    -0.0577279   -0.0378283     0.0418634 
 -0.0120407   -0.0545211    -0.102045    -0.0827903   0.0697687   -0.100444     -0.0102344   0.133171      0.102489     -0.195365    -0.00448131  -0.0369817   -0.0582678    0.0849716    0.0317601    0.0118582   -0.0371548    -0.0290878   -0.00678543  -0.0581709    -0.0696441   -0.105164    -0.0915444    -0.0880112   -0.0306736     0.0642113 
  0.0117255    0.100854      0.176642    -0.0718723  -0.249729    -0.166925      0.0728705   0.0394439     0.122422     -0.0783683   -0.17948      0.0223888   -0.120118    -0.0222216   -0.135298     0.0652681    0.104459      0.00598416  -0.0291924    0.0959197     0.116722     0.0475542   -0.210377      0.0347015   -0.0219877     0.0185563 
 -0.0288756    0.009669     -0.0500394   -0.0234441   0.0196189   -0.0198594    -0.0315552  -0.00152584   -0.0607879     0.0682228   -0.0705357    0.0834187    0.0207976   -0.0886997   -0.0816919   -0.0440363   -0.0319686     0.0313483   -0.0725667   -0.0848696     0.0694076    0.0224051   -0.320828     -0.0612956    0.0365898    -0.0548896 
  0.220862    -0.0617398    -0.0103911    0.0333965   1.47892     -0.041852     -0.0562249   0.234499     -0.0660962     0.049275    -0.0935541   -0.00985618   0.0515857   -0.0560949   -0.074144     0.119667     0.110934      0.130355     0.00951287   0.213978     -0.0800632    0.162704     0.0733287    -0.159178    -0.148662      0.198218  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 2 3 9 23 27 28
INFO: iteration 1, average log likelihood -1.105136
WARNING: Variances had to be floored 2 3 4 9 16 23 27 28 31
INFO: iteration 2, average log likelihood -1.053157
WARNING: Variances had to be floored 1 2 3 8 9 14 20 23 27 28
INFO: iteration 3, average log likelihood -1.048783
WARNING: Variances had to be floored 2 3 4 6 9 16 23 27 28 31
INFO: iteration 4, average log likelihood -1.071917
WARNING: Variances had to be floored 2 3 9 17 23 27
INFO: iteration 5, average log likelihood -1.065471
WARNING: Variances had to be floored 1 2 3 4 9 16 20 23 27 28 31
INFO: iteration 6, average log likelihood -1.053178
WARNING: Variances had to be floored 2 3 6 8 9 14 23 27 28
INFO: iteration 7, average log likelihood -1.066486
WARNING: Variances had to be floored 2 3 4 9 17 23 27 28 31
INFO: iteration 8, average log likelihood -1.055558
WARNING: Variances had to be floored 1 2 3 9 16 20 23 27 28
INFO: iteration 9, average log likelihood -1.065353
WARNING: Variances had to be floored 2 3 4 6 9 14 23 27 28 31
INFO: iteration 10, average log likelihood -1.059556
INFO: EM with 100000 data points 10 iterations avll -1.059556
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.212809     0.055974     0.0617518    0.0167923   -0.076401   -0.131648     0.0639604   -0.0983617    0.114015    -0.0807632   -0.132817    -0.165176    -0.0702739   -0.121558     0.0485884    0.00957332   -0.00228625    0.0204922    0.08648     0.0574257    0.0264977   -0.137563    -0.0106657   -0.118232     0.0353767   -0.0563729
 -0.0244374    0.00673311   0.041772    -0.13482      0.197416    0.0787162   -0.0491826   -0.119012     0.00697725  -0.0871338    0.141203    -0.0503127    0.0848739    0.123435    -0.0808046   -0.222078      0.0964673    -0.125533    -0.0496013   0.0518026    0.16974     -0.0431388    0.0398576   -0.220767    -0.086427    -0.157542 
  0.132473    -0.0585402   -0.00212259  -0.118279    -0.0882802  -0.111144    -0.0130325    0.243571    -0.0434065    0.024685    -0.0889452    0.0779374    0.0144532   -0.119575    -0.104651    -0.0436245    -0.0499511     0.0825712    0.0163573   0.234615     0.00207007   0.0595911   -0.0828323    0.0740202   -0.1533      -0.0598153
  0.0176408   -0.0652592   -0.155018     0.170881    -0.100077    0.120646     0.0757161   -0.179372     0.182838     0.0230495   -0.00542305   0.232233     0.145143     0.00744141   0.0133806    0.0820331    -0.122316     -0.019114    -0.0699048   0.147703     0.0941101    0.00906196   0.0530709   -0.0501986   -0.0595446   -0.037688 
 -0.118081    -0.115444    -0.144144     0.132381    -0.053633    0.0395936    0.0370453    0.0899711    0.134475     0.085227    -0.0321337   -0.0370137    0.0194638    0.0299174    0.0162266    0.00259708    0.0655066     0.0319697    0.0704084  -0.0561269    0.0247481   -0.0477275    0.106472    -0.153669     0.0518435   -0.122795 
 -0.0318217    0.120643     0.14743      0.0739543   -0.0864492  -0.0835382    0.0403928   -0.158292    -0.0658115   -0.0104108    0.0442353   -0.00718631   0.0942467   -0.0955102    0.0948481   -0.0601024     0.1006       -0.164811     0.0731858  -0.037761     0.0554249   -0.0021155   -0.0989533   -0.105992    -0.021762    -0.0619855
 -0.0145213   -0.127087    -0.17289     -0.218724    -0.237003    0.0312517   -0.13032      0.0820745   -0.00737594  -0.0221028   -0.0303604    0.0450781    0.140246    -0.175764     0.192164     0.029733     -0.156526     -0.0145508    0.0512757  -0.058142    -0.0295762   -0.164527    -0.0031379    0.0201737    0.00793377  -0.0465975
  0.0191833   -0.15447     -0.113711     0.0389838   -0.0496935   0.0529148    0.0195122   -0.0615838   -0.199356     0.199399     0.0085405   -0.148278    -0.0558561    0.0929844   -0.0471489   -0.0638752     0.064379      0.00265926   0.0114966  -0.057838     0.0699312    0.155379     6.11553e-5   0.0694182    0.127338    -0.108137 
 -0.0950488   -0.0310857    0.0137241    0.00450987   0.0251125   0.00727453   0.0975951    0.00465356   0.00596998  -0.0990874   -0.134562     0.104735    -0.0401214    0.0912311    0.172062    -0.0949342     0.00185158   -0.158282    -0.118692   -0.0435629    0.0925576   -0.0441694    0.0406538   -0.0917154    0.0611372    0.153153 
  0.0977259    0.0249813    0.195098     0.191005    -0.137582    0.110546    -0.0798758    0.0510258    0.0481698    0.0489002    0.0235354   -0.0849952   -0.0398372    0.00914851  -0.0993271   -0.00741195   -0.00627766    0.098184     0.0468376   0.0712941   -0.184415    -0.0494212    0.0258659    0.129609    -0.034883    -0.0818167
  0.0158282    0.0553279   -0.0686998    0.0916648   -0.0387979   0.00687416  -0.0499267    0.217988    -0.00370887  -0.0286046    0.142545    -0.0044501   -0.00231695   0.0833376    0.0153649    0.00524249    0.0266346     0.0369597   -0.16356     0.0396794    0.00682261   0.340519    -0.290861     0.030987    -0.0442033   -0.0750843
  0.06922     -0.0218472   -0.0673749    0.0611826   -0.0281996   0.085196     0.100721    -0.0461379    0.122334     0.125165    -0.143005     0.144521     0.204098     0.161585    -0.136273    -0.257459      0.079478     -0.1362      -0.185418    0.0850378    0.0391287   -0.0216319   -0.121799    -0.0726826   -0.124968     0.0983843
  0.110763     0.123434     0.152463     0.108531    -0.0756579  -0.00125601  -0.305899    -0.0585383    0.049006    -0.0484653   -0.0127783    0.136079    -0.0812483   -0.0416889    0.0696066   -0.00887038    0.0989031     0.113542     0.015118    0.15728      0.0841075   -0.0487223   -0.154031    -0.14022      0.132726    -0.134755 
  0.113062     0.185385    -0.218763    -0.039336    -0.0546527  -0.0204579   -0.0546396   -0.047052    -0.0889435    0.00233428  -0.0116078    0.0480029   -0.12454     -0.0774134   -0.0874385    0.173723     -0.0263892     0.0326646   -0.0766123   0.103212     0.0692898   -0.0292055   -0.127588     0.0325189    0.0235556    0.155325 
  0.00530082   0.0205486    0.0234468   -0.0495341    0.0920812  -0.154628     0.122022    -0.131488     0.126576    -0.104296     0.0145622   -0.0359199   -0.0918013   -0.0512134   -0.0372629    0.124734     -0.178739     -0.0627095    0.113597   -0.055569    -0.093332     0.0426796    0.114008    -0.0282685    0.0185933   -0.112313 
 -0.146471    -0.128022     0.0655397   -0.122432     0.225621   -0.0189159    0.0922562    0.160397     0.128979     0.127841    -0.0701554   -0.0422407   -0.018054    -0.0299403    0.0877024    0.149135     -0.094826     -0.00264605  -0.0233335  -0.0575712    0.121221    -0.0371842   -0.0613993   -0.0831093   -0.189097     0.182714 
  0.0480384    0.0136462    0.00887793   0.00253454   0.0536817   0.0126995    0.0520862   -0.201508     0.0661429    0.00669355  -0.0647413    0.0330203    0.149495     0.10672      0.0472386    0.143946      0.017581     -0.160426     0.038228   -0.0163528   -0.0538177   -0.00374724   0.078869    -0.0845006    0.0824663    0.0634354
 -0.105455     0.0187057   -0.095824     0.180031     0.191981   -0.0812852    0.0879747    0.126074    -0.140909     0.167972    -0.0635944    0.00172411  -0.189499     0.353223     0.0308047   -0.0529494    -0.0908787    -0.201915    -0.114028    0.0606209    0.0339819    0.0709492    0.14652      0.0201275    0.0868332    0.020322 
  0.00460753  -0.0482792   -0.0860786   -0.0766803    0.152154    0.0237359    0.033545     0.00382857   0.0390943   -0.0655763   -0.0219586   -0.0669042    0.0614664    0.0575195    0.0420548   -0.000289954   0.094559      0.0238059   -0.0771497   0.0327767   -0.134793    -0.109958     0.0143501    0.0813029   -0.0223932    0.0487394
  0.0489033   -0.0290129   -0.0890799    0.00207168   0.107183   -0.00723689   0.11144     -0.152778     0.0116105   -0.08037      0.117152    -0.00187527  -0.14596      0.0344503    0.219816     0.0330818    -0.118798      0.113103    -0.0453489  -0.00518162   0.165754    -0.00517316  -0.0171909   -0.0345707   -0.0262014    0.0182063
  0.0784472   -0.0143668    0.0172805    0.0499061   -0.037142   -0.0423145   -0.113952     0.0995738    0.0323815   -0.0164033    0.0671832    0.0314675    0.0673599    0.0605709    0.102625    -0.216165     -0.0750542    -0.129386    -0.0210739   0.0991612    0.112832    -0.0219396    0.0772387   -0.081062    -0.0423634   -0.022021 
 -0.0666822   -0.0829136    0.099001    -0.0887876   -0.0982951  -0.0848823   -0.0955056   -0.00708745  -0.0154854   -0.0718473   -0.0996438   -0.0182765   -0.138149     0.0850329   -0.0147652    0.0738143     0.108307      0.00519596   0.201807    0.0398559    0.126348    -0.0895352   -0.0144464    0.106029    -0.12675     -0.167212 
  0.0628468   -0.117675    -0.1078       0.137127    -0.100272    0.0750706   -0.0583846   -0.176299     0.00760868   0.0299147    0.0847141    0.101117     0.127062    -0.124754     0.0684214    0.0940629    -0.0229147    -0.0757788   -0.0579441  -0.0482357    0.125657    -0.129867     0.00243803   0.00964715  -0.126248    -0.0190334
 -0.0701062    0.0979816   -0.258566     0.02054      0.189653   -0.0213854    0.00121636  -0.00239575  -0.155112     0.200171    -0.142882    -0.0750126    0.173063    -0.115738     0.0959044    0.164893      0.0792409     0.091304    -0.0761903  -0.00907009  -0.138334    -0.146209     0.00557207  -0.0289502   -0.093521    -0.13155  
 -0.0122202   -0.100338    -0.0524509   -0.0706441   -0.0142871   0.0289987    0.145177    -0.0273979    0.0103463   -0.227765     0.174665    -0.00854717   0.145609    -0.0207597    0.0281903    0.117012     -0.150089     -0.0888949   -0.032801   -0.139842    -0.0524099   -0.0601005   -0.144687     0.0199842    0.0607818   -0.0410971
 -0.107146    -0.0146288    0.0945387   -0.088885     0.0443249   0.0266354   -0.118742     0.0328684    0.17706      0.00157307  -0.00561204  -0.0290751    0.0241786    0.0373126    0.0501236    0.0602298     0.0732002    -0.090197     0.0551252  -0.0692128   -0.0222433    0.16009     -0.0416787   -0.0475524   -0.128196    -0.0692511
  0.0143955   -0.16273      0.0293104    0.0960243    0.112227   -0.0103722   -0.0349308   -0.0369881   -0.00167374   0.0856845    0.0346009    0.0130245    0.0993333    0.0555782    0.0312959   -0.0463669     0.0221742     0.107216     0.0903702   0.0252712   -0.140013     0.0250908   -0.104905    -0.091269     0.169664     0.104368 
 -0.0221577    0.138015    -0.100485     0.0651485   -0.0494044   0.00514606   0.0813657   -0.162962     0.0794024    0.0773288    0.10529     -0.275597    -0.0468056    0.0650439   -0.0142908   -0.0726477     0.0400429     0.0256961   -0.0924453  -0.0116552    0.0186919    0.0248253    0.0574433   -0.0869976   -0.0263254   -0.0324205
 -0.0491828   -0.106165     0.0113715    0.00277088  -0.14799    -0.0813908   -0.053607    -0.0273115   -0.0733419   -0.108197     0.0524895    0.128666    -0.035496    -0.0887678    0.121348     0.205442      0.000338176   0.0343361   -0.0402505   0.165576    -0.149569     0.069762    -0.0475798    0.180652     0.0293113    0.114975 
 -0.202776    -0.0049174    0.0902696   -0.0249677    0.0781468   0.0465981   -0.0710649   -0.313848    -0.0872773   -0.14399      0.0522492   -0.0701779   -0.0895127    0.112383     0.00816589  -0.0998648     0.0750146    -0.211901     0.0467784  -0.151452    -0.0976302   -0.104602     0.0183027    0.0688039    0.231495     0.0161812
 -0.0746836   -0.0513803   -0.116478     0.111965    -0.0400015  -0.0715169   -0.196895     0.103165     0.168249    -0.0616221   -0.00754467  -0.0039708    0.104673    -0.0176973   -0.0082423    0.0339076    -0.00521296   -0.106939    -0.0732093  -0.0545486   -0.0151589   -0.0949459   -0.178629    -0.110398     0.0192113   -0.0319783
 -0.105041     0.208451    -0.0743843    0.0452031   -0.0317793  -0.0857266   -0.0729698   -0.00907526  -0.0543485   -0.0265505   -0.0217099   -0.226941     0.14129      0.100788     0.0613546    0.0290572     0.145082      0.173081     0.129202    0.0132482    0.0649859    0.0787302   -0.0909355    0.0863317    0.05183     -0.0924388kind full, method split
0: avll = -1.4177571512381357
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.417776
INFO: iteration 2, average log likelihood -1.417724
INFO: iteration 3, average log likelihood -1.417691
INFO: iteration 4, average log likelihood -1.417651
INFO: iteration 5, average log likelihood -1.417593
INFO: iteration 6, average log likelihood -1.417482
INFO: iteration 7, average log likelihood -1.417227
INFO: iteration 8, average log likelihood -1.416638
INFO: iteration 9, average log likelihood -1.415535
INFO: iteration 10, average log likelihood -1.414190
INFO: iteration 11, average log likelihood -1.413247
INFO: iteration 12, average log likelihood -1.412841
INFO: iteration 13, average log likelihood -1.412703
INFO: iteration 14, average log likelihood -1.412658
INFO: iteration 15, average log likelihood -1.412642
INFO: iteration 16, average log likelihood -1.412636
INFO: iteration 17, average log likelihood -1.412633
INFO: iteration 18, average log likelihood -1.412632
INFO: iteration 19, average log likelihood -1.412631
INFO: iteration 20, average log likelihood -1.412631
INFO: iteration 21, average log likelihood -1.412631
INFO: iteration 22, average log likelihood -1.412631
INFO: iteration 23, average log likelihood -1.412630
INFO: iteration 24, average log likelihood -1.412630
INFO: iteration 25, average log likelihood -1.412630
INFO: iteration 26, average log likelihood -1.412630
INFO: iteration 27, average log likelihood -1.412630
INFO: iteration 28, average log likelihood -1.412629
INFO: iteration 29, average log likelihood -1.412629
INFO: iteration 30, average log likelihood -1.412629
INFO: iteration 31, average log likelihood -1.412629
INFO: iteration 32, average log likelihood -1.412629
INFO: iteration 33, average log likelihood -1.412629
INFO: iteration 34, average log likelihood -1.412629
INFO: iteration 35, average log likelihood -1.412629
INFO: iteration 36, average log likelihood -1.412629
INFO: iteration 37, average log likelihood -1.412629
INFO: iteration 38, average log likelihood -1.412629
INFO: iteration 39, average log likelihood -1.412629
INFO: iteration 40, average log likelihood -1.412629
INFO: iteration 41, average log likelihood -1.412629
INFO: iteration 42, average log likelihood -1.412629
INFO: iteration 43, average log likelihood -1.412628
INFO: iteration 44, average log likelihood -1.412628
INFO: iteration 45, average log likelihood -1.412628
INFO: iteration 46, average log likelihood -1.412628
INFO: iteration 47, average log likelihood -1.412628
INFO: iteration 48, average log likelihood -1.412628
INFO: iteration 49, average log likelihood -1.412628
INFO: iteration 50, average log likelihood -1.412628
INFO: EM with 100000 data points 50 iterations avll -1.412628
952.4 data points per parameter
1: avll = [-1.41778,-1.41772,-1.41769,-1.41765,-1.41759,-1.41748,-1.41723,-1.41664,-1.41554,-1.41419,-1.41325,-1.41284,-1.4127,-1.41266,-1.41264,-1.41264,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.412643
INFO: iteration 2, average log likelihood -1.412596
INFO: iteration 3, average log likelihood -1.412563
INFO: iteration 4, average log likelihood -1.412528
INFO: iteration 5, average log likelihood -1.412489
INFO: iteration 6, average log likelihood -1.412445
INFO: iteration 7, average log likelihood -1.412396
INFO: iteration 8, average log likelihood -1.412344
INFO: iteration 9, average log likelihood -1.412291
INFO: iteration 10, average log likelihood -1.412241
INFO: iteration 11, average log likelihood -1.412194
INFO: iteration 12, average log likelihood -1.412153
INFO: iteration 13, average log likelihood -1.412117
INFO: iteration 14, average log likelihood -1.412086
INFO: iteration 15, average log likelihood -1.412060
INFO: iteration 16, average log likelihood -1.412039
INFO: iteration 17, average log likelihood -1.412020
INFO: iteration 18, average log likelihood -1.412004
INFO: iteration 19, average log likelihood -1.411988
INFO: iteration 20, average log likelihood -1.411974
INFO: iteration 21, average log likelihood -1.411959
INFO: iteration 22, average log likelihood -1.411945
INFO: iteration 23, average log likelihood -1.411930
INFO: iteration 24, average log likelihood -1.411915
INFO: iteration 25, average log likelihood -1.411899
INFO: iteration 26, average log likelihood -1.411884
INFO: iteration 27, average log likelihood -1.411868
INFO: iteration 28, average log likelihood -1.411852
INFO: iteration 29, average log likelihood -1.411835
INFO: iteration 30, average log likelihood -1.411819
INFO: iteration 31, average log likelihood -1.411803
INFO: iteration 32, average log likelihood -1.411788
INFO: iteration 33, average log likelihood -1.411773
INFO: iteration 34, average log likelihood -1.411758
INFO: iteration 35, average log likelihood -1.411744
INFO: iteration 36, average log likelihood -1.411731
INFO: iteration 37, average log likelihood -1.411719
INFO: iteration 38, average log likelihood -1.411707
INFO: iteration 39, average log likelihood -1.411696
INFO: iteration 40, average log likelihood -1.411686
INFO: iteration 41, average log likelihood -1.411676
INFO: iteration 42, average log likelihood -1.411667
INFO: iteration 43, average log likelihood -1.411659
INFO: iteration 44, average log likelihood -1.411651
INFO: iteration 45, average log likelihood -1.411643
INFO: iteration 46, average log likelihood -1.411637
INFO: iteration 47, average log likelihood -1.411630
INFO: iteration 48, average log likelihood -1.411624
INFO: iteration 49, average log likelihood -1.411619
INFO: iteration 50, average log likelihood -1.411613
INFO: EM with 100000 data points 50 iterations avll -1.411613
473.9 data points per parameter
2: avll = [-1.41264,-1.4126,-1.41256,-1.41253,-1.41249,-1.41244,-1.4124,-1.41234,-1.41229,-1.41224,-1.41219,-1.41215,-1.41212,-1.41209,-1.41206,-1.41204,-1.41202,-1.412,-1.41199,-1.41197,-1.41196,-1.41194,-1.41193,-1.41191,-1.4119,-1.41188,-1.41187,-1.41185,-1.41184,-1.41182,-1.4118,-1.41179,-1.41177,-1.41176,-1.41174,-1.41173,-1.41172,-1.41171,-1.4117,-1.41169,-1.41168,-1.41167,-1.41166,-1.41165,-1.41164,-1.41164,-1.41163,-1.41162,-1.41162,-1.41161]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.411619
INFO: iteration 2, average log likelihood -1.411561
INFO: iteration 3, average log likelihood -1.411510
INFO: iteration 4, average log likelihood -1.411448
INFO: iteration 5, average log likelihood -1.411373
INFO: iteration 6, average log likelihood -1.411281
INFO: iteration 7, average log likelihood -1.411175
INFO: iteration 8, average log likelihood -1.411060
INFO: iteration 9, average log likelihood -1.410945
INFO: iteration 10, average log likelihood -1.410840
INFO: iteration 11, average log likelihood -1.410749
INFO: iteration 12, average log likelihood -1.410676
INFO: iteration 13, average log likelihood -1.410619
INFO: iteration 14, average log likelihood -1.410576
INFO: iteration 15, average log likelihood -1.410543
INFO: iteration 16, average log likelihood -1.410518
INFO: iteration 17, average log likelihood -1.410498
INFO: iteration 18, average log likelihood -1.410482
INFO: iteration 19, average log likelihood -1.410469
INFO: iteration 20, average log likelihood -1.410458
INFO: iteration 21, average log likelihood -1.410448
INFO: iteration 22, average log likelihood -1.410439
INFO: iteration 23, average log likelihood -1.410431
INFO: iteration 24, average log likelihood -1.410424
INFO: iteration 25, average log likelihood -1.410417
INFO: iteration 26, average log likelihood -1.410410
INFO: iteration 27, average log likelihood -1.410404
INFO: iteration 28, average log likelihood -1.410398
INFO: iteration 29, average log likelihood -1.410392
INFO: iteration 30, average log likelihood -1.410387
INFO: iteration 31, average log likelihood -1.410381
INFO: iteration 32, average log likelihood -1.410376
INFO: iteration 33, average log likelihood -1.410371
INFO: iteration 34, average log likelihood -1.410366
INFO: iteration 35, average log likelihood -1.410361
INFO: iteration 36, average log likelihood -1.410357
INFO: iteration 37, average log likelihood -1.410352
INFO: iteration 38, average log likelihood -1.410348
INFO: iteration 39, average log likelihood -1.410343
INFO: iteration 40, average log likelihood -1.410339
INFO: iteration 41, average log likelihood -1.410334
INFO: iteration 42, average log likelihood -1.410330
INFO: iteration 43, average log likelihood -1.410326
INFO: iteration 44, average log likelihood -1.410322
INFO: iteration 45, average log likelihood -1.410317
INFO: iteration 46, average log likelihood -1.410313
INFO: iteration 47, average log likelihood -1.410309
INFO: iteration 48, average log likelihood -1.410305
INFO: iteration 49, average log likelihood -1.410300
INFO: iteration 50, average log likelihood -1.410296
INFO: EM with 100000 data points 50 iterations avll -1.410296
236.4 data points per parameter
3: avll = [-1.41162,-1.41156,-1.41151,-1.41145,-1.41137,-1.41128,-1.41117,-1.41106,-1.41095,-1.41084,-1.41075,-1.41068,-1.41062,-1.41058,-1.41054,-1.41052,-1.4105,-1.41048,-1.41047,-1.41046,-1.41045,-1.41044,-1.41043,-1.41042,-1.41042,-1.41041,-1.4104,-1.4104,-1.41039,-1.41039,-1.41038,-1.41038,-1.41037,-1.41037,-1.41036,-1.41036,-1.41035,-1.41035,-1.41034,-1.41034,-1.41033,-1.41033,-1.41033,-1.41032,-1.41032,-1.41031,-1.41031,-1.4103,-1.4103,-1.4103]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.410304
INFO: iteration 2, average log likelihood -1.410250
INFO: iteration 3, average log likelihood -1.410206
INFO: iteration 4, average log likelihood -1.410157
INFO: iteration 5, average log likelihood -1.410099
INFO: iteration 6, average log likelihood -1.410031
INFO: iteration 7, average log likelihood -1.409953
INFO: iteration 8, average log likelihood -1.409866
INFO: iteration 9, average log likelihood -1.409775
INFO: iteration 10, average log likelihood -1.409682
INFO: iteration 11, average log likelihood -1.409590
INFO: iteration 12, average log likelihood -1.409503
INFO: iteration 13, average log likelihood -1.409422
INFO: iteration 14, average log likelihood -1.409347
INFO: iteration 15, average log likelihood -1.409279
INFO: iteration 16, average log likelihood -1.409217
INFO: iteration 17, average log likelihood -1.409161
INFO: iteration 18, average log likelihood -1.409111
INFO: iteration 19, average log likelihood -1.409064
INFO: iteration 20, average log likelihood -1.409022
INFO: iteration 21, average log likelihood -1.408983
INFO: iteration 22, average log likelihood -1.408948
INFO: iteration 23, average log likelihood -1.408915
INFO: iteration 24, average log likelihood -1.408884
INFO: iteration 25, average log likelihood -1.408856
INFO: iteration 26, average log likelihood -1.408829
INFO: iteration 27, average log likelihood -1.408803
INFO: iteration 28, average log likelihood -1.408780
INFO: iteration 29, average log likelihood -1.408757
INFO: iteration 30, average log likelihood -1.408735
INFO: iteration 31, average log likelihood -1.408715
INFO: iteration 32, average log likelihood -1.408695
INFO: iteration 33, average log likelihood -1.408677
INFO: iteration 34, average log likelihood -1.408659
INFO: iteration 35, average log likelihood -1.408642
INFO: iteration 36, average log likelihood -1.408625
INFO: iteration 37, average log likelihood -1.408609
INFO: iteration 38, average log likelihood -1.408594
INFO: iteration 39, average log likelihood -1.408579
INFO: iteration 40, average log likelihood -1.408564
INFO: iteration 41, average log likelihood -1.408550
INFO: iteration 42, average log likelihood -1.408537
INFO: iteration 43, average log likelihood -1.408524
INFO: iteration 44, average log likelihood -1.408511
INFO: iteration 45, average log likelihood -1.408498
INFO: iteration 46, average log likelihood -1.408486
INFO: iteration 47, average log likelihood -1.408474
INFO: iteration 48, average log likelihood -1.408463
INFO: iteration 49, average log likelihood -1.408452
INFO: iteration 50, average log likelihood -1.408441
INFO: EM with 100000 data points 50 iterations avll -1.408441
118.1 data points per parameter
4: avll = [-1.4103,-1.41025,-1.41021,-1.41016,-1.4101,-1.41003,-1.40995,-1.40987,-1.40977,-1.40968,-1.40959,-1.4095,-1.40942,-1.40935,-1.40928,-1.40922,-1.40916,-1.40911,-1.40906,-1.40902,-1.40898,-1.40895,-1.40891,-1.40888,-1.40886,-1.40883,-1.4088,-1.40878,-1.40876,-1.40874,-1.40871,-1.4087,-1.40868,-1.40866,-1.40864,-1.40862,-1.40861,-1.40859,-1.40858,-1.40856,-1.40855,-1.40854,-1.40852,-1.40851,-1.4085,-1.40849,-1.40847,-1.40846,-1.40845,-1.40844]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.408438
INFO: iteration 2, average log likelihood -1.408374
INFO: iteration 3, average log likelihood -1.408313
INFO: iteration 4, average log likelihood -1.408242
INFO: iteration 5, average log likelihood -1.408155
INFO: iteration 6, average log likelihood -1.408050
INFO: iteration 7, average log likelihood -1.407928
INFO: iteration 8, average log likelihood -1.407791
INFO: iteration 9, average log likelihood -1.407648
INFO: iteration 10, average log likelihood -1.407504
INFO: iteration 11, average log likelihood -1.407365
INFO: iteration 12, average log likelihood -1.407234
INFO: iteration 13, average log likelihood -1.407115
INFO: iteration 14, average log likelihood -1.407008
INFO: iteration 15, average log likelihood -1.406912
INFO: iteration 16, average log likelihood -1.406828
INFO: iteration 17, average log likelihood -1.406754
INFO: iteration 18, average log likelihood -1.406689
INFO: iteration 19, average log likelihood -1.406631
INFO: iteration 20, average log likelihood -1.406580
INFO: iteration 21, average log likelihood -1.406534
INFO: iteration 22, average log likelihood -1.406493
INFO: iteration 23, average log likelihood -1.406455
INFO: iteration 24, average log likelihood -1.406419
INFO: iteration 25, average log likelihood -1.406387
INFO: iteration 26, average log likelihood -1.406356
INFO: iteration 27, average log likelihood -1.406326
INFO: iteration 28, average log likelihood -1.406298
INFO: iteration 29, average log likelihood -1.406272
INFO: iteration 30, average log likelihood -1.406246
INFO: iteration 31, average log likelihood -1.406221
INFO: iteration 32, average log likelihood -1.406196
INFO: iteration 33, average log likelihood -1.406173
INFO: iteration 34, average log likelihood -1.406150
INFO: iteration 35, average log likelihood -1.406127
INFO: iteration 36, average log likelihood -1.406105
INFO: iteration 37, average log likelihood -1.406083
INFO: iteration 38, average log likelihood -1.406062
INFO: iteration 39, average log likelihood -1.406041
INFO: iteration 40, average log likelihood -1.406021
INFO: iteration 41, average log likelihood -1.406001
INFO: iteration 42, average log likelihood -1.405982
INFO: iteration 43, average log likelihood -1.405963
INFO: iteration 44, average log likelihood -1.405945
INFO: iteration 45, average log likelihood -1.405927
INFO: iteration 46, average log likelihood -1.405910
INFO: iteration 47, average log likelihood -1.405894
INFO: iteration 48, average log likelihood -1.405878
INFO: iteration 49, average log likelihood -1.405862
INFO: iteration 50, average log likelihood -1.405847
INFO: EM with 100000 data points 50 iterations avll -1.405847
59.0 data points per parameter
5: avll = [-1.40844,-1.40837,-1.40831,-1.40824,-1.40816,-1.40805,-1.40793,-1.40779,-1.40765,-1.4075,-1.40736,-1.40723,-1.40712,-1.40701,-1.40691,-1.40683,-1.40675,-1.40669,-1.40663,-1.40658,-1.40653,-1.40649,-1.40645,-1.40642,-1.40639,-1.40636,-1.40633,-1.4063,-1.40627,-1.40625,-1.40622,-1.4062,-1.40617,-1.40615,-1.40613,-1.4061,-1.40608,-1.40606,-1.40604,-1.40602,-1.406,-1.40598,-1.40596,-1.40594,-1.40593,-1.40591,-1.40589,-1.40588,-1.40586,-1.40585]
[-1.41776,-1.41778,-1.41772,-1.41769,-1.41765,-1.41759,-1.41748,-1.41723,-1.41664,-1.41554,-1.41419,-1.41325,-1.41284,-1.4127,-1.41266,-1.41264,-1.41264,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41263,-1.41264,-1.4126,-1.41256,-1.41253,-1.41249,-1.41244,-1.4124,-1.41234,-1.41229,-1.41224,-1.41219,-1.41215,-1.41212,-1.41209,-1.41206,-1.41204,-1.41202,-1.412,-1.41199,-1.41197,-1.41196,-1.41194,-1.41193,-1.41191,-1.4119,-1.41188,-1.41187,-1.41185,-1.41184,-1.41182,-1.4118,-1.41179,-1.41177,-1.41176,-1.41174,-1.41173,-1.41172,-1.41171,-1.4117,-1.41169,-1.41168,-1.41167,-1.41166,-1.41165,-1.41164,-1.41164,-1.41163,-1.41162,-1.41162,-1.41161,-1.41162,-1.41156,-1.41151,-1.41145,-1.41137,-1.41128,-1.41117,-1.41106,-1.41095,-1.41084,-1.41075,-1.41068,-1.41062,-1.41058,-1.41054,-1.41052,-1.4105,-1.41048,-1.41047,-1.41046,-1.41045,-1.41044,-1.41043,-1.41042,-1.41042,-1.41041,-1.4104,-1.4104,-1.41039,-1.41039,-1.41038,-1.41038,-1.41037,-1.41037,-1.41036,-1.41036,-1.41035,-1.41035,-1.41034,-1.41034,-1.41033,-1.41033,-1.41033,-1.41032,-1.41032,-1.41031,-1.41031,-1.4103,-1.4103,-1.4103,-1.4103,-1.41025,-1.41021,-1.41016,-1.4101,-1.41003,-1.40995,-1.40987,-1.40977,-1.40968,-1.40959,-1.4095,-1.40942,-1.40935,-1.40928,-1.40922,-1.40916,-1.40911,-1.40906,-1.40902,-1.40898,-1.40895,-1.40891,-1.40888,-1.40886,-1.40883,-1.4088,-1.40878,-1.40876,-1.40874,-1.40871,-1.4087,-1.40868,-1.40866,-1.40864,-1.40862,-1.40861,-1.40859,-1.40858,-1.40856,-1.40855,-1.40854,-1.40852,-1.40851,-1.4085,-1.40849,-1.40847,-1.40846,-1.40845,-1.40844,-1.40844,-1.40837,-1.40831,-1.40824,-1.40816,-1.40805,-1.40793,-1.40779,-1.40765,-1.4075,-1.40736,-1.40723,-1.40712,-1.40701,-1.40691,-1.40683,-1.40675,-1.40669,-1.40663,-1.40658,-1.40653,-1.40649,-1.40645,-1.40642,-1.40639,-1.40636,-1.40633,-1.4063,-1.40627,-1.40625,-1.40622,-1.4062,-1.40617,-1.40615,-1.40613,-1.4061,-1.40608,-1.40606,-1.40604,-1.40602,-1.406,-1.40598,-1.40596,-1.40594,-1.40593,-1.40591,-1.40589,-1.40588,-1.40586,-1.40585]
32×26 Array{Float64,2}:
 -0.340931      0.202138    0.537868    -0.447807     0.176561    0.0785017     0.533495     0.939745   -0.568896     0.157435    0.180079    0.055643   -0.565982      0.141332   -0.0343834   0.172512   -0.132094     0.0479578    0.110823     -0.473245    0.157056     0.0522637  -0.844977    -0.163331     0.124921    0.287647  
  0.38762       0.615026    0.314279     0.411329    -0.518373   -0.700873      0.345678     0.569217    0.146652    -0.341825   -0.263538    0.0183302  -0.0872126    -0.210384    0.734436   -0.121541   -0.293894     0.0804577    0.0511199    -0.266221   -0.420727     0.423722   -0.586308    -0.289232    -0.580109    0.25134   
 -0.132576     -0.243807   -0.0259921    0.116562    -0.100715   -0.000360509   0.0368073   -0.151323   -0.115212     0.0471804  -0.162135    0.0530716   0.22934      -0.0704069   0.061997    0.291731    0.00699219  -0.285832    -0.0556563     0.0233225   0.0982046   -0.162192   -0.067633     0.0504502   -0.150884    0.190946  
 -0.0493449     0.0123277   0.262015    -0.304695    -0.102678    0.0278901    -0.860737     0.116372   -0.196906     0.372552   -0.0904538   0.243552   -0.217322      0.840494   -0.521927    0.071112    0.107056     0.615625     0.00677809   -0.24604     0.294513    -0.166253    0.418895    -0.0537804   -0.414052   -0.602003  
  0.117087      0.675398   -0.506867     0.367331    -0.717999   -0.400191      0.102495    -0.315713    0.124146    -0.0246355  -0.0576392  -0.0242445   0.0709151    -0.033777   -0.0417104   0.511403   -0.21048      0.369428    -0.0282697    -0.385992    0.68222     -0.4326     -0.412361    -0.46614      0.532651   -0.726073  
  0.286392     -0.298853   -0.293336     0.123841    -0.169084   -0.570846     -0.0444764   -0.384045   -0.244209     0.398409   -0.618088   -0.200448    0.416447      0.510774    0.238019    0.1838     -0.409465     0.896674    -0.131679     -0.392212    0.184684     0.0885445  -0.0689411   -0.275918     0.135572    0.350207  
 -0.626531     -0.462418   -0.477423     0.0360075    0.0470636   0.00335753   -0.364651    -0.115953    0.498547    -0.643846    0.0254944  -0.120793    0.430319      0.220694    0.454979    0.343564    0.289137    -0.156302    -0.494635     -0.0854627   0.0932405   -0.966106    0.110345    -0.816549    -0.0245412  -0.0721189 
 -0.0664559     0.453882   -0.0959388    0.163838     0.154472    0.863677      0.0941886   -0.0489048   0.525755    -0.114382    0.697429   -0.0275284  -0.264937     -0.220261    0.242161    0.20466     0.402851    -0.252104    -0.349988      0.294723    0.170662    -0.552941   -0.148631    -0.0147641   -0.131355   -0.550272  
  0.30359       0.351356    0.0213906    0.320075     0.350328   -0.370578     -0.0738551    0.0209965   0.186629    -0.221314    0.403259   -0.0840045  -0.779975     -0.0928435  -0.168612   -0.819854    0.122633     0.0970598    0.26109       0.115553   -0.449433     0.309397   -0.0555605   -0.0433232    0.385414   -0.254557  
  0.520862     -0.172243   -0.402347     0.205723     0.404996   -0.59163      -0.126703     0.0343292   0.307987    -0.171421    0.290438    0.202017    0.735313     -0.777586   -0.158435   -0.322088   -0.170813     0.46153     -0.371805      0.440534   -0.340017     0.433633    0.466522    -0.00392791   0.2886     -0.212203  
 -0.0875745    -0.871317   -0.136263    -0.209547     0.307401    0.404775     -0.0199794   -0.162041    0.184426    -0.0775882   0.562279   -0.326742    0.0176291     0.0219981  -0.12494    -0.607844    0.0556149   -0.391874    -0.0658094     0.308483   -0.37391      0.673278    0.0575235    0.551998    -0.371728    0.454728  
  0.214545      0.949565   -0.0602319   -0.632289    -0.0446746   0.860921      0.0178268   -0.0898728   0.0531749   -0.33772     0.312762   -0.116832   -0.000201522   0.229771    0.313064   -0.462468   -0.44898      0.500612    -0.156079      0.514389   -0.968045     0.355828   -0.0459169    0.354923     0.0143405   0.0260346 
  0.0529099    -0.406789    0.0521784   -0.422622    -0.387665   -0.801826      0.387469     0.186912    0.169255    -0.307521   -0.474698   -0.18019    -0.125678      0.394288   -0.305218   -0.672639   -0.255093    -0.199432     0.871746     -0.341351    0.140482     0.355133    0.176634     0.185611    -0.0648085  -0.0723311 
 -0.342741      0.149916    0.248165    -0.0912084   -0.356116   -0.209029      0.849386    -0.109212   -0.0785079    0.447571   -0.502432    0.268604    0.135451      0.388647    1.18203    -0.3261     -0.019118    -0.976798     0.0956039    -0.140048    0.0381922    0.0915449   0.829766     0.0217569    0.274179   -0.34726   
 -0.6495       -0.124736   -0.186471    -0.282107     0.192997   -0.406926      0.605009    -0.0433649   0.160966     0.676752    0.316982   -0.078324   -0.424095      0.107805    0.156978    0.116591    0.0308105   -0.00645587   0.319903      0.337899    0.0324383    0.540735    0.00386879   0.375972     0.492192   -0.471536  
  0.182047      0.394641    0.567356     0.197053     0.183936    0.314055     -0.165033    -0.142455   -0.0410491    0.710201   -0.233754   -0.298994   -0.446595     -0.167703   -0.417698   -0.255479    0.256749    -0.00369488   0.39405       0.585129    0.00317304   0.621253   -0.110718     0.0645043   -0.0113638   0.223307  
 -0.0742021    -0.0223493  -0.453828     0.218452    -0.0713879  -0.371882     -0.00411604   0.344236   -0.345029    -0.239217    0.331604    0.767033    0.0745301    -0.205386    0.38429     0.0960649  -0.0609001    0.0117395   -0.28624      -0.178708    0.563399    -0.642458   -0.247695    -0.109186    -0.0311338  -0.26203   
 -0.628346     -0.17038    -0.081763     0.533595    -0.20639    -0.590752     -0.212964     0.106227   -0.852055    -0.234549    0.621524    0.542368   -0.0265166     0.702358    0.230572    0.20127    -0.140054    -0.589342    -0.197572     -0.0747915   0.309427     0.210244    0.190242     0.00919213  -0.41368    -0.177996  
  0.00772399    0.121781    0.00961777  -0.115622     0.0213867  -0.0721983     0.0729398    0.0513599  -0.0587111    0.0644991  -0.0491993   0.0668868   0.0469215    -0.169775    0.0432566  -0.0103938  -0.330439     0.147501    -0.0409104     0.159606   -0.174959     0.211682   -0.12145      0.222269    -0.0540265   0.0922707 
 -0.0532509    -0.128801    0.00743638  -0.00152982  -0.0639459   0.0512893     0.00543612  -0.0428074   0.086303    -0.0500958   0.0181025  -0.105434   -0.0210379     0.118615    0.0445262   0.0413826   0.152403    -0.097523     0.0232341    -0.0713568   0.0897383   -0.151796   -0.0660065   -0.182827     0.0553538  -0.0590079 
  0.365729     -0.0777687   0.322265     0.119931     0.430005   -0.213425      0.240885     0.172753    0.229048     0.139671   -0.137993   -0.0906746  -0.291034      0.191897    0.306945   -0.047668    0.656758     0.289077    -0.261068     -0.708172   -0.0643891    0.158662    0.761097    -0.509103    -0.254931   -0.12091   
  0.0317144     0.0810994  -0.199557     0.149585    -0.0583163   0.0298765    -0.0386421    0.23994     0.297652     0.150563    0.0127305  -0.0644993   0.0416825     0.17594     0.206541   -0.361024    0.424008     0.276393     0.395163      0.423904   -0.426145     0.300882    0.465992    -0.0227956   -0.0525047  -0.50616   
  0.000348266   0.177599    0.262951    -0.319169     0.460192   -0.619644     -0.374373     0.42339     0.456949    -0.18789    -0.161752   -0.17242    -0.217837      0.099566    0.0475119  -0.370411   -0.209399     0.186828    -0.00758662   -0.0891239  -0.0613548    0.126082   -0.253838    -0.363236     0.403115   -0.379484  
  0.140039     -0.0666681  -0.0957225   -0.509859     0.472024    0.485766      0.35798      0.0223699   0.333759     0.193963   -0.638248   -0.833202    0.0262237    -0.144667   -0.0309062  -0.247522   -0.148661     0.094722     0.0246289    -0.0368264  -0.0590553   -0.24564    -0.095233    -0.0990409    0.923809    0.19693   
  0.285324      0.0665065   0.418242     0.224011     0.131524    0.116683     -0.989482    -0.224713   -0.0557903   -0.668758   -0.162094   -0.169394   -0.0303378    -0.129292   -0.215368    0.161548   -0.136473     0.086292    -0.134993      0.037927   -0.205404    -0.280942   -0.413405    -0.435334    -0.314585    0.212241  
  0.550853      0.299551   -0.0313104    0.491052     0.0955493   0.0813451    -0.352842    -0.0291083  -0.398693    -0.0609588   0.105053    0.368788   -0.111219     -0.0836378  -0.320028    0.010816    0.0505299    0.0674955   -0.268361     -0.0713401   0.0117181    0.145895    0.0344036    0.386202    -0.585988    0.419271  
 -0.112962      0.406413   -0.525136     0.167339     0.248803   -0.105971     -0.427331    -0.494355    0.00561097  -0.433179    0.0596274  -0.16673    -0.430634      0.130305   -0.609461   -0.0688556  -0.260743     0.0426198   -0.0453483     0.382047    0.529493    -0.122109    0.200768    -0.187014     0.435522    0.142959  
 -0.610503     -0.593939   -0.0352086    0.0191773    0.0849828   0.426349     -0.466788    -0.282986   -0.18402      0.221557    0.369206   -0.181154   -0.127195     -0.048507   -0.515622    0.402392    0.664754    -0.123066     0.005307      0.279696    0.270013    -0.304849    0.122473    -0.204497     0.161982   -0.00625866
 -0.114286     -0.486422    0.085644    -0.295967    -0.109178    0.243279      0.219616     0.309949   -0.120727     0.450166   -0.171271    0.374666    0.767873     -0.377123    0.394642    0.239888   -0.166528    -0.126884    -0.29052      -0.0663271  -0.467226    -0.175518   -0.267261     0.202859    -0.298492    0.109514  
 -0.146982     -0.204404   -0.0381807   -0.322863    -0.363523    0.350486      0.0134082   -0.156787   -0.016724     0.281129   -0.557021   -0.153761    0.204397      0.129045    0.160308    0.840028    0.118164    -0.313645    -0.000664039   0.0517676   0.625555    -0.196856   -0.217921     0.207395    -0.327565    0.167274  
  0.139203     -0.575825   -0.30344      0.444849    -0.223083   -0.0628004     0.315055    -0.521326    0.0212183   -0.313915    0.192723    0.150293    0.417432      0.154162   -0.124618   -0.0595141   0.195845    -0.68725      0.0776724    -0.680105    0.144838    -0.0998576   0.0696147    0.00203907   0.203352   -0.0634168 
 -0.0663151     0.129268    0.00107767   0.295877    -0.567626    0.39467       0.349445    -0.694784    0.00159618  -0.149292    0.0354498   0.0626502   0.59959      -0.3073      0.0962558   0.133351   -0.526727    -0.550884     0.394285      0.373496   -0.0279407   -0.1656     -0.0396962    0.295616     0.0283142   0.421023  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.405833
INFO: iteration 2, average log likelihood -1.405819
INFO: iteration 3, average log likelihood -1.405806
INFO: iteration 4, average log likelihood -1.405793
INFO: iteration 5, average log likelihood -1.405780
INFO: iteration 6, average log likelihood -1.405768
INFO: iteration 7, average log likelihood -1.405757
INFO: iteration 8, average log likelihood -1.405745
INFO: iteration 9, average log likelihood -1.405734
INFO: iteration 10, average log likelihood -1.405724
INFO: EM with 100000 data points 10 iterations avll -1.405724
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.995554e+05
      1       6.959655e+05      -2.035899e+05 |       32
      2       6.840077e+05      -1.195779e+04 |       32
      3       6.792077e+05      -4.799965e+03 |       32
      4       6.765158e+05      -2.691947e+03 |       32
      5       6.748426e+05      -1.673218e+03 |       32
      6       6.737430e+05      -1.099521e+03 |       32
      7       6.728736e+05      -8.694351e+02 |       32
      8       6.721612e+05      -7.123727e+02 |       32
      9       6.716148e+05      -5.464182e+02 |       32
     10       6.712219e+05      -3.929252e+02 |       32
     11       6.708989e+05      -3.230365e+02 |       32
     12       6.706208e+05      -2.781015e+02 |       32
     13       6.703773e+05      -2.434133e+02 |       32
     14       6.701735e+05      -2.038438e+02 |       32
     15       6.699851e+05      -1.883784e+02 |       32
     16       6.698112e+05      -1.739266e+02 |       32
     17       6.696585e+05      -1.526790e+02 |       32
     18       6.695250e+05      -1.335205e+02 |       32
     19       6.694043e+05      -1.207111e+02 |       32
     20       6.692871e+05      -1.172118e+02 |       32
     21       6.691944e+05      -9.262382e+01 |       32
     22       6.691164e+05      -7.807587e+01 |       32
     23       6.690502e+05      -6.613389e+01 |       32
     24       6.689852e+05      -6.501456e+01 |       32
     25       6.689271e+05      -5.808046e+01 |       32
     26       6.688755e+05      -5.166915e+01 |       32
     27       6.688306e+05      -4.488359e+01 |       32
     28       6.687873e+05      -4.325932e+01 |       32
     29       6.687466e+05      -4.069010e+01 |       32
     30       6.687023e+05      -4.437120e+01 |       32
     31       6.686629e+05      -3.941183e+01 |       32
     32       6.686292e+05      -3.361954e+01 |       32
     33       6.685992e+05      -3.003759e+01 |       32
     34       6.685732e+05      -2.598229e+01 |       32
     35       6.685486e+05      -2.465339e+01 |       32
     36       6.685219e+05      -2.664024e+01 |       32
     37       6.684962e+05      -2.575089e+01 |       32
     38       6.684719e+05      -2.424489e+01 |       32
     39       6.684497e+05      -2.223087e+01 |       32
     40       6.684279e+05      -2.182090e+01 |       32
     41       6.684070e+05      -2.086481e+01 |       32
     42       6.683856e+05      -2.137181e+01 |       32
     43       6.683649e+05      -2.070539e+01 |       32
     44       6.683454e+05      -1.956545e+01 |       32
     45       6.683243e+05      -2.103968e+01 |       32
     46       6.682989e+05      -2.540790e+01 |       32
     47       6.682746e+05      -2.434113e+01 |       32
     48       6.682507e+05      -2.390656e+01 |       32
     49       6.682255e+05      -2.520428e+01 |       32
     50       6.682037e+05      -2.177506e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 668203.6934695046)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.417897
INFO: iteration 2, average log likelihood -1.412717
INFO: iteration 3, average log likelihood -1.411333
INFO: iteration 4, average log likelihood -1.410318
INFO: iteration 5, average log likelihood -1.409235
INFO: iteration 6, average log likelihood -1.408223
INFO: iteration 7, average log likelihood -1.407536
INFO: iteration 8, average log likelihood -1.407164
INFO: iteration 9, average log likelihood -1.406959
INFO: iteration 10, average log likelihood -1.406826
INFO: iteration 11, average log likelihood -1.406727
INFO: iteration 12, average log likelihood -1.406647
INFO: iteration 13, average log likelihood -1.406579
INFO: iteration 14, average log likelihood -1.406518
INFO: iteration 15, average log likelihood -1.406464
INFO: iteration 16, average log likelihood -1.406415
INFO: iteration 17, average log likelihood -1.406370
INFO: iteration 18, average log likelihood -1.406328
INFO: iteration 19, average log likelihood -1.406290
INFO: iteration 20, average log likelihood -1.406254
INFO: iteration 21, average log likelihood -1.406221
INFO: iteration 22, average log likelihood -1.406190
INFO: iteration 23, average log likelihood -1.406162
INFO: iteration 24, average log likelihood -1.406135
INFO: iteration 25, average log likelihood -1.406109
INFO: iteration 26, average log likelihood -1.406086
INFO: iteration 27, average log likelihood -1.406063
INFO: iteration 28, average log likelihood -1.406042
INFO: iteration 29, average log likelihood -1.406023
INFO: iteration 30, average log likelihood -1.406004
INFO: iteration 31, average log likelihood -1.405986
INFO: iteration 32, average log likelihood -1.405970
INFO: iteration 33, average log likelihood -1.405954
INFO: iteration 34, average log likelihood -1.405939
INFO: iteration 35, average log likelihood -1.405924
INFO: iteration 36, average log likelihood -1.405911
INFO: iteration 37, average log likelihood -1.405898
INFO: iteration 38, average log likelihood -1.405885
INFO: iteration 39, average log likelihood -1.405874
INFO: iteration 40, average log likelihood -1.405862
INFO: iteration 41, average log likelihood -1.405851
INFO: iteration 42, average log likelihood -1.405841
INFO: iteration 43, average log likelihood -1.405831
INFO: iteration 44, average log likelihood -1.405822
INFO: iteration 45, average log likelihood -1.405812
INFO: iteration 46, average log likelihood -1.405804
INFO: iteration 47, average log likelihood -1.405795
INFO: iteration 48, average log likelihood -1.405787
INFO: iteration 49, average log likelihood -1.405779
INFO: iteration 50, average log likelihood -1.405771
INFO: EM with 100000 data points 50 iterations avll -1.405771
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.439364    -0.214274    -0.32028      0.0151998   -0.020745    0.0770446  -0.0868716   -0.113816    0.164583    -0.312621    -0.0473289  -0.210539     0.211012    0.178412     0.130152     0.531668     0.283884     -0.239695   -0.27096    -0.105857     0.290219    -0.815868   -0.0743193   -0.442147     0.160917   -0.204057 
 -0.210916    -0.652982     0.309661    -0.410773    -0.235751    0.159754    0.381367     0.278429   -0.388998     0.765668    -0.363672    0.472836     0.348704   -0.14749      0.458217     0.498519    -0.181638     -0.350816   -0.0738896  -0.335915     0.0151042   -0.0777835  -0.579181     0.149351    -0.273723    0.141926 
  0.0779105    0.151722     0.2332       0.0594813   -0.137149    0.0931903  -0.0805265    0.0965873   0.0174239   -0.0150488   -0.054485    0.0484149    0.0741821  -0.0842658    0.152373    -0.00105776  -0.0755658     0.0336343  -0.0538213   0.0297842   -0.370369     0.0582784  -0.12086      0.114375    -0.318277    0.0107538
  0.0934339    0.193965     0.490479    -0.0899576    0.694642    0.148138    0.00907823   0.0901754   0.118808     0.829585    -0.826369   -0.893195    -0.439321   -0.0934947   -0.156429    -0.0966823    0.287111      0.285461    0.212567   -0.0455602    0.238397     0.388427    0.243078    -0.0671752    0.153814    0.265068 
 -0.317964    -0.156662    -0.342821     0.425081    -0.191181   -0.604548   -0.0851855    0.226861   -0.552136    -0.539663     0.62505     0.759639     0.0518256   0.245413     0.314598     0.104936     0.000659138  -0.422091   -0.326061   -0.310056     0.399902    -0.283796   -0.0863716   -0.100456    -0.196488   -0.323746 
 -0.00702887   0.1196       0.0207037   -0.44547      0.719749   -0.277021    0.116437     0.36811    -0.153445     0.2518       0.258145    0.0935164   -0.729768    0.197387    -0.320615    -0.334379    -0.00760443    0.0143528   0.341198    0.259773     0.18373      0.205289   -0.632871     0.607137     0.510017   -0.247926 
 -0.102078     0.22092     -0.0416061   -0.101284     0.230027   -0.290308    0.188225     0.396589    0.320604     0.349763    -0.0703508   0.132571    -0.0543331   0.204705     0.425811    -0.169104     0.498813      0.526204    0.172401   -0.162836    -0.354554     0.273648    0.70955     -0.432744     0.203572   -0.829435 
 -0.19023     -0.0912141   -0.396713     0.334156    -0.464124    0.138512    0.298249    -0.943445    0.00780214  -0.0389418    0.0930615   0.00111443   0.411565   -0.0497101   -0.205038     0.445034    -0.364103     -0.616798    0.568493   -0.0743528    0.277343    -0.112056   -0.00543615   0.255018     0.363409   -0.0533845
  0.225622    -0.0790409    0.130669    -0.226914    -0.392985   -0.708235    0.229134     0.406566    0.393634    -0.630832    -0.386063   -0.304293    -0.168777    0.369514    -0.0965677   -0.752149    -0.113905     -0.213559    0.750331   -0.311258     0.0376825    0.237801   -0.0926376   -0.167126     0.0180492  -0.117139 
  0.0997597   -0.393426    -0.699519    -0.142037    -0.0554535  -0.568912    0.175579    -0.431956   -0.234056     0.270488    -0.500442   -0.427532    -0.0404642   0.800398     0.441002     0.239233    -0.504259      0.55203    -0.0547553  -0.645457     0.357849     0.0544037   0.121106    -0.143251     0.163011    0.121576 
  0.0564929    0.0399698    0.22438      0.0214748    0.375333   -0.160201   -0.933419    -0.115886    0.264954    -0.581435    -0.12844    -0.429768    -0.144712   -0.124377    -0.196377     0.087419    -0.0736957     0.291922   -0.226904    0.00851649  -0.285962    -0.216638   -0.542497    -0.73379     -0.034084   -0.179364 
  0.268497    -0.00443748   0.421982     0.188298    -0.286479    0.447019   -0.498151    -0.162465   -0.456994    -0.498081    -0.217877    0.0736493    0.0676215   0.0409569   -0.144757     0.367401     0.0983055    -0.24128     0.121331   -0.053131     0.238644    -0.505351   -0.525721    -0.0155355   -0.334077    0.676148 
 -0.057455     0.396981     0.0660759    0.16535     -0.0478027   0.802872    0.0771879   -0.0599434   0.559674    -0.0782897    0.657034   -0.0442975   -0.296588   -0.102255     0.225479     0.117432     0.450116     -0.137192   -0.185887    0.257195     0.135343    -0.575201   -0.167494    -0.0644538   -0.202773   -0.587725 
  0.124611     0.221602    -0.613457     0.00743895   0.526732    0.427495   -0.165616     0.0462068  -0.222922     0.308769     0.0914514   0.379358     0.334088   -0.604533    -0.0261803    0.752134    -0.175313      0.0201928  -0.781631    0.207771     0.101982    -0.337906   -0.303363     0.342643    -0.0374441   0.338464 
 -0.124783    -0.728802    -0.763743    -0.0226725   -0.0477986   0.362607    0.128181    -0.418081    0.263616    -0.390078     0.245813    0.02629      0.687701    0.166425     0.00129047  -0.312444     0.051078     -0.389533   -0.259654   -0.00972067   0.00995948  -0.323694   -0.0296938   -0.00401973  -0.0290337   0.36514  
  0.500335     0.472947     0.385149    -0.113374    -0.26193     0.199011   -0.250007    -0.541989   -0.215677     0.325481    -0.137344    0.00968585   0.203855    0.0524498   -0.460119    -0.403182    -0.490409      0.426761    0.0662587   0.396909    -0.31918      0.740908   -0.370145     0.21094     -0.0363864   0.385127 
 -0.5399       0.0736518    0.159138    -0.108672    -0.264007   -0.141604    0.742301    -0.195063   -0.0833255    0.564448    -0.459457    0.301168     0.0681327   0.627055     0.849941    -0.178107     0.00327461   -1.20901     0.119502    0.0497435    0.213917    -0.0286138   0.811613     0.104088     0.206791   -0.382887 
  0.271717     0.521495    -0.29792      0.550569     0.223331   -0.150007   -0.403684    -0.339511    0.017196    -0.468972     0.189736   -0.0439515   -0.548553    0.214778    -0.524936    -0.280208     0.105336      0.117069   -0.0510774   0.213686     0.157287     0.14541     0.602222    -0.152691     0.135429    0.112332 
 -0.0443951   -0.175184    -0.327744    -0.0977761   -0.526959    0.101205    0.00720017  -0.0229984   0.452693     0.420982    -0.601801   -0.426944     0.550632   -0.24612      0.0346546    0.643308     0.325116      0.21163     0.117795    0.59149      0.110845     0.313883    0.13964      0.292112    -0.549629   -0.170091 
 -0.253696    -0.0439519    0.00622898  -0.501716     0.195757    0.249271    0.255051    -0.0306937   0.33499      0.221801    -0.388429   -0.598262     0.0588807   0.0144785    0.101165     0.00125661  -0.208666      0.0131606   0.021985    0.135647    -0.0328646   -0.163475   -0.0552183   -0.185676     0.712448   -0.0384831
  0.224382     0.277944    -0.240751    -0.705664     0.281862    0.720711   -0.0115889    0.057496    0.160583    -0.464055     0.344017   -0.332403    -0.0403666   0.087609     0.31464     -0.551956    -0.140303      0.293667   -0.0451119   0.414026    -0.988401     0.323405    0.0944129    0.500602     0.0304347   0.0487613
  0.213941     0.316409     0.242363     0.0382239   -0.535066   -0.0773456   0.779133    -0.0186927  -0.0335628   -0.107481    -0.181948    0.15643      0.368629   -0.46668      0.52367     -0.108625    -0.36996      -0.525845    0.0821395   0.0129396   -0.439579     0.292842    0.0991913    0.50497     -0.2619      0.505525 
 -0.0372434   -0.136284    -0.230546    -0.0682213   -0.0835571  -0.229715    0.4019       0.201199   -0.180222     0.284032     0.0867131   0.266126     0.0270635  -0.00806824   0.18964     -0.240594    -0.0386949     0.14443     0.333919    0.133633    -0.0456946    0.497206    0.0995515    0.33864      0.0609801  -0.112643 
  0.113005     0.493986    -0.193347     0.395323    -0.651949   -0.555779   -0.110596    -0.116038    0.0220002   -0.00601226  -0.285787    0.360738     0.0987208  -0.177388     0.173279     0.409087    -0.355241      0.546061   -0.118186   -0.125246     0.849183    -0.614707   -0.268605    -0.536758     0.204392   -0.455791 
 -0.208859    -0.304519     0.276173     0.507865     0.229246   -0.0764351   0.436994    -0.281033    0.419948     0.191658     0.705109   -0.41424     -0.538754   -0.493923     0.0164517   -0.479393     0.283413     -0.440842    0.219296    0.355782    -0.296251     0.856307   -0.0333488    0.158931     0.0870757  -0.0291528
 -0.136981     0.421385     0.589389    -0.00205428   0.0380824  -0.229327    0.370659     1.06092    -0.372778     0.111264     0.206303    0.0617065   -0.367707    0.0862075    0.136943     0.040122    -0.192103      0.318111   -0.0587231  -0.366609    -0.0287083    0.219497   -0.861564    -0.322532    -0.108948    0.293007 
  0.215988    -0.56961      0.231885     0.36805      0.147331   -0.22725    -0.0299716   -0.111886    0.283281    -0.119653    -0.319565   -0.0895949    0.355175   -0.0307316    0.248528     0.0773074    0.430934     -0.147658   -0.392565   -0.497158     0.0195119    0.138267    0.74756     -0.436947    -0.663774    0.302643 
 -0.51444     -0.433987    -0.0226918   -0.0888542    0.0408288   0.253671   -0.5326      -0.182143   -0.473007     0.368851     0.358351   -0.0239938   -0.291568    0.270108    -0.391179     0.111507     0.39875      -0.320957    0.0714555   0.309232     0.271542     0.148837    0.474544     0.186882    -0.218189    0.0711889
 -0.0646637   -0.141557    -0.115616    -0.00505988   0.0397095  -0.0310725  -0.078138    -0.114526    0.0247866   -0.104202     0.0217476  -0.0802713   -0.0586951   0.0776779   -0.0878114    0.144447     0.0911617    -0.0943574  -0.0174846  -0.0755354    0.315564    -0.198628   -0.0956501   -0.180024     0.151372   -0.0228787
 -0.134452    -0.662692     0.967586    -0.0745747   -0.351178   -0.058752   -0.704705    -0.0510623   0.068783     0.610579     0.215348    0.817349     0.183242    0.865294    -0.699752     0.252388    -0.131042      0.912022   -0.0246713  -0.496017     0.157299    -0.304022    0.238462     0.080481    -0.457565   -0.66569  
  0.183652     0.0718344   -0.0876537    0.209223    -0.0284067  -0.118615   -0.165372    -0.144345   -0.0136419   -0.032343     0.148484    0.146306    -0.0922663  -0.0290849   -0.0904379   -0.281148    -0.0872018     0.112094    0.0858512   0.126275    -0.173493     0.216522    0.0240238    0.062973    -0.0250554  -0.106453 
  0.306747    -0.12042     -0.0924498    0.278295     0.530117   -0.447382   -0.308787     0.24285     0.0403447   -0.0568545    0.187233    0.359509     0.491816   -0.468434    -0.0915268   -0.560784    -0.397045      0.27722    -0.438236    0.272331    -0.46702      0.0748684   0.299986    -0.170978     0.145429    0.0635012INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.405763
INFO: iteration 2, average log likelihood -1.405756
INFO: iteration 3, average log likelihood -1.405748
INFO: iteration 4, average log likelihood -1.405741
INFO: iteration 5, average log likelihood -1.405734
INFO: iteration 6, average log likelihood -1.405727
INFO: iteration 7, average log likelihood -1.405720
INFO: iteration 8, average log likelihood -1.405713
INFO: iteration 9, average log likelihood -1.405706
INFO: iteration 10, average log likelihood -1.405700
INFO: EM with 100000 data points 10 iterations avll -1.405700
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
