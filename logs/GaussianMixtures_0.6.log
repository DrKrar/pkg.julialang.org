>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.6.6
INFO: Installing JLD v0.6.5
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.0
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.2.0
INFO: Installing StaticArrays v0.0.11
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1172
Commit 7aba3f5 (2016-11-01 17:23 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-100-generic #147-Ubuntu SMP Tue Oct 18 16:48:51 UTC 2016 x86_64 x86_64
Memory: 2.939289093017578 GB (663.6953125 MB free)
Uptime: 23398.0 sec
Load Avg:  0.95703125  0.962890625  1.02392578125
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3499 MHz    1441838 s       4051 s     146034 s     511273 s         56 s
#2  3499 MHz     632904 s       2423 s      77506 s    1551817 s          1 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.3
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.6.6
 - JLD                           0.6.5
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.0
 - Rmath                         0.1.4
 - SHA                           0.2.1
 - ScikitLearnBase               0.2.0
 - StaticArrays                  0.0.11
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-5.8010182540058e6,[9.0,99991.0],
[4.68901 -26.2963 23.6876; -348.808 32.7581 294.69],

Array{Float64,2}[
[5.47002 -13.7235 10.3612; -13.7235 78.6018 -67.8944; 10.3612 -67.8944 65.1386],

[99914.3 -285.261 425.963; -285.261 99586.4 564.597; 425.963 564.597 99622.1]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.466187e+03
      1       9.406277e+02      -5.255597e+02 |        8
      2       9.057135e+02      -3.491413e+01 |        0
      3       9.057135e+02       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 905.7135331958352)
INFO: K-means with 272 data points using 3 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.073516
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.774826
INFO: iteration 2, lowerbound -3.637691
INFO: iteration 3, lowerbound -3.489116
INFO: iteration 4, lowerbound -3.316855
INFO: dropping number of Gaussions to 7
INFO: iteration 5, lowerbound -3.135694
INFO: iteration 6, lowerbound -2.974477
INFO: iteration 7, lowerbound -2.866930
INFO: dropping number of Gaussions to 6
INFO: iteration 8, lowerbound -2.815067
INFO: dropping number of Gaussions to 5
INFO: iteration 9, lowerbound -2.796854
INFO: dropping number of Gaussions to 3
INFO: iteration 10, lowerbound -2.782768
INFO: iteration 11, lowerbound -2.771163
INFO: iteration 12, lowerbound -2.764951
INFO: iteration 13, lowerbound -2.756044
INFO: iteration 14, lowerbound -2.743465
INFO: iteration 15, lowerbound -2.726158
INFO: iteration 16, lowerbound -2.703174
INFO: iteration 17, lowerbound -2.673955
INFO: iteration 18, lowerbound -2.638628
INFO: iteration 19, lowerbound -2.598217
INFO: iteration 20, lowerbound -2.554679
INFO: iteration 21, lowerbound -2.510610
INFO: iteration 22, lowerbound -2.468516
INFO: iteration 23, lowerbound -2.429897
INFO: iteration 24, lowerbound -2.394870
INFO: iteration 25, lowerbound -2.362924
INFO: iteration 26, lowerbound -2.334899
INFO: iteration 27, lowerbound -2.314747
INFO: iteration 28, lowerbound -2.307411
INFO: dropping number of Gaussions to 2
INFO: iteration 29, lowerbound -2.302944
INFO: iteration 30, lowerbound -2.299261
INFO: iteration 31, lowerbound -2.299256
INFO: iteration 32, lowerbound -2.299255
INFO: iteration 33, lowerbound -2.299254
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Wed 02 Nov 2016 10:59:41 AM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Wed 02 Nov 2016 10:59:42 AM UTC: K-means with 272 data points using 3 iterations
11.3 data points per parameter
,Wed 02 Nov 2016 10:59:44 AM UTC: EM with 272 data points 0 iterations avll -2.073516
5.8 data points per parameter
,Wed 02 Nov 2016 10:59:44 AM UTC: GMM converted to Variational GMM
,Wed 02 Nov 2016 10:59:46 AM UTC: iteration 1, lowerbound -3.774826
,Wed 02 Nov 2016 10:59:46 AM UTC: iteration 2, lowerbound -3.637691
,Wed 02 Nov 2016 10:59:47 AM UTC: iteration 3, lowerbound -3.489116
,Wed 02 Nov 2016 10:59:47 AM UTC: iteration 4, lowerbound -3.316855
,Wed 02 Nov 2016 10:59:47 AM UTC: dropping number of Gaussions to 7
,Wed 02 Nov 2016 10:59:47 AM UTC: iteration 5, lowerbound -3.135694
,Wed 02 Nov 2016 10:59:47 AM UTC: iteration 6, lowerbound -2.974477
,Wed 02 Nov 2016 10:59:47 AM UTC: iteration 7, lowerbound -2.866930
,Wed 02 Nov 2016 10:59:47 AM UTC: dropping number of Gaussions to 6
,Wed 02 Nov 2016 10:59:47 AM UTC: iteration 8, lowerbound -2.815067
,Wed 02 Nov 2016 10:59:47 AM UTC: dropping number of Gaussions to 5
,Wed 02 Nov 2016 10:59:47 AM UTC: iteration 9, lowerbound -2.796854
,Wed 02 Nov 2016 10:59:47 AM UTC: dropping number of Gaussions to 3
,Wed 02 Nov 2016 10:59:47 AM UTC: iteration 10, lowerbound -2.782768
,Wed 02 Nov 2016 10:59:47 AM UTC: iteration 11, lowerbound -2.771163
,Wed 02 Nov 2016 10:59:47 AM UTC: iteration 12, lowerbound -2.764951
,Wed 02 Nov 2016 10:59:48 AM UTC: iteration 13, lowerbound -2.756044
,Wed 02 Nov 2016 10:59:48 AM UTC: iteration 14, lowerbound -2.743465
,Wed 02 Nov 2016 10:59:48 AM UTC: iteration 15, lowerbound -2.726158
,Wed 02 Nov 2016 10:59:48 AM UTC: iteration 16, lowerbound -2.703174
,Wed 02 Nov 2016 10:59:48 AM UTC: iteration 17, lowerbound -2.673955
,Wed 02 Nov 2016 10:59:48 AM UTC: iteration 18, lowerbound -2.638628
,Wed 02 Nov 2016 10:59:48 AM UTC: iteration 19, lowerbound -2.598217
,Wed 02 Nov 2016 10:59:48 AM UTC: iteration 20, lowerbound -2.554679
,Wed 02 Nov 2016 10:59:48 AM UTC: iteration 21, lowerbound -2.510610
,Wed 02 Nov 2016 10:59:48 AM UTC: iteration 22, lowerbound -2.468516
,Wed 02 Nov 2016 10:59:48 AM UTC: iteration 23, lowerbound -2.429897
,Wed 02 Nov 2016 10:59:48 AM UTC: iteration 24, lowerbound -2.394870
,Wed 02 Nov 2016 10:59:48 AM UTC: iteration 25, lowerbound -2.362924
,Wed 02 Nov 2016 10:59:48 AM UTC: iteration 26, lowerbound -2.334899
,Wed 02 Nov 2016 10:59:49 AM UTC: iteration 27, lowerbound -2.314747
,Wed 02 Nov 2016 10:59:49 AM UTC: iteration 28, lowerbound -2.307411
,Wed 02 Nov 2016 10:59:49 AM UTC: dropping number of Gaussions to 2
,Wed 02 Nov 2016 10:59:49 AM UTC: iteration 29, lowerbound -2.302944
,Wed 02 Nov 2016 10:59:49 AM UTC: iteration 30, lowerbound -2.299261
,Wed 02 Nov 2016 10:59:49 AM UTC: iteration 31, lowerbound -2.299256
,Wed 02 Nov 2016 10:59:49 AM UTC: iteration 32, lowerbound -2.299255
,Wed 02 Nov 2016 10:59:49 AM UTC: iteration 33, lowerbound -2.299254
,Wed 02 Nov 2016 10:59:49 AM UTC: iteration 34, lowerbound -2.299253
,Wed 02 Nov 2016 10:59:49 AM UTC: iteration 35, lowerbound -2.299253
,Wed 02 Nov 2016 10:59:49 AM UTC: iteration 36, lowerbound -2.299253
,Wed 02 Nov 2016 10:59:49 AM UTC: iteration 37, lowerbound -2.299253
,Wed 02 Nov 2016 10:59:49 AM UTC: iteration 38, lowerbound -2.299253
,Wed 02 Nov 2016 10:59:49 AM UTC: iteration 39, lowerbound -2.299253
,Wed 02 Nov 2016 10:59:50 AM UTC: iteration 40, lowerbound -2.299253
,Wed 02 Nov 2016 10:59:50 AM UTC: iteration 41, lowerbound -2.299253
,Wed 02 Nov 2016 10:59:50 AM UTC: iteration 42, lowerbound -2.299253
,Wed 02 Nov 2016 10:59:50 AM UTC: iteration 43, lowerbound -2.299253
,Wed 02 Nov 2016 10:59:50 AM UTC: iteration 44, lowerbound -2.299253
,Wed 02 Nov 2016 10:59:50 AM UTC: iteration 45, lowerbound -2.299253
,Wed 02 Nov 2016 10:59:50 AM UTC: iteration 46, lowerbound -2.299253
,Wed 02 Nov 2016 10:59:50 AM UTC: iteration 47, lowerbound -2.299253
,Wed 02 Nov 2016 10:59:50 AM UTC: iteration 48, lowerbound -2.299253
,Wed 02 Nov 2016 10:59:50 AM UTC: iteration 49, lowerbound -2.299253
,Wed 02 Nov 2016 10:59:50 AM UTC: iteration 50, lowerbound -2.299253
,Wed 02 Nov 2016 10:59:50 AM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.045,95.9549]
β = [178.045,95.9549]
m = [4.2503 79.2869; 2.00023 53.852]
ν = [180.045,97.9549]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.184042 -0.00764405; 0.0 0.00858171],

[0.375876 -0.00895312; 0.0 0.0127487]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -0.9751426462738398
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9751426462738512
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9751426462738512
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
WARNING: is is deprecated, use === instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in is(::Array{Float64,2}, ::Vararg{Array{Float64,2},N}) at ./deprecated.jl:30
 in _rcopy!(::Array{Float64,2}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/PDMats/src/utils.jl:10
 in unwhiten!(::Array{Float64,2}, ::PDMats.PDMat{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/PDMats/src/pdmat.jl:60
 in rand(::Distributions.MvNormal{Float64,PDMats.PDMat{Float64,Array{Float64,2}},Array{Float64,1}}, ::Int64) at /home/vagrant/.julia/v0.6/Distributions/src/multivariates.jl:18
 in rand(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:60
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.00000000001
avll from stats: -0.9603170288580334
avll from llpg:  -0.9603170288580334
avll direct:     -0.9603170288580334
sum posterior: 100000.0
32×26 Array{Float64,2}:
  0.128349    -0.22258     -0.0527219   -0.0590448   0.0155221    0.234689     0.0905972   0.0166833    0.00546357  -0.237368     0.0324417   -0.0849377   -0.13315      -0.143845      0.106591     0.152022    -0.045605     0.0281288   -0.197666    0.0472005   -0.178015     0.00367572  -0.130578    -0.0589343   -0.0427756   -0.0733306 
  0.0108916   -0.108662     0.0681758    0.122541   -0.00432827   0.0224296   -0.054841    0.0407946   -0.0668334    0.0753528   -0.0291783    0.130288    -0.0243811    -0.183941     -0.212173    -0.0579355   -0.081805    -0.0929951    0.0969326  -0.0190063   -0.15198     -0.0021255   -0.0772332   -0.00307088   0.069452    -0.157464  
  0.148575    -0.0301122   -0.135426    -0.085386    0.079166     0.0966821    0.108429   -0.0944617   -0.139923    -0.00420894   0.138353     0.239078     0.0556164     0.019949      0.103138    -0.0394211   -0.090261    -0.0567201    0.12654     0.0717334   -0.0635373   -0.154879    -0.0622873   -0.00817604   0.0557808   -0.134187  
 -0.248595    -0.115927    -0.00156903   0.0706171   0.108584     0.0517309    0.0161793  -0.0833084   -0.0538595    0.075934    -0.0569826   -0.0239726   -0.174767     -0.0225274     0.106168    -0.0371649    0.00318738   0.137008     0.0735668  -0.0575759    0.219535     0.0560464    0.112423    -0.0486359   -0.0387431   -0.0378041 
 -0.0658407   -0.0314458   -0.0230062    0.0596829  -0.109131     0.10929     -0.0535339  -0.166263    -0.14336      0.0392419    0.0437017    0.0527792    0.117327     -0.0401956    -0.223806     0.067353    -0.0697266   -0.0795646    0.173588   -0.0587581   -0.0335019   -0.0292336    0.0543243    0.04445      0.135387     0.157504  
  0.0879279    0.0403819   -0.107371    -0.0787933  -0.0813521    0.0441425    0.0148346  -0.0620464    0.146287     0.0546602    0.0557054   -0.024709    -0.157558     -0.0743064     0.113343     0.0535255   -0.072705     0.103613    -0.0186327  -0.0157845    0.100584    -0.141445    -0.0144858    0.015743     0.0342666   -0.134524  
  0.0474621    0.148497     0.0713286    0.0609526  -0.0193556   -0.02624      0.105575    0.0658764   -0.0814923   -0.0418688    0.12243     -0.0259487    0.0463231     0.0783988    -0.192097    -0.0169899    0.0516328   -0.054689     0.0322959  -0.0566788   -0.0752605    0.11641      0.0129918   -0.0826296    0.0551747    0.0978753 
  0.0451934   -0.00845498   0.0879463   -0.0452381  -0.00885621  -0.0793834    0.0352791  -0.00245162   0.0785395    0.0467817    0.0710741   -0.0486565    0.0923634    -0.00738161    0.0492863    0.0681622   -0.0781537   -0.149045    -0.0592953   0.0589737   -0.0679889   -0.176976    -0.107745     0.0431997    0.0434844   -0.0734012 
 -0.0711896   -0.0318752   -0.118178     0.0834494   0.065392    -0.00591754   0.0711287   0.0403942    0.0945602    0.0614643   -0.157902     0.121888     0.0671935     0.0498927     0.0628075    0.1953      -0.0112059   -0.0464312    0.0775533   0.0408706   -0.0326397   -0.064672    -0.149746     0.0464829    0.0429897   -0.0218996 
  0.153785     0.0613957   -0.0288894   -0.0371157  -0.00786676  -0.0797596   -0.0218617   0.150588    -0.114472     0.115579    -0.0134261   -0.119136    -0.015836      0.0739212    -0.221773    -0.155865    -0.00851668   0.00507725   0.148582    0.0480084   -0.196095    -0.124034    -0.124787    -0.131879    -0.0013307    0.0140343 
 -0.119386    -0.0583989    0.0251638   -0.164207    0.0568556    0.0657123    0.141302   -0.171906    -0.0203914   -0.0341713    0.0152539   -0.0144124    0.0271924    -0.094885      0.0674854   -0.0299358    0.0392122   -0.100782     0.0120914   0.00278251   0.0761439    0.0550225   -0.040239     0.147094    -0.081508     0.162153  
  0.00391697   0.0226318   -0.120123     0.164662   -0.0220856   -0.0486052   -0.0214939  -0.191587    -0.122347     0.0384904    0.00451084   0.209398    -0.108424      0.00751909    0.0652947    0.0497766   -0.00786023  -0.0826949   -0.168719   -0.0730237   -0.00548615  -0.0133668   -0.111922     0.188311    -0.0460884    0.13877   
  0.137838    -0.11535      0.118713    -0.0615083   0.0455947    0.0164837    0.0155324  -0.0177316    0.10118     -0.123626     0.0149549    0.00620653  -0.176028      0.0709186    -0.101663    -0.0270429   -0.0456696   -0.019914     0.160037   -0.178618    -0.103496    -0.217055     0.159481    -0.021209    -0.151431     0.0642407 
 -0.10744     -0.0272108   -0.0112091   -0.0935412  -0.087928    -0.220778    -0.0704936  -0.0433466    0.00121505  -0.167552    -0.0938412   -0.123332    -0.00206222    0.0323542    -0.0456915   -0.190665    -0.0219924    0.194922     0.117797    0.0967385   -0.0804433    0.108112    -0.155961    -0.105494     0.0309535    0.0910701 
 -0.166264     0.109914    -0.0201707    0.195304    0.159456     0.0368005   -0.0061049  -0.0613028    0.148354    -0.173586     0.020813     0.0337141   -0.0143842     0.138243     -0.136174     0.065653     0.159543     0.0306308    0.121751   -0.0276818    0.182307     0.0242267    0.0223859    0.072503     0.0401785    0.0621587 
 -0.0485809    0.0157073   -0.0381704   -0.0620386   0.105525     0.17316     -0.104902   -0.0564326    0.0550273    0.0834458    0.125577    -0.0476001   -0.00513142    0.128953     -0.0484161    0.0710675   -0.0281424    0.201871     0.0498895  -0.0635267   -0.115705    -0.0754138   -0.0426921   -0.27177     -0.144235    -0.149708  
  0.038404     0.0190818   -0.115466    -0.202163    0.0424047   -0.140821    -0.0251657   0.0204095   -0.0725393   -0.156503    -0.0327072    0.164127     0.0770499    -0.0442916     0.115991     0.105301    -0.0937434    0.0630319    0.108561    0.073732     0.10972      0.0289196    0.0614975    0.0819636    0.13086      0.177861  
  0.193869     0.170129     0.0167549   -0.210735   -0.0645563    0.1131      -0.1022      0.101003     0.082432     0.207889     0.0899285    0.0842204   -0.0151241    -0.00290271   -0.0101265    0.0663747   -0.04276      0.0946406    0.151663   -0.119368    -0.0627945   -0.0458307    0.0552204   -0.112191    -0.0749124    0.0550608 
 -0.0704549    0.0754611   -0.135559    -0.0947219   0.0433828   -0.221043     0.0364408  -0.0569661   -0.100532     0.0851994   -0.124327     0.0903726    0.208851      0.0294401     0.0258263   -0.0338362    0.075058    -0.0154108   -0.0596051   0.0063424   -0.189556     0.0358079    0.142577    -0.0422071   -0.0878378   -0.0308853 
 -0.14903      0.0582704   -0.0868868    0.0605488  -0.0154787    0.0742291   -0.0427143  -0.00589786   0.0930669    0.141302    -0.209239    -0.0535518   -0.135495     -0.0635435     0.0658925    0.105884    -0.0463446    0.00241181   0.102653   -0.042413     0.0118527   -0.026272    -0.0475928    0.0293048   -0.05368      0.10555   
  0.00606758  -0.0788295   -0.0439604   -0.0420586  -0.0608575    0.171911    -0.0561382   0.0542408    0.0300013   -0.227687    -0.0416014    0.0928889    0.000297308   0.0574683    -0.157785     0.0574433   -0.0484516    0.112144     0.198257   -0.110755     0.0114256    0.107298     0.0775764    0.128237    -0.106418     0.141524  
 -0.050609     0.046223     0.0400792   -0.0426276   0.132379     0.0824636    0.130322   -0.038       -0.0563501    0.051891     0.0451517    0.124138     0.00952531   -0.101599     -0.130236     0.0681797    0.119352     0.111012    -0.116125    0.17261     -0.0449575    0.215111    -0.029412     0.0881491   -0.17681      0.00525741
 -0.103419     0.0180695   -0.0290961   -0.0129988  -0.0306058    0.0599914   -0.0416759  -0.0728455   -0.0969067    0.0139112   -0.0895887   -0.115366    -0.0644739    -0.0853149    -0.0843711   -0.0513807   -0.0254797   -0.088103     0.0414338   0.106975     0.0230794   -0.0191696   -0.00876201  -0.0721092    0.0549587   -0.0451993 
 -0.19401      0.166924    -0.0909882   -0.144812    0.139691    -0.212514     0.207046    0.0486277   -0.0636105   -0.0760296   -0.0367088   -0.0737016   -0.10221      -0.0232093    -0.00698064   0.00627065   0.0682384    0.00121126   0.0107028  -0.0709101    0.0624787    0.24957      0.0100543    0.0331087   -0.0257462    0.0654905 
  0.0811942    0.187568     0.0215471   -0.0449453   0.0667965    0.0576087    0.0325383   0.021999     0.0786311   -0.0204871   -0.123503    -0.152901     0.129496     -0.0023514    -0.00307044   0.0302627    0.00326374  -0.0170692    0.0558509  -0.0219461   -0.109476     0.124763     0.0197456    0.0796504    0.104592    -0.0271064 
  0.050346     0.103185     0.0124494   -0.0164328   0.114876     0.0418484   -0.0213988   0.00865941   0.0602693   -0.169691     0.0929047    0.0107456   -0.194012     -0.0530585     0.00605297  -0.0120918    0.094151    -0.146032     0.275236    0.0375818    0.0986636    0.162002     0.00896345  -0.0556448    0.0191406    0.0633361 
  0.186499     0.0286926   -0.111553     0.0503476   0.159778    -0.0636053   -0.0629005  -0.1709       0.0701954   -0.0707597    0.0258864    0.110451    -0.0188747    -0.158461     -0.0385589   -0.0240352   -0.229405     0.0849699   -0.0537249   0.0355226    0.0175958    0.00858574   0.00755968   0.055231    -0.0556533   -0.206599  
 -0.0540111   -0.0248678   -0.043357     0.02016    -0.0819831   -0.0485728   -0.0390261  -0.100205    -0.00950399   0.0445229    0.0397893   -0.017213     0.0261694    -0.131523     -0.179636    -0.0513322   -0.165505    -0.162434    -0.122888    0.00466639   0.0794458    0.103071     0.118004     0.0053902    0.130645    -0.0522981 
 -0.0511984   -0.0588643    0.0970961   -0.0477404   0.151507     0.0177959    0.106848    0.0514634    0.123721    -0.102325     0.134739    -0.0258953   -0.0269395     0.208232     -0.0419626   -0.00439774  -0.0808874    0.265437     0.119318   -0.0203608    0.108527     0.040443    -0.0216494    0.106514     0.00804736  -0.034461  
 -0.0721164    0.0476397   -0.00746531  -0.0772324  -0.00824318  -0.122942    -0.0383399   0.0459883   -0.077669     0.0500545    0.0867109    0.188624     0.00335102    0.118507     -0.10364     -0.0465582    0.0824064    0.0622959   -0.0602477   0.0575829   -0.0407019    0.0651733    0.13234      0.0791448    0.0763773    0.0652094 
  0.0661722    0.0684303   -0.0375044    0.0908902   0.0118545   -0.0942414   -0.0553902  -0.0134613   -0.0372795    0.107743     0.044452    -0.13276     -0.0565747     0.0777751     0.0379346   -0.151159    -0.148104    -0.0986224   -0.119437    0.0803262   -0.0326767    0.0406677   -0.154589     0.00848751   0.0699972    0.033413  
  0.0403884    0.0323784   -0.0592974   -0.0483341   0.117626     0.245697    -0.0146724  -0.167568    -0.0319055    0.0672057    0.0110418   -0.0972027   -0.0477226     0.000258847   0.018251    -0.331584     0.047221     0.18819     -0.0182829  -0.0730388    0.0494093   -0.195579    -0.0734961   -0.0420409    0.0247012    0.111945  kind diag, method split
0: avll = -1.3785869788639795
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.378632
INFO: iteration 2, average log likelihood -1.378577
INFO: iteration 3, average log likelihood -1.378138
INFO: iteration 4, average log likelihood -1.373362
INFO: iteration 5, average log likelihood -1.356707
INFO: iteration 6, average log likelihood -1.345234
INFO: iteration 7, average log likelihood -1.341619
INFO: iteration 8, average log likelihood -1.340222
INFO: iteration 9, average log likelihood -1.339582
INFO: iteration 10, average log likelihood -1.339213
INFO: iteration 11, average log likelihood -1.338963
INFO: iteration 12, average log likelihood -1.338778
INFO: iteration 13, average log likelihood -1.338634
INFO: iteration 14, average log likelihood -1.338522
INFO: iteration 15, average log likelihood -1.338434
INFO: iteration 16, average log likelihood -1.338364
INFO: iteration 17, average log likelihood -1.338310
INFO: iteration 18, average log likelihood -1.338268
INFO: iteration 19, average log likelihood -1.338235
INFO: iteration 20, average log likelihood -1.338210
INFO: iteration 21, average log likelihood -1.338190
INFO: iteration 22, average log likelihood -1.338174
INFO: iteration 23, average log likelihood -1.338162
INFO: iteration 24, average log likelihood -1.338153
INFO: iteration 25, average log likelihood -1.338146
INFO: iteration 26, average log likelihood -1.338141
INFO: iteration 27, average log likelihood -1.338136
INFO: iteration 28, average log likelihood -1.338133
INFO: iteration 29, average log likelihood -1.338130
INFO: iteration 30, average log likelihood -1.338128
INFO: iteration 31, average log likelihood -1.338126
INFO: iteration 32, average log likelihood -1.338125
INFO: iteration 33, average log likelihood -1.338123
INFO: iteration 34, average log likelihood -1.338122
INFO: iteration 35, average log likelihood -1.338121
INFO: iteration 36, average log likelihood -1.338119
INFO: iteration 37, average log likelihood -1.338118
INFO: iteration 38, average log likelihood -1.338116
INFO: iteration 39, average log likelihood -1.338114
INFO: iteration 40, average log likelihood -1.338111
INFO: iteration 41, average log likelihood -1.338108
INFO: iteration 42, average log likelihood -1.338104
INFO: iteration 43, average log likelihood -1.338099
INFO: iteration 44, average log likelihood -1.338093
INFO: iteration 45, average log likelihood -1.338087
INFO: iteration 46, average log likelihood -1.338080
INFO: iteration 47, average log likelihood -1.338072
INFO: iteration 48, average log likelihood -1.338065
INFO: iteration 49, average log likelihood -1.338058
INFO: iteration 50, average log likelihood -1.338053
INFO: EM with 100000 data points 50 iterations avll -1.338053
952.4 data points per parameter
1: avll = [-1.37863,-1.37858,-1.37814,-1.37336,-1.35671,-1.34523,-1.34162,-1.34022,-1.33958,-1.33921,-1.33896,-1.33878,-1.33863,-1.33852,-1.33843,-1.33836,-1.33831,-1.33827,-1.33824,-1.33821,-1.33819,-1.33817,-1.33816,-1.33815,-1.33815,-1.33814,-1.33814,-1.33813,-1.33813,-1.33813,-1.33813,-1.33812,-1.33812,-1.33812,-1.33812,-1.33812,-1.33812,-1.33812,-1.33811,-1.33811,-1.33811,-1.3381,-1.3381,-1.33809,-1.33809,-1.33808,-1.33807,-1.33806,-1.33806,-1.33805]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.338147
INFO: iteration 2, average log likelihood -1.338063
INFO: iteration 3, average log likelihood -1.337737
INFO: iteration 4, average log likelihood -1.334200
INFO: iteration 5, average log likelihood -1.320283
INFO: iteration 6, average log likelihood -1.307131
INFO: iteration 7, average log likelihood -1.302270
INFO: iteration 8, average log likelihood -1.300253
INFO: iteration 9, average log likelihood -1.299072
INFO: iteration 10, average log likelihood -1.298211
INFO: iteration 11, average log likelihood -1.297563
INFO: iteration 12, average log likelihood -1.297096
INFO: iteration 13, average log likelihood -1.296759
INFO: iteration 14, average log likelihood -1.296476
INFO: iteration 15, average log likelihood -1.296181
INFO: iteration 16, average log likelihood -1.295835
INFO: iteration 17, average log likelihood -1.295387
INFO: iteration 18, average log likelihood -1.294796
INFO: iteration 19, average log likelihood -1.294063
INFO: iteration 20, average log likelihood -1.293206
INFO: iteration 21, average log likelihood -1.292271
INFO: iteration 22, average log likelihood -1.291333
INFO: iteration 23, average log likelihood -1.290472
INFO: iteration 24, average log likelihood -1.289777
INFO: iteration 25, average log likelihood -1.289274
INFO: iteration 26, average log likelihood -1.288934
INFO: iteration 27, average log likelihood -1.288714
INFO: iteration 28, average log likelihood -1.288576
INFO: iteration 29, average log likelihood -1.288490
INFO: iteration 30, average log likelihood -1.288437
INFO: iteration 31, average log likelihood -1.288405
INFO: iteration 32, average log likelihood -1.288386
INFO: iteration 33, average log likelihood -1.288375
INFO: iteration 34, average log likelihood -1.288368
INFO: iteration 35, average log likelihood -1.288364
INFO: iteration 36, average log likelihood -1.288361
INFO: iteration 37, average log likelihood -1.288359
INFO: iteration 38, average log likelihood -1.288358
INFO: iteration 39, average log likelihood -1.288357
INFO: iteration 40, average log likelihood -1.288355
INFO: iteration 41, average log likelihood -1.288354
INFO: iteration 42, average log likelihood -1.288353
INFO: iteration 43, average log likelihood -1.288353
INFO: iteration 44, average log likelihood -1.288352
INFO: iteration 45, average log likelihood -1.288351
INFO: iteration 46, average log likelihood -1.288350
INFO: iteration 47, average log likelihood -1.288349
INFO: iteration 48, average log likelihood -1.288349
INFO: iteration 49, average log likelihood -1.288348
INFO: iteration 50, average log likelihood -1.288348
INFO: EM with 100000 data points 50 iterations avll -1.288348
473.9 data points per parameter
2: avll = [-1.33815,-1.33806,-1.33774,-1.3342,-1.32028,-1.30713,-1.30227,-1.30025,-1.29907,-1.29821,-1.29756,-1.2971,-1.29676,-1.29648,-1.29618,-1.29583,-1.29539,-1.2948,-1.29406,-1.29321,-1.29227,-1.29133,-1.29047,-1.28978,-1.28927,-1.28893,-1.28871,-1.28858,-1.28849,-1.28844,-1.28841,-1.28839,-1.28838,-1.28837,-1.28836,-1.28836,-1.28836,-1.28836,-1.28836,-1.28836,-1.28835,-1.28835,-1.28835,-1.28835,-1.28835,-1.28835,-1.28835,-1.28835,-1.28835,-1.28835]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.288468
INFO: iteration 2, average log likelihood -1.288329
INFO: iteration 3, average log likelihood -1.287527
INFO: iteration 4, average log likelihood -1.280499
INFO: iteration 5, average log likelihood -1.262265
INFO: iteration 6, average log likelihood -1.245108
INFO: iteration 7, average log likelihood -1.237853
INFO: iteration 8, average log likelihood -1.235386
INFO: iteration 9, average log likelihood -1.234277
INFO: iteration 10, average log likelihood -1.233579
INFO: iteration 11, average log likelihood -1.233058
INFO: iteration 12, average log likelihood -1.232609
INFO: iteration 13, average log likelihood -1.232124
INFO: iteration 14, average log likelihood -1.231511
INFO: iteration 15, average log likelihood -1.230722
INFO: iteration 16, average log likelihood -1.229825
INFO: iteration 17, average log likelihood -1.228963
INFO: iteration 18, average log likelihood -1.228117
INFO: iteration 19, average log likelihood -1.227236
INFO: iteration 20, average log likelihood -1.226535
INFO: iteration 21, average log likelihood -1.226252
INFO: iteration 22, average log likelihood -1.226191
INFO: iteration 23, average log likelihood -1.226161
INFO: iteration 24, average log likelihood -1.226135
INFO: iteration 25, average log likelihood -1.226108
INFO: iteration 26, average log likelihood -1.226079
INFO: iteration 27, average log likelihood -1.226045
INFO: iteration 28, average log likelihood -1.226005
INFO: iteration 29, average log likelihood -1.225955
INFO: iteration 30, average log likelihood -1.225888
INFO: iteration 31, average log likelihood -1.225793
INFO: iteration 32, average log likelihood -1.225650
INFO: iteration 33, average log likelihood -1.225436
INFO: iteration 34, average log likelihood -1.225139
INFO: iteration 35, average log likelihood -1.224844
INFO: iteration 36, average log likelihood -1.224638
INFO: iteration 37, average log likelihood -1.224511
INFO: iteration 38, average log likelihood -1.224424
INFO: iteration 39, average log likelihood -1.224355
INFO: iteration 40, average log likelihood -1.224302
INFO: iteration 41, average log likelihood -1.224262
INFO: iteration 42, average log likelihood -1.224233
INFO: iteration 43, average log likelihood -1.224213
INFO: iteration 44, average log likelihood -1.224197
INFO: iteration 45, average log likelihood -1.224185
INFO: iteration 46, average log likelihood -1.224173
INFO: iteration 47, average log likelihood -1.224162
INFO: iteration 48, average log likelihood -1.224152
INFO: iteration 49, average log likelihood -1.224142
INFO: iteration 50, average log likelihood -1.224131
INFO: EM with 100000 data points 50 iterations avll -1.224131
236.4 data points per parameter
3: avll = [-1.28847,-1.28833,-1.28753,-1.2805,-1.26227,-1.24511,-1.23785,-1.23539,-1.23428,-1.23358,-1.23306,-1.23261,-1.23212,-1.23151,-1.23072,-1.22982,-1.22896,-1.22812,-1.22724,-1.22654,-1.22625,-1.22619,-1.22616,-1.22613,-1.22611,-1.22608,-1.22605,-1.22601,-1.22596,-1.22589,-1.22579,-1.22565,-1.22544,-1.22514,-1.22484,-1.22464,-1.22451,-1.22442,-1.22436,-1.2243,-1.22426,-1.22423,-1.22421,-1.2242,-1.22418,-1.22417,-1.22416,-1.22415,-1.22414,-1.22413]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.224267
INFO: iteration 2, average log likelihood -1.224077
INFO: iteration 3, average log likelihood -1.222818
INFO: iteration 4, average log likelihood -1.206859
WARNING: Variances had to be floored 4
INFO: iteration 5, average log likelihood -1.163362
INFO: iteration 6, average log likelihood -1.147390
WARNING: Variances had to be floored 4
INFO: iteration 7, average log likelihood -1.130544
INFO: iteration 8, average log likelihood -1.135373
WARNING: Variances had to be floored 4
INFO: iteration 9, average log likelihood -1.123805
WARNING: Variances had to be floored 15
INFO: iteration 10, average log likelihood -1.129388
WARNING: Variances had to be floored 4
INFO: iteration 11, average log likelihood -1.126085
INFO: iteration 12, average log likelihood -1.126449
WARNING: Variances had to be floored 4
INFO: iteration 13, average log likelihood -1.112178
WARNING: Variances had to be floored 15
INFO: iteration 14, average log likelihood -1.118777
WARNING: Variances had to be floored 4
INFO: iteration 15, average log likelihood -1.117675
INFO: iteration 16, average log likelihood -1.122082
WARNING: Variances had to be floored 4
INFO: iteration 17, average log likelihood -1.111152
WARNING: Variances had to be floored 15
INFO: iteration 18, average log likelihood -1.118481
WARNING: Variances had to be floored 4
INFO: iteration 19, average log likelihood -1.117653
INFO: iteration 20, average log likelihood -1.122058
WARNING: Variances had to be floored 4
INFO: iteration 21, average log likelihood -1.111117
WARNING: Variances had to be floored 15
INFO: iteration 22, average log likelihood -1.118451
WARNING: Variances had to be floored 4
INFO: iteration 23, average log likelihood -1.117626
INFO: iteration 24, average log likelihood -1.122038
WARNING: Variances had to be floored 4
INFO: iteration 25, average log likelihood -1.111095
WARNING: Variances had to be floored 15
INFO: iteration 26, average log likelihood -1.118433
WARNING: Variances had to be floored 4
INFO: iteration 27, average log likelihood -1.117612
INFO: iteration 28, average log likelihood -1.122028
WARNING: Variances had to be floored 4
INFO: iteration 29, average log likelihood -1.111087
WARNING: Variances had to be floored 15
INFO: iteration 30, average log likelihood -1.118427
WARNING: Variances had to be floored 4
INFO: iteration 31, average log likelihood -1.117609
INFO: iteration 32, average log likelihood -1.122026
WARNING: Variances had to be floored 4
INFO: iteration 33, average log likelihood -1.111086
WARNING: Variances had to be floored 15
INFO: iteration 34, average log likelihood -1.118426
WARNING: Variances had to be floored 4
INFO: iteration 35, average log likelihood -1.117608
INFO: iteration 36, average log likelihood -1.122026
WARNING: Variances had to be floored 4
INFO: iteration 37, average log likelihood -1.111086
WARNING: Variances had to be floored 15
INFO: iteration 38, average log likelihood -1.118426
WARNING: Variances had to be floored 4
INFO: iteration 39, average log likelihood -1.117608
INFO: iteration 40, average log likelihood -1.122026
WARNING: Variances had to be floored 4
INFO: iteration 41, average log likelihood -1.111086
WARNING: Variances had to be floored 15
INFO: iteration 42, average log likelihood -1.118426
WARNING: Variances had to be floored 4
INFO: iteration 43, average log likelihood -1.117608
INFO: iteration 44, average log likelihood -1.122026
WARNING: Variances had to be floored 4
INFO: iteration 45, average log likelihood -1.111086
WARNING: Variances had to be floored 15
INFO: iteration 46, average log likelihood -1.118426
WARNING: Variances had to be floored 4
INFO: iteration 47, average log likelihood -1.117608
INFO: iteration 48, average log likelihood -1.122026
WARNING: Variances had to be floored 4
INFO: iteration 49, average log likelihood -1.111086
WARNING: Variances had to be floored 15
INFO: iteration 50, average log likelihood -1.118426
INFO: EM with 100000 data points 50 iterations avll -1.118426
118.1 data points per parameter
4: avll = [-1.22427,-1.22408,-1.22282,-1.20686,-1.16336,-1.14739,-1.13054,-1.13537,-1.12381,-1.12939,-1.12609,-1.12645,-1.11218,-1.11878,-1.11767,-1.12208,-1.11115,-1.11848,-1.11765,-1.12206,-1.11112,-1.11845,-1.11763,-1.12204,-1.11109,-1.11843,-1.11761,-1.12203,-1.11109,-1.11843,-1.11761,-1.12203,-1.11109,-1.11843,-1.11761,-1.12203,-1.11109,-1.11843,-1.11761,-1.12203,-1.11109,-1.11843,-1.11761,-1.12203,-1.11109,-1.11843,-1.11761,-1.12203,-1.11109,-1.11843]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 7 8
INFO: iteration 1, average log likelihood -1.117769
WARNING: Variances had to be floored 7 8
INFO: iteration 2, average log likelihood -1.113071
WARNING: Variances had to be floored 7 8
INFO: iteration 3, average log likelihood -1.109963
WARNING: Variances had to be floored 7 8 16 29 30
INFO: iteration 4, average log likelihood -1.091748
WARNING: Variances had to be floored 1 7 8 15 16 27
INFO: iteration 5, average log likelihood -1.051424
WARNING: Variances had to be floored 4 7 8
INFO: iteration 6, average log likelihood -1.036286
WARNING: Variances had to be floored 1 7 8 15 16 22 27 29 30
INFO: iteration 7, average log likelihood -1.013401
WARNING: Variances had to be floored 7 8
INFO: iteration 8, average log likelihood -1.044375
WARNING: Variances had to be floored 1 4 7 8 15 16 27
INFO: iteration 9, average log likelihood -1.009807
WARNING: Variances had to be floored 7 8 22 30
INFO: iteration 10, average log likelihood -1.030940
WARNING: Variances had to be floored 1 7 8 15 16 27 29
INFO: iteration 11, average log likelihood -1.013197
WARNING: Variances had to be floored 4 7 8
INFO: iteration 12, average log likelihood -1.034870
WARNING: Variances had to be floored 1 7 8 15 16 22 27
INFO: iteration 13, average log likelihood -1.012915
WARNING: Variances had to be floored 7 8 29
INFO: iteration 14, average log likelihood -1.034051
WARNING: Variances had to be floored 1 4 7 8 15 16 27
INFO: iteration 15, average log likelihood -1.013901
WARNING: Variances had to be floored 7 8 22
INFO: iteration 16, average log likelihood -1.031467
WARNING: Variances had to be floored 1 7 8 15 16 27 29
INFO: iteration 17, average log likelihood -1.008661
WARNING: Variances had to be floored 4 7 8 9
INFO: iteration 18, average log likelihood -1.025564
WARNING: Variances had to be floored 1 7 8 15 16 22 27
INFO: iteration 19, average log likelihood -1.013508
WARNING: Variances had to be floored 7 8 29
INFO: iteration 20, average log likelihood -1.028745
WARNING: Variances had to be floored 1 4 7 8 9 15 16 27
INFO: iteration 21, average log likelihood -1.005971
WARNING: Variances had to be floored 7 8 22
INFO: iteration 22, average log likelihood -1.034098
WARNING: Variances had to be floored 1 7 8 15 16 27 29
INFO: iteration 23, average log likelihood -1.008731
WARNING: Variances had to be floored 4 7 8 9
INFO: iteration 24, average log likelihood -1.026997
WARNING: Variances had to be floored 1 7 8 15 16 22 27
INFO: iteration 25, average log likelihood -1.013555
WARNING: Variances had to be floored 7 8 29
INFO: iteration 26, average log likelihood -1.028908
WARNING: Variances had to be floored 1 4 7 8 9 15 16 27
INFO: iteration 27, average log likelihood -1.006483
WARNING: Variances had to be floored 7 8 22
INFO: iteration 28, average log likelihood -1.034094
WARNING: Variances had to be floored 1 7 8 15 16 27 29
INFO: iteration 29, average log likelihood -1.008799
WARNING: Variances had to be floored 4 7 8 9
INFO: iteration 30, average log likelihood -1.027231
WARNING: Variances had to be floored 1 7 8 15 16 22 27
INFO: iteration 31, average log likelihood -1.013554
WARNING: Variances had to be floored 7 8 29
INFO: iteration 32, average log likelihood -1.028944
WARNING: Variances had to be floored 1 4 7 8 9 15 16 27
INFO: iteration 33, average log likelihood -1.006607
WARNING: Variances had to be floored 7 8 22
INFO: iteration 34, average log likelihood -1.034089
WARNING: Variances had to be floored 1 7 8 15 16 27 29
INFO: iteration 35, average log likelihood -1.008818
WARNING: Variances had to be floored 4 7 8
INFO: iteration 36, average log likelihood -1.027302
WARNING: Variances had to be floored 1 7 8 9 15 16 22 27
INFO: iteration 37, average log likelihood -1.002799
WARNING: Variances had to be floored 7 8 29
INFO: iteration 38, average log likelihood -1.035315
WARNING: Variances had to be floored 1 4 7 8 15 16 27
INFO: iteration 39, average log likelihood -1.008959
WARNING: Variances had to be floored 7 8 9 22
INFO: iteration 40, average log likelihood -1.024753
WARNING: Variances had to be floored 1 7 8 15 16 27 29
INFO: iteration 41, average log likelihood -1.014849
WARNING: Variances had to be floored 4 7 8
INFO: iteration 42, average log likelihood -1.029307
WARNING: Variances had to be floored 1 7 8 9 15 16 22 27
INFO: iteration 43, average log likelihood -1.004341
WARNING: Variances had to be floored 7 8 29
INFO: iteration 44, average log likelihood -1.035278
WARNING: Variances had to be floored 1 4 7 8 15 16 27
INFO: iteration 45, average log likelihood -1.009074
WARNING: Variances had to be floored 7 8 9 22
INFO: iteration 46, average log likelihood -1.025037
WARNING: Variances had to be floored 1 7 8 15 16 27 29
INFO: iteration 47, average log likelihood -1.014839
WARNING: Variances had to be floored 4 7 8
INFO: iteration 48, average log likelihood -1.029355
WARNING: Variances had to be floored 1 7 8 9 15 16 22 27
INFO: iteration 49, average log likelihood -1.004466
WARNING: Variances had to be floored 7 8 29
INFO: iteration 50, average log likelihood -1.035268
INFO: EM with 100000 data points 50 iterations avll -1.035268
59.0 data points per parameter
5: avll = [-1.11777,-1.11307,-1.10996,-1.09175,-1.05142,-1.03629,-1.0134,-1.04437,-1.00981,-1.03094,-1.0132,-1.03487,-1.01291,-1.03405,-1.0139,-1.03147,-1.00866,-1.02556,-1.01351,-1.02874,-1.00597,-1.0341,-1.00873,-1.027,-1.01356,-1.02891,-1.00648,-1.03409,-1.0088,-1.02723,-1.01355,-1.02894,-1.00661,-1.03409,-1.00882,-1.0273,-1.0028,-1.03531,-1.00896,-1.02475,-1.01485,-1.02931,-1.00434,-1.03528,-1.00907,-1.02504,-1.01484,-1.02935,-1.00447,-1.03527]
[-1.37859,-1.37863,-1.37858,-1.37814,-1.37336,-1.35671,-1.34523,-1.34162,-1.34022,-1.33958,-1.33921,-1.33896,-1.33878,-1.33863,-1.33852,-1.33843,-1.33836,-1.33831,-1.33827,-1.33824,-1.33821,-1.33819,-1.33817,-1.33816,-1.33815,-1.33815,-1.33814,-1.33814,-1.33813,-1.33813,-1.33813,-1.33813,-1.33812,-1.33812,-1.33812,-1.33812,-1.33812,-1.33812,-1.33812,-1.33811,-1.33811,-1.33811,-1.3381,-1.3381,-1.33809,-1.33809,-1.33808,-1.33807,-1.33806,-1.33806,-1.33805,-1.33815,-1.33806,-1.33774,-1.3342,-1.32028,-1.30713,-1.30227,-1.30025,-1.29907,-1.29821,-1.29756,-1.2971,-1.29676,-1.29648,-1.29618,-1.29583,-1.29539,-1.2948,-1.29406,-1.29321,-1.29227,-1.29133,-1.29047,-1.28978,-1.28927,-1.28893,-1.28871,-1.28858,-1.28849,-1.28844,-1.28841,-1.28839,-1.28838,-1.28837,-1.28836,-1.28836,-1.28836,-1.28836,-1.28836,-1.28836,-1.28835,-1.28835,-1.28835,-1.28835,-1.28835,-1.28835,-1.28835,-1.28835,-1.28835,-1.28835,-1.28847,-1.28833,-1.28753,-1.2805,-1.26227,-1.24511,-1.23785,-1.23539,-1.23428,-1.23358,-1.23306,-1.23261,-1.23212,-1.23151,-1.23072,-1.22982,-1.22896,-1.22812,-1.22724,-1.22654,-1.22625,-1.22619,-1.22616,-1.22613,-1.22611,-1.22608,-1.22605,-1.22601,-1.22596,-1.22589,-1.22579,-1.22565,-1.22544,-1.22514,-1.22484,-1.22464,-1.22451,-1.22442,-1.22436,-1.2243,-1.22426,-1.22423,-1.22421,-1.2242,-1.22418,-1.22417,-1.22416,-1.22415,-1.22414,-1.22413,-1.22427,-1.22408,-1.22282,-1.20686,-1.16336,-1.14739,-1.13054,-1.13537,-1.12381,-1.12939,-1.12609,-1.12645,-1.11218,-1.11878,-1.11767,-1.12208,-1.11115,-1.11848,-1.11765,-1.12206,-1.11112,-1.11845,-1.11763,-1.12204,-1.11109,-1.11843,-1.11761,-1.12203,-1.11109,-1.11843,-1.11761,-1.12203,-1.11109,-1.11843,-1.11761,-1.12203,-1.11109,-1.11843,-1.11761,-1.12203,-1.11109,-1.11843,-1.11761,-1.12203,-1.11109,-1.11843,-1.11761,-1.12203,-1.11109,-1.11843,-1.11777,-1.11307,-1.10996,-1.09175,-1.05142,-1.03629,-1.0134,-1.04437,-1.00981,-1.03094,-1.0132,-1.03487,-1.01291,-1.03405,-1.0139,-1.03147,-1.00866,-1.02556,-1.01351,-1.02874,-1.00597,-1.0341,-1.00873,-1.027,-1.01356,-1.02891,-1.00648,-1.03409,-1.0088,-1.02723,-1.01355,-1.02894,-1.00661,-1.03409,-1.00882,-1.0273,-1.0028,-1.03531,-1.00896,-1.02475,-1.01485,-1.02931,-1.00434,-1.03528,-1.00907,-1.02504,-1.01484,-1.02935,-1.00447,-1.03527]
32×26 Array{Float64,2}:
 -0.0506417    0.0749863  -0.104736    -0.0903299    0.0543116    -0.237387    0.0375765  -0.054207    -0.132257     0.0761116   -0.128616     0.104241     0.204461      0.0189362    0.0246897   -0.0328606    0.0738886   -0.0149179   -0.0723052   0.00937117  -0.152541     0.043896     0.0991539   -0.0773435   -0.0879395   -0.0218356 
  0.0727645    0.0492464  -0.0872124   -0.0662156   -0.0595932     0.0457655   0.0189537  -0.0552813    0.153982     0.0666747    0.0643841   -0.0130511   -0.160618     -0.0938977    0.129616     0.0538465   -0.0638392    0.10342     -0.019923   -0.0164018    0.102994    -0.118088    -0.0243842    0.0234169    0.034602    -0.175584  
  0.145806    -0.0127968  -0.136255    -0.0849459    0.0730271     0.103054    0.11762    -0.0825246   -0.133809    -0.00708202   0.137641     0.239841     0.0530632     0.0445347    0.0940015   -0.0398207   -0.0918606   -0.0589193    0.120351    0.0791402   -0.0709545   -0.135869    -0.0631467    0.0142808    0.0567413   -0.143026  
  0.0700032    0.0187662  -0.110528    -0.200832     0.0749074    -0.130168   -0.0360605   0.012475    -0.0606226   -0.156501    -0.0348667    0.165636     0.121606     -0.039213     0.127507     0.100211    -0.0880042    0.0603699    0.0877269   0.0580166    0.120605     0.0306759    0.048021     0.075625     0.130675     0.179761  
 -0.197308     0.0144782  -0.0265612   -0.00745578  -0.0178509     0.0817102  -0.193341   -0.0816564   -0.315424    -0.0196084   -0.0992112   -0.0742112   -0.0734997    -0.0781158   -0.0908615    0.00502078  -0.0258228   -0.0791698    0.0386267   0.0895052    0.0242143   -0.120994    -0.0160288   -0.0991461    0.0639417   -0.165555  
  0.0512806    0.022552   -0.0373717   -0.0336973   -0.0588931     0.0405888   0.156217   -0.0672106    0.203599     0.0577484   -0.0714155   -0.179273    -0.0556752    -0.0892384   -0.0782527   -0.02665     -0.0225471   -0.100537     0.107561    0.132415     0.0225554    0.0391019    0.0203927   -0.093056     0.0428764   -0.00585622
  0.0902165    0.0149991  -0.0695558   -0.148345     0.106209      0.204677   -0.104848   -0.0626808    0.0532562    0.0637811    0.165734    -0.0587112    0.14908       0.128865    -0.372243     0.0634398   -0.0297       0.207381     0.0550474   0.126168    -0.126605    -0.050029    -0.0350894   -0.365732    -0.0818545   -0.147591  
 -0.109855     0.0174446   0.0457161    0.0248645    0.106149      0.0856049  -0.10528    -0.0512935    0.0691564    0.114345     0.105662    -0.00235243  -0.130844      0.127636     0.264442     0.0231645   -0.0270513    0.197943     0.0340211  -0.159134    -0.104983    -0.110366    -0.0611182   -0.113297    -0.179997    -0.148244  
 -0.114029    -0.0402701   0.0117537   -0.154087     0.0761446     0.0494206   0.127694   -0.123581    -0.0209245   -0.0309632    0.00978238  -0.0059748    0.0175913    -0.0683528    0.0784715   -0.0396231    0.0471786   -0.1225      -0.0166161  -0.00855084   0.0631838    0.0619144   -0.0253875    0.139385    -0.0408161    0.150297  
 -0.0290908    0.036356   -0.0610702    0.0701975   -0.0238931    -0.0884588  -0.056203   -0.0768225   -0.109296     0.0476223    0.044703     0.161117    -0.0553348     0.0596746    0.0014247    0.0259923    0.0270604   -0.010867    -0.115444    0.00952557  -0.0178365    0.0201323   -0.00781434   0.125232     0.00185855   0.0944486 
 -0.0811705   -0.0267143  -0.0131074    0.0329419   -0.107484      0.0767614  -0.0540057  -0.162911    -0.146151     0.0416854    0.0468226    0.0792862    0.102208      0.00621981  -0.226018     0.0256461   -0.0701614   -0.0620569    0.208816   -0.0610267   -0.0280297   -0.0299466    0.0571443    0.0607632    0.131185     0.156837  
 -0.100341    -0.0109651  -0.0121734   -0.103248    -0.0924677    -0.218892   -0.0431635  -0.0471918    0.0102843   -0.158773    -0.0928183   -0.0960573   -0.000492546  -0.0368619   -0.0547597   -0.18349     -0.0404146    0.202201     0.0852722   0.0989168   -0.091998     0.107623    -0.161313    -0.072389     0.0331598    0.0885389 
 -0.135703     0.0770927  -0.0686508   -0.0606225    0.0376685    -0.152821    0.0901853  -0.0573892   -0.0262287   -0.033715    -0.00369089  -0.0625705   -0.0380992    -0.0740848   -0.0698018   -0.00751452  -0.0460399   -0.0516076   -0.0370728  -0.0326874    0.0784917    0.156435     0.0720532    0.0215098    0.0439528    0.00232096
 -0.0398705    0.061348   -0.0439104    0.0537858    0.141244      0.149373   -0.0136358  -0.121773     0.0480207   -0.030745     0.0303119   -0.0392352   -0.0365315     0.0677623   -0.048768    -0.178933     0.0957897    0.125476     0.0561236  -0.0558257    0.101062    -0.111423    -0.0217306    0.0018715    0.0336457    0.127022  
  0.15051      0.0634543  -0.0518099   -0.0421689   -0.00938216   -0.0745245  -0.0221252   0.16145     -0.109983     0.131243    -0.0200194   -0.154744    -0.0241909     0.0827396   -0.228699    -0.192549     0.0312963   -0.00017519   0.147369    0.072261    -0.209961    -0.139085    -0.123027    -0.133595    -0.0191704    0.00793181
  0.050262     0.0165562   0.0898626   -0.0588184   -0.000792069  -0.0901491   0.0357548   0.016833     0.0774932    0.0473076    0.0743785   -0.0854923    0.108924     -0.0129881    0.0617754    0.0794322   -0.123548    -0.141847    -0.0605546   0.0556673   -0.0659861   -0.166901    -0.106948     0.0505263    0.0538003   -0.0683898 
  0.0585174    0.0677052  -0.038456     0.0954493    0.0143091    -0.0625639  -0.110959   -0.0184544   -0.0442851    0.11114      0.0748456   -0.133385    -0.0549371     0.0858206    0.0458782   -0.169541    -0.147179    -0.095427    -0.095341    0.0816158   -0.072766     0.040304    -0.154807     0.0167957    0.0620009    0.0343542 
  0.0519583    0.120741    0.042455     0.00867885   0.0388951     0.0104991   0.0619961   0.026087    -0.00967809  -0.117728     0.0945811   -0.00741819  -0.0932543     0.0120817   -0.116898    -0.0183444    0.0709987   -0.141524     0.196438   -0.0100306    0.0122405    0.178928     0.0102593   -0.0621475    0.0242579    0.0826798 
  0.175572     0.0347885  -0.09691      0.0178439    0.193414     -0.0702579  -0.0650987  -0.169886     0.0649914   -0.0661688    0.0190176    0.10884     -0.0232311    -0.158792    -0.0453482   -0.00270112  -0.227132     0.0843469   -0.0513755   0.0291385    0.0213764    0.00565498   0.0357831    0.0573449   -0.0613247   -0.223244  
 -0.256209    -0.114632    0.00649557   0.0841877    0.0929715     0.0140668  -0.0122653  -0.0979542   -0.0198729    0.0699224   -0.0931097   -0.0223578   -0.181616     -0.0254185    0.118826    -0.0280041   -0.00614366   0.0992497    0.0731226  -0.048871     0.225751     0.0543044    0.112155    -0.0530211   -0.0375998   -0.0305599 
  0.055898     0.0521189   0.0731702   -0.13464      0.051416      0.060187    0.0102468   0.0745896    0.101619     0.0878576    0.127567     0.0327332   -0.000137749   0.125018    -0.0424136    0.00303203  -0.0472381    0.199444     0.146205   -0.0707299    0.0441179    0.00235482   0.0293289    0.00264606  -0.0141522    0.0199311 
  0.114721    -0.197608   -0.0375706   -0.0560308    0.0164689     0.240658    0.0800326   0.0265496    0.0521077   -0.229913     0.0434247   -0.088865    -0.148024     -0.141407     0.0988766    0.153522    -0.0327826    0.0425984   -0.185046    0.0364022   -0.133887     0.0174183   -0.122868    -0.0549144   -0.0505105   -0.0605567 
 -0.141851     0.0268327  -0.164717     0.0941142    0.0459967     0.066582   -0.0418899   0.00607515   0.0930835   -0.128737    -0.184458    -0.120247    -0.138281     -0.0907512    0.0845381    0.101774    -0.0429304   -0.0661344    0.113619   -0.126698    -0.0275908   -0.0245809   -0.0341428    0.0299239   -0.0631638    0.176913  
 -0.156644     0.0760511  -0.0176648    0.0443516   -0.0760475     0.115281   -0.082914   -0.0369183    0.094541     0.340028    -0.25911      0.00501092  -0.137023     -0.0342327    0.00591655   0.0890374   -0.0469625    0.0661498    0.094362    0.0563947    0.0934128   -0.0220203   -0.0496215    0.0294579   -0.0580987    0.0268667 
 -0.085362    -0.0224568  -0.17095      0.0659376    0.0829241    -0.0465461   0.0499845  -0.0735878    0.194651     0.0989811   -0.130786     0.150251     0.0741417     0.0474478    0.0528796    0.195444     0.00287901   0.0960296    0.0637368  -0.0295285   -0.0365698   -0.112695    -0.129088     0.025571    -0.568665    -0.0198413 
 -0.0637141   -0.0332662  -0.139876     0.102677     0.0609941     0.0458775   0.0871186   0.138438     0.0223566    0.0326242   -0.253371     0.105619     0.0626888     0.0537662   -0.0766154    0.189319    -0.0145698   -0.217455     0.0777241   0.108032    -0.0375205    0.0199262   -0.173095     0.0713021    0.699989    -0.0545224 
  0.00390659  -0.0761337  -0.0336149   -0.0263151   -0.0565669     0.20958    -0.0484934   0.0459171    0.0308034   -0.231749    -0.0472102    0.0913836   -0.00507418    0.0499277   -0.144181     0.0568914   -0.0503065    0.105614     0.149799   -0.118351     0.00952447   0.120304     0.0981841    0.105626    -0.141778     0.136983  
 -0.0503489    0.0498894   0.0720446   -0.0432258    0.131254      0.0607767   0.130795   -0.0594827   -0.0912114    0.050535     0.0505106    0.123042     0.00943816   -0.113169    -0.127712     0.0689944    0.126633     0.0943958   -0.125127    0.171876    -0.0402628    0.191787    -0.0491294    0.0980502   -0.182815     0.0113574 
  0.0825546    0.1618      0.0346174   -0.0610216    0.0663687     0.0523828   0.0364925   0.0222029    0.0847923   -0.018046    -0.0977584   -0.148518     0.132121      0.00531041  -0.0196376    0.056722     0.00724601  -0.00602535   0.0504751   0.00273518  -0.116292     0.152578     0.0194556    0.0669897    0.0850818   -0.0326268 
  0.00883069  -0.0954351   0.082635     0.116108    -0.00987093    0.0284894  -0.0591577  -0.00706194  -0.0698507    0.0718658   -0.0898495    0.125404    -0.00850266   -0.179942    -0.21994     -0.0576691   -0.0742579   -0.0921644    0.107906   -0.0392205   -0.148234    -6.92161e-5  -0.0718646    0.00234059   0.0755426   -0.152617  
  0.136779    -0.0962101   0.103316    -0.0641753    0.0426164    -0.0227411   0.12423    -0.00669119   0.101322    -0.118918    -0.876221     0.0116608   -0.163229      0.110489    -0.0645529   -0.0266242   -0.026733    -0.00912581   0.148483   -0.186254    -0.0268705   -0.347778     0.111001     0.00697344  -0.105106     0.073816  
  0.139668    -0.117406    0.171189    -0.055292     0.0459145     0.0448315  -0.030581   -0.00884402   0.10031     -0.0827276    0.945407     0.00112813  -0.162268      0.0395653   -0.104953    -0.0289259   -0.0736174   -0.0188764    0.203248   -0.180244    -0.105384    -0.0890276    0.0658762   -0.0986273   -0.158985     0.058242  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 4 7 8 15 16 27
INFO: iteration 1, average log likelihood -1.009094
WARNING: Variances had to be floored 1 4 7 8 9 15 16 22 27
INFO: iteration 2, average log likelihood -0.997157
WARNING: Variances had to be floored 1 4 7 8 15 16 27 29
INFO: iteration 3, average log likelihood -1.000198
WARNING: Variances had to be floored 1 4 7 8 9 15 16 22 27
INFO: iteration 4, average log likelihood -1.002244
WARNING: Variances had to be floored 1 4 7 8 15 16 27
INFO: iteration 5, average log likelihood -1.003059
WARNING: Variances had to be floored 1 4 7 8 9 15 16 22 27 29
INFO: iteration 6, average log likelihood -0.993308
WARNING: Variances had to be floored 1 4 7 8 15 16 27
INFO: iteration 7, average log likelihood -1.009002
WARNING: Variances had to be floored 1 4 7 8 9 15 16 22 27
INFO: iteration 8, average log likelihood -0.996191
WARNING: Variances had to be floored 1 4 7 8 15 16 27 29
INFO: iteration 9, average log likelihood -1.000201
WARNING: Variances had to be floored 1 4 7 8 9 15 16 22 27
INFO: iteration 10, average log likelihood -1.002122
INFO: EM with 100000 data points 10 iterations avll -1.002122
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.250609e+05
      1       6.417405e+05      -1.833204e+05 |       32
      2       6.172832e+05      -2.445735e+04 |       32
      3       6.017047e+05      -1.557842e+04 |       32
      4       5.917195e+05      -9.985267e+03 |       32
      5       5.856404e+05      -6.079033e+03 |       32
      6       5.801367e+05      -5.503746e+03 |       32
      7       5.764014e+05      -3.735335e+03 |       32
      8       5.745531e+05      -1.848220e+03 |       32
      9       5.732935e+05      -1.259654e+03 |       32
     10       5.726766e+05      -6.169247e+02 |       32
     11       5.724422e+05      -2.343471e+02 |       32
     12       5.723454e+05      -9.682417e+01 |       32
     13       5.722899e+05      -5.547930e+01 |       32
     14       5.722529e+05      -3.701063e+01 |       32
     15       5.722136e+05      -3.934389e+01 |       32
     16       5.721607e+05      -5.287070e+01 |       32
     17       5.720737e+05      -8.701063e+01 |       32
     18       5.719186e+05      -1.551189e+02 |       32
     19       5.716417e+05      -2.768136e+02 |       32
     20       5.711716e+05      -4.701665e+02 |       32
     21       5.706890e+05      -4.825989e+02 |       32
     22       5.703050e+05      -3.839688e+02 |       32
     23       5.699847e+05      -3.202598e+02 |       32
     24       5.695970e+05      -3.877390e+02 |       32
     25       5.690680e+05      -5.290384e+02 |       32
     26       5.686243e+05      -4.436807e+02 |       32
     27       5.684946e+05      -1.296553e+02 |       32
     28       5.684744e+05      -2.027369e+01 |       29
     29       5.684677e+05      -6.612826e+00 |       29
     30       5.684628e+05      -4.910201e+00 |       29
     31       5.684572e+05      -5.678194e+00 |       22
     32       5.684529e+05      -4.218669e+00 |       22
     33       5.684480e+05      -4.933871e+00 |       23
     34       5.684444e+05      -3.614202e+00 |       25
     35       5.684398e+05      -4.581971e+00 |       26
     36       5.684357e+05      -4.086189e+00 |       24
     37       5.684333e+05      -2.455392e+00 |       19
     38       5.684318e+05      -1.496767e+00 |       14
     39       5.684301e+05      -1.638459e+00 |       16
     40       5.684278e+05      -2.324528e+00 |       22
     41       5.684239e+05      -3.931513e+00 |       21
     42       5.684189e+05      -5.004412e+00 |       27
     43       5.684105e+05      -8.363452e+00 |       21
     44       5.684051e+05      -5.415329e+00 |       25
     45       5.683993e+05      -5.798557e+00 |       21
     46       5.683936e+05      -5.709612e+00 |       24
     47       5.683883e+05      -5.288982e+00 |       24
     48       5.683849e+05      -3.442490e+00 |       21
     49       5.683820e+05      -2.850604e+00 |       24
     50       5.683783e+05      -3.698835e+00 |       21
K-means terminated without convergence after 50 iterations (objv = 568378.3029335648)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.295903
INFO: iteration 2, average log likelihood -1.264171
INFO: iteration 3, average log likelihood -1.232182
INFO: iteration 4, average log likelihood -1.194011
INFO: iteration 5, average log likelihood -1.149845
WARNING: Variances had to be floored 9
INFO: iteration 6, average log likelihood -1.100555
WARNING: Variances had to be floored 3 20 25
INFO: iteration 7, average log likelihood -1.064595
WARNING: Variances had to be floored 7 12 16 29
INFO: iteration 8, average log likelihood -1.067287
INFO: iteration 9, average log likelihood -1.075043
WARNING: Variances had to be floored 3 9 13 23
INFO: iteration 10, average log likelihood -1.020906
WARNING: Variances had to be floored 16 20 25
INFO: iteration 11, average log likelihood -1.034689
WARNING: Variances had to be floored 7 12 29
INFO: iteration 12, average log likelihood -1.036457
WARNING: Variances had to be floored 9
INFO: iteration 13, average log likelihood -1.031415
WARNING: Variances had to be floored 3 16 20 23 25
INFO: iteration 14, average log likelihood -1.006760
WARNING: Variances had to be floored 7 13
INFO: iteration 15, average log likelihood -1.043203
WARNING: Variances had to be floored 9 12 15 29
INFO: iteration 16, average log likelihood -1.015975
WARNING: Variances had to be floored 3 16
INFO: iteration 17, average log likelihood -1.034904
WARNING: Variances had to be floored 20 23 25
INFO: iteration 18, average log likelihood -1.012475
WARNING: Variances had to be floored 7 29
INFO: iteration 19, average log likelihood -1.008901
WARNING: Variances had to be floored 3 5 9 12 13 15 16
INFO: iteration 20, average log likelihood -0.993501
INFO: iteration 21, average log likelihood -1.061267
WARNING: Variances had to be floored 20 23 25
INFO: iteration 22, average log likelihood -1.002975
WARNING: Variances had to be floored 3 7 16 29
INFO: iteration 23, average log likelihood -1.004965
WARNING: Variances had to be floored 9 12 13
INFO: iteration 24, average log likelihood -1.025574
INFO: iteration 25, average log likelihood -1.028075
WARNING: Variances had to be floored 3 7 16 20 23 25
INFO: iteration 26, average log likelihood -0.981204
WARNING: Variances had to be floored 9 12 15 29
INFO: iteration 27, average log likelihood -1.026288
WARNING: Variances had to be floored 13
INFO: iteration 28, average log likelihood -1.045428
WARNING: Variances had to be floored 16
INFO: iteration 29, average log likelihood -1.014519
WARNING: Variances had to be floored 3 7 9 20 23 25 29
INFO: iteration 30, average log likelihood -0.976785
WARNING: Variances had to be floored 12
INFO: iteration 31, average log likelihood -1.051816
WARNING: Variances had to be floored 5 13 15 16
INFO: iteration 32, average log likelihood -1.014933
INFO: iteration 33, average log likelihood -1.027049
WARNING: Variances had to be floored 3 7 9 20 23 25 29
INFO: iteration 34, average log likelihood -0.969299
WARNING: Variances had to be floored 12 16
INFO: iteration 35, average log likelihood -1.051485
WARNING: Variances had to be floored 13
INFO: iteration 36, average log likelihood -1.037100
WARNING: Variances had to be floored 3 7 29
INFO: iteration 37, average log likelihood -0.997587
WARNING: Variances had to be floored 5 9 12 15 16 20 23 25
INFO: iteration 38, average log likelihood -0.983391
INFO: iteration 39, average log likelihood -1.070136
WARNING: Variances had to be floored 13
INFO: iteration 40, average log likelihood -1.022265
WARNING: Variances had to be floored 3 7 16 29
INFO: iteration 41, average log likelihood -0.994593
WARNING: Variances had to be floored 9 20 23 25
INFO: iteration 42, average log likelihood -1.010165
WARNING: Variances had to be floored 12 13
INFO: iteration 43, average log likelihood -1.032233
WARNING: Variances had to be floored 3 7 16
INFO: iteration 44, average log likelihood -1.017887
WARNING: Variances had to be floored 9 29
INFO: iteration 45, average log likelihood -1.007622
WARNING: Variances had to be floored 5 12 13 15 20 23 25
INFO: iteration 46, average log likelihood -0.989985
WARNING: Variances had to be floored 3 16
INFO: iteration 47, average log likelihood -1.055404
WARNING: Variances had to be floored 7 9
INFO: iteration 48, average log likelihood -1.037841
WARNING: Variances had to be floored 29
INFO: iteration 49, average log likelihood -1.010445
WARNING: Variances had to be floored 3 12 13 16 20 23 25
INFO: iteration 50, average log likelihood -0.982385
INFO: EM with 100000 data points 50 iterations avll -0.982385
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0779651   -0.0236277  -0.0422062    0.0105819  -0.0881422   -0.0452367   -0.0348494   -0.138303    -0.0151962     0.0478865    0.0351622  -0.0155314    0.0262429   -0.129342    -0.147558    -0.0361344   -0.190449    -0.144133    -0.098281     0.00168249   0.0949853    0.0790702    0.118936     0.00654916    0.138531    -0.0518981
  0.058619     0.0685894  -0.0386113    0.0962399   0.0117108   -0.0620305   -0.11079     -0.0175508   -0.0444867     0.115529     0.0739341  -0.134126    -0.0540069    0.0868712    0.0447543   -0.170945    -0.149202    -0.0973792   -0.0997139    0.0815397   -0.0733149    0.0414329   -0.156905     0.0173248     0.0623834    0.0341414
 -0.0552095    0.0784971  -0.110055    -0.0930644   0.0522184   -0.239516     0.0382461   -0.0567555   -0.133695      0.0793492   -0.13436     0.104967     0.210708     0.0187721    0.0252507   -0.0342005    0.0729982   -0.0158021   -0.0804902    0.0123082   -0.155675     0.0418531    0.105015    -0.0840163    -0.0871445   -0.0232137
 -0.0330259   -0.081146    0.11052     -0.0451934   0.160057     0.029767     0.10792      0.0315355    0.113686     -0.052801     0.134648   -0.0347572   -0.0141529    0.206992    -0.0376928   -0.0476819   -0.0996175    0.289717     0.106943    -0.0163298    0.118821     0.0321526   -0.0143629    0.0996406     0.022104    -0.0306218
  0.0916353    0.0333627   0.0326722   -0.0504153  -0.00570526  -0.0865596    0.00913233   0.0842321   -0.000333241   0.0821484    0.0359968  -0.127869     0.0541447    0.0310118   -0.0640025   -0.0318214   -0.0696254   -0.08205      0.0201498    0.0627282   -0.125746    -0.157141    -0.114212    -0.0287331     0.0288822   -0.0363787
 -0.172382     0.10267    -0.0313489    0.19737     0.164633     0.0275471   -0.00473302  -0.0593502    0.139849     -0.165919     0.0214119   0.0345083   -0.0144092    0.160693    -0.133656     0.0623264    0.16425      0.0293691    0.125215    -0.0270733    0.171742     0.0313615    0.0275299    0.0665735     0.0356529    0.0850742
  0.0842117    0.0259825  -0.0569044   -0.0382628   0.120926     0.260378    -0.0172934   -0.17009     -0.00539306    0.094528     0.0510411  -0.0858426   -0.0481621   -0.0234039    0.00955091  -0.341732     0.0488869    0.216275     0.00606272  -0.0676865    0.049163    -0.247978    -0.0663689   -0.062698      0.0197192    0.151403 
  0.159075     0.160431    0.0293934   -0.227688   -0.0661864    0.107665    -0.0868773    0.122885     0.0863779     0.192565     0.105377    0.0876394   -0.00978507   0.00833717  -0.0299908    0.0717476    0.0141041    0.0853918    0.147716    -0.120285    -0.0802077   -0.0250365    0.0600315   -0.111674     -0.061327     0.0547033
 -0.176755     0.157942   -0.0876404   -0.140565    0.134674    -0.233542     0.186092     0.0110377   -0.0544171    -0.0798381   -0.0329799  -0.12267     -0.0961034   -0.029952     0.013755    -0.0107616    0.0728453    0.0345721    0.0100104   -0.0711815    0.0526858    0.212688     0.0224724    0.0360705    -0.0228872    0.0420715
  0.145491    -0.0115247  -0.136306    -0.0836239   0.0738826    0.101202     0.117264    -0.0844297   -0.134565     -0.00692707   0.138316    0.240086     0.0521134    0.0478896    0.0938755   -0.0400503   -0.0915735   -0.0602316    0.119685     0.0770421   -0.0719785   -0.136137    -0.0645737    0.0202833     0.0567267   -0.14137  
 -0.0783312    0.0434564   0.00751286  -0.0697911  -0.0121228   -0.127173    -0.0326885    0.0415519   -0.0734337     0.0385354    0.0381107   0.108426     0.00879237   0.119108    -0.0438002   -0.0521796    0.0813343    0.0757856   -0.0555825    0.0656751   -0.0527676    0.0652312    0.120805     0.0995998     0.0786835    0.0569557
 -0.10829     -0.0336397   0.0106749   -0.158765    0.0819602    0.0832203    0.137632    -0.151964    -0.0216895    -0.0321682    0.0122244  -0.0207241    0.0125044   -0.106216     0.0660131   -0.0323245    0.0376598   -0.13272     -0.00380807  -0.00468604   0.083551     0.0599018   -0.0536813    0.130466     -0.0703483    0.15714  
  0.082946     0.1612      0.033701    -0.0599922   0.0666105    0.051127     0.0365512    0.0215257    0.0830504    -0.0162082   -0.09668    -0.149083     0.131089     0.00296507  -0.0200686    0.0551407    0.00484501  -0.00426926   0.0501866    0.00271925  -0.116659     0.151347     0.0195502    0.0665742     0.0846814   -0.0301473
  0.00888053  -0.0866166   0.0571065    0.121325   -0.00981664   0.0231622   -0.0624897   -0.0227757   -0.0772963     0.0712341   -0.0703313   0.129284    -0.0172464   -0.17898     -0.196476    -0.0466842   -0.0679829   -0.0892306    0.0942339   -0.0328003   -0.141568    -0.00155292  -0.0766873   -0.000959355   0.0585039   -0.130102 
  0.158743     0.05468    -0.033473    -0.0272512  -0.0207901   -0.0273176   -0.00879166   0.0769457   -0.101206      0.0918726   -0.0312482   0.0451515   -0.0176572    0.0384394   -0.190048    -0.270017     0.254166     0.0217885    0.184696     0.108399    -0.221249    -0.228367    -0.128849    -0.199411     -0.0879246    0.0407886
  0.0720759    0.0125004  -0.111266    -0.203441    0.0762718   -0.132982    -0.0354389    0.0158724   -0.0639788    -0.156163    -0.0341125   0.16859      0.122144    -0.0391058    0.128378     0.100551    -0.0876694    0.0606508    0.0955544    0.0543661    0.119953     0.0291968    0.0472979    0.0779943     0.131384     0.180784 
  0.183921     0.0320266  -0.097524     0.0102638   0.196249    -0.0759243   -0.0666186   -0.171279     0.0637836    -0.0641173    0.0185292   0.10996     -0.0240478   -0.159165    -0.0428527   -0.0143722   -0.239948     0.0859      -0.0542462    0.0303697    0.0158499    0.00402049   0.0348939    0.0579184    -0.0612115   -0.224833 
 -0.0498731    0.0480987   0.0709778   -0.042222    0.126234     0.062142     0.127776    -0.0552415   -0.0900458     0.050983     0.049468    0.12277      0.00819795  -0.110517    -0.130582     0.0702644    0.12416      0.0979575   -0.123726     0.172424    -0.0412926    0.192085    -0.0436349    0.0993239    -0.18108      0.0136457
 -0.149344     0.0505878  -0.0970673    0.0706714  -0.0122584    0.0893051   -0.0607893   -0.0148599    0.0936742     0.0980254   -0.227015   -0.0624444   -0.141335    -0.0625046    0.0533003    0.0972243   -0.0457326   -0.00414407   0.10426     -0.0377351    0.0259746   -0.0231294   -0.0432007    0.0304632    -0.0575525    0.105214 
 -0.0109575    0.0162033  -0.011511    -0.0608122   0.106119     0.144568    -0.105175    -0.0568986    0.0613625     0.0894519    0.135466   -0.0300379    0.00753122   0.128262    -0.0505814    0.0432624   -0.0283557    0.202697     0.0445176   -0.0181919   -0.115756    -0.08054     -0.048434    -0.238558     -0.132127    -0.147801 
  0.138581    -0.10831     0.1393      -0.0602726   0.0449361    0.0115956    0.0424427   -0.00862598   0.100918     -0.0997806    0.0615892   0.00600126  -0.164476     0.0737305   -0.0848989   -0.0278357   -0.050252    -0.0142384    0.1773      -0.183131    -0.0674792   -0.216008     0.0887687   -0.0500273    -0.133219     0.0656746
  0.0547299    0.0912429   0.0107515   -0.0261596   0.0979749    0.0363034   -0.0168483    0.00494142   0.061506     -0.193661     0.0644746   0.00973422  -0.23965     -0.0584889   -0.00119064  -0.0303141    0.083849    -0.206406     0.271476     0.0335153    0.102003     0.227183     0.0284851   -0.0432934    -0.00786356   0.0616241
  0.00620463  -0.0810254  -0.0365426   -0.0296834  -0.0599271    0.212645    -0.0545779    0.0552719    0.0358894    -0.240067    -0.048928    0.0927591   -0.00313894   0.0559364   -0.146685     0.0627593   -0.0562191    0.108937     0.164075    -0.126174     0.0161096    0.120334     0.10675      0.109192     -0.153282     0.138201 
 -0.0709673    0.0185574  -0.032095    -0.0214428  -0.0383993    0.0608316   -0.0172154   -0.0744348   -0.0546679     0.0208507   -0.0855633  -0.127648    -0.0646425   -0.0827983   -0.0845011   -0.0114071   -0.0240944   -0.0901054    0.0732923    0.110817     0.0233399   -0.0400938    0.00150602  -0.0965525     0.0534186   -0.0848682
  0.125277    -0.219755   -0.0492317   -0.0549798   0.0191882    0.250441     0.0916196    0.0205173    0.030669     -0.239931     0.0420386  -0.0971721   -0.141571    -0.161819     0.106659     0.14023     -0.0548203    0.0300792   -0.200866     0.0498295   -0.142108     0.0100231   -0.139247    -0.0589148    -0.0573385   -0.0538823
  0.059266     0.145933    0.0713177    0.0453324  -0.0212202   -0.00971644   0.141973     0.0467559   -0.0821704    -0.0597581    0.126356   -0.0245434    0.045524     0.0808668   -0.224923    -0.00799136   0.0557191   -0.0709408    0.112169    -0.0517023   -0.0778529    0.128764    -0.0103733   -0.0801855     0.0536511    0.100116 
 -0.100434    -0.0066162  -0.0248735   -0.0951582  -0.0891474   -0.211686    -0.0480739   -0.0530171    0.00887737   -0.151982    -0.094375   -0.0998816   -0.00172914  -0.0531697   -0.0517648   -0.181245    -0.0392281    0.206425     0.0777601    0.102941    -0.08649      0.106674    -0.160601    -0.0653797     0.0304257    0.0934936
 -0.256595    -0.114856    0.00784847   0.084602    0.0928459    0.0174169   -0.0153541   -0.0988277   -0.0207477     0.0713861   -0.0978528  -0.023029    -0.184391    -0.0260192    0.119316    -0.0304772   -0.0072983    0.101828     0.0748775   -0.0497933    0.226003     0.0539295    0.111618    -0.0519309    -0.0383027   -0.0261871
  0.00656899   0.0316814  -0.0932719    0.150614   -0.0226006   -0.0461744   -0.0495244   -0.173646    -0.126077      0.0385678    0.0124473   0.215581    -0.0986856    0.03334      0.0507416    0.0694489   -0.00858092  -0.112558    -0.158146    -0.070254     0.00901677  -0.0109822   -0.1143       0.180638     -0.0523052    0.13501  
 -0.0731492   -0.0282417  -0.16214      0.0870399   0.0735857   -0.00172454   0.0693808    0.0329986    0.105566      0.0654015   -0.202756    0.13007      0.0683605    0.0508532   -0.00727316   0.193995    -0.00920472  -0.0642738    0.0691534    0.0389021   -0.0402269   -0.0488782   -0.155901     0.049519      0.0756606   -0.0392594
 -0.0808115   -0.0336992  -0.0130818    0.0391154  -0.105783     0.0716136   -0.053912    -0.17311     -0.146077      0.0464744    0.0496527   0.0844014    0.102306     0.022495    -0.215371     0.033646    -0.0719881   -0.0695061    0.213384    -0.0689887   -0.0181063   -0.0321392    0.0549157    0.0523876     0.126269     0.157134 
  0.0754353    0.0494377  -0.0896071   -0.0643543  -0.0582976    0.0456612    0.0175735   -0.0561911    0.153976      0.0669943    0.0632742  -0.00871338  -0.157671    -0.094146     0.130412     0.0535025   -0.062646     0.103096    -0.021829    -0.0167408    0.0998735   -0.116813    -0.0248853    0.0225253     0.034607    -0.173612 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 7 9
INFO: iteration 1, average log likelihood -1.051548
WARNING: Variances had to be floored 5 7 9 15 29
INFO: iteration 2, average log likelihood -1.001980
WARNING: Variances had to be floored 3 5 7 9 12 16 23 25
INFO: iteration 3, average log likelihood -0.977001
WARNING: Variances had to be floored 5 7 9 13 15 20 29
INFO: iteration 4, average log likelihood -1.002078
WARNING: Variances had to be floored 7 9
INFO: iteration 5, average log likelihood -1.022994
WARNING: Variances had to be floored 3 5 7 9 12 15 16 23 29
INFO: iteration 6, average log likelihood -0.968936
WARNING: Variances had to be floored 7 9 13 20 25
INFO: iteration 7, average log likelihood -1.004148
WARNING: Variances had to be floored 5 7 9 15 29
INFO: iteration 8, average log likelihood -1.010031
WARNING: Variances had to be floored 3 5 7 9 12 16 23
INFO: iteration 9, average log likelihood -0.985059
WARNING: Variances had to be floored 5 7 9 13 15 25 29
INFO: iteration 10, average log likelihood -0.992368
INFO: EM with 100000 data points 10 iterations avll -0.992368
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.121296    0.026694      0.170654    -0.103094    0.076838     0.0814419    0.00185993  -0.000700469  -0.196428     0.0462384    0.131744    -0.17962     -0.0555686   -0.00913815  -0.0511253    0.00612576   0.133863     0.137774      0.259286     0.0867915    0.0247554    0.168142     0.054394    -0.0585083    0.10577     -0.105402  
  0.115949   -0.0141307    -0.0615908   -0.0878961  -0.051982    -0.058171    -0.0619319   -0.0652601     0.00313634   0.05492      0.0692487   -0.0793321   -0.0507098   -0.00790099  -0.10332     -0.198692    -0.110385     0.0916116    -0.127579     0.0791802    0.254332    -0.0243401    0.160576    -0.101036     0.0321034   -0.0898164 
 -0.0130256   0.0726808     0.0935766    0.0911497  -0.154528    -0.100455     0.0832907   -0.0264061     0.0200302   -0.043646     0.0163168    0.123764     0.00693308  -0.0159758    0.189077     0.0614811    0.150586    -0.062226     -0.0278919   -0.0455142   -0.0471154   -0.121678     0.0731874   -0.00991564   0.025265     0.00389669
 -0.0222933   0.0109037    -0.123896    -0.0963159  -0.0738349    0.131124    -0.0669544    0.0340708     0.0904763    0.0110853    0.204525     0.0384888    0.0801936    0.0740711    0.0413738   -0.0747144    0.14136     -0.000515816   0.0923713   -0.0395938    0.156379     0.00688882   0.0249106   -0.00314776   0.0402392    0.0358426 
 -0.035676   -0.0337974     0.0384139    0.10188    -0.0496666   -0.0522747   -0.0109694    0.135139      0.139901     0.0446283   -0.0374372   -0.00368088  -0.0725612   -0.0248413    0.10255     -0.0778775   -0.124302     0.00866208    0.181015    -0.0729409    0.0163481    0.143257    -0.0710647   -0.158354    -0.08816     -0.00196098
 -0.0570624   0.0631002     0.0214796    0.0988038  -0.146171    -0.0617071   -0.0771485    0.0696893     0.0824215   -0.0821688   -0.048484     0.0894449   -0.0171085    0.179517     0.0732878    0.0398047    0.00861393   0.0673353     0.297773    -0.166907     0.0549536   -0.0302577   -0.0466036   -0.113084     0.0238551    0.0157411 
  0.134953    0.00430148    0.209298    -0.0364949   0.114125     0.0318152   -0.0430028    0.115238     -0.00541709  -0.125471     0.108098     0.0216321   -0.0841926    0.0474754   -0.0939779    0.0019442   -0.0642662   -0.0293002    -0.152554     0.0807721    0.0153197   -0.137212    -0.159361     0.166053     0.0430225   -0.109608  
 -0.0935062   0.0492407     0.061921     0.049193    0.0921122    0.0357046    0.0125201   -0.0124431    -0.0435412   -0.116378    -0.0818522    0.0562391   -0.114188    -0.0239851   -0.0744516   -0.0155073    0.00533532  -0.134192     -0.0792567    0.017632     0.134062    -0.0687239   -0.120056    -0.0740484    0.0428747    0.0654248 
  0.168006    0.0890213    -0.103132    -0.138598   -0.131651    -0.079095    -0.0727724    0.0023384    -0.115232     0.0356433   -0.00457096   0.141517     0.129684     0.0638553    0.00159123   0.00711129   0.0225242   -0.018136     -0.0723914    0.0850512    0.066982     0.0604568    0.0419838    0.0414648   -0.00819284   0.02164   
  0.217109    0.0270752     0.16382      0.0396776  -0.15422     -0.0358587    0.027548    -0.115584      0.0709003   -0.0295064    0.0321873    0.103963    -0.0797107   -0.0492294    0.0366294    0.0270736    0.256955     0.0207285     0.0115629    0.0548539   -0.113495     0.051961     0.192422     0.124845    -0.0511085   -0.131945  
 -0.0755872   0.13961       0.0814972   -0.0289473   0.0532151    0.15505     -0.00342842   0.0292258     0.0644429    0.0527175    0.0113161   -0.0399842    0.0612762   -0.0820797   -0.0601204   -0.11412     -0.0522575    0.111534      0.0683454   -0.0684131    0.251974    -0.027614     0.198317    -0.109031     0.0901881    0.214171  
  0.136174   -0.0168826     0.0060193   -0.0896238   0.036075     0.0684221    0.0666018    0.198218      0.0108484    0.0786653   -0.0266963    0.00490709  -0.00919759   0.0340536   -0.014359    -0.0568749    0.251609    -0.103111     -0.0938216   -0.00129882   0.0920255   -0.08914     -0.112361    -0.0178773    0.0803433   -0.110639  
  0.129796    0.195476      0.0648019    0.1246     -0.0131083    0.0197826    0.0199671    0.0595579     0.0369234   -0.0274381    0.199781     0.0264378   -0.058793     0.0713101    0.0299524    0.0807973   -0.0552447    0.217432     -0.151605     0.04708      0.128588     0.182534    -0.0696742   -0.00776752   0.156972     0.0556892 
 -0.0167704  -0.00799497   -0.0211167    0.125711   -0.0947999   -0.0109464   -0.0559652   -0.0271226    -0.127298     0.0398194   -0.0753838   -0.00888395  -0.105774    -0.0988821   -0.0594452    0.163296     0.0688467   -0.0133777     0.0692327   -0.09493      0.0609184    0.111161    -0.23987     -0.0818068    0.0982675   -0.129238  
  0.205969    0.107095      0.0804093   -0.185143    0.0665502    0.13568      0.00767504  -0.119235      0.107894    -0.0190197   -0.101272    -0.00468482   0.0436865   -0.0762245    0.0800824    0.152211    -0.0786361    0.020973      0.01154     -0.0383725    0.0503661   -0.142731    -0.0645792   -0.146564    -0.173155     0.142564  
 -0.0635806  -0.000159051   0.120371    -0.0862601   0.112314    -0.0427523    0.146625    -0.104562     -0.18213     -0.0438645    0.0168961    0.0581093    0.0651922   -0.0176041   -0.079423     0.106289     0.0515982    0.0670259    -0.00518721   0.154213    -0.118507    -0.0314669   -0.0131663   -0.217013    -0.0827255    0.164448  
 -0.206817   -0.0531192     0.0643925    0.084135    0.00895371  -0.00667883   0.238952    -0.0223873    -0.238112     0.142143     0.0614132   -0.0855598   -0.0414707    0.0558153   -0.0276827    0.072538    -0.054185     0.0688482    -0.0035687   -0.100101    -0.0766016    0.00749315   0.201472     0.0319744    0.0600311   -0.0465229 
  0.069286    0.0203088     0.151274    -0.0108579   0.0661686   -0.0428551   -0.0818134   -0.080252     -0.216075     0.0444088   -0.0536626   -0.0105454    0.030556    -0.0167289   -0.0573136    0.00615458   0.0145541   -0.0516914    -0.0950762    0.022965     0.0202902    0.0119169    0.0439042   -0.0320992    0.0742576    0.174486  
  0.0571224   0.00871131   -0.00646555  -0.0817998  -0.0341549    0.136344     0.130665    -0.0480434    -0.0347753    0.0125079    0.0335665    0.0627963   -0.0468852   -0.0939931   -0.210399     0.183733     0.0513684   -0.0838467     0.0915345    0.0666224   -0.121244     0.0641968   -0.0647325   -0.174908    -0.0206725    0.0756124 
  0.0862386  -0.157656     -0.00951498  -0.0799648   0.00941539   0.0525997    0.0762049   -0.104793      0.189943     0.170736    -0.048595    -0.0764179    0.0515266    0.0469015    0.21419     -0.0480828    0.0302202    0.0256118     0.0824769   -0.00515705   0.0245483    0.140578     0.129442     0.114233     0.0886254    0.0629996 
  0.0591941  -0.142826     -0.208457     0.107324    0.0459829    0.051938     0.0444386    0.135319     -0.0728248    0.0436989    0.00247471   0.0374101   -0.07716      0.131014     0.00752078   0.0904035   -0.0821431   -0.126855      0.256452    -0.0841753    0.0968203   -0.0348905   -0.0490303   -0.224777     0.101668     0.0849488 
 -0.046355   -0.165708     -0.0119767   -0.0162724  -0.10885     -0.0215974   -0.0288486    0.0316033    -0.091595     0.0950642   -0.0643886   -0.088023    -0.101721    -0.0757427    0.0374312    0.102864     0.14458     -0.027935     -0.123201    -0.0146079    0.0758422   -0.0319526   -0.0916406    0.00198621   0.134345    -0.0126288 
 -0.0180667   0.095853      0.0643785    0.103153   -0.166485    -0.0117842   -0.107023     0.0204278     0.269177     0.0248297    0.144273     0.185356    -0.0432725   -0.0960145   -0.042389     0.0790516    0.065819     0.0976998    -0.0370205   -0.116813     0.0251214   -0.0517498   -0.175966     0.0847187    0.107352    -0.00726195
 -0.0288679  -0.125108      0.0250893   -0.166293    0.0584492   -0.121307     0.141909     0.149748     -0.0355157   -0.0252235   -0.0872534   -0.14116      0.0361482    0.202198     0.0563855    0.0259329    0.172011    -0.177537      0.00986045  -0.064116    -0.0168634    0.0949744    0.00612222   0.140905    -0.158286    -0.0917361 
  0.094345    0.020348      0.0250133    0.0970506  -0.0414522    0.0645287   -0.0438579   -0.0616998    -0.025498     0.059538    -0.106413     0.0340634   -0.00653344   0.00320311   0.142075    -0.140688     0.00277842  -0.00112699    0.045287     0.047816    -0.0995977    0.0448468   -0.0667146    0.0355563   -0.143215     0.208926  
  0.136336   -0.0183199     0.122452    -0.126041   -0.22129      0.0964022    0.00985972   0.0312939    -0.202759     0.00433123  -0.181091    -0.205974     0.0105227    0.0484454   -0.00266174   0.0229921   -0.0521492   -0.0113406    -0.196677     0.0704907    0.192864     0.11123      0.0573096    0.0228989    0.15578     -0.172371  
 -0.0867449   0.118461     -0.0910135    0.0422147   0.0691483   -0.167168     0.00277232   0.083044      0.104122     0.0283681    0.00195085   0.0886522   -0.00531926   0.14656      0.0230244   -0.145572     0.0390209    0.0532712    -0.100566     0.198127     0.0656016    0.175615    -0.102311     0.0224293    0.112341    -0.0997423 
  0.0663313  -0.125329      0.16852      0.0799837  -0.0338025    0.00534695  -0.156197     0.143009     -0.176651    -0.0106936    0.0222907    0.0760403   -0.171458    -0.00115721  -0.10986      0.0238235    0.0106399   -0.0981961     0.0679313    0.01527      0.0696583    0.0421648   -0.120262     0.150984    -0.0807502   -0.030754  
  0.156091    0.082064     -0.0176625    0.0345505   0.110433     0.204234    -0.0134374   -0.0567307     0.0511707   -0.0431587    0.180102    -0.0123216    0.216808     0.118892    -0.0663547   -0.0228177    0.00628777  -0.0547413    -0.0275748   -0.140438    -0.282474    -0.0303515    0.0786946    0.0256718    0.0233642   -0.0741728 
 -0.0950154   0.00437646    0.0824316    0.0669472   0.105858    -0.0679848   -0.0668767    0.00392192   -0.0254075   -0.13805      0.0476727   -0.127534     0.112755    -0.08296     -0.0648353   -0.0016911    0.109741     0.0482854    -0.0864223   -0.111018     0.163516     0.185026     0.104336     0.164221     0.329919     0.0117641 
  0.21612     0.0295889     0.016811     0.15671     0.0416687   -0.00812853  -0.052605    -0.131132     -0.0567756    0.116564    -0.073745    -0.0572547    0.0613879   -0.0367799   -0.0064653   -0.143272     0.0608759   -0.0350462     0.0769234   -0.0500009    0.00108679   0.0355734    0.0522649   -0.0404122    0.032121     0.0281571 
  0.122327   -0.202073     -0.0340122    0.0905915   0.0198601   -0.162303     0.0500665   -0.228578      0.0488924    0.0335774   -0.167737     0.179591     0.0243258    0.131985     0.092241    -0.0444832    0.0605081    0.00918817    0.222457     0.0492328    0.0547602   -0.0560889   -0.0287316   -0.0180714    0.111804     0.0198705 kind full, method split
0: avll = -1.4253506410593533
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.425370
INFO: iteration 2, average log likelihood -1.425297
INFO: iteration 3, average log likelihood -1.425240
INFO: iteration 4, average log likelihood -1.425176
INFO: iteration 5, average log likelihood -1.425099
INFO: iteration 6, average log likelihood -1.425004
INFO: iteration 7, average log likelihood -1.424880
INFO: iteration 8, average log likelihood -1.424696
INFO: iteration 9, average log likelihood -1.424382
INFO: iteration 10, average log likelihood -1.423826
INFO: iteration 11, average log likelihood -1.422953
INFO: iteration 12, average log likelihood -1.421902
INFO: iteration 13, average log likelihood -1.421026
INFO: iteration 14, average log likelihood -1.420521
INFO: iteration 15, average log likelihood -1.420293
INFO: iteration 16, average log likelihood -1.420199
INFO: iteration 17, average log likelihood -1.420161
INFO: iteration 18, average log likelihood -1.420145
INFO: iteration 19, average log likelihood -1.420138
INFO: iteration 20, average log likelihood -1.420136
INFO: iteration 21, average log likelihood -1.420134
INFO: iteration 22, average log likelihood -1.420134
INFO: iteration 23, average log likelihood -1.420133
INFO: iteration 24, average log likelihood -1.420133
INFO: iteration 25, average log likelihood -1.420133
INFO: iteration 26, average log likelihood -1.420133
INFO: iteration 27, average log likelihood -1.420132
INFO: iteration 28, average log likelihood -1.420132
INFO: iteration 29, average log likelihood -1.420132
INFO: iteration 30, average log likelihood -1.420132
INFO: iteration 31, average log likelihood -1.420132
INFO: iteration 32, average log likelihood -1.420132
INFO: iteration 33, average log likelihood -1.420132
INFO: iteration 34, average log likelihood -1.420132
INFO: iteration 35, average log likelihood -1.420132
INFO: iteration 36, average log likelihood -1.420132
INFO: iteration 37, average log likelihood -1.420132
INFO: iteration 38, average log likelihood -1.420132
INFO: iteration 39, average log likelihood -1.420132
INFO: iteration 40, average log likelihood -1.420132
INFO: iteration 41, average log likelihood -1.420132
INFO: iteration 42, average log likelihood -1.420132
INFO: iteration 43, average log likelihood -1.420132
INFO: iteration 44, average log likelihood -1.420132
INFO: iteration 45, average log likelihood -1.420132
INFO: iteration 46, average log likelihood -1.420132
INFO: iteration 47, average log likelihood -1.420132
INFO: iteration 48, average log likelihood -1.420132
INFO: iteration 49, average log likelihood -1.420132
INFO: iteration 50, average log likelihood -1.420132
INFO: EM with 100000 data points 50 iterations avll -1.420132
952.4 data points per parameter
1: avll = [-1.42537,-1.4253,-1.42524,-1.42518,-1.4251,-1.425,-1.42488,-1.4247,-1.42438,-1.42383,-1.42295,-1.4219,-1.42103,-1.42052,-1.42029,-1.4202,-1.42016,-1.42015,-1.42014,-1.42014,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.420150
INFO: iteration 2, average log likelihood -1.420074
INFO: iteration 3, average log likelihood -1.420015
INFO: iteration 4, average log likelihood -1.419948
INFO: iteration 5, average log likelihood -1.419871
INFO: iteration 6, average log likelihood -1.419786
INFO: iteration 7, average log likelihood -1.419702
INFO: iteration 8, average log likelihood -1.419624
INFO: iteration 9, average log likelihood -1.419559
INFO: iteration 10, average log likelihood -1.419508
INFO: iteration 11, average log likelihood -1.419469
INFO: iteration 12, average log likelihood -1.419438
INFO: iteration 13, average log likelihood -1.419413
INFO: iteration 14, average log likelihood -1.419391
INFO: iteration 15, average log likelihood -1.419371
INFO: iteration 16, average log likelihood -1.419353
INFO: iteration 17, average log likelihood -1.419333
INFO: iteration 18, average log likelihood -1.419313
INFO: iteration 19, average log likelihood -1.419291
INFO: iteration 20, average log likelihood -1.419265
INFO: iteration 21, average log likelihood -1.419237
INFO: iteration 22, average log likelihood -1.419206
INFO: iteration 23, average log likelihood -1.419172
INFO: iteration 24, average log likelihood -1.419136
INFO: iteration 25, average log likelihood -1.419099
INFO: iteration 26, average log likelihood -1.419063
INFO: iteration 27, average log likelihood -1.419028
INFO: iteration 28, average log likelihood -1.418997
INFO: iteration 29, average log likelihood -1.418969
INFO: iteration 30, average log likelihood -1.418946
INFO: iteration 31, average log likelihood -1.418926
INFO: iteration 32, average log likelihood -1.418910
INFO: iteration 33, average log likelihood -1.418897
INFO: iteration 34, average log likelihood -1.418886
INFO: iteration 35, average log likelihood -1.418877
INFO: iteration 36, average log likelihood -1.418869
INFO: iteration 37, average log likelihood -1.418863
INFO: iteration 38, average log likelihood -1.418857
INFO: iteration 39, average log likelihood -1.418852
INFO: iteration 40, average log likelihood -1.418847
INFO: iteration 41, average log likelihood -1.418843
INFO: iteration 42, average log likelihood -1.418839
INFO: iteration 43, average log likelihood -1.418835
INFO: iteration 44, average log likelihood -1.418832
INFO: iteration 45, average log likelihood -1.418829
INFO: iteration 46, average log likelihood -1.418825
INFO: iteration 47, average log likelihood -1.418822
INFO: iteration 48, average log likelihood -1.418819
INFO: iteration 49, average log likelihood -1.418816
INFO: iteration 50, average log likelihood -1.418814
INFO: EM with 100000 data points 50 iterations avll -1.418814
473.9 data points per parameter
2: avll = [-1.42015,-1.42007,-1.42002,-1.41995,-1.41987,-1.41979,-1.4197,-1.41962,-1.41956,-1.41951,-1.41947,-1.41944,-1.41941,-1.41939,-1.41937,-1.41935,-1.41933,-1.41931,-1.41929,-1.41927,-1.41924,-1.41921,-1.41917,-1.41914,-1.4191,-1.41906,-1.41903,-1.419,-1.41897,-1.41895,-1.41893,-1.41891,-1.4189,-1.41889,-1.41888,-1.41887,-1.41886,-1.41886,-1.41885,-1.41885,-1.41884,-1.41884,-1.41884,-1.41883,-1.41883,-1.41883,-1.41882,-1.41882,-1.41882,-1.41881]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.418822
INFO: iteration 2, average log likelihood -1.418770
INFO: iteration 3, average log likelihood -1.418728
INFO: iteration 4, average log likelihood -1.418682
INFO: iteration 5, average log likelihood -1.418628
INFO: iteration 6, average log likelihood -1.418564
INFO: iteration 7, average log likelihood -1.418488
INFO: iteration 8, average log likelihood -1.418405
INFO: iteration 9, average log likelihood -1.418316
INFO: iteration 10, average log likelihood -1.418229
INFO: iteration 11, average log likelihood -1.418147
INFO: iteration 12, average log likelihood -1.418075
INFO: iteration 13, average log likelihood -1.418014
INFO: iteration 14, average log likelihood -1.417963
INFO: iteration 15, average log likelihood -1.417923
INFO: iteration 16, average log likelihood -1.417890
INFO: iteration 17, average log likelihood -1.417862
INFO: iteration 18, average log likelihood -1.417840
INFO: iteration 19, average log likelihood -1.417821
INFO: iteration 20, average log likelihood -1.417804
INFO: iteration 21, average log likelihood -1.417790
INFO: iteration 22, average log likelihood -1.417777
INFO: iteration 23, average log likelihood -1.417765
INFO: iteration 24, average log likelihood -1.417753
INFO: iteration 25, average log likelihood -1.417743
INFO: iteration 26, average log likelihood -1.417733
INFO: iteration 27, average log likelihood -1.417723
INFO: iteration 28, average log likelihood -1.417713
INFO: iteration 29, average log likelihood -1.417704
INFO: iteration 30, average log likelihood -1.417695
INFO: iteration 31, average log likelihood -1.417686
INFO: iteration 32, average log likelihood -1.417678
INFO: iteration 33, average log likelihood -1.417669
INFO: iteration 34, average log likelihood -1.417661
INFO: iteration 35, average log likelihood -1.417653
INFO: iteration 36, average log likelihood -1.417645
INFO: iteration 37, average log likelihood -1.417637
INFO: iteration 38, average log likelihood -1.417630
INFO: iteration 39, average log likelihood -1.417622
INFO: iteration 40, average log likelihood -1.417615
INFO: iteration 41, average log likelihood -1.417608
INFO: iteration 42, average log likelihood -1.417601
INFO: iteration 43, average log likelihood -1.417594
INFO: iteration 44, average log likelihood -1.417587
INFO: iteration 45, average log likelihood -1.417580
INFO: iteration 46, average log likelihood -1.417573
INFO: iteration 47, average log likelihood -1.417567
INFO: iteration 48, average log likelihood -1.417560
INFO: iteration 49, average log likelihood -1.417554
INFO: iteration 50, average log likelihood -1.417547
INFO: EM with 100000 data points 50 iterations avll -1.417547
236.4 data points per parameter
3: avll = [-1.41882,-1.41877,-1.41873,-1.41868,-1.41863,-1.41856,-1.41849,-1.4184,-1.41832,-1.41823,-1.41815,-1.41807,-1.41801,-1.41796,-1.41792,-1.41789,-1.41786,-1.41784,-1.41782,-1.4178,-1.41779,-1.41778,-1.41776,-1.41775,-1.41774,-1.41773,-1.41772,-1.41771,-1.4177,-1.41769,-1.41769,-1.41768,-1.41767,-1.41766,-1.41765,-1.41764,-1.41764,-1.41763,-1.41762,-1.41761,-1.41761,-1.4176,-1.41759,-1.41759,-1.41758,-1.41757,-1.41757,-1.41756,-1.41755,-1.41755]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.417549
INFO: iteration 2, average log likelihood -1.417493
INFO: iteration 3, average log likelihood -1.417439
INFO: iteration 4, average log likelihood -1.417378
INFO: iteration 5, average log likelihood -1.417302
INFO: iteration 6, average log likelihood -1.417211
INFO: iteration 7, average log likelihood -1.417105
INFO: iteration 8, average log likelihood -1.416991
INFO: iteration 9, average log likelihood -1.416875
INFO: iteration 10, average log likelihood -1.416765
INFO: iteration 11, average log likelihood -1.416664
INFO: iteration 12, average log likelihood -1.416573
INFO: iteration 13, average log likelihood -1.416493
INFO: iteration 14, average log likelihood -1.416423
INFO: iteration 15, average log likelihood -1.416360
INFO: iteration 16, average log likelihood -1.416305
INFO: iteration 17, average log likelihood -1.416258
INFO: iteration 18, average log likelihood -1.416216
INFO: iteration 19, average log likelihood -1.416179
INFO: iteration 20, average log likelihood -1.416147
INFO: iteration 21, average log likelihood -1.416119
INFO: iteration 22, average log likelihood -1.416094
INFO: iteration 23, average log likelihood -1.416072
INFO: iteration 24, average log likelihood -1.416052
INFO: iteration 25, average log likelihood -1.416033
INFO: iteration 26, average log likelihood -1.416016
INFO: iteration 27, average log likelihood -1.415999
INFO: iteration 28, average log likelihood -1.415984
INFO: iteration 29, average log likelihood -1.415970
INFO: iteration 30, average log likelihood -1.415956
INFO: iteration 31, average log likelihood -1.415943
INFO: iteration 32, average log likelihood -1.415931
INFO: iteration 33, average log likelihood -1.415919
INFO: iteration 34, average log likelihood -1.415907
INFO: iteration 35, average log likelihood -1.415896
INFO: iteration 36, average log likelihood -1.415886
INFO: iteration 37, average log likelihood -1.415875
INFO: iteration 38, average log likelihood -1.415865
INFO: iteration 39, average log likelihood -1.415856
INFO: iteration 40, average log likelihood -1.415846
INFO: iteration 41, average log likelihood -1.415837
INFO: iteration 42, average log likelihood -1.415829
INFO: iteration 43, average log likelihood -1.415820
INFO: iteration 44, average log likelihood -1.415812
INFO: iteration 45, average log likelihood -1.415804
INFO: iteration 46, average log likelihood -1.415796
INFO: iteration 47, average log likelihood -1.415788
INFO: iteration 48, average log likelihood -1.415781
INFO: iteration 49, average log likelihood -1.415774
INFO: iteration 50, average log likelihood -1.415767
INFO: EM with 100000 data points 50 iterations avll -1.415767
118.1 data points per parameter
4: avll = [-1.41755,-1.41749,-1.41744,-1.41738,-1.4173,-1.41721,-1.41711,-1.41699,-1.41687,-1.41676,-1.41666,-1.41657,-1.41649,-1.41642,-1.41636,-1.41631,-1.41626,-1.41622,-1.41618,-1.41615,-1.41612,-1.41609,-1.41607,-1.41605,-1.41603,-1.41602,-1.416,-1.41598,-1.41597,-1.41596,-1.41594,-1.41593,-1.41592,-1.41591,-1.4159,-1.41589,-1.41588,-1.41587,-1.41586,-1.41585,-1.41584,-1.41583,-1.41582,-1.41581,-1.4158,-1.4158,-1.41579,-1.41578,-1.41577,-1.41577]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.415768
INFO: iteration 2, average log likelihood -1.415709
INFO: iteration 3, average log likelihood -1.415651
INFO: iteration 4, average log likelihood -1.415581
INFO: iteration 5, average log likelihood -1.415492
INFO: iteration 6, average log likelihood -1.415381
INFO: iteration 7, average log likelihood -1.415254
INFO: iteration 8, average log likelihood -1.415115
INFO: iteration 9, average log likelihood -1.414972
INFO: iteration 10, average log likelihood -1.414831
INFO: iteration 11, average log likelihood -1.414697
INFO: iteration 12, average log likelihood -1.414570
INFO: iteration 13, average log likelihood -1.414454
INFO: iteration 14, average log likelihood -1.414348
INFO: iteration 15, average log likelihood -1.414252
INFO: iteration 16, average log likelihood -1.414167
INFO: iteration 17, average log likelihood -1.414090
INFO: iteration 18, average log likelihood -1.414021
INFO: iteration 19, average log likelihood -1.413959
INFO: iteration 20, average log likelihood -1.413904
INFO: iteration 21, average log likelihood -1.413854
INFO: iteration 22, average log likelihood -1.413808
INFO: iteration 23, average log likelihood -1.413767
INFO: iteration 24, average log likelihood -1.413729
INFO: iteration 25, average log likelihood -1.413694
INFO: iteration 26, average log likelihood -1.413661
INFO: iteration 27, average log likelihood -1.413630
INFO: iteration 28, average log likelihood -1.413602
INFO: iteration 29, average log likelihood -1.413574
INFO: iteration 30, average log likelihood -1.413548
INFO: iteration 31, average log likelihood -1.413523
INFO: iteration 32, average log likelihood -1.413500
INFO: iteration 33, average log likelihood -1.413477
INFO: iteration 34, average log likelihood -1.413454
INFO: iteration 35, average log likelihood -1.413433
INFO: iteration 36, average log likelihood -1.413412
INFO: iteration 37, average log likelihood -1.413392
INFO: iteration 38, average log likelihood -1.413372
INFO: iteration 39, average log likelihood -1.413353
INFO: iteration 40, average log likelihood -1.413334
INFO: iteration 41, average log likelihood -1.413316
INFO: iteration 42, average log likelihood -1.413298
INFO: iteration 43, average log likelihood -1.413280
INFO: iteration 44, average log likelihood -1.413264
INFO: iteration 45, average log likelihood -1.413247
INFO: iteration 46, average log likelihood -1.413231
INFO: iteration 47, average log likelihood -1.413216
INFO: iteration 48, average log likelihood -1.413201
INFO: iteration 49, average log likelihood -1.413187
INFO: iteration 50, average log likelihood -1.413173
INFO: EM with 100000 data points 50 iterations avll -1.413173
59.0 data points per parameter
5: avll = [-1.41577,-1.41571,-1.41565,-1.41558,-1.41549,-1.41538,-1.41525,-1.41511,-1.41497,-1.41483,-1.4147,-1.41457,-1.41445,-1.41435,-1.41425,-1.41417,-1.41409,-1.41402,-1.41396,-1.4139,-1.41385,-1.41381,-1.41377,-1.41373,-1.41369,-1.41366,-1.41363,-1.4136,-1.41357,-1.41355,-1.41352,-1.4135,-1.41348,-1.41345,-1.41343,-1.41341,-1.41339,-1.41337,-1.41335,-1.41333,-1.41332,-1.4133,-1.41328,-1.41326,-1.41325,-1.41323,-1.41322,-1.4132,-1.41319,-1.41317]
[-1.42535,-1.42537,-1.4253,-1.42524,-1.42518,-1.4251,-1.425,-1.42488,-1.4247,-1.42438,-1.42383,-1.42295,-1.4219,-1.42103,-1.42052,-1.42029,-1.4202,-1.42016,-1.42015,-1.42014,-1.42014,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42015,-1.42007,-1.42002,-1.41995,-1.41987,-1.41979,-1.4197,-1.41962,-1.41956,-1.41951,-1.41947,-1.41944,-1.41941,-1.41939,-1.41937,-1.41935,-1.41933,-1.41931,-1.41929,-1.41927,-1.41924,-1.41921,-1.41917,-1.41914,-1.4191,-1.41906,-1.41903,-1.419,-1.41897,-1.41895,-1.41893,-1.41891,-1.4189,-1.41889,-1.41888,-1.41887,-1.41886,-1.41886,-1.41885,-1.41885,-1.41884,-1.41884,-1.41884,-1.41883,-1.41883,-1.41883,-1.41882,-1.41882,-1.41882,-1.41881,-1.41882,-1.41877,-1.41873,-1.41868,-1.41863,-1.41856,-1.41849,-1.4184,-1.41832,-1.41823,-1.41815,-1.41807,-1.41801,-1.41796,-1.41792,-1.41789,-1.41786,-1.41784,-1.41782,-1.4178,-1.41779,-1.41778,-1.41776,-1.41775,-1.41774,-1.41773,-1.41772,-1.41771,-1.4177,-1.41769,-1.41769,-1.41768,-1.41767,-1.41766,-1.41765,-1.41764,-1.41764,-1.41763,-1.41762,-1.41761,-1.41761,-1.4176,-1.41759,-1.41759,-1.41758,-1.41757,-1.41757,-1.41756,-1.41755,-1.41755,-1.41755,-1.41749,-1.41744,-1.41738,-1.4173,-1.41721,-1.41711,-1.41699,-1.41687,-1.41676,-1.41666,-1.41657,-1.41649,-1.41642,-1.41636,-1.41631,-1.41626,-1.41622,-1.41618,-1.41615,-1.41612,-1.41609,-1.41607,-1.41605,-1.41603,-1.41602,-1.416,-1.41598,-1.41597,-1.41596,-1.41594,-1.41593,-1.41592,-1.41591,-1.4159,-1.41589,-1.41588,-1.41587,-1.41586,-1.41585,-1.41584,-1.41583,-1.41582,-1.41581,-1.4158,-1.4158,-1.41579,-1.41578,-1.41577,-1.41577,-1.41577,-1.41571,-1.41565,-1.41558,-1.41549,-1.41538,-1.41525,-1.41511,-1.41497,-1.41483,-1.4147,-1.41457,-1.41445,-1.41435,-1.41425,-1.41417,-1.41409,-1.41402,-1.41396,-1.4139,-1.41385,-1.41381,-1.41377,-1.41373,-1.41369,-1.41366,-1.41363,-1.4136,-1.41357,-1.41355,-1.41352,-1.4135,-1.41348,-1.41345,-1.41343,-1.41341,-1.41339,-1.41337,-1.41335,-1.41333,-1.41332,-1.4133,-1.41328,-1.41326,-1.41325,-1.41323,-1.41322,-1.4132,-1.41319,-1.41317]
32×26 Array{Float64,2}:
  0.0337282    0.002692    -0.0346494  -0.111002     0.123252    0.0679047   -0.0642692  -0.024118   -0.227239   -0.0217592  -0.0654041    0.000433385  -0.033868   -0.0953445   -0.0657931    0.167279     0.0255411   0.020358   -0.118728    -0.0305245    -0.257752     0.155018     0.0223992   -0.136552   -0.0273138    0.0588334 
 -0.0185636    0.105279     0.0501049  -0.092189    -0.110619   -0.0316507   -0.0633896   0.1416      0.150103    0.32725    -0.0671957   -0.0920089     0.150524    0.277053     0.0485872   -0.0819647   -0.0807043   0.0547337   0.161876     0.173699      0.460045    -0.0343378   -0.0959913   -0.0641812   0.130723    -0.152333  
 -0.467631     0.190116    -0.272805   -0.0746375    0.28357    -0.356581     0.205891   -0.160425    0.154676   -0.0795243   0.0211764   -0.498709     -0.155301   -0.3451       0.243156    -0.320871    -0.0308967  -0.201656   -0.332803    -0.102895      0.0627121   -0.183522     0.22702     -0.164776    0.00370603  -0.121886  
 -0.232478     0.0797786   -0.301264    0.0538803    0.26491     0.280868     0.0127921  -0.28347     0.404974    0.0633825   0.23212     -0.210635      0.323503   -0.431145    -0.0472811   -0.00193338  -0.651496   -0.214287    0.694644    -0.0560552    -0.123623     0.0479594   -0.0685174    0.163244   -0.0565187   -0.0707058 
  0.0294629   -0.287987     0.0917782   0.326221    -0.0938411  -0.209286    -0.504334   -0.228413    0.385841   -0.433955    0.515739    -0.227125      0.468754   -0.390697    -0.0717713   -0.0139054    0.249325    0.0627229   0.0565139   -0.431928      0.101066     0.620692    -0.775209    -0.108865   -0.0988156   -0.419033  
  0.179629    -0.313609    -0.311908    0.686558    -0.0930308   0.0935883   -0.0395401   0.0625672   0.351396   -0.258064   -0.158762     0.535825     -0.304052    0.0447689   -0.563909    -0.385384    -0.433829    0.104003    0.634058    -0.281344      0.443776     0.322783    -0.217702    -0.516721   -0.365895    -0.177706  
  0.0897676    0.0373792    0.607656   -0.103622     0.151778   -0.249559    -0.0405303  -0.174667   -0.317229    0.0769219   0.00450002  -0.131406      0.017175   -0.154333    -0.403246    -0.552502    -0.107491    0.0417369   0.595557     0.382253      0.0582505   -0.431283    -0.101737    -0.0447349   0.375983    -0.00022377
  0.348724    -0.142711     0.259363    0.171112     0.442899    0.810098     0.435322    0.0452274  -0.287853    0.352092   -0.479922     0.372343     -0.116572    0.570705     0.135701    -0.3179      -0.0139684   0.0267431   0.42017      0.326097     -0.215112    -0.153359     0.376941     0.201085    0.226247     0.421792  
  0.131257    -0.0196693    0.188228    0.523407    -0.248      -0.193223     0.147579   -0.259283   -0.0102314  -0.422988    0.203        0.28549      -0.309687    0.062501    -0.002372    -0.370652     0.422799   -0.172553    0.191739    -0.323294     -0.0468013   -0.159453     0.0962122    0.143236    0.125794     0.0454058 
  0.683442    -0.168579     0.581114    0.161466    -0.225922   -0.0162439   -0.166932    0.118245   -0.0791923  -0.558017   -0.124831     0.660383     -0.265109    0.00368965  -0.16655      0.183455     0.261322    0.607333   -0.364481     0.196815     -0.00180983   0.0702994    0.155312    -0.122162    0.0871324    0.179642  
  0.0232893    0.14691     -0.365994    0.14576     -0.322356    0.0543463   -0.142983    0.146688   -0.210995    0.193551    0.286056     0.38771      -0.319991   -0.313827     0.252322     0.269633     0.0504496  -0.12682    -0.14146     -0.0961775     0.0405486    0.279834     0.0116022    0.497474   -0.191057     0.563802  
  0.578486    -0.416463     0.190711   -0.0807169    0.0736066   0.120883     0.478921   -0.104007   -0.102135   -0.200021    0.434821    -0.0149873    -0.430381    0.180861     0.311231     0.57777     -0.0704649  -0.131067   -0.355704    -0.201076      0.122968     0.0924598    0.217633     0.784238   -0.506188    -0.121408  
 -0.41265     -0.0927799   -0.0278774  -0.280944     0.0219551  -0.0267879    0.0544836  -0.0903536  -0.216263    0.204164   -0.105503    -0.395074     -0.0450616   0.515333     0.0196283    0.0490191    0.267485    0.516489   -0.424102    -0.630054     -0.573182     0.761382     0.230939     0.181745   -0.0374954   -0.619656  
  0.26581     -0.00863966   0.526926   -0.185902     0.382907   -0.0559052    0.68032     0.307965    0.133203   -0.301044   -0.0613103   -0.387359      0.370588    0.460253     0.235001     0.0495243    0.568612    0.218793   -0.306099    -0.0964129     0.38285      0.320958    -0.185439     0.0226166   0.145533    -0.208527  
  0.00992312   0.0989906    0.521414   -0.0772564   -0.607104    0.180558     0.316518    0.355941   -0.531677    0.330317   -0.023272    -0.0827419     0.068038    0.275127     0.203661     0.702816     0.830273   -0.410091   -0.00504451  -0.117693     -0.273934    -0.275925     0.0504894   -0.0287857   0.542275     0.264267  
 -0.191947    -0.0506257   -0.309309   -0.103782    -0.477769   -0.0325796    0.0463518   0.26417     0.24567     0.401644    0.235439    -0.378367      0.535988    0.540868     0.0552976    0.13346      0.328983   -0.180315    0.216404    -0.51172       0.453947    -0.434507     0.160162     0.128953    0.431807     0.0950499 
  0.104469    -0.419158     0.159748    0.0244      -0.0966073  -0.0426909    0.226232   -0.337036    0.310774   -0.298281    0.688283     0.0378248     0.173205   -0.554812    -0.685919     0.317543    -0.754118    0.0396196   0.506428    -0.313604      0.0900348   -0.601583    -0.166864     0.43166     0.41571     -0.128905  
  0.0520274   -0.678116     0.406721   -0.0297052   -0.44646    -0.485436     0.154214    0.17295     0.626466   -0.0357736   0.311052     0.078029      0.227294   -0.338937     0.711715    -0.0879127   -0.185599   -0.158088    0.0691412    0.230191      0.0684433   -0.796272    -0.0825133   -0.172167    0.112012    -0.290581  
  0.436928     0.359932     0.184287   -0.209626     0.0102073  -0.303121    -0.306099   -0.0867843  -0.169208   -0.328738    0.21992      0.256551      0.0437145  -0.366959    -0.212145    -0.0373002    0.0346327   0.23476    -0.156808     0.280179     -0.00659944   0.175417    -0.0545418   -0.0838485   0.146632     0.231498  
  0.277742    -0.479646     0.332385    0.103026     0.0582394  -0.508918    -0.193184    0.221578   -0.361437   -0.755401    0.340328     0.704632      0.127964    0.687913     0.0545006    0.200203     0.868955    0.198412   -0.717693    -0.23278      -0.543617     0.161352     0.0548629    0.585407    0.258468    -0.415724  
  0.375551    -0.333612     0.450274    0.615949    -0.465177    0.360494     0.103786   -0.313337   -0.120134   -0.134452    0.395132     0.165887     -0.377216   -1.04736      0.480205    -0.160742     0.870242   -0.109905    0.0824594   -0.34336      -0.620738    -0.163197    -0.0113368    0.0129456  -0.108613     0.732986  
 -0.342211     0.710955    -0.138491    0.325797    -0.258308    0.0423608   -0.0842456  -0.473283   -0.389147   -0.0671379  -0.253434     0.333845     -0.118495    0.298565    -0.108247    -0.0308235    0.136977    0.0221129   0.199723    -0.898233     -0.458371     0.364997    -0.0415849    0.802385   -0.0307752    0.155428  
  0.48063     -0.140605     0.463561    0.771241     0.24114     0.750578     0.51579    -0.432157    0.0264621  -0.318118    0.235438    -0.614856     -0.121867   -0.17107      0.268711     0.139546     0.216774   -1.1428     -0.371871     0.000825411   0.924391    -0.017695    -0.330808     0.28863     0.286494     0.148631  
  0.0995276   -0.395392     0.257322    0.319934    -0.227993    0.167486     0.227091    0.265295   -0.113113    0.0405974  -0.28474     -0.0466028    -0.346124    0.898271     0.00694289  -0.165623     0.479002   -0.0132704   0.0202528   -0.129277      0.471899    -0.283486     0.244641    -0.315995    0.194608     0.0816074 
 -0.15827      0.0375624   -0.044909   -0.43139      0.196163    0.246549     0.419612   -0.0417357   0.274728    0.494729   -0.284356    -0.444667      0.246272   -0.275145     0.217888     0.379908    -0.591176   -0.18386     0.450486     0.0951654     0.381169    -0.0531354   -0.570088    -0.0809987  -0.623776    -0.427551  
 -0.369877     0.270242    -0.141334   -0.54819      0.281698    0.102729     0.220784    0.181378    0.387787    0.4453     -0.723298    -1.30426      -0.155149    0.375149    -0.19187     -0.632702    -0.358792   -0.0518143   0.499206     0.344372      0.40445      0.410886     0.0917338   -0.135418    0.192976    -0.132695  
 -0.268806    -0.00918005  -0.516586   -0.00536966   0.730289    0.146286    -0.236327   -0.122508   -0.138465   -0.075035    0.0212235    0.636826     -0.442076   -0.586089    -0.178243    -0.243618    -0.813679    0.225626    0.262615     0.34806      -0.641722     0.331853     0.264065    -0.231374   -0.307781    -0.0859497 
  0.0550806    0.0992789   -0.614923   -0.911507     0.139808   -0.163462    -0.320103    0.334443    0.130795    0.570198   -0.378053     0.436531      0.40533     0.484833    -0.260735     0.244825    -0.602824    0.659641   -0.0379225    0.382202      0.282274     0.39332     -0.00986028   0.0217327  -0.200741    -0.172603  
 -0.7542       0.16039     -0.128999    0.196704    -0.345771    0.0996729   -0.843667   -0.0370864  -0.222292    0.211574   -0.784817     0.140195      0.133483    0.013926    -0.165966    -0.535533     0.0854158   0.0215395   0.19417      0.423959     -0.113112     0.0113279   -0.211272    -0.906959    0.448602     0.0800054 
 -0.239623     0.488971    -0.48428     0.140572    -0.294446    0.00208612  -0.0589131   0.747728   -0.238674    0.779946   -0.387965     0.442704     -0.38646     0.637345     1.00299     -0.716223     0.727234    0.247803   -0.899323     0.252406     -0.0995261    0.936029    -0.0139725   -0.249411    0.0332642   -0.353904  
  0.249832     0.672694    -0.087015   -0.201943    -0.0319305  -0.651465    -0.388206    0.159734   -0.287152   -0.198326   -0.223004     0.231635     -0.0436914   0.0402157    0.152722     0.460684     0.268037   -0.170322   -0.627512     0.488684      0.094147    -0.164225     0.203131    -0.458848   -0.0150505    0.0620375 
  0.31132      0.194247    -0.17994    -0.37767      0.229538    0.630488    -0.458225   -0.013959   -0.130402    0.628417   -0.0489643   -0.484679      0.134669   -0.362374    -0.341745     0.384174    -0.241619   -0.248293   -0.347044     0.344513      0.0519738   -0.00984313  -0.0796631   -0.434794   -0.195088     0.725383  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.413159
INFO: iteration 2, average log likelihood -1.413146
INFO: iteration 3, average log likelihood -1.413134
INFO: iteration 4, average log likelihood -1.413122
INFO: iteration 5, average log likelihood -1.413110
INFO: iteration 6, average log likelihood -1.413099
INFO: iteration 7, average log likelihood -1.413088
INFO: iteration 8, average log likelihood -1.413077
INFO: iteration 9, average log likelihood -1.413067
INFO: iteration 10, average log likelihood -1.413057
INFO: EM with 100000 data points 10 iterations avll -1.413057
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.219209e+05
      1       7.120018e+05      -2.099191e+05 |       32
      2       6.982683e+05      -1.373346e+04 |       32
      3       6.928785e+05      -5.389857e+03 |       32
      4       6.902516e+05      -2.626834e+03 |       32
      5       6.886802e+05      -1.571461e+03 |       32
      6       6.875131e+05      -1.167043e+03 |       32
      7       6.865614e+05      -9.517573e+02 |       32
      8       6.858193e+05      -7.420800e+02 |       32
      9       6.852314e+05      -5.878796e+02 |       32
     10       6.847436e+05      -4.878645e+02 |       32
     11       6.843361e+05      -4.074556e+02 |       32
     12       6.839939e+05      -3.421709e+02 |       32
     13       6.837102e+05      -2.837325e+02 |       32
     14       6.834590e+05      -2.511935e+02 |       32
     15       6.832257e+05      -2.332970e+02 |       32
     16       6.830205e+05      -2.051757e+02 |       32
     17       6.828311e+05      -1.894818e+02 |       32
     18       6.826456e+05      -1.855001e+02 |       32
     19       6.824447e+05      -2.008549e+02 |       32
     20       6.822636e+05      -1.810749e+02 |       32
     21       6.820794e+05      -1.842049e+02 |       32
     22       6.819149e+05      -1.645298e+02 |       32
     23       6.817744e+05      -1.404793e+02 |       32
     24       6.816441e+05      -1.302850e+02 |       32
     25       6.815334e+05      -1.107208e+02 |       32
     26       6.814343e+05      -9.909782e+01 |       32
     27       6.813344e+05      -9.992355e+01 |       32
     28       6.812385e+05      -9.584227e+01 |       32
     29       6.811416e+05      -9.691801e+01 |       32
     30       6.810561e+05      -8.548022e+01 |       32
     31       6.809740e+05      -8.215819e+01 |       32
     32       6.808996e+05      -7.437245e+01 |       32
     33       6.808320e+05      -6.764186e+01 |       32
     34       6.807602e+05      -7.174618e+01 |       32
     35       6.806924e+05      -6.779268e+01 |       32
     36       6.806295e+05      -6.291205e+01 |       32
     37       6.805708e+05      -5.870204e+01 |       32
     38       6.805192e+05      -5.157145e+01 |       32
     39       6.804615e+05      -5.774912e+01 |       32
     40       6.804054e+05      -5.611122e+01 |       32
     41       6.803494e+05      -5.595705e+01 |       32
     42       6.802952e+05      -5.419481e+01 |       32
     43       6.802444e+05      -5.080163e+01 |       32
     44       6.801977e+05      -4.668854e+01 |       32
     45       6.801455e+05      -5.226931e+01 |       32
     46       6.800907e+05      -5.479699e+01 |       32
     47       6.800361e+05      -5.454654e+01 |       32
     48       6.799796e+05      -5.655617e+01 |       32
     49       6.799218e+05      -5.774162e+01 |       32
     50       6.798632e+05      -5.859033e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 679863.2454498609)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.425067
INFO: iteration 2, average log likelihood -1.420083
INFO: iteration 3, average log likelihood -1.418855
INFO: iteration 4, average log likelihood -1.418066
INFO: iteration 5, average log likelihood -1.417199
INFO: iteration 6, average log likelihood -1.416188
INFO: iteration 7, average log likelihood -1.415270
INFO: iteration 8, average log likelihood -1.414666
INFO: iteration 9, average log likelihood -1.414339
INFO: iteration 10, average log likelihood -1.414158
INFO: iteration 11, average log likelihood -1.414043
INFO: iteration 12, average log likelihood -1.413959
INFO: iteration 13, average log likelihood -1.413892
INFO: iteration 14, average log likelihood -1.413835
INFO: iteration 15, average log likelihood -1.413785
INFO: iteration 16, average log likelihood -1.413741
INFO: iteration 17, average log likelihood -1.413701
INFO: iteration 18, average log likelihood -1.413665
INFO: iteration 19, average log likelihood -1.413632
INFO: iteration 20, average log likelihood -1.413601
INFO: iteration 21, average log likelihood -1.413573
INFO: iteration 22, average log likelihood -1.413546
INFO: iteration 23, average log likelihood -1.413521
INFO: iteration 24, average log likelihood -1.413498
INFO: iteration 25, average log likelihood -1.413477
INFO: iteration 26, average log likelihood -1.413456
INFO: iteration 27, average log likelihood -1.413437
INFO: iteration 28, average log likelihood -1.413419
INFO: iteration 29, average log likelihood -1.413402
INFO: iteration 30, average log likelihood -1.413386
INFO: iteration 31, average log likelihood -1.413370
INFO: iteration 32, average log likelihood -1.413356
INFO: iteration 33, average log likelihood -1.413342
INFO: iteration 34, average log likelihood -1.413329
INFO: iteration 35, average log likelihood -1.413316
INFO: iteration 36, average log likelihood -1.413304
INFO: iteration 37, average log likelihood -1.413292
INFO: iteration 38, average log likelihood -1.413281
INFO: iteration 39, average log likelihood -1.413270
INFO: iteration 40, average log likelihood -1.413259
INFO: iteration 41, average log likelihood -1.413249
INFO: iteration 42, average log likelihood -1.413239
INFO: iteration 43, average log likelihood -1.413229
INFO: iteration 44, average log likelihood -1.413219
INFO: iteration 45, average log likelihood -1.413210
INFO: iteration 46, average log likelihood -1.413200
INFO: iteration 47, average log likelihood -1.413191
INFO: iteration 48, average log likelihood -1.413181
INFO: iteration 49, average log likelihood -1.413172
INFO: iteration 50, average log likelihood -1.413163
INFO: EM with 100000 data points 50 iterations avll -1.413163
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.231142     0.58301     0.144266    -0.369158   -0.406307    0.635645      0.492272     0.32829    -0.298014     0.768555    -0.265352   -0.397606   -0.469425   -0.096329    -0.0826037    0.442151    0.0509069  -0.694954      0.465101      0.320787    0.297755    -0.0268056    0.0786112   -0.481584     0.000430829   0.532264   
 -0.665381     0.156456    0.0384431   -0.328458    0.308369   -0.203796      0.222962     0.0282417   0.396221    -0.360287    -0.585281   -0.853924    0.0331008  -0.00673731   0.175508    -0.208083    0.160078    0.0305288    -0.191158      0.279176    0.00391527  -0.049265     0.263388    -0.791583     0.0348337    -0.0481242  
  0.781865     0.0990877   0.390477     0.258813   -0.330175   -0.16763      -0.311619    -0.26775    -0.272978    -0.591985     0.485435    0.462219   -0.557319   -0.515564    -0.121994    -0.277781    0.43771     0.33424      -0.300074      0.0846482  -0.148013     0.295996     0.121398     0.122716    -0.0100171     0.660849   
  0.330826     0.231107   -0.261698    -0.426055    0.253539    0.347951     -0.645605    -0.0255395  -0.172909     0.625465     0.0440609  -0.481858    0.216786   -0.394533    -0.33823      0.349003   -0.24969    -0.248752     -0.491839      0.392486    0.00169131  -0.0739333   -0.0921208   -0.37532     -0.171397      0.669597   
 -0.276795     0.130446   -0.462748     0.112555    0.0586964   0.180714      0.169069     0.0892995  -0.181156     0.194824    -0.229353    0.241797   -0.0664321   0.175386     0.29115      0.318067   -0.0206112  -0.270353     -0.260689     -0.353507    0.0352672    0.0627533    0.0602796    0.117664    -0.0941321     0.0657988  
  0.0162418   -0.218126    0.00315594   0.024103   -0.393304    0.303538      0.541006    -0.713809    0.332235     0.490962     0.0918603  -0.222347   -0.0330015  -0.871259     0.168713    -0.119169   -0.247231   -0.216868      0.186037     -0.586498    0.168435    -0.519646    -0.8148       0.597863    -0.33193       0.54726    
 -0.267821    -0.0155209  -0.453985    -0.0454993   0.716012    0.192687     -0.269512    -0.0785919  -0.0522714   -0.00330985   0.105288    0.507299   -0.24191    -0.626702    -0.119302    -0.0759386  -0.715867    0.177447      0.370516      0.353843   -0.684737     0.370176     0.286646    -0.112339    -0.291023      0.00510219 
  0.00548727   0.0912662   0.0146358   -0.0214481   0.0849222  -0.100335     -0.0294635   -0.0543938   0.00354594   0.0128657    0.062062   -0.0694211  -0.0476519  -0.0890223    0.00995669  -0.0852959  -0.0720189  -0.000676166   0.0084611     0.059571    0.118595     0.00974071   0.0150403   -0.034623     0.00754223   -0.0356512  
 -0.16905      0.532549   -0.295217     0.0109923  -0.14614    -0.25818      -0.233245     0.646514   -0.34626      0.367005    -0.247773    0.232437   -0.179168    0.648441     0.540033    -0.373006    0.782576    0.182043     -0.86134       0.330248    0.112074     0.863125    -0.00955535  -0.262314     0.119819     -0.198853   
  0.682449     0.223779    0.362668    -0.0382093   0.0532878  -0.292197     -0.460014     0.276529   -0.240498    -0.512132    -0.417504    0.565131   -0.162568    0.200709     0.142369     0.7492      0.254071    0.0779763    -0.916774      0.439348   -0.187983    -0.17989      0.224348    -0.317302     0.0738558    -0.0945848  
  0.128716    -0.403239   -0.187201     0.518567   -0.0922384  -0.000742005   0.0650939   -0.10385     0.283477    -0.240156    -0.090268    0.457016   -0.499791    0.0805704   -0.520622    -0.444456   -0.349948    0.15699       0.60726      -0.210185    0.400346     0.105599    -0.030303    -0.372558    -0.442396     -0.312039   
 -0.0831708    0.0619496  -0.335915    -0.504254    0.171091    0.0175658    -0.151753     0.095656   -0.00589821   0.405754    -0.459262   -0.109575   -0.0884112   0.225355    -0.14837     -0.179559   -0.494515    0.289392      0.241021      0.308827    0.148339     0.23902     -0.029398    -0.196554    -0.136732     -0.126615   
  0.752404    -0.475862    0.159652    -0.256736   -0.0510596  -0.145547      0.610948     0.171691   -0.154352     0.0183828    0.703391    0.0418967  -0.202338    0.293316     0.291619     0.77417     0.178058   -0.149311     -0.134527     -0.233613    0.11834      0.0299823    0.202861     0.827444    -0.601589     -0.0418078  
  0.424778    -0.352662    0.60339      0.273207   -0.31954     0.116004     -0.00388279   0.294188   -0.180746    -0.302599     0.0352273   0.303437   -0.0808803   0.522055    -0.107757    -0.0368302   0.499327    0.334052      0.0166341     0.0244706   0.210359     0.202287    -0.0551395    0.022982     0.1933        0.130382   
 -0.240225    -0.302862    0.368681    -0.466248    0.325527   -0.116851      0.522827     0.268021    0.319836     0.470254    -0.0131257  -0.220773    0.258102   -0.495853     0.434305     0.178225   -0.41901    -0.385397     -0.00181829    0.534167    0.417184    -0.539252    -0.246064    -0.33205     -0.281458     -0.763446   
  0.615675     0.279909    0.62606      0.266385    0.0268339  -0.342827      0.134722    -0.380673   -0.128607    -0.475818     0.175032    0.0321485   0.176288   -0.381721    -0.319939    -0.148484   -0.156937   -0.295134      0.444134      0.281449    0.343994    -0.442851    -0.144854     0.152472     0.387571      0.171396   
  0.248904    -0.0106054   0.202588     0.0707366  -0.176125    0.162804      0.110827    -0.0933198  -0.294295    -0.209137     0.0337204   0.148993   -0.117414   -0.274931    -0.0670319    0.181357    0.373448    0.0175242    -0.0608551    -0.325607   -0.573676     0.0477643   -0.0204927   -0.0291912    0.0735486     0.27229    
 -0.325066    -0.361149   -0.114611    -0.0665219   0.0449524  -0.135839      0.146678    -0.302874    0.426823    -0.365338     0.751214   -0.110325    0.203482   -0.73296     -0.288527     0.214357   -0.857773    0.0796522     0.370367     -0.317747   -0.0976312   -0.218363    -0.0463627    0.317891     0.197393     -0.266376   
 -0.162551    -0.185642    0.258514     0.147735    0.0567326  -0.610885     -0.0764486   -0.2694     -0.0948819   -1.09395      0.35269     0.666709    0.0793324   0.190818     0.0804711   -0.2365      0.602949    0.292866     -0.254566     -0.3809     -0.405705    -0.151896    -0.108551     0.460952     0.312161     -0.702423   
  0.138413    -0.364101   -0.0377484    0.415111   -0.657001   -0.294478     -0.0489639    0.0230086   0.317182     0.163409    -0.111838    0.220108   -0.423167   -0.0374281    0.777092    -0.26944     0.377619   -0.400088      0.0113401     0.0266903  -0.168751    -0.766865     0.277902    -0.162956    -0.311568      0.133031   
 -0.0749258   -0.240273   -0.0824359    0.423618    0.640834    0.474814      0.156049    -0.125548    0.352096    -0.397013     0.347605   -0.429998   -0.474066   -0.190346     0.546111     0.065086    0.183236   -0.720874     -0.507747      0.0703875   0.441071     0.612579    -0.0337416    0.319756    -0.154341      0.122155   
 -0.0246845   -0.234717   -0.131573     0.14517    -0.434884   -0.311956     -0.63856     -0.0505242   0.307323    -0.176056     0.42147    -0.265959    0.545529   -0.272006     0.0363275    0.304662    0.208037   -0.075581      0.0319802    -0.245737    0.0871549    0.673968    -0.711372    -0.227693    -0.22343      -0.411202   
 -0.221425    -0.0022056  -0.147205    -0.0370941   0.178273    0.41754      -0.13013     -0.13969     0.28292      0.523134    -0.0215582  -0.431803    0.550218    0.297972    -0.208583    -0.183105   -0.478462   -0.187289      0.928459      0.165809    0.203144    -0.0281487   -0.202138     0.103694     0.124833     -0.313221   
 -0.525358     0.237083    0.0351954    0.298103   -0.0163503   0.0277448    -0.700279    -0.0876936  -0.0835036    0.0590445   -0.470087    0.107354    0.0461022  -0.21445     -0.382664    -1.03116    -0.129164   -0.00455024    0.34798       0.285755    0.0792659   -0.124945    -0.535363    -0.80767      0.559695     -0.000958486
  0.401458    -0.0466465   0.393388     0.0861576   0.663683    0.567158      0.584886     0.0538439  -0.314822     0.301487    -0.506205    0.217826   -0.202121    0.470865     0.0942932   -0.447378    0.0502743   0.031539      0.236195      0.319737   -0.214162    -0.136423     0.471453     0.264678     0.34009       0.502272   
  0.908283     0.455569   -0.402751    -0.483096    0.824439    0.0169434     0.139982     0.233387    0.495147    -0.513807    -0.161344    0.271469    0.966546   -0.0624123   -0.280079     0.206833   -0.298888    0.707424      0.2837       -0.236336    0.358542     0.294733    -0.320572    -0.0117541   -0.503551      0.0926676  
 -0.00819369   0.0399297   0.123182    -0.239938   -0.272028   -0.205824      0.276825     0.270999    0.217026     0.277466     0.118531   -0.418587    0.449946    0.452417     0.13556      0.0476774   0.356962   -0.0732209    -0.000451204  -0.299064    0.435216    -0.284945     0.0146255    0.0719039    0.642725     -0.0802353  
 -0.192084     0.380817   -0.366869    -0.573251   -0.462279   -0.280855     -0.367799     0.171788   -0.185041     0.596523    -0.359241    0.433718    0.203758    0.274363    -0.0221399    0.366257   -0.29484     0.477917     -0.00502605    0.365135    0.122236     0.0749594    0.0557869    0.0308934   -0.0203445     0.0163168  
 -0.546019     0.899332   -0.192813     0.411351   -0.200411    0.0578136    -0.0986602   -0.447069   -0.391785     0.0143049   -0.262596    0.319149   -0.246885    0.173212    -0.00690094  -0.0240092   0.12499    -0.172714      0.173071     -0.856597   -0.47854      0.576935     0.0244537    0.794084    -0.329361      0.0766541  
 -0.212812    -0.104283    0.0464138   -0.373222    0.182273    0.0774417     0.182665    -0.111616   -0.244525     0.22819     -0.221188   -0.430306   -0.0535514   0.597267     0.0128724    0.0188871   0.298692    0.59584      -0.540022     -0.589412   -0.353616     0.690965     0.169273     0.220323    -0.0955926    -0.593845   
 -0.480355    -0.181768    0.0400673    0.560283   -0.730398    0.0423208    -0.2498       0.214753   -0.519488     0.200566     0.320793    0.163958   -0.253322   -0.00968701   0.0515614    0.103986    0.380731   -0.162754      0.160278     -0.182024   -0.289177    -0.373395     0.65865      0.0745274    0.772503      0.483553   
  0.311105    -0.288892    0.557928     0.421567   -0.193463    0.206819      0.415395     0.0318363  -0.0264746   -0.208241     0.169641   -0.175665    0.222561    0.248506     0.0952997    0.208173    0.64775    -0.338239     -0.0116609    -0.301139    0.179907    -0.296903    -0.0961451    0.00987921   0.421964      0.0854056  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.413154
INFO: iteration 2, average log likelihood -1.413145
INFO: iteration 3, average log likelihood -1.413136
INFO: iteration 4, average log likelihood -1.413127
INFO: iteration 5, average log likelihood -1.413118
INFO: iteration 6, average log likelihood -1.413109
INFO: iteration 7, average log likelihood -1.413100
INFO: iteration 8, average log likelihood -1.413091
INFO: iteration 9, average log likelihood -1.413082
INFO: iteration 10, average log likelihood -1.413074
INFO: EM with 100000 data points 10 iterations avll -1.413074
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
