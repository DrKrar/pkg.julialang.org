>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.6.6
INFO: Installing JLD v0.6.4
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.0
INFO: Installing Rmath v0.1.3
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.1.1
INFO: Installing StaticArrays v0.0.8
INFO: Installing StatsBase v0.11.0
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.787
Commit c71f205 (2016-09-26 16:28 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-96-generic #143-Ubuntu SMP Mon Aug 29 20:15:20 UTC 2016 x86_64 x86_64
Memory: 2.9392929077148438 GB (685.765625 MB free)
Uptime: 27560.0 sec
Load Avg:  1.07568359375  1.0146484375  1.0078125
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3504 MHz    1600316 s         57 s     163910 s     730672 s         57 s
#2  3504 MHz     667707 s       8200 s      91525 s    1900146 s          1 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.2
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.6.6
 - JLD                           0.6.4
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.0
 - Rmath                         0.1.3
 - SHA                           0.2.1
 - ScikitLearnBase               0.1.1
 - StaticArrays                  0.0.8
 - StatsBase                     0.11.0
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:345
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:378
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:346
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1739
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-695796.7931874642,[91584.6,8415.36],
[4576.81 4178.55 -10618.5; -4393.31 -4292.12 10707.9],

Array{Float64,2}[
[94204.7 -916.521 7025.44; -916.521 88680.3 4281.82; 7025.44 4281.82 84517.0],

[5655.4 950.015 -6909.86; 950.015 11002.8 -4241.77; -6909.86 -4241.77 15766.3]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.527597e+03
      1       8.618970e+02      -6.657003e+02 |        4
      2       8.228134e+02      -3.908355e+01 |        2
      3       7.929165e+02      -2.989688e+01 |        2
      4       7.865888e+02      -6.327700e+00 |        0
      5       7.865888e+02       0.000000e+00 |        0
K-means converged with 5 iterations (objv = 786.588816895412)
INFO: K-means with 272 data points using 5 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.058735
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.798893
INFO: iteration 2, lowerbound -3.666454
INFO: iteration 3, lowerbound -3.515042
INFO: iteration 4, lowerbound -3.338638
INFO: iteration 5, lowerbound -3.162980
INFO: iteration 6, lowerbound -3.016237
INFO: dropping number of Gaussions to 7
INFO: iteration 7, lowerbound -2.911191
INFO: dropping number of Gaussions to 6
INFO: iteration 8, lowerbound -2.836118
INFO: dropping number of Gaussions to 5
INFO: iteration 9, lowerbound -2.790899
INFO: dropping number of Gaussions to 4
INFO: iteration 10, lowerbound -2.761103
INFO: dropping number of Gaussions to 3
INFO: iteration 11, lowerbound -2.733156
INFO: iteration 12, lowerbound -2.698177
INFO: iteration 13, lowerbound -2.652949
INFO: iteration 14, lowerbound -2.593521
INFO: iteration 15, lowerbound -2.525402
INFO: iteration 16, lowerbound -2.459327
INFO: iteration 17, lowerbound -2.404434
INFO: iteration 18, lowerbound -2.362760
INFO: iteration 19, lowerbound -2.332285
INFO: iteration 20, lowerbound -2.313046
INFO: iteration 21, lowerbound -2.307465
INFO: dropping number of Gaussions to 2
INFO: iteration 22, lowerbound -2.302927
INFO: iteration 23, lowerbound -2.299261
INFO: iteration 24, lowerbound -2.299256
INFO: iteration 25, lowerbound -2.299255
INFO: iteration 26, lowerbound -2.299254
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Fri 07 Oct 2016 12:09:37 PM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Fri 07 Oct 2016 12:09:39 PM UTC: K-means with 272 data points using 5 iterations
11.3 data points per parameter
,Fri 07 Oct 2016 12:09:40 PM UTC: EM with 272 data points 0 iterations avll -2.058735
5.8 data points per parameter
,Fri 07 Oct 2016 12:09:41 PM UTC: GMM converted to Variational GMM
,Fri 07 Oct 2016 12:09:42 PM UTC: iteration 1, lowerbound -3.798893
,Fri 07 Oct 2016 12:09:42 PM UTC: iteration 2, lowerbound -3.666454
,Fri 07 Oct 2016 12:09:42 PM UTC: iteration 3, lowerbound -3.515042
,Fri 07 Oct 2016 12:09:43 PM UTC: iteration 4, lowerbound -3.338638
,Fri 07 Oct 2016 12:09:43 PM UTC: iteration 5, lowerbound -3.162980
,Fri 07 Oct 2016 12:09:43 PM UTC: iteration 6, lowerbound -3.016237
,Fri 07 Oct 2016 12:09:43 PM UTC: dropping number of Gaussions to 7
,Fri 07 Oct 2016 12:09:43 PM UTC: iteration 7, lowerbound -2.911191
,Fri 07 Oct 2016 12:09:43 PM UTC: dropping number of Gaussions to 6
,Fri 07 Oct 2016 12:09:43 PM UTC: iteration 8, lowerbound -2.836118
,Fri 07 Oct 2016 12:09:43 PM UTC: dropping number of Gaussions to 5
,Fri 07 Oct 2016 12:09:43 PM UTC: iteration 9, lowerbound -2.790899
,Fri 07 Oct 2016 12:09:43 PM UTC: dropping number of Gaussions to 4
,Fri 07 Oct 2016 12:09:43 PM UTC: iteration 10, lowerbound -2.761103
,Fri 07 Oct 2016 12:09:43 PM UTC: dropping number of Gaussions to 3
,Fri 07 Oct 2016 12:09:43 PM UTC: iteration 11, lowerbound -2.733156
,Fri 07 Oct 2016 12:09:43 PM UTC: iteration 12, lowerbound -2.698177
,Fri 07 Oct 2016 12:09:43 PM UTC: iteration 13, lowerbound -2.652949
,Fri 07 Oct 2016 12:09:43 PM UTC: iteration 14, lowerbound -2.593521
,Fri 07 Oct 2016 12:09:43 PM UTC: iteration 15, lowerbound -2.525402
,Fri 07 Oct 2016 12:09:43 PM UTC: iteration 16, lowerbound -2.459327
,Fri 07 Oct 2016 12:09:43 PM UTC: iteration 17, lowerbound -2.404434
,Fri 07 Oct 2016 12:09:43 PM UTC: iteration 18, lowerbound -2.362760
,Fri 07 Oct 2016 12:09:44 PM UTC: iteration 19, lowerbound -2.332285
,Fri 07 Oct 2016 12:09:44 PM UTC: iteration 20, lowerbound -2.313046
,Fri 07 Oct 2016 12:09:44 PM UTC: iteration 21, lowerbound -2.307465
,Fri 07 Oct 2016 12:09:44 PM UTC: dropping number of Gaussions to 2
,Fri 07 Oct 2016 12:09:44 PM UTC: iteration 22, lowerbound -2.302927
,Fri 07 Oct 2016 12:09:44 PM UTC: iteration 23, lowerbound -2.299261
,Fri 07 Oct 2016 12:09:44 PM UTC: iteration 24, lowerbound -2.299256
,Fri 07 Oct 2016 12:09:44 PM UTC: iteration 25, lowerbound -2.299255
,Fri 07 Oct 2016 12:09:44 PM UTC: iteration 26, lowerbound -2.299254
,Fri 07 Oct 2016 12:09:44 PM UTC: iteration 27, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:44 PM UTC: iteration 28, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:44 PM UTC: iteration 29, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:44 PM UTC: iteration 30, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:44 PM UTC: iteration 31, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:44 PM UTC: iteration 32, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:44 PM UTC: iteration 33, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:44 PM UTC: iteration 34, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:44 PM UTC: iteration 35, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:44 PM UTC: iteration 36, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:44 PM UTC: iteration 37, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:44 PM UTC: iteration 38, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:44 PM UTC: iteration 39, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:45 PM UTC: iteration 40, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:45 PM UTC: iteration 41, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:45 PM UTC: iteration 42, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:45 PM UTC: iteration 43, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:45 PM UTC: iteration 44, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:45 PM UTC: iteration 45, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:45 PM UTC: iteration 46, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:45 PM UTC: iteration 47, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:45 PM UTC: iteration 48, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:45 PM UTC: iteration 49, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:45 PM UTC: iteration 50, lowerbound -2.299253
,Fri 07 Oct 2016 12:09:45 PM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.9549,178.045]
β = [95.9549,178.045]
m = [2.00023 53.852; 4.2503 79.2869]
ν = [97.9549,180.045]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.375876 -0.00895312; 0.0 0.0127487],

[0.184042 -0.00764405; 0.0 0.00858171]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 99999.99999999997
avll from stats: -0.9803279926937764
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.980327992693746
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.980327992693746
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.00000000001
avll from stats: -1.0038330430228695
avll from llpg:  -1.0038330430228695
avll direct:     -1.0038330430228695
sum posterior: 100000.0
32×26 Array{Float64,2}:
 -0.0778328    0.103627     0.0244062    -0.023948    0.101731   -0.102906    -0.146519    -0.0537008    0.0480293    0.00975535    0.0346812   0.0148893   -0.157072      0.0909572    0.0319866   -0.070318    -0.0271602   -0.00176268   0.131129     0.0409934   -0.0104581    0.101912    -0.0702758    0.152333     0.217686    -0.0146373 
 -0.100964    -0.141869     0.000670213  -0.103156   -0.133442    0.0171607   -0.0023445    0.149187    -0.0977946   -0.016081     -0.102364   -0.0232444    0.0359244     0.108919    -0.0271382    0.0572758   -0.00604817   0.0292      -0.27078     -0.156551     0.0277496    0.0472204    0.028886     0.0990434   -0.14714      0.105098  
  0.075132    -0.0241781    0.128952      0.0404504   0.0517139   0.0551999    0.0162184    0.0105787   -0.101085    -0.027809     -0.107361   -0.0769749   -0.084262      0.0241525   -0.0116814    0.140599    -0.141718    -0.0220409   -0.00934248   0.151049     0.0453173   -0.119485    -0.176491    -0.0305266    0.0894854   -0.0847372 
 -0.355551     0.0234675    0.161219     -0.0435542  -0.0306494  -0.0449262    0.204622    -0.121855     0.112526    -0.116016      0.0732413  -0.0663362    0.0748762    -0.0166328    0.0693428   -0.064445    -0.0120035   -0.0386986    0.0335004    0.0206007   -0.127536     0.0149167    0.0501551   -0.0374687    0.103379     0.0268788 
  0.0957483    0.0260014   -0.0573598     0.093622    0.0231104   0.081336    -0.134991    -0.185132    -0.170134    -0.0709692    -0.102041   -0.219991    -0.0707512    -0.0728848    0.0220616   -0.0558013    0.127272    -0.174722     0.00676791   0.0563019    0.0927435   -0.165481    -0.0593093    0.0993466   -0.00694156  -0.0252337 
  0.101998    -0.156192     0.0641407     0.156873    0.10612     0.146319     0.225305     0.0220629    0.102069    -0.0167205    -0.0850712   0.0144229   -0.056354      0.0256156    0.107973     0.0309253   -0.0511465    0.0852474   -0.15179      0.125732    -0.163881    -0.0206758    0.00508587  -0.0318129   -0.00267425  -0.0544077 
  0.0913051   -0.0676688    0.0438913    -0.014279    0.0327058  -0.169234    -0.12924     -0.0486208   -0.0465583    0.10903      -0.0841313  -0.0773954   -0.0529872     0.0804262    0.0992468   -0.0747381    0.162716    -0.0138847   -0.0997816    0.00212953  -0.0405265   -0.0769529   -0.0683816   -0.00888748   0.207499     0.16134   
  0.130925    -0.0391557   -0.0194116    -0.098589    0.0182162  -0.0527793   -0.0708345   -0.00518938  -0.0847069   -0.0261935     0.179817   -0.185294    -0.000965648  -0.0940606    0.0131663    0.14952     -0.0315615    0.151935     0.0418244   -0.0231228   -0.0772952    0.0600459   -0.185067     0.222043    -0.0430079   -0.0171117 
 -0.0047466   -0.228251     0.100548      0.0927015   0.174824   -0.0957798    0.157267     0.0706982    0.123714     0.0205438    -0.137873   -0.0816066    0.0540593     0.00375749   0.0276265   -0.103275    -0.19766     -0.0436221    0.134781     0.0570825   -0.130311    -0.0658253   -0.0179522    0.0156948   -0.101433    -0.191948  
  0.0828739   -0.0196404   -0.0141317     0.0670665   0.096303    0.0485704    0.160351    -0.0115076   -0.0773964   -0.0111613    -0.0255823   0.00241417   0.0251319    -0.12255     -0.0528132    0.0799298   -0.0986147   -0.0648366    0.0165011   -0.0846608   -0.0904064   -0.149008    -0.015775     0.163394    -0.00101858  -0.0121003 
  0.0394331    0.07171      0.0152279     0.0148957  -0.0857957   0.038152    -0.0632325    0.242996     0.0769176   -0.178328     -0.105014    0.0908242    0.227668     -0.0126311    0.0072015   -0.114908     0.0266417   -0.027351    -0.0157518   -0.0178692    0.0863327    0.0599818   -0.0870722   -0.151179     0.0940802   -0.0945917 
  0.022351    -0.0249047   -0.179573     -0.0751382   0.0460221  -0.00822501   0.0898547    0.0256931    0.0391877   -0.0240539    -0.0206917  -0.06637     -0.0368163    -0.0638728   -0.0129359    0.238709     0.14581     -0.0471647    0.0685999    0.02838      0.132471    -0.116455    -0.00532143  -0.0195592    0.0196962    0.0545111 
  0.0756699    0.0134374   -0.187964      0.0466357   0.0593081   0.0111329    0.0472549   -0.0104991    0.105575    -0.133573     -0.0504065  -0.0259873   -0.0605933     0.0630074   -0.00232036  -0.0711991   -0.0407641   -0.00165753  -0.116299    -0.0185685   -0.0157245   -0.163775    -0.154063     0.0148735   -0.118966     0.0745511 
 -0.0426236    0.0223078    0.0305977     0.0983208  -0.103449   -0.0737712    0.0533195   -0.00358537  -0.124817    -0.00385381   -0.135145    0.130816    -0.028398     -0.246647    -0.0676368    0.125872     0.177343    -0.100948    -0.0780908    0.196171    -0.24189      0.122557    -0.0257235   -0.0556742    0.138454     0.0773977 
  0.0774737    0.0881254    0.0215        0.0422397  -0.18437     0.115674     0.0761759    0.0034255    0.0142463    0.0920176     0.0490278   0.0490115    0.0212826    -0.0642804   -0.0875795   -0.031827     0.00797652   0.124518     0.07529     -0.16367     -0.0782478    0.0069067    0.0207631   -0.0539642   -0.236451    -0.0799141 
  0.156734    -0.0724297   -0.00408238   -0.114269   -0.0997794   0.00695297   0.0118634   -0.15492      0.00428593   0.0887016     0.0828169  -0.00668794  -0.0946475    -0.0476782    0.0424493   -0.198647     0.0478005   -0.0222588   -0.131955     0.0638899   -0.00390333  -0.119585     0.0165124    0.161624    -0.12667      0.0368942 
 -0.0152009    0.010632    -0.0340852    -0.0525402  -0.0998368   0.0150691    0.0436335   -0.165337     0.049968    -0.00103536    0.0401498   0.0694549   -0.00469693    0.265138     0.0178833    0.0326797    0.0870732    0.00212135   0.00534774   0.0548285    0.0438365   -0.143588     0.0681351   -0.0862896    0.0276996    0.157314  
 -0.0131739    0.0240924    0.0250482     0.0123445   0.0960058  -0.0210791   -0.0626463   -0.0368022   -0.220426     0.131926      0.0361035  -0.0340516    0.0795629     0.0475344    0.117859     0.0671501    0.00410393  -0.0618222   -0.144126     0.217401     0.0595866    0.265248    -0.0995826   -0.0179501   -0.161037     0.0414884 
  0.0511045    0.0216627   -0.0156522     0.0584161   0.0812355  -0.0242315   -0.0721121   -0.00380086  -0.0122287   -0.0807183    -0.118637   -0.129492     0.0973187     0.053605    -0.0681174   -0.0848748    0.146974    -0.0595539    0.0424431   -0.145763     0.102097    -0.0207605   -0.10623      0.128398    -0.125021    -0.177179  
 -0.027176    -0.143488     0.111678      0.118779   -0.0875838  -0.00656323  -0.00792422  -0.00039334   0.0128793    0.0512892    -0.109734    0.101373     0.235866      0.0850889    0.0140994   -0.03984      0.0936359    0.157461    -0.0710742   -0.152071    -0.0517513   -0.0247859   -0.0221932    0.114289    -0.0855643   -0.109287  
  0.131035    -0.0495429   -0.0925934     0.0137104   0.0417148  -0.0121352    0.225966    -0.0357627   -0.112175    -0.0316051    -0.0277464  -0.0511549   -0.175208     -0.0488188   -0.0112333    0.0422026    0.0718222    0.0304761    0.00831448   0.0192376    0.0549397    0.0025952   -0.00348291   0.113877    -0.15661     -0.158612  
 -0.106533     0.110688    -0.0243233     0.0580307  -0.119526    0.0153688   -0.097448    -0.143169    -0.0867818    0.0420532     0.0087132  -0.0319076   -0.0175979    -0.124068     0.0555845    0.0516098    0.0569494    0.230917     0.0668825    0.00222427  -0.00446593  -0.0273356   -0.102146     0.113233     0.018393    -0.10732   
  0.0378656    0.200647     0.0118641    -0.0900134  -0.289391   -0.15526     -0.17455      0.0861786    0.147997    -0.128445      0.0695091  -0.0348915    0.11491       0.076556     0.0486306    0.0365574    0.0927718    0.0165049   -0.0236351    0.113833    -0.0177735   -0.137435     0.0592084   -0.104643    -0.0977238   -0.22634   
  0.00284152  -0.0308181   -0.0996031    -0.0162904   0.12586     0.0638839   -0.143295    -0.100972    -0.054442    -0.0606201    -0.0877895  -0.0561095   -0.0748035    -0.00659691   0.15597     -0.0710733   -0.0792903    0.00660481  -0.134116    -0.0851216    0.0396317   -0.21248      0.0218317   -0.0193233   -0.0141862   -0.260088  
  0.0913231    0.0478479    0.0597516    -0.0156036  -0.0968138   0.0107897    0.0515761    0.0668406   -0.00210951   0.0315795    -0.0322791   0.0182542   -0.00515261    0.259438    -0.00197945  -0.0972232   -0.110975    -0.20581     -0.139781     0.0228567    0.136151    -0.00770099  -0.0246757    0.0857101    0.110757    -0.141027  
 -7.74587e-7  -0.0290444   -0.0234048    -0.027003    0.0328242   0.0465818    0.0719734    0.145303    -0.073538     0.100339      0.0150372  -0.0597934   -0.0185999     0.109172    -0.0220996    0.00242321   0.022464     0.0475449    0.0349808   -0.142175     0.161612     0.156834    -0.218739    -0.108702     0.114552    -0.0585634 
 -0.0682666   -0.141304     0.188217     -0.106562   -0.129673   -0.0349444   -0.0466328   -0.0368939   -0.0902265    0.0975869    -0.0249448   0.112971    -0.176223     -0.128596     0.0946465   -0.0674384   -0.0914141    0.0687116    0.0704995   -0.0950297   -0.00270445   0.0608593   -0.102568    -0.00189055   0.0345515    0.00160797
  0.217127     0.00259727  -0.144094     -0.190034   -0.0527991   0.258032    -0.0191383    0.197293     0.0631875    0.000601324  -0.0407442   0.052993    -0.0965569     0.0112149   -0.0505962   -0.0803743   -0.0186554   -0.183831     0.0121182    0.10292      0.0507725    0.0755181    0.094396     0.00663501   0.00172681   0.232466  
  0.0953629    0.0712904   -8.96005e-5   -0.144936   -0.0786302  -0.10108     -0.0661335    0.165002    -0.0392147   -0.0578104     0.0747716  -0.0506896    0.116582     -0.0162295   -0.126272     0.0263679   -0.19229     -0.00638133   0.00648017   0.0676792    0.0408128   -0.0307578    0.0684094    0.0614914   -0.0664417    0.0421288 
  0.0666643    0.134332    -0.0769762     0.0454852  -0.042483   -0.0371102    0.0260692   -0.0952652   -0.0947121    0.100481      0.102699   -0.0256405    0.0258656    -0.0684917   -0.182759    -0.0126086   -0.0764602    0.156968    -0.0795927    0.0157855    0.104401    -0.0363255   -0.0251188    0.1448      -0.0989394   -0.0253061 
 -0.0118895    0.193823    -0.116446     -0.0702319  -0.0419253  -0.124451     0.0724793    0.112838    -0.0188083   -0.3105        0.108422    0.0506137    0.0607       -0.0912652    0.144397     0.152597     0.0250157    0.0575896    0.109518     0.166166    -0.0084877   -0.0935118   -0.0335261   -0.0292948    0.0662489   -0.0224102 
  0.0998292    0.0713728   -0.0607384     0.0516444   0.121805   -0.068851     0.087062    -0.0442818    0.172496    -0.168996     -0.115382   -0.140179     0.101314     -0.0084262   -0.0118619   -0.0504689    0.0139552    0.100734    -0.0146749   -0.0673622   -0.0129197    0.00071332  -0.059458    -0.0505749   -0.0234263    0.188113  kind diag, method split
0: avll = -1.4594335813600356
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.459520
INFO: iteration 2, average log likelihood -1.459441
INFO: iteration 3, average log likelihood -1.458837
INFO: iteration 4, average log likelihood -1.451294
INFO: iteration 5, average log likelihood -1.429612
INFO: iteration 6, average log likelihood -1.421006
INFO: iteration 7, average log likelihood -1.419092
INFO: iteration 8, average log likelihood -1.418470
INFO: iteration 9, average log likelihood -1.418241
INFO: iteration 10, average log likelihood -1.418127
INFO: iteration 11, average log likelihood -1.418056
INFO: iteration 12, average log likelihood -1.418007
INFO: iteration 13, average log likelihood -1.417972
INFO: iteration 14, average log likelihood -1.417946
INFO: iteration 15, average log likelihood -1.417928
INFO: iteration 16, average log likelihood -1.417915
INFO: iteration 17, average log likelihood -1.417905
INFO: iteration 18, average log likelihood -1.417898
INFO: iteration 19, average log likelihood -1.417893
INFO: iteration 20, average log likelihood -1.417888
INFO: iteration 21, average log likelihood -1.417885
INFO: iteration 22, average log likelihood -1.417883
INFO: iteration 23, average log likelihood -1.417881
INFO: iteration 24, average log likelihood -1.417879
INFO: iteration 25, average log likelihood -1.417878
INFO: iteration 26, average log likelihood -1.417877
INFO: iteration 27, average log likelihood -1.417876
INFO: iteration 28, average log likelihood -1.417876
INFO: iteration 29, average log likelihood -1.417875
INFO: iteration 30, average log likelihood -1.417875
INFO: iteration 31, average log likelihood -1.417874
INFO: iteration 32, average log likelihood -1.417874
INFO: iteration 33, average log likelihood -1.417874
INFO: iteration 34, average log likelihood -1.417874
INFO: iteration 35, average log likelihood -1.417874
INFO: iteration 36, average log likelihood -1.417874
INFO: iteration 37, average log likelihood -1.417873
INFO: iteration 38, average log likelihood -1.417873
INFO: iteration 39, average log likelihood -1.417873
INFO: iteration 40, average log likelihood -1.417873
INFO: iteration 41, average log likelihood -1.417873
INFO: iteration 42, average log likelihood -1.417873
INFO: iteration 43, average log likelihood -1.417873
INFO: iteration 44, average log likelihood -1.417873
INFO: iteration 45, average log likelihood -1.417873
INFO: iteration 46, average log likelihood -1.417873
INFO: iteration 47, average log likelihood -1.417873
INFO: iteration 48, average log likelihood -1.417873
INFO: iteration 49, average log likelihood -1.417873
INFO: iteration 50, average log likelihood -1.417873
INFO: EM with 100000 data points 50 iterations avll -1.417873
952.4 data points per parameter
1: avll = [-1.45952,-1.45944,-1.45884,-1.45129,-1.42961,-1.42101,-1.41909,-1.41847,-1.41824,-1.41813,-1.41806,-1.41801,-1.41797,-1.41795,-1.41793,-1.41792,-1.41791,-1.4179,-1.41789,-1.41789,-1.41789,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.418051
INFO: iteration 2, average log likelihood -1.417902
INFO: iteration 3, average log likelihood -1.417295
INFO: iteration 4, average log likelihood -1.410674
INFO: iteration 5, average log likelihood -1.392363
INFO: iteration 6, average log likelihood -1.381751
INFO: iteration 7, average log likelihood -1.377810
INFO: iteration 8, average log likelihood -1.375793
INFO: iteration 9, average log likelihood -1.374522
INFO: iteration 10, average log likelihood -1.373441
INFO: iteration 11, average log likelihood -1.372459
INFO: iteration 12, average log likelihood -1.371924
INFO: iteration 13, average log likelihood -1.371672
INFO: iteration 14, average log likelihood -1.371531
INFO: iteration 15, average log likelihood -1.371430
INFO: iteration 16, average log likelihood -1.371345
INFO: iteration 17, average log likelihood -1.371263
INFO: iteration 18, average log likelihood -1.371180
INFO: iteration 19, average log likelihood -1.371093
INFO: iteration 20, average log likelihood -1.370998
INFO: iteration 21, average log likelihood -1.370890
INFO: iteration 22, average log likelihood -1.370762
INFO: iteration 23, average log likelihood -1.370608
INFO: iteration 24, average log likelihood -1.370424
INFO: iteration 25, average log likelihood -1.370215
INFO: iteration 26, average log likelihood -1.369986
INFO: iteration 27, average log likelihood -1.369730
INFO: iteration 28, average log likelihood -1.369442
INFO: iteration 29, average log likelihood -1.369106
INFO: iteration 30, average log likelihood -1.368739
INFO: iteration 31, average log likelihood -1.368354
INFO: iteration 32, average log likelihood -1.367953
INFO: iteration 33, average log likelihood -1.367520
INFO: iteration 34, average log likelihood -1.367073
INFO: iteration 35, average log likelihood -1.366659
INFO: iteration 36, average log likelihood -1.366312
INFO: iteration 37, average log likelihood -1.366048
INFO: iteration 38, average log likelihood -1.365855
INFO: iteration 39, average log likelihood -1.365711
INFO: iteration 40, average log likelihood -1.365597
INFO: iteration 41, average log likelihood -1.365502
INFO: iteration 42, average log likelihood -1.365420
INFO: iteration 43, average log likelihood -1.365347
INFO: iteration 44, average log likelihood -1.365278
INFO: iteration 45, average log likelihood -1.365209
INFO: iteration 46, average log likelihood -1.365135
INFO: iteration 47, average log likelihood -1.365053
INFO: iteration 48, average log likelihood -1.364957
INFO: iteration 49, average log likelihood -1.364844
INFO: iteration 50, average log likelihood -1.364707
INFO: EM with 100000 data points 50 iterations avll -1.364707
473.9 data points per parameter
2: avll = [-1.41805,-1.4179,-1.4173,-1.41067,-1.39236,-1.38175,-1.37781,-1.37579,-1.37452,-1.37344,-1.37246,-1.37192,-1.37167,-1.37153,-1.37143,-1.37134,-1.37126,-1.37118,-1.37109,-1.371,-1.37089,-1.37076,-1.37061,-1.37042,-1.37022,-1.36999,-1.36973,-1.36944,-1.36911,-1.36874,-1.36835,-1.36795,-1.36752,-1.36707,-1.36666,-1.36631,-1.36605,-1.36586,-1.36571,-1.3656,-1.3655,-1.36542,-1.36535,-1.36528,-1.36521,-1.36514,-1.36505,-1.36496,-1.36484,-1.36471]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.364772
INFO: iteration 2, average log likelihood -1.364368
INFO: iteration 3, average log likelihood -1.363011
INFO: iteration 4, average log likelihood -1.351226
INFO: iteration 5, average log likelihood -1.326758
INFO: iteration 6, average log likelihood -1.313522
INFO: iteration 7, average log likelihood -1.308576
INFO: iteration 8, average log likelihood -1.305676
INFO: iteration 9, average log likelihood -1.303541
INFO: iteration 10, average log likelihood -1.302029
INFO: iteration 11, average log likelihood -1.300938
INFO: iteration 12, average log likelihood -1.300044
INFO: iteration 13, average log likelihood -1.299300
INFO: iteration 14, average log likelihood -1.298761
INFO: iteration 15, average log likelihood -1.298405
INFO: iteration 16, average log likelihood -1.298165
INFO: iteration 17, average log likelihood -1.297994
INFO: iteration 18, average log likelihood -1.297859
INFO: iteration 19, average log likelihood -1.297746
INFO: iteration 20, average log likelihood -1.297642
INFO: iteration 21, average log likelihood -1.297540
INFO: iteration 22, average log likelihood -1.297437
INFO: iteration 23, average log likelihood -1.297329
INFO: iteration 24, average log likelihood -1.297211
INFO: iteration 25, average log likelihood -1.297075
INFO: iteration 26, average log likelihood -1.296913
INFO: iteration 27, average log likelihood -1.296710
INFO: iteration 28, average log likelihood -1.296448
INFO: iteration 29, average log likelihood -1.296128
INFO: iteration 30, average log likelihood -1.295838
INFO: iteration 31, average log likelihood -1.295612
INFO: iteration 32, average log likelihood -1.295436
INFO: iteration 33, average log likelihood -1.295286
INFO: iteration 34, average log likelihood -1.295141
INFO: iteration 35, average log likelihood -1.294998
INFO: iteration 36, average log likelihood -1.294854
INFO: iteration 37, average log likelihood -1.294719
INFO: iteration 38, average log likelihood -1.294608
INFO: iteration 39, average log likelihood -1.294515
INFO: iteration 40, average log likelihood -1.294428
INFO: iteration 41, average log likelihood -1.294331
INFO: iteration 42, average log likelihood -1.294206
INFO: iteration 43, average log likelihood -1.294023
INFO: iteration 44, average log likelihood -1.293742
INFO: iteration 45, average log likelihood -1.293312
INFO: iteration 46, average log likelihood -1.292717
INFO: iteration 47, average log likelihood -1.292062
INFO: iteration 48, average log likelihood -1.291633
INFO: iteration 49, average log likelihood -1.291503
INFO: iteration 50, average log likelihood -1.291482
INFO: EM with 100000 data points 50 iterations avll -1.291482
236.4 data points per parameter
3: avll = [-1.36477,-1.36437,-1.36301,-1.35123,-1.32676,-1.31352,-1.30858,-1.30568,-1.30354,-1.30203,-1.30094,-1.30004,-1.2993,-1.29876,-1.2984,-1.29817,-1.29799,-1.29786,-1.29775,-1.29764,-1.29754,-1.29744,-1.29733,-1.29721,-1.29708,-1.29691,-1.29671,-1.29645,-1.29613,-1.29584,-1.29561,-1.29544,-1.29529,-1.29514,-1.295,-1.29485,-1.29472,-1.29461,-1.29452,-1.29443,-1.29433,-1.29421,-1.29402,-1.29374,-1.29331,-1.29272,-1.29206,-1.29163,-1.2915,-1.29148]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.291787
INFO: iteration 2, average log likelihood -1.291490
INFO: iteration 3, average log likelihood -1.290637
INFO: iteration 4, average log likelihood -1.280776
INFO: iteration 5, average log likelihood -1.245061
WARNING: Variances had to be floored 3 13 14
INFO: iteration 6, average log likelihood -1.213156
INFO: iteration 7, average log likelihood -1.234131
WARNING: Variances had to be floored 3 6
INFO: iteration 8, average log likelihood -1.220157
INFO: iteration 9, average log likelihood -1.221349
WARNING: Variances had to be floored 3 13 14
INFO: iteration 10, average log likelihood -1.199778
INFO: iteration 11, average log likelihood -1.227209
WARNING: Variances had to be floored 3
INFO: iteration 12, average log likelihood -1.213935
WARNING: Variances had to be floored 6
INFO: iteration 13, average log likelihood -1.204378
WARNING: Variances had to be floored 3 12 13 14
INFO: iteration 14, average log likelihood -1.196023
INFO: iteration 15, average log likelihood -1.233122
WARNING: Variances had to be floored 3
INFO: iteration 16, average log likelihood -1.215877
INFO: iteration 17, average log likelihood -1.205303
WARNING: Variances had to be floored 3 6 13 14
INFO: iteration 18, average log likelihood -1.186941
WARNING: Variances had to be floored 12
INFO: iteration 19, average log likelihood -1.225662
WARNING: Variances had to be floored 3
INFO: iteration 20, average log likelihood -1.222360
INFO: iteration 21, average log likelihood -1.209687
WARNING: Variances had to be floored 3 13 14
INFO: iteration 22, average log likelihood -1.190096
WARNING: Variances had to be floored 6
INFO: iteration 23, average log likelihood -1.218164
WARNING: Variances had to be floored 3 12
INFO: iteration 24, average log likelihood -1.215660
INFO: iteration 25, average log likelihood -1.216287
WARNING: Variances had to be floored 3 13 14
INFO: iteration 26, average log likelihood -1.194796
INFO: iteration 27, average log likelihood -1.221146
WARNING: Variances had to be floored 3 6
INFO: iteration 28, average log likelihood -1.208248
WARNING: Variances had to be floored 12
INFO: iteration 29, average log likelihood -1.209729
WARNING: Variances had to be floored 3 13 14
INFO: iteration 30, average log likelihood -1.201188
INFO: iteration 31, average log likelihood -1.225407
WARNING: Variances had to be floored 3
INFO: iteration 32, average log likelihood -1.211116
WARNING: Variances had to be floored 6
INFO: iteration 33, average log likelihood -1.202250
WARNING: Variances had to be floored 3 12 13 14
INFO: iteration 34, average log likelihood -1.194378
INFO: iteration 35, average log likelihood -1.232270
WARNING: Variances had to be floored 3
INFO: iteration 36, average log likelihood -1.215524
INFO: iteration 37, average log likelihood -1.205253
WARNING: Variances had to be floored 3 6 13 14
INFO: iteration 38, average log likelihood -1.186967
WARNING: Variances had to be floored 12
INFO: iteration 39, average log likelihood -1.225641
WARNING: Variances had to be floored 3
INFO: iteration 40, average log likelihood -1.222346
INFO: iteration 41, average log likelihood -1.209680
WARNING: Variances had to be floored 3 13 14
INFO: iteration 42, average log likelihood -1.190127
WARNING: Variances had to be floored 6
INFO: iteration 43, average log likelihood -1.218189
WARNING: Variances had to be floored 3 12
INFO: iteration 44, average log likelihood -1.215665
INFO: iteration 45, average log likelihood -1.216288
WARNING: Variances had to be floored 3 13 14
INFO: iteration 46, average log likelihood -1.194797
INFO: iteration 47, average log likelihood -1.221154
WARNING: Variances had to be floored 3 6
INFO: iteration 48, average log likelihood -1.208256
WARNING: Variances had to be floored 12
INFO: iteration 49, average log likelihood -1.209732
WARNING: Variances had to be floored 3 13 14
INFO: iteration 50, average log likelihood -1.201190
INFO: EM with 100000 data points 50 iterations avll -1.201190
118.1 data points per parameter
4: avll = [-1.29179,-1.29149,-1.29064,-1.28078,-1.24506,-1.21316,-1.23413,-1.22016,-1.22135,-1.19978,-1.22721,-1.21394,-1.20438,-1.19602,-1.23312,-1.21588,-1.2053,-1.18694,-1.22566,-1.22236,-1.20969,-1.1901,-1.21816,-1.21566,-1.21629,-1.1948,-1.22115,-1.20825,-1.20973,-1.20119,-1.22541,-1.21112,-1.20225,-1.19438,-1.23227,-1.21552,-1.20525,-1.18697,-1.22564,-1.22235,-1.20968,-1.19013,-1.21819,-1.21567,-1.21629,-1.1948,-1.22115,-1.20826,-1.20973,-1.20119]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.225772
WARNING: Variances had to be floored 5 6
INFO: iteration 2, average log likelihood -1.211101
WARNING: Variances had to be floored 11 12 27
INFO: iteration 3, average log likelihood -1.200649
WARNING: Variances had to be floored 5 6 17 18 23 24 25 26 28
INFO: iteration 4, average log likelihood -1.166296
WARNING: Variances had to be floored 11 12
INFO: iteration 5, average log likelihood -1.163646
WARNING: Variances had to be floored 5 6 17 18 29
INFO: iteration 6, average log likelihood -1.107326
WARNING: Variances had to be floored 2 11 12 22 23 24 25 26 28 31
INFO: iteration 7, average log likelihood -1.099554
WARNING: Variances had to be floored 5 6 17 18 20
INFO: iteration 8, average log likelihood -1.122522
WARNING: Variances had to be floored 11 12 29
INFO: iteration 9, average log likelihood -1.116108
WARNING: Variances had to be floored 5 6 17 18 22 23 24 25 26 28 31
INFO: iteration 10, average log likelihood -1.080400
WARNING: Variances had to be floored 2 11 12
INFO: iteration 11, average log likelihood -1.117240
WARNING: Variances had to be floored 3 5 6 17 18 23 24 29
INFO: iteration 12, average log likelihood -1.090865
WARNING: Variances had to be floored 11 12 20 22 25 26 28 31
INFO: iteration 13, average log likelihood -1.102882
WARNING: Variances had to be floored 5 6 17 18
INFO: iteration 14, average log likelihood -1.114566
WARNING: Variances had to be floored 2 11 12 23 24 29
INFO: iteration 15, average log likelihood -1.095241
WARNING: Variances had to be floored 5 6 17 18 22 25 26 28 31
INFO: iteration 16, average log likelihood -1.093812
WARNING: Variances had to be floored 3 11 12 23 24
INFO: iteration 17, average log likelihood -1.118728
WARNING: Variances had to be floored 5 6 17 18 20 29
INFO: iteration 18, average log likelihood -1.089421
WARNING: Variances had to be floored 2 11 12 22 23 24 25 26 28 31
INFO: iteration 19, average log likelihood -1.098022
WARNING: Variances had to be floored 5 6 17 18
INFO: iteration 20, average log likelihood -1.124625
WARNING: Variances had to be floored 11 12 23 24 29
INFO: iteration 21, average log likelihood -1.103593
WARNING: Variances had to be floored 3 5 6 17 18 22 25 26 28 31
INFO: iteration 22, average log likelihood -1.080134
WARNING: Variances had to be floored 2 11 12 20 23 24
INFO: iteration 23, average log likelihood -1.112876
WARNING: Variances had to be floored 5 6 17 18 29
INFO: iteration 24, average log likelihood -1.110345
WARNING: Variances had to be floored 11 12 22 23 24 25 26 28 31
INFO: iteration 25, average log likelihood -1.102162
WARNING: Variances had to be floored 5 6 17 18
INFO: iteration 26, average log likelihood -1.108973
WARNING: Variances had to be floored 2 3 11 12 23 24 29
INFO: iteration 27, average log likelihood -1.088966
WARNING: Variances had to be floored 5 6 17 18 20 22 25 26 28 31
INFO: iteration 28, average log likelihood -1.093933
WARNING: Variances had to be floored 11 12 23
INFO: iteration 29, average log likelihood -1.133646
WARNING: Variances had to be floored 5 6 17 18 24 29
INFO: iteration 30, average log likelihood -1.088297
WARNING: Variances had to be floored 2 11 12 22 25 26 28 31
INFO: iteration 31, average log likelihood -1.092657
WARNING: Variances had to be floored 3 5 6 17 18 24
INFO: iteration 32, average log likelihood -1.110225
WARNING: Variances had to be floored 11 12 20 23 29
INFO: iteration 33, average log likelihood -1.106906
WARNING: Variances had to be floored 5 6 17 18 22 25 26 28 31
INFO: iteration 34, average log likelihood -1.092908
WARNING: Variances had to be floored 2 11 12 24
INFO: iteration 35, average log likelihood -1.116085
WARNING: Variances had to be floored 5 6 17 18 29
INFO: iteration 36, average log likelihood -1.099310
WARNING: Variances had to be floored 3 11 12 22 23 24 25 26 28 31
INFO: iteration 37, average log likelihood -1.093193
WARNING: Variances had to be floored 5 6 17 18 20
INFO: iteration 38, average log likelihood -1.109190
WARNING: Variances had to be floored 2 11 12 29
INFO: iteration 39, average log likelihood -1.103774
WARNING: Variances had to be floored 5 6 17 18 22 23 24 25 26 28 31
INFO: iteration 40, average log likelihood -1.091544
WARNING: Variances had to be floored 11 12
INFO: iteration 41, average log likelihood -1.128080
WARNING: Variances had to be floored 3 5 6 17 18 24 29
INFO: iteration 42, average log likelihood -1.080734
WARNING: Variances had to be floored 2 11 12 20 22 23 25 26 28 31
INFO: iteration 43, average log likelihood -1.086944
WARNING: Variances had to be floored 5 6 17 18
INFO: iteration 44, average log likelihood -1.130140
WARNING: Variances had to be floored 11 12 23 29
INFO: iteration 45, average log likelihood -1.108777
WARNING: Variances had to be floored 5 6 17 18 22 25 26 28 31
INFO: iteration 46, average log likelihood -1.083257
WARNING: Variances had to be floored 2 3 11 12 24
INFO: iteration 47, average log likelihood -1.107989
WARNING: Variances had to be floored 5 6 17 18 20 23 29
INFO: iteration 48, average log likelihood -1.093835
WARNING: Variances had to be floored 11 12 22 25 26 28 31
INFO: iteration 49, average log likelihood -1.113679
WARNING: Variances had to be floored 5 6 17 18 23
INFO: iteration 50, average log likelihood -1.109270
INFO: EM with 100000 data points 50 iterations avll -1.109270
59.0 data points per parameter
5: avll = [-1.22577,-1.2111,-1.20065,-1.1663,-1.16365,-1.10733,-1.09955,-1.12252,-1.11611,-1.0804,-1.11724,-1.09086,-1.10288,-1.11457,-1.09524,-1.09381,-1.11873,-1.08942,-1.09802,-1.12462,-1.10359,-1.08013,-1.11288,-1.11035,-1.10216,-1.10897,-1.08897,-1.09393,-1.13365,-1.0883,-1.09266,-1.11022,-1.10691,-1.09291,-1.11608,-1.09931,-1.09319,-1.10919,-1.10377,-1.09154,-1.12808,-1.08073,-1.08694,-1.13014,-1.10878,-1.08326,-1.10799,-1.09384,-1.11368,-1.10927]
[-1.45943,-1.45952,-1.45944,-1.45884,-1.45129,-1.42961,-1.42101,-1.41909,-1.41847,-1.41824,-1.41813,-1.41806,-1.41801,-1.41797,-1.41795,-1.41793,-1.41792,-1.41791,-1.4179,-1.41789,-1.41789,-1.41789,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41805,-1.4179,-1.4173,-1.41067,-1.39236,-1.38175,-1.37781,-1.37579,-1.37452,-1.37344,-1.37246,-1.37192,-1.37167,-1.37153,-1.37143,-1.37134,-1.37126,-1.37118,-1.37109,-1.371,-1.37089,-1.37076,-1.37061,-1.37042,-1.37022,-1.36999,-1.36973,-1.36944,-1.36911,-1.36874,-1.36835,-1.36795,-1.36752,-1.36707,-1.36666,-1.36631,-1.36605,-1.36586,-1.36571,-1.3656,-1.3655,-1.36542,-1.36535,-1.36528,-1.36521,-1.36514,-1.36505,-1.36496,-1.36484,-1.36471,-1.36477,-1.36437,-1.36301,-1.35123,-1.32676,-1.31352,-1.30858,-1.30568,-1.30354,-1.30203,-1.30094,-1.30004,-1.2993,-1.29876,-1.2984,-1.29817,-1.29799,-1.29786,-1.29775,-1.29764,-1.29754,-1.29744,-1.29733,-1.29721,-1.29708,-1.29691,-1.29671,-1.29645,-1.29613,-1.29584,-1.29561,-1.29544,-1.29529,-1.29514,-1.295,-1.29485,-1.29472,-1.29461,-1.29452,-1.29443,-1.29433,-1.29421,-1.29402,-1.29374,-1.29331,-1.29272,-1.29206,-1.29163,-1.2915,-1.29148,-1.29179,-1.29149,-1.29064,-1.28078,-1.24506,-1.21316,-1.23413,-1.22016,-1.22135,-1.19978,-1.22721,-1.21394,-1.20438,-1.19602,-1.23312,-1.21588,-1.2053,-1.18694,-1.22566,-1.22236,-1.20969,-1.1901,-1.21816,-1.21566,-1.21629,-1.1948,-1.22115,-1.20825,-1.20973,-1.20119,-1.22541,-1.21112,-1.20225,-1.19438,-1.23227,-1.21552,-1.20525,-1.18697,-1.22564,-1.22235,-1.20968,-1.19013,-1.21819,-1.21567,-1.21629,-1.1948,-1.22115,-1.20826,-1.20973,-1.20119,-1.22577,-1.2111,-1.20065,-1.1663,-1.16365,-1.10733,-1.09955,-1.12252,-1.11611,-1.0804,-1.11724,-1.09086,-1.10288,-1.11457,-1.09524,-1.09381,-1.11873,-1.08942,-1.09802,-1.12462,-1.10359,-1.08013,-1.11288,-1.11035,-1.10216,-1.10897,-1.08897,-1.09393,-1.13365,-1.0883,-1.09266,-1.11022,-1.10691,-1.09291,-1.11608,-1.09931,-1.09319,-1.10919,-1.10377,-1.09154,-1.12808,-1.08073,-1.08694,-1.13014,-1.10878,-1.08326,-1.10799,-1.09384,-1.11368,-1.10927]
32×26 Array{Float64,2}:
 -0.00373309   -0.0327041    -0.0247925   -0.0941348   -0.031872   -0.0524227    0.0210217    -0.0373834   -0.0728709   0.0645006    0.00687293   0.00590379  -0.0922226   -0.0737032    -0.0251406    0.0596706  -0.0130181    0.0753123     0.0560165   -0.0261682    0.07045     -0.0335238   -0.0438798     0.0334658   -0.000749724   0.0119016 
  0.217516      0.000554679  -0.140589    -0.175875    -0.128065    0.232158    -0.0209696     0.168637     0.0793992   0.0112945   -0.0651129    0.0536731   -0.0914876   -0.00596023   -0.0542076   -0.077285   -0.0029248   -0.195568      0.0147967    0.100096     0.0590493    0.0714936    0.101991      0.0631253    0.00540434    0.230377  
 -0.011581      0.0603903     0.0406773    0.00668137  -0.114976    0.00769783  -0.0792166     0.210253     0.070873   -0.200125    -0.106567     0.0626341    0.201197    -0.0111648    -0.00145749  -0.100527    0.0150506   -0.0204107    -0.0149943   -0.049998     0.0844332    0.0712476   -0.0939628    -0.171608     0.0358999    -0.0995267 
  0.102585     -0.0175604    -0.0323329    0.0418651    0.0353015   0.0339681    0.0177456    -0.0713464   -0.107387   -0.0187535    0.00663962  -0.124842    -0.00340306  -0.100884     -0.01159      0.0689577  -0.00402468  -0.0458364     0.010477    -0.0228117   -0.0171156   -0.101534    -0.086576      0.161239    -0.0112229    -0.0262811 
  0.138234     -0.0464628    -0.181144     0.0116023    0.0370577   0.0167967    0.208661     -0.0407442   -0.237157   -0.0315824    0.0111232   -0.136631    -0.352847    -0.0230089    -0.0118306   -1.15306     0.0749586    0.0107179     0.00486998   0.0906409    0.0520342    0.0610874   -0.0106286     0.11652     -0.11892      -0.146954  
  0.112938     -0.063601     -0.0682447   -0.00694769   0.048142   -0.0153468    0.222132     -0.0266893    0.0224281  -0.0188266   -0.0356961   -0.0292319    0.0133179    0.00401283   -0.0116196    1.1298      0.0686052    0.0936395     0.0072412   -0.0962219    0.0518396    0.024175     0.000516591   0.113656    -0.183192     -0.184195  
 -0.0819575     0.285615     -0.864905    -0.121352    -0.0511472  -0.142595     0.0934845     0.101228    -0.0131018  -0.356137     0.0871203    0.0135809    0.0601149   -0.0405627     0.138558     0.102401   -0.0518154   -0.000879657   0.176123     0.140442    -0.0191551   -0.190251    -0.0328445    -0.0163804    0.0747954    -0.0134242 
  0.0620904     0.188729      0.752312    -0.0530125   -0.0560393  -0.104114     0.0625118     0.12747     -0.0286885  -0.193469     0.139878     0.0788704    0.0577689   -0.123792      0.149929     0.203783    0.116597     0.0756013     0.0159901    0.226727    -0.00744943  -0.015002    -0.0369127    -0.131694     0.0530982    -0.0267682 
 -0.0416426    -0.0404517    -4.12892e-5  -0.0241353    0.028536    0.0717392    0.0690074     0.165841    -0.0817453   0.0970796    0.0142267   -0.0542942   -0.0190082    0.115331     -0.0373778    0.0243984   0.0183901    0.0765849     0.0303757   -0.133223     0.160091     0.152674    -0.221794     -0.147795     0.118792     -0.0307851 
  0.117363      0.0718655    -0.082814     0.0521045    0.118301   -0.0832558    0.0805981    -0.0182559    0.180257   -0.149486    -0.115792    -0.136985     0.103327     0.00971049   -0.0370696   -0.0622379   0.0124231    0.101289     -0.0181699   -0.0239561   -0.00301779   0.00704019  -0.059568     -0.0544148   -0.0325962     0.195025  
  0.000940669  -0.28398       0.00282229   0.0919455    0.127064   -0.0867371    0.128839      0.0811962    0.127054   -0.0638619   -0.137177    -0.106659     0.0612086   -0.000109519   0.0446945   -0.125559   -0.193498     0.0203885    -0.630444     0.178109     0.00289011  -0.185672    -0.0484332     0.0141546   -0.126648     -0.189787  
 -0.0367447    -0.146931      0.164315     0.0921738    0.240575   -0.094827     0.169271      0.125306     0.0911239   0.14721     -0.135058    -0.0391058    0.078234     0.00633261    0.00765948  -0.0974287  -0.19709     -0.13394       0.869417    -0.0739509   -0.33709      0.0422535   -0.00694694    0.0195599   -0.0447272    -0.178026  
 -0.360811      0.0276729     0.166894    -0.0467322   -0.0257219  -0.0421557    0.208636     -0.119894     0.106306   -0.118556     0.0728447   -0.0571505    0.087826     0.0137632     0.0476348   -0.0309554  -0.00771402  -0.0380633     0.0442675    0.0161723   -0.114638     0.022023     0.0466165    -0.0597988    0.0838613     0.034321  
  0.0776693     0.0834925     0.0217246    0.0331479   -0.182078    0.154751     0.0839409     0.00595611   0.0224028   0.0936643    0.0488969    0.0568169    0.021204    -0.0601645    -0.0653891   -0.0348964   0.002707     0.111751      0.0637134   -0.164349    -0.0887597    0.047556     0.0148055    -0.0833906   -0.239759     -0.109289  
  0.0619041     0.198122     -0.0501494   -0.0680692   -0.28987    -0.150708    -0.571729      0.164574     0.16597    -0.179281     0.0783541   -0.0562141    0.149919     0.04944       0.0815136    0.061799    0.129054     0.0030801    -0.0986999    0.113382     0.209379     0.0664897    0.154625     -0.297672    -0.0991646    -0.209669  
  0.0152318     0.202182      0.0472294   -0.0827021   -0.29532    -0.0635807    0.191451      0.0697669    0.140311   -0.0732735    0.0650279   -0.0856737    0.0542178    0.0196507     0.0096781    0.0214857   0.047099     0.0211676     0.0740717    0.1281      -0.304835    -0.254105    -0.0264366    -0.0531099   -0.0959972    -0.238457  
  0.00945499   -0.13303       0.099489     0.114319    -0.0928412  -0.00665222  -0.029166     -0.00245268   0.0114951  -0.00283245  -0.124628     0.100386     0.197597     0.0682232    -0.0119491   -0.0554058   0.0784889    0.155513     -0.0841589   -0.157285    -0.0436902   -0.0173754   -0.0306846     0.114183    -0.0959967    -0.094169  
  0.0952507     0.00618794   -0.157612     0.0435582    0.0501032   0.00404241   0.0384026    -0.0150556    0.11213    -0.121862    -0.0470543   -0.0313119   -0.0484342    0.115856     -0.0195178   -0.016584   -0.0646502   -0.00144796   -0.105646    -0.00588875  -0.0128353   -0.164115    -0.176973      0.0153572   -0.119053      0.0820489 
  0.0078116     0.0803706     0.00926017  -0.0426097   -0.105779   -0.0391776   -0.0823408     0.0105559   -0.0729193  -0.013049     0.033746    -0.0390712    0.0570952   -0.0659007     0.0152567    0.0466231  -0.0457377    0.131859      0.0376344    0.0345559    0.0146201   -0.0336125   -0.0239028     0.0915444   -0.0395898    -0.048713  
 -0.0589581     0.0975716     0.0176704   -0.0284181    0.0970759  -0.0800125   -0.193654     -0.0401596    0.0401622   0.00851787   0.0644523   -0.00185545  -0.133826     0.0772239    -0.00585363  -0.0582062  -0.0349344    0.00656338    0.12647      0.018301    -0.00317994   0.0967244   -0.0617672     0.151462     0.185424      0.00149743
  0.0534769    -0.0172921    -0.0937293   -0.0156996    0.122929    0.0669749   -0.143576     -0.106377    -0.0302993  -0.0632016   -0.0845257   -0.044223    -0.0982848   -0.00558038    0.159828    -0.0692032  -0.0762525    0.0127719    -0.129136    -0.096592     0.0622766   -0.20549      0.0502273     0.00232071  -0.0234801    -0.254654  
  0.063106     -0.0204        0.141808     0.0413588    0.045939    0.05092      0.0166599     0.0528456   -0.116166   -0.0483351   -0.127895    -0.0760165   -0.0908634    0.0272035    -0.0123809    0.131456   -0.147143    -0.00152795   -0.00887857   0.142527     0.0789939   -0.116505    -0.162006     -0.00702836   0.0912033    -0.0733474 
  0.104786     -0.145903      0.0712307    0.155717     0.120239    0.143117     0.230722      0.039629     0.100369   -0.0110033   -0.0934528    0.0143546   -0.0463186    0.023032      0.112875     0.0353462  -0.051582     0.0812187    -0.147325     0.127196    -0.154114    -0.0202276   -0.00328227   -0.0461477   -0.0440008    -0.0533878 
  0.104578      0.0181685     0.0746959    0.121927     0.0209582   0.10155     -0.0237434     0.0127336   -0.0219625  -0.00159599  -0.0327463   -0.0075486    0.0508888    0.0169399     0.0283342    0.0270266  -0.0519292    0.0684355    -0.116889     0.1005      -0.0502306   -0.0359674   -0.0413896     0.103751     0.01824      -0.0354302 
  0.0503443     0.0246988    -0.0476285    0.306111     0.0808391   0.0259218   -0.0389923     0.128871     0.0265757  -0.0999638   -0.115802    -0.130904     0.0971842    0.0458872    -0.140826    -2.0867      0.123777    -0.192949      0.0992676   -0.0392679    0.146406    -0.04114     -0.235111      0.173909    -0.172116     -0.186899  
  0.0504109     0.0432394    -0.020495    -0.0302567    0.0859755  -0.0255682   -0.111015     -0.0682886   -0.0353341  -0.0299211   -0.121063    -0.137368     0.0964929    0.0528993    -0.0716039    1.2549      0.164788    -0.0186409     0.014423    -0.210815     0.192095    -0.00642227  -0.0437202     0.0873806   -0.168129     -0.210612  
 -0.0128374     0.0208111     0.0264892    0.0212217    0.0930637  -0.0122934   -0.0603031    -0.0598279   -0.211399    0.13659      0.0510174   -0.0358097    0.0730361    0.0462614     0.118015     0.0683379   0.00696762  -0.0619942    -0.131614     0.196958     0.0598085    0.265738    -0.0728491    -0.0145279   -0.165633      0.0656664 
  0.0891377    -0.0652898     0.0363183   -0.0589035    0.033902   -0.16526     -0.131504     -0.0473616   -0.12318     0.132865    -0.0913207   -0.0741219   -0.0451379    0.0860629     0.102098    -0.0687333   0.158351    -0.0158905    -0.0916559    0.00365089  -0.0486622   -0.0785583   -0.0610137    -0.0140914    0.191672      0.150464  
 -0.0955494    -0.131331     -0.0217623   -0.080471    -0.131612    0.0180389   -0.000974889   0.15754     -0.0615515  -0.0260329   -0.095321    -0.0197657    0.0324718    0.10907      -0.0258106    0.0565415  -0.00218224   0.0199459    -0.243385    -0.0904446   -0.0343056    0.0455368    0.00620394    0.0938649   -0.104583      0.0854724 
  0.0265709     0.044714      0.0303439    0.0331875   -0.0936472  -0.0296081    0.0527264     0.016737    -0.0586773   0.0237654   -0.0913942    0.108867    -0.0147996   -0.00242764   -0.0718595    0.0105651   0.0446084   -0.155436     -0.0972607    0.109437    -0.0451827    0.0640617   -0.00616968    0.00969176   0.128738     -0.0475448 
  0.155336     -0.0676101    -0.0234571   -0.109733    -0.100989    0.0110576    0.00847868   -0.171423     0.0017697   0.0425026    0.153766    -0.0138345   -0.0868869   -0.0776229     0.0340692   -0.195941   -0.0236468   -0.0374207    -0.118262     0.033672     0.0503413   -0.0821826    0.0100516     0.152473    -0.128118      0.030589  
 -0.0221258     0.00371544   -0.0268592   -0.0215692   -0.0994466   0.00823081   0.0245128    -0.141705     0.052718    0.0208076   -0.0132189    0.0704056   -0.010745     0.260711      0.012504     0.0333236   0.0945804    0.00141996    0.00576882   0.0671281    0.0140598   -0.129687     0.0667953    -0.0839563    0.0268031     0.2245    INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 2 11 12 29
INFO: iteration 1, average log likelihood -1.097842
WARNING: Variances had to be floored 2 3 5 6 11 12 17 18 22 23 25 26 28 29 31
INFO: iteration 2, average log likelihood -1.052926
WARNING: Variances had to be floored 2 11 12 20 24 29
INFO: iteration 3, average log likelihood -1.090146
WARNING: Variances had to be floored 2 3 5 6 11 12 17 18 22 23 25 26 28 29 31
INFO: iteration 4, average log likelihood -1.056609
WARNING: Variances had to be floored 2 11 12 20 29
INFO: iteration 5, average log likelihood -1.093406
WARNING: Variances had to be floored 2 3 5 6 11 12 17 18 22 23 25 26 28 29 31
INFO: iteration 6, average log likelihood -1.055562
WARNING: Variances had to be floored 2 11 12 20 24 29
INFO: iteration 7, average log likelihood -1.091891
WARNING: Variances had to be floored 2 3 5 6 11 12 17 18 22 23 25 26 28 29 31
INFO: iteration 8, average log likelihood -1.057387
WARNING: Variances had to be floored 2 11 12 20 29
INFO: iteration 9, average log likelihood -1.092660
WARNING: Variances had to be floored 2 3 5 6 11 12 17 18 22 24 25 26 28 29 31
INFO: iteration 10, average log likelihood -1.054917
INFO: EM with 100000 data points 10 iterations avll -1.054917
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.620131e+05
      1       7.500475e+05      -2.119656e+05 |       32
      2       7.189162e+05      -3.113125e+04 |       32
      3       7.031682e+05      -1.574800e+04 |       32
      4       6.915992e+05      -1.156900e+04 |       32
      5       6.818232e+05      -9.776068e+03 |       32
      6       6.750279e+05      -6.795273e+03 |       32
      7       6.712988e+05      -3.729153e+03 |       32
      8       6.694672e+05      -1.831578e+03 |       32
      9       6.680871e+05      -1.380046e+03 |       32
     10       6.663304e+05      -1.756730e+03 |       32
     11       6.637801e+05      -2.550317e+03 |       32
     12       6.610212e+05      -2.758872e+03 |       32
     13       6.594676e+05      -1.553630e+03 |       32
     14       6.586992e+05      -7.684198e+02 |       32
     15       6.581822e+05      -5.170090e+02 |       32
     16       6.575995e+05      -5.826757e+02 |       32
     17       6.568414e+05      -7.580792e+02 |       32
     18       6.562183e+05      -6.231404e+02 |       32
     19       6.559326e+05      -2.857093e+02 |       32
     20       6.557923e+05      -1.402036e+02 |       32
     21       6.557410e+05      -5.134164e+01 |       31
     22       6.557181e+05      -2.290353e+01 |       32
     23       6.557089e+05      -9.215457e+00 |       28
     24       6.557018e+05      -7.103726e+00 |       27
     25       6.556964e+05      -5.388494e+00 |       25
     26       6.556929e+05      -3.478432e+00 |       28
     27       6.556908e+05      -2.136708e+00 |       24
     28       6.556892e+05      -1.611155e+00 |       25
     29       6.556878e+05      -1.337705e+00 |       23
     30       6.556866e+05      -1.233843e+00 |       20
     31       6.556852e+05      -1.364847e+00 |       14
     32       6.556845e+05      -7.224900e-01 |       15
     33       6.556840e+05      -5.339445e-01 |       12
     34       6.556834e+05      -5.505445e-01 |       12
     35       6.556828e+05      -6.025801e-01 |       10
     36       6.556821e+05      -7.238973e-01 |       12
     37       6.556816e+05      -5.358821e-01 |        7
     38       6.556814e+05      -1.516539e-01 |        2
     39       6.556813e+05      -7.201983e-02 |        0
     40       6.556813e+05       0.000000e+00 |        0
K-means converged with 40 iterations (objv = 655681.3381239403)
INFO: K-means with 32000 data points using 40 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.356367
INFO: iteration 2, average log likelihood -1.323426
INFO: iteration 3, average log likelihood -1.288664
INFO: iteration 4, average log likelihood -1.245650
WARNING: Variances had to be floored 24
INFO: iteration 5, average log likelihood -1.186623
WARNING: Variances had to be floored 7 9 22
INFO: iteration 6, average log likelihood -1.130186
WARNING: Variances had to be floored 1 2 17
INFO: iteration 7, average log likelihood -1.114378
WARNING: Variances had to be floored 15 16 19 27
INFO: iteration 8, average log likelihood -1.124649
WARNING: Variances had to be floored 5 10 18 22 24 25
INFO: iteration 9, average log likelihood -1.112390
WARNING: Variances had to be floored 2 7
INFO: iteration 10, average log likelihood -1.122917
WARNING: Variances had to be floored 1 11 17
INFO: iteration 11, average log likelihood -1.104987
WARNING: Variances had to be floored 19 22 23
INFO: iteration 12, average log likelihood -1.115589
WARNING: Variances had to be floored 2 15 18 24 27
INFO: iteration 13, average log likelihood -1.093158
WARNING: Variances had to be floored 1 5 7
INFO: iteration 14, average log likelihood -1.118801
WARNING: Variances had to be floored 11 17 22
INFO: iteration 15, average log likelihood -1.105811
WARNING: Variances had to be floored 19 23
INFO: iteration 16, average log likelihood -1.099981
WARNING: Variances had to be floored 1 2 15 16 18 24
INFO: iteration 17, average log likelihood -1.051182
WARNING: Variances had to be floored 7 17 22 27
INFO: iteration 18, average log likelihood -1.123227
WARNING: Variances had to be floored 5
INFO: iteration 19, average log likelihood -1.119650
WARNING: Variances had to be floored 1 2 11 19 23
INFO: iteration 20, average log likelihood -1.055642
WARNING: Variances had to be floored 15 17 18 22 24
INFO: iteration 21, average log likelihood -1.099951
WARNING: Variances had to be floored 7 16
INFO: iteration 22, average log likelihood -1.123293
WARNING: Variances had to be floored 2 27
INFO: iteration 23, average log likelihood -1.088308
WARNING: Variances had to be floored 1 19 22 23 24
INFO: iteration 24, average log likelihood -1.066545
WARNING: Variances had to be floored 5 15 17 18
INFO: iteration 25, average log likelihood -1.093300
WARNING: Variances had to be floored 2 7 16 26
INFO: iteration 26, average log likelihood -1.102390
INFO: iteration 27, average log likelihood -1.115767
WARNING: Variances had to be floored 1 19 22 24
INFO: iteration 28, average log likelihood -1.050756
WARNING: Variances had to be floored 2 5 11 15 17 18 23
INFO: iteration 29, average log likelihood -1.063863
WARNING: Variances had to be floored 7 16 26 27
INFO: iteration 30, average log likelihood -1.111085
WARNING: Variances had to be floored 1 24
INFO: iteration 31, average log likelihood -1.120221
WARNING: Variances had to be floored 19 22 25
INFO: iteration 32, average log likelihood -1.101038
WARNING: Variances had to be floored 2 17
INFO: iteration 33, average log likelihood -1.079688
WARNING: Variances had to be floored 1 7 15 16 18 24 26
INFO: iteration 34, average log likelihood -1.051622
WARNING: Variances had to be floored 11 22 23
INFO: iteration 35, average log likelihood -1.115477
WARNING: Variances had to be floored 2 5 19 27
INFO: iteration 36, average log likelihood -1.096979
WARNING: Variances had to be floored 17 18 26
INFO: iteration 37, average log likelihood -1.083328
WARNING: Variances had to be floored 1 7 15 16 22 24 25
INFO: iteration 38, average log likelihood -1.065376
INFO: iteration 39, average log likelihood -1.137244
WARNING: Variances had to be floored 2 11 19
INFO: iteration 40, average log likelihood -1.067458
WARNING: Variances had to be floored 1 7 16 17 18 22 23 26 27
INFO: iteration 41, average log likelihood -1.049742
WARNING: Variances had to be floored 5 24
INFO: iteration 42, average log likelihood -1.150784
WARNING: Variances had to be floored 2 15
INFO: iteration 43, average log likelihood -1.106720
WARNING: Variances had to be floored 11 19 22
INFO: iteration 44, average log likelihood -1.081081
WARNING: Variances had to be floored 1 7 16 17 18 23 25
INFO: iteration 45, average log likelihood -1.056236
WARNING: Variances had to be floored 2 5 24 26
INFO: iteration 46, average log likelihood -1.107825
WARNING: Variances had to be floored 15 22 27
INFO: iteration 47, average log likelihood -1.105527
WARNING: Variances had to be floored 7 11 16 18 19
INFO: iteration 48, average log likelihood -1.083840
WARNING: Variances had to be floored 1 17 23
INFO: iteration 49, average log likelihood -1.109168
WARNING: Variances had to be floored 2 22 24
INFO: iteration 50, average log likelihood -1.108004
INFO: EM with 100000 data points 50 iterations avll -1.108004
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.103808    -0.0739175    0.0543811    -0.0399748    0.0401005   -0.139141    -0.113073    -0.0368672   -0.136048     0.101864     -0.0982954   -0.0596259  -0.0569258    0.0857857    0.146436    -0.0670652    0.124629     -0.00558171  -0.114499     0.0274711   -0.0602593    -0.0767668   -0.0625331   -0.0287413    0.226703     0.205051   
  0.219872     0.0014369   -0.145221     -0.177566    -0.127925     0.231934    -0.0238002    0.171883     0.0806095    0.000995147  -0.0626543    0.0538351  -0.0907424   -0.0218958   -0.0546739   -0.0795321   -0.00497983   -0.204963     0.0145515    0.0989781    0.0612133     0.0763834    0.108573     0.0643045    0.00538115   0.232447   
  0.0930306    0.0417675    0.0360278    -0.0153006   -0.0757752   -0.0129197    0.035738     0.0287817    0.00258176   0.0689475    -0.0531897    0.0342092  -0.00706474   0.229085    -0.0604299   -0.100303    -0.0511396    -0.182102    -0.133782     0.0253902    0.109577     -0.0102861   -0.0042993    0.0737342    0.120972    -0.134899   
  0.118908     0.0703838   -0.0825499     0.0519       0.120577    -0.0830709    0.0811774   -0.0197629    0.182951    -0.147663     -0.11581     -0.137209    0.103082     0.0109858   -0.0357671   -0.0626949    0.0123552     0.101698    -0.0239086   -0.0186311   -0.00605383    0.00644753  -0.0596705   -0.0536684   -0.0334431    0.19063    
 -0.0836024   -0.121987     0.169441     -0.086119    -0.0962502   -0.0621234   -0.0458263   -0.0361947   -0.100799     0.0817031    -0.040585     0.0965721  -0.163947    -0.0993605    0.0791972   -0.0610748   -0.0903871     0.0913955    0.0739449   -0.123822     0.000504437   0.0453906   -0.106714     0.0187693    0.0492582   -0.0176651  
 -0.0228738    0.00264381  -0.0250713    -0.0222385   -0.0994119    0.00827564   0.0245466   -0.141749     0.0523846    0.0207427    -0.0112728    0.0704947  -0.0113691    0.260217     0.0128774    0.032966     0.0965148     0.00129617   0.00558738   0.0674057    0.0146523    -0.129158     0.0618935   -0.0845554    0.026808     0.224711   
 -0.0686665    0.0942065    0.0201032    -0.00186021   0.12338     -0.0722826   -0.180981    -0.0384314    0.045855     0.00947923    0.045593     0.0150704  -0.157938     0.0718304   -0.0128426   -0.0726477   -0.0356962     0.00647395   0.108678     0.0138091   -0.013543      0.0858902   -0.0606064    0.146814     0.200322     0.000690902
 -0.0653635    0.0905045    0.00684813    0.0657687   -0.116726     0.00749375  -0.0891624   -0.123141    -0.0599843    0.0268468     0.0115117   -0.0314494   0.00277655  -0.097728     0.109448     0.036344     0.0652961     0.222449     0.0674637   -0.00822646  -0.005936     -0.0420543   -0.105182     0.114745    -0.0105715   -0.0948815  
  0.0414305   -0.0263089   -0.0728719    -0.00904889   0.128024     0.0673276   -0.118736    -0.0896789   -0.0185966   -0.0676845    -0.082319    -0.039762   -0.0956488    0.0110426    0.159614    -0.0626779   -0.0738151     0.0146702   -0.130103    -0.0890923    0.0557195    -0.193709     0.0338191    0.00552226  -0.013712    -0.228702   
  0.0952352    0.0611812   -0.0184528    -0.1458      -0.0813703   -0.0838975   -0.0727175    0.148992    -0.0577056   -0.0553403     0.0500004   -0.031194    0.111332     0.0161721   -0.11684      0.0399725   -0.179544     -0.00550141   0.00460073   0.0816108    0.0386998    -0.0260819    0.050569     0.0618273   -0.0671976    0.0354563  
 -0.0741383   -0.279234     0.125681      0.123596     0.0375397   -0.0496728    0.0898674    0.10855      0.0458294    0.0177261    -0.117423    -0.0569598   0.0362911    0.0285019    0.0019713   -0.0603503   -0.101725     -0.0667166   -0.00557333   0.0864593   -0.31019      -0.0232865   -0.00542343   0.0790672   -0.116214    -0.0872386  
 -0.0393596   -0.0428264    0.00589911   -0.0180397    0.0327417    0.0689326    0.0760544    0.166501    -0.067862     0.0977628     0.00873997  -0.0532064  -0.0186052    0.106929    -0.0321304    0.019705     0.0123171     0.0726446    0.0274424   -0.122678     0.158624      0.154874    -0.221222    -0.148137     0.112274    -0.0351434  
 -0.00674146   0.233956    -0.0272567    -0.090151    -0.0533717   -0.122455     0.0794145    0.114692    -0.0210834   -0.277437      0.114572     0.0469905   0.0598076   -0.0819474    0.144087     0.154891     0.0314822     0.0393463    0.0944746    0.187383    -0.0118076    -0.101656    -0.0349469   -0.0778087    0.0636316   -0.0198909  
  0.128382    -0.0394559   -0.0190265    -0.0379884    0.0181678   -0.0451966   -0.0744935   -0.00432524  -0.0778241   -0.0262552     0.157963    -0.175269    0.0091475   -0.0904649    0.0190371    0.168074    -0.0361982     0.146411     0.0430159   -0.0585073   -0.072698      0.0544535   -0.172446     0.22925     -0.0236921   -0.0411374  
  0.0824018   -0.023941    -0.0134159     0.0322456    0.0761542    0.0270307    0.2052      -0.0328613   -0.0744181    0.0230788    -0.033116     0.0133705   0.0289995   -0.114955    -0.0567918    0.0156897   -0.107763     -0.0665253    0.0238667   -0.0980479   -0.0775179    -0.177205    -0.0151303    0.161561     0.00531159  -0.0232801  
 -0.0940087   -0.158072    -0.0904018    -0.172973    -0.104724     0.0146612   -0.00505614   0.205979    -0.0570473   -0.0234134    -0.103717    -0.0300644   0.0122251    0.151963    -0.0295715    0.0460799   -0.0194662     0.0390785   -0.263029    -0.212235     0.0726952     0.0392182    0.0366121    0.0940314   -0.119719     0.0872091  
  0.0519989    0.0277705   -0.0353852     0.107272     0.076178    -0.00688918  -0.0837502    0.00459076   0.0115588   -0.0636781    -0.10393     -0.105494    0.0867073    0.0564217   -0.0794483   -0.138961     0.126441     -0.0797634    0.0296342   -0.110012     0.137282     -0.0397174   -0.173143     0.104555    -0.152816    -0.167885   
  0.0101817   -0.170175     0.104699      0.108841    -0.054335    -0.0126299    0.0342797    0.0223916    0.0298589   -0.000600944  -0.136726     0.0743513   0.214787     0.0916009   -0.0112697   -0.0784126    0.0337962     0.123664    -0.0919675   -0.140375    -0.0684771    -0.025254    -0.0262283    0.0953635   -0.129067    -0.105565   
  0.0515786    0.11008     -0.0979716     0.0493612   -0.00643712  -0.0486978    0.0532252   -0.0829869   -0.0890821    0.0977582     0.0686189   -0.0600986  -0.0217584   -0.0624713   -0.153223    -0.0155141   -0.083446      0.164576    -0.128262     0.0471052    0.111698     -0.0323825   -0.00462308   0.13341     -0.0975219   -0.0399888  
  0.0378068    0.200188     0.000313718  -0.0754746   -0.292655    -0.106173    -0.175896     0.115282     0.152548    -0.125223      0.0711321   -0.0709126   0.102746     0.0332152    0.0452568    0.0412994    0.0870523     0.013194    -0.0064128    0.120771    -0.0585709    -0.101034     0.0624516   -0.17123     -0.0975996   -0.224144   
  0.0773877    0.083289     0.0218584     0.0317921   -0.181835     0.15502      0.0839648    0.00448781   0.0230918    0.0941186     0.0486383    0.0543727   0.0217015   -0.0590065   -0.0651407   -0.034904     0.00297378    0.112091     0.0639509   -0.163934    -0.0900884     0.0470939    0.014799    -0.0845832   -0.239942    -0.109278   
  0.153612    -0.0713071   -0.0224742    -0.108109    -0.101781     0.00564205   0.0133022   -0.171482     0.0031082    0.0548358     0.136789    -0.0098894  -0.0942579   -0.0748765    0.0436531   -0.199439     0.000609952  -0.0280423   -0.121954     0.0466662    0.0452675    -0.0774906    0.0149318    0.154524    -0.134304     0.034237   
  0.125369    -0.0350864   -0.211285      0.0497271    0.113864    -0.0167729    0.0734581    0.00619062   0.181791    -0.119845     -0.0785748   -0.0581726  -0.0757123    0.103394    -0.00625598   0.0108743   -0.197719     -0.0243873   -0.102908     0.0162893   -0.00706037   -0.204762    -0.168102     0.00238002  -0.115351     0.0857259  
  0.122196    -0.0579798   -0.125122      0.00405826   0.0453066    0.00191372   0.21447     -0.0308596   -0.0922153   -0.0254403    -0.0144203   -0.0799513  -0.158365    -0.00323785  -0.0119262    0.0531554    0.0720089     0.0554305    0.00338049  -0.00939454   0.0522091     0.0423145   -0.00532249   0.113675    -0.154586    -0.163615   
 -0.047173     0.0255478    0.0233274     0.0759743   -0.101188    -0.0584153    0.0491562   -0.00342416  -0.123165    -0.00985524   -0.133757     0.147476   -0.0233208   -0.255035    -0.0597862    0.11986      0.180255     -0.083561    -0.0554684    0.18493     -0.217037      0.1201      -0.0210358   -0.0545927    0.130033     0.0602135  
 -0.0687057    0.0120048    0.0195654     0.042395    -0.154715     0.00594646  -0.0417729    0.186253     0.0555716   -0.173662     -0.129707     0.108479    0.14646     -0.0168507    0.0126203    0.00722355   0.0267978    -0.0812148   -0.012209     0.00692641   0.0691062     0.115696    -0.0818415   -0.117472     0.0864955   -0.0440004  
  0.0905428    0.0913259    0.0583514     0.0213584   -0.0295315    0.0161423   -0.0702875    0.286        0.0940688   -0.257598     -0.152273     0.0350502   0.278575     0.00670422  -0.00992218  -0.203057    -0.0257128    -0.0251633    0.0225467   -0.0797677    0.0734154     0.0146216   -0.0866469   -0.306054    -0.046711    -0.193971   
 -0.0133718    0.0203383    0.0269089     0.0238064    0.0953598   -0.00964487  -0.0597889   -0.0666594   -0.210732     0.136013      0.0521379   -0.0358557   0.0721058    0.0462404    0.118251     0.0633443    0.00806341   -0.0620558   -0.134274     0.195311     0.0591169     0.267014    -0.0778286   -0.0143622   -0.1645       0.064475   
 -0.354852     0.027503     0.165354     -0.0496943   -0.0252861   -0.04015      0.205817    -0.119038     0.108097    -0.119835      0.0725831   -0.0564186   0.0877713    0.0169003    0.0465175   -0.0325954   -0.00727432   -0.0378058    0.0407705    0.0184814   -0.115889      0.0203445    0.0445406   -0.0610704    0.084048     0.035165   
  0.0169412   -0.0392243   -0.131329     -0.161065     0.03342     -0.0439288    0.0597619    0.00731366   0.0148487    0.00258697   -0.0288794   -0.0600632  -0.0361604   -0.00831171  -0.0242799    0.190199     0.126535     -0.0413183    0.120988     0.0262771    0.0811151    -0.115738    -0.011883    -0.0525618    0.0187904    0.0622118  
  0.0953777    0.0268857   -0.0574115     0.105974     0.0220502    0.0982949   -0.088787    -0.167848    -0.157393    -0.033037     -0.0957082   -0.21445    -0.0500155   -0.0798003    0.0203803    0.0171784    0.122529     -0.17524     -0.0211774    0.0667882    0.0932858    -0.167163    -0.0753884    0.095748    -0.00391398  -0.0275478  
  0.0859808   -0.0876509    0.111951      0.0932619    0.0804864    0.0903333    0.112145     0.0499649   -0.0237845   -0.0325748    -0.115509    -0.0345558  -0.0695632    0.0212282    0.0451581    0.0942013   -0.103939      0.0309655   -0.0694766    0.139843    -0.0238987    -0.0804261   -0.100896    -0.0278196    0.0220688   -0.069921   INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 15 25
INFO: iteration 1, average log likelihood -1.095672
WARNING: Variances had to be floored 1 5 7 15 16 17 18 19 22 25 26
INFO: iteration 2, average log likelihood -1.028681
WARNING: Variances had to be floored 2 11 15 19 23 24 25 27
INFO: iteration 3, average log likelihood -1.048554
WARNING: Variances had to be floored 1 5 7 15 16 17 18 22 25 26
INFO: iteration 4, average log likelihood -1.047278
WARNING: Variances had to be floored 15 19 25
INFO: iteration 5, average log likelihood -1.067831
WARNING: Variances had to be floored 1 2 5 7 11 15 16 17 18 19 22 23 24 25 26
INFO: iteration 6, average log likelihood -0.999818
WARNING: Variances had to be floored 15 25
INFO: iteration 7, average log likelihood -1.094843
WARNING: Variances had to be floored 1 5 7 15 16 17 18 19 22 25 26
INFO: iteration 8, average log likelihood -1.027947
WARNING: Variances had to be floored 2 11 15 19 23 24 25
INFO: iteration 9, average log likelihood -1.048937
WARNING: Variances had to be floored 1 5 7 15 16 17 18 22 25 26 27
INFO: iteration 10, average log likelihood -1.046986
INFO: EM with 100000 data points 10 iterations avll -1.046986
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0229735    0.148634     0.0613711   -0.0254438    0.218876    -0.11339      0.0623655    0.142608     0.154972     0.0715586    0.0756342   -0.0967133   -0.00445931   -0.133608     0.0120477    0.05053     -0.159458     0.0671322     0.015581    -0.05505      0.0229708   -0.213599     0.121084      0.128798     0.0476048    -0.0633124 
  0.0179393   -0.0762371   -0.0852524   -0.0376936    0.0740559   -0.0820896   -0.00137248   0.0116266   -0.125703    -0.0120399    0.134846     0.123719    -0.0140863    -0.124571    -0.0535562   -0.179662     0.0148066    0.0211862    -0.182539     0.00907412  -0.120169     0.0244691   -0.0205407    -0.0571774    0.114896      0.0866953 
 -0.0242573    0.0591605   -0.0936278    0.0457584    0.0284745    0.12232      0.0868791    0.164462     0.037497    -0.0725261   -0.130779     0.10652     -0.185104     -0.12934     -0.0329747    0.100786     0.105041    -0.0376282     0.171876    -0.0371679   -0.104623    -0.0248531    0.0724464    -0.0405153    0.0487998     0.109767  
  0.0886716   -0.159469    -0.0477148    0.0912232   -0.0177295    0.0547951   -0.014311     0.12135      0.0467746    0.215797     0.1148      -0.0376839   -0.0795243     0.0415143   -0.0502324    0.0960821   -0.0416605   -0.108087      0.136465    -0.0719876   -0.0425262   -0.205106     0.0585139     0.124614    -0.113485      0.0186815 
  0.143846    -0.0550574   -0.100309    -0.00652184   0.0799201    0.00945253   0.0157374    0.100468     0.0896673    0.0470628   -0.120051     0.19679      0.0149753    -0.023551     0.0532606   -0.0534368    0.12451      0.0526284    -0.0466976   -0.132059     0.159329     0.206784     0.138413      0.0827825    0.018051     -0.113182  
  0.0210995   -0.0505664   -0.0935922   -0.145489     0.100579    -0.0951592    0.0500294   -0.00896699   0.0407136   -0.060771     0.141165    -0.0939831    0.0759452    -0.214582     0.0421158    0.110254    -0.0821761   -0.0836157     0.0590349   -0.0069365    0.0388848    0.203804     0.0694959    -0.0591774   -0.088907      0.0319201 
 -0.163209    -0.111344     0.0408329    0.0922772   -0.110713     0.0619387   -0.176506     0.0859722   -0.141456    -0.0391111   -0.0190307    0.00554513  -0.062502     -0.0200513    0.128145    -0.0252931   -0.0784076    0.0166844    -0.156446    -0.141354     0.0758281   -0.00313935   0.157193     -0.0932383   -0.0180854    -0.0591264 
 -0.00188242  -0.00448309   0.0613344   -0.166823    -0.0276776   -0.0714237    0.041433     0.0463395   -0.00656788  -0.248355     0.0869201    0.0497096   -0.0353636     0.0669581    0.137905    -0.102849    -0.0345781    0.0567792     0.118991    -0.179557    -0.061306    -0.0582558   -0.0737298     0.00582433  -0.0318142    -0.175434  
  0.0723541   -0.0954127    0.0957225   -0.053547    -0.0179089   -0.064459    -0.100294    -0.0559638   -0.0816444   -0.0655807   -0.115363     0.169006     0.0387744     0.0510719    0.0121071    0.0753538    0.0200061   -0.0130782     0.0431904   -0.0182475   -0.139574     0.0251178   -0.0912827     0.100227    -0.131203      0.065977  
 -0.0487308   -0.0510095   -0.123812     0.0857513   -0.0689452    0.0275387    0.0406506   -0.00159696  -0.0968726    0.108491     0.0467129    0.0840115    0.216443     -0.0322271    0.0459618   -0.0753816   -0.14406     -0.000478893  -0.0188429    0.0321523    0.0738164   -0.0306627    0.00254433   -0.0540557   -0.0565503     0.027282  
 -0.0482622   -0.118211     0.15357     -0.0885742    0.126092    -0.0234715   -0.00345881   0.0551826   -0.125749     0.0197408   -0.0689869   -0.0732999    0.0497184     0.068355    -0.0462614   -0.154852    -0.0418213    0.145401     -0.00225554   0.0950349   -0.15565      0.192994     0.0411225    -0.124341     0.0187675    -0.0547064 
 -0.243511     0.0896925   -0.240653     0.0519077    0.0698328    0.0349247    0.0826072   -0.0596839   -0.0790973    0.056426    -0.149235    -0.159774    -0.0823673     0.0291625    0.0212826   -0.0260919   -0.170894     0.101454     -0.0104124    0.099234    -0.053726     0.0304925    0.0215799     0.0641553   -0.157109     -0.0825297 
  0.120805     0.0253228   -0.00818878   0.0399319   -0.0428099   -0.183684     0.0325787   -0.0210848    0.136323     0.199349     0.00358986   0.0626492    0.0581002     0.00651539  -0.0697323    0.156392    -0.00814777  -0.0764638    -0.0786234    0.222816    -0.0721562    0.0251203    0.00681186   -0.0643197   -0.106723      0.0267828 
 -0.116762    -0.0340722    0.147137    -0.0321496    0.0289935    0.0830072    0.102984    -0.0645948   -0.0143947    0.138843     0.0560822   -0.119384    -0.015103      0.0595179   -0.044319    -0.0983513   -0.0981095   -0.0391741     0.0633586    0.0220311   -0.0508402   -0.0976237    0.123231      0.0630641   -0.125284      0.196556  
  0.0611691   -0.0189779    0.0423974   -0.124985     0.0573486    0.0263158   -0.0569955   -0.151276    -0.0924065   -0.0717829   -0.166379     0.0357187   -0.0822552     0.168556     0.00541288   0.165357     0.0280489   -0.0578496    -0.0373283   -0.0846699    0.149762    -0.0431372   -0.0888926     0.0746295   -0.163095     -0.0469844 
 -0.164634     0.128165     0.00427135  -0.194152     0.18591     -0.0156134    0.00262174   0.102771    -0.002695    -0.00850794  -0.00595467   0.122179     0.0403512     0.13016     -0.0509235    0.154101    -0.0539103   -0.204936      0.00869004  -0.0259757    0.00947172   0.103947     0.0578324     0.0479516    0.0525401    -0.0645404 
 -0.0278413    0.115781    -0.0083153   -0.0415961   -0.00767452   0.145244    -0.133982     0.0879048   -0.0215995    0.0132823    0.226764    -0.25662     -0.0290461     0.103833    -0.0399445   -0.00685778  -0.00453156   0.0692443    -0.0735952    0.0836825   -0.127238     0.0896564   -0.109541     -0.0247909    0.0845351    -0.0187352 
 -0.133971    -0.115453     0.16088      0.103697    -0.0550948    0.0457398    0.140828    -0.0210393    0.10035      0.150063    -0.0484876   -0.0313225    0.213947     -0.0162808   -0.0804937    0.0650573   -0.222625    -0.081474      0.106905    -0.0325085    0.179423    -0.104923     0.0599747     0.0539612   -0.0104445     0.0625455 
  0.0413799    0.00571964  -0.1176      -0.0564777    0.0418665   -0.130943     0.0360494    0.105901     0.182782    -0.111378     0.090926    -0.0187428   -0.0232491     0.0363497    0.110353     0.0261636    0.164584     0.199374     -0.113153     0.0439672    0.0941884    0.12097     -0.0664917     0.276066     0.185621      0.0869725 
 -0.1117       0.113879     0.0272849    0.146103    -0.0355265   -0.153449     0.0210955    0.0383662   -0.0490813    0.103205    -0.0524354   -0.0696421   -0.0258681     0.0189138    0.121036     0.116195     0.0569812    0.0454042     0.178384     0.014479     0.050757     0.0651338    0.118825     -0.188251    -0.0532704     0.0313806 
 -0.0442011    0.0048369   -0.0573455    0.0845128    0.106901     0.162212     0.301357     0.0452461   -0.0757245   -0.0729519   -0.0177755    0.00852275   0.0266482    -0.0110501    0.117857     0.0107494    0.010144     0.0625472     0.0183726   -0.0423672   -0.200343    -0.0563661    0.31262      -0.0867474   -0.000459889   0.0260052 
 -0.0614499    0.0110276   -0.134161     0.166132    -0.0669024   -0.0165903   -0.0825437   -0.157193     0.168297    -0.169656     0.156087     0.036308    -0.0576451    -0.149865    -0.152984     0.00224576  -0.0459886   -0.0777969     0.0961846    0.180934     0.116719     0.108351     0.0237204    -0.0443395   -0.0270063    -0.0149355 
  0.0152874    0.00470295   0.174055    -0.113257    -0.0214395   -0.0389737    0.0498625    0.0550868   -0.206409     0.117541    -0.0264859    0.0297188    0.0176184    -0.094017     0.00643376   0.0626037   -0.125412     0.0393588     0.12726      0.179918    -0.111915    -0.0356812    0.0565488     0.0019332   -0.0501109    -0.0761197 
  0.0883638   -0.183531     0.134283    -0.261077     0.0638343   -0.0603795    0.0901158    0.0501022   -0.0466498   -0.11925     -0.0291667    0.0624255   -0.181925     -0.171008     0.0129972   -0.148434    -0.114848     0.0560341     0.00803152  -0.0270497   -0.00907534  -0.0201788    0.0575471    -0.0241813   -0.00545489   -0.0293838 
  0.0222696   -0.0602603   -0.123153     0.0582976    0.0799799    0.00318922  -0.248116     0.19858      0.0953422    0.00844937   0.10499     -0.0197231   -0.0446172     0.102789     0.128428     0.0538188    0.0236069    0.172586      0.00953124   0.00432087  -0.130545     0.0774374   -0.113594      0.0226455   -0.0491271     0.0510037 
  0.0436644    0.0393361   -0.130696    -0.0493574   -0.154299     0.0928025   -0.199062    -0.119156     0.0703306    0.0377382   -0.0528978    0.0127623    0.0181283    -0.0134452   -0.171724     0.0348248    0.215478     0.0722782    -0.114878    -0.013906    -0.00521611  -0.0435667   -0.123033      0.0606158   -0.0382113     0.150264  
 -0.0946643   -0.0174282    0.110625    -0.0752823    0.0639239   -0.11078     -0.0136644   -0.213566    -0.159821    -0.118714    -0.0531875    0.084235    -0.00359725    0.0897649   -0.0968042    0.119169     0.0290247   -0.105979     -0.138683     0.0373803    0.0302797   -0.126054     0.0218475    -0.0528678    0.119967      0.0648871 
  0.0553574   -0.0850835   -0.182699    -0.0726331    0.0667851   -0.0570383    0.108775    -0.038415     0.139754     0.0855871    0.128171    -0.107846     0.112428      0.0534083   -0.0438373   -0.00322598   0.0779398    0.0794302    -0.0816748   -0.0505652    0.00813758  -0.156013     0.0582519    -0.00131592  -0.08938       0.115929  
 -0.0296029   -0.00400321   0.0981345    0.0259316    0.0525818   -0.0143339    0.028651    -0.0268358    0.00367265  -0.043556    -0.127145     0.1163       0.170457      0.117365     0.0512063   -0.125066     0.0910879    0.145365      0.234734     0.125241    -0.186652    -0.103083    -0.000420462   0.0336143   -0.0439586    -0.00435456
  0.118105     0.019013     0.0377588    0.0290488   -0.104344     0.0831582    0.061245    -0.0484899    0.00211183   0.00389104  -0.0693489    0.0196974    0.000667031   0.112802     0.0319774   -0.070529     0.137487    -0.158966      0.12348      0.0784228   -0.0387114   -0.118837    -0.0242075     0.0366561    0.132133     -0.0541015 
  0.0208924    0.0823115    0.0422272    0.121054    -0.00416673  -0.159441    -0.048452     0.0458791   -0.061526     0.0851887    0.0388411   -0.0861817   -0.0569576    -0.0788563   -0.023764     0.0962813    0.0584055   -0.103191     -0.203355     0.159363     0.0233596   -0.00977785   0.0711213     0.119103    -0.00464773   -0.0723326 
  0.177335    -0.11981      0.0389109    0.0616701    0.0726978    0.0523557    0.0540152   -0.0430852    0.0280404   -0.172371     0.00939335   0.0722419   -0.117635     -0.159944    -0.0140249    0.131897     0.0429711    0.138275     -0.10065      0.093128    -0.00865066  -0.0473651   -0.0597759    -0.060815     0.00376017    0.0135477 kind full, method split
0: avll = -1.4230959072796818
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.423116
INFO: iteration 2, average log likelihood -1.423057
INFO: iteration 3, average log likelihood -1.423014
INFO: iteration 4, average log likelihood -1.422964
INFO: iteration 5, average log likelihood -1.422904
INFO: iteration 6, average log likelihood -1.422831
INFO: iteration 7, average log likelihood -1.422750
INFO: iteration 8, average log likelihood -1.422667
INFO: iteration 9, average log likelihood -1.422586
INFO: iteration 10, average log likelihood -1.422510
INFO: iteration 11, average log likelihood -1.422428
INFO: iteration 12, average log likelihood -1.422313
INFO: iteration 13, average log likelihood -1.422112
INFO: iteration 14, average log likelihood -1.421737
INFO: iteration 15, average log likelihood -1.421089
INFO: iteration 16, average log likelihood -1.420163
INFO: iteration 17, average log likelihood -1.419186
INFO: iteration 18, average log likelihood -1.418464
INFO: iteration 19, average log likelihood -1.418073
INFO: iteration 20, average log likelihood -1.417898
INFO: iteration 21, average log likelihood -1.417824
INFO: iteration 22, average log likelihood -1.417794
INFO: iteration 23, average log likelihood -1.417781
INFO: iteration 24, average log likelihood -1.417775
INFO: iteration 25, average log likelihood -1.417773
INFO: iteration 26, average log likelihood -1.417771
INFO: iteration 27, average log likelihood -1.417770
INFO: iteration 28, average log likelihood -1.417769
INFO: iteration 29, average log likelihood -1.417769
INFO: iteration 30, average log likelihood -1.417768
INFO: iteration 31, average log likelihood -1.417768
INFO: iteration 32, average log likelihood -1.417767
INFO: iteration 33, average log likelihood -1.417767
INFO: iteration 34, average log likelihood -1.417767
INFO: iteration 35, average log likelihood -1.417766
INFO: iteration 36, average log likelihood -1.417766
INFO: iteration 37, average log likelihood -1.417766
INFO: iteration 38, average log likelihood -1.417766
INFO: iteration 39, average log likelihood -1.417766
INFO: iteration 40, average log likelihood -1.417765
INFO: iteration 41, average log likelihood -1.417765
INFO: iteration 42, average log likelihood -1.417765
INFO: iteration 43, average log likelihood -1.417765
INFO: iteration 44, average log likelihood -1.417765
INFO: iteration 45, average log likelihood -1.417765
INFO: iteration 46, average log likelihood -1.417765
INFO: iteration 47, average log likelihood -1.417765
INFO: iteration 48, average log likelihood -1.417765
INFO: iteration 49, average log likelihood -1.417765
INFO: iteration 50, average log likelihood -1.417764
INFO: EM with 100000 data points 50 iterations avll -1.417764
952.4 data points per parameter
1: avll = [-1.42312,-1.42306,-1.42301,-1.42296,-1.4229,-1.42283,-1.42275,-1.42267,-1.42259,-1.42251,-1.42243,-1.42231,-1.42211,-1.42174,-1.42109,-1.42016,-1.41919,-1.41846,-1.41807,-1.4179,-1.41782,-1.41779,-1.41778,-1.41778,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41776,-1.41776,-1.41776,-1.41776,-1.41776,-1.41776,-1.41776]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.417785
INFO: iteration 2, average log likelihood -1.417723
INFO: iteration 3, average log likelihood -1.417679
INFO: iteration 4, average log likelihood -1.417627
INFO: iteration 5, average log likelihood -1.417563
INFO: iteration 6, average log likelihood -1.417489
INFO: iteration 7, average log likelihood -1.417407
INFO: iteration 8, average log likelihood -1.417325
INFO: iteration 9, average log likelihood -1.417252
INFO: iteration 10, average log likelihood -1.417192
INFO: iteration 11, average log likelihood -1.417146
INFO: iteration 12, average log likelihood -1.417111
INFO: iteration 13, average log likelihood -1.417084
INFO: iteration 14, average log likelihood -1.417062
INFO: iteration 15, average log likelihood -1.417043
INFO: iteration 16, average log likelihood -1.417026
INFO: iteration 17, average log likelihood -1.417010
INFO: iteration 18, average log likelihood -1.416995
INFO: iteration 19, average log likelihood -1.416980
INFO: iteration 20, average log likelihood -1.416966
INFO: iteration 21, average log likelihood -1.416951
INFO: iteration 22, average log likelihood -1.416936
INFO: iteration 23, average log likelihood -1.416920
INFO: iteration 24, average log likelihood -1.416904
INFO: iteration 25, average log likelihood -1.416888
INFO: iteration 26, average log likelihood -1.416872
INFO: iteration 27, average log likelihood -1.416856
INFO: iteration 28, average log likelihood -1.416841
INFO: iteration 29, average log likelihood -1.416826
INFO: iteration 30, average log likelihood -1.416811
INFO: iteration 31, average log likelihood -1.416798
INFO: iteration 32, average log likelihood -1.416786
INFO: iteration 33, average log likelihood -1.416774
INFO: iteration 34, average log likelihood -1.416764
INFO: iteration 35, average log likelihood -1.416755
INFO: iteration 36, average log likelihood -1.416747
INFO: iteration 37, average log likelihood -1.416740
INFO: iteration 38, average log likelihood -1.416733
INFO: iteration 39, average log likelihood -1.416728
INFO: iteration 40, average log likelihood -1.416723
INFO: iteration 41, average log likelihood -1.416718
INFO: iteration 42, average log likelihood -1.416714
INFO: iteration 43, average log likelihood -1.416710
INFO: iteration 44, average log likelihood -1.416707
INFO: iteration 45, average log likelihood -1.416704
INFO: iteration 46, average log likelihood -1.416701
INFO: iteration 47, average log likelihood -1.416698
INFO: iteration 48, average log likelihood -1.416696
INFO: iteration 49, average log likelihood -1.416693
INFO: iteration 50, average log likelihood -1.416691
INFO: EM with 100000 data points 50 iterations avll -1.416691
473.9 data points per parameter
2: avll = [-1.41778,-1.41772,-1.41768,-1.41763,-1.41756,-1.41749,-1.41741,-1.41732,-1.41725,-1.41719,-1.41715,-1.41711,-1.41708,-1.41706,-1.41704,-1.41703,-1.41701,-1.41699,-1.41698,-1.41697,-1.41695,-1.41694,-1.41692,-1.4169,-1.41689,-1.41687,-1.41686,-1.41684,-1.41683,-1.41681,-1.4168,-1.41679,-1.41677,-1.41676,-1.41676,-1.41675,-1.41674,-1.41673,-1.41673,-1.41672,-1.41672,-1.41671,-1.41671,-1.41671,-1.4167,-1.4167,-1.4167,-1.4167,-1.41669,-1.41669]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.416702
INFO: iteration 2, average log likelihood -1.416650
INFO: iteration 3, average log likelihood -1.416609
INFO: iteration 4, average log likelihood -1.416565
INFO: iteration 5, average log likelihood -1.416511
INFO: iteration 6, average log likelihood -1.416448
INFO: iteration 7, average log likelihood -1.416374
INFO: iteration 8, average log likelihood -1.416294
INFO: iteration 9, average log likelihood -1.416210
INFO: iteration 10, average log likelihood -1.416128
INFO: iteration 11, average log likelihood -1.416052
INFO: iteration 12, average log likelihood -1.415981
INFO: iteration 13, average log likelihood -1.415918
INFO: iteration 14, average log likelihood -1.415859
INFO: iteration 15, average log likelihood -1.415806
INFO: iteration 16, average log likelihood -1.415756
INFO: iteration 17, average log likelihood -1.415709
INFO: iteration 18, average log likelihood -1.415664
INFO: iteration 19, average log likelihood -1.415621
INFO: iteration 20, average log likelihood -1.415579
INFO: iteration 21, average log likelihood -1.415538
INFO: iteration 22, average log likelihood -1.415499
INFO: iteration 23, average log likelihood -1.415462
INFO: iteration 24, average log likelihood -1.415427
INFO: iteration 25, average log likelihood -1.415395
INFO: iteration 26, average log likelihood -1.415365
INFO: iteration 27, average log likelihood -1.415338
INFO: iteration 28, average log likelihood -1.415313
INFO: iteration 29, average log likelihood -1.415290
INFO: iteration 30, average log likelihood -1.415270
INFO: iteration 31, average log likelihood -1.415252
INFO: iteration 32, average log likelihood -1.415236
INFO: iteration 33, average log likelihood -1.415222
INFO: iteration 34, average log likelihood -1.415209
INFO: iteration 35, average log likelihood -1.415197
INFO: iteration 36, average log likelihood -1.415186
INFO: iteration 37, average log likelihood -1.415176
INFO: iteration 38, average log likelihood -1.415166
INFO: iteration 39, average log likelihood -1.415158
INFO: iteration 40, average log likelihood -1.415149
INFO: iteration 41, average log likelihood -1.415142
INFO: iteration 42, average log likelihood -1.415134
INFO: iteration 43, average log likelihood -1.415127
INFO: iteration 44, average log likelihood -1.415121
INFO: iteration 45, average log likelihood -1.415114
INFO: iteration 46, average log likelihood -1.415108
INFO: iteration 47, average log likelihood -1.415102
INFO: iteration 48, average log likelihood -1.415097
INFO: iteration 49, average log likelihood -1.415091
INFO: iteration 50, average log likelihood -1.415086
INFO: EM with 100000 data points 50 iterations avll -1.415086
236.4 data points per parameter
3: avll = [-1.4167,-1.41665,-1.41661,-1.41656,-1.41651,-1.41645,-1.41637,-1.41629,-1.41621,-1.41613,-1.41605,-1.41598,-1.41592,-1.41586,-1.41581,-1.41576,-1.41571,-1.41566,-1.41562,-1.41558,-1.41554,-1.4155,-1.41546,-1.41543,-1.41539,-1.41536,-1.41534,-1.41531,-1.41529,-1.41527,-1.41525,-1.41524,-1.41522,-1.41521,-1.4152,-1.41519,-1.41518,-1.41517,-1.41516,-1.41515,-1.41514,-1.41513,-1.41513,-1.41512,-1.41511,-1.41511,-1.4151,-1.4151,-1.41509,-1.41509]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.415089
INFO: iteration 2, average log likelihood -1.415025
INFO: iteration 3, average log likelihood -1.414966
INFO: iteration 4, average log likelihood -1.414897
INFO: iteration 5, average log likelihood -1.414815
INFO: iteration 6, average log likelihood -1.414717
INFO: iteration 7, average log likelihood -1.414605
INFO: iteration 8, average log likelihood -1.414485
INFO: iteration 9, average log likelihood -1.414364
INFO: iteration 10, average log likelihood -1.414250
INFO: iteration 11, average log likelihood -1.414144
INFO: iteration 12, average log likelihood -1.414050
INFO: iteration 13, average log likelihood -1.413967
INFO: iteration 14, average log likelihood -1.413895
INFO: iteration 15, average log likelihood -1.413833
INFO: iteration 16, average log likelihood -1.413780
INFO: iteration 17, average log likelihood -1.413735
INFO: iteration 18, average log likelihood -1.413696
INFO: iteration 19, average log likelihood -1.413663
INFO: iteration 20, average log likelihood -1.413634
INFO: iteration 21, average log likelihood -1.413609
INFO: iteration 22, average log likelihood -1.413586
INFO: iteration 23, average log likelihood -1.413566
INFO: iteration 24, average log likelihood -1.413547
INFO: iteration 25, average log likelihood -1.413530
INFO: iteration 26, average log likelihood -1.413513
INFO: iteration 27, average log likelihood -1.413498
INFO: iteration 28, average log likelihood -1.413483
INFO: iteration 29, average log likelihood -1.413469
INFO: iteration 30, average log likelihood -1.413455
INFO: iteration 31, average log likelihood -1.413441
INFO: iteration 32, average log likelihood -1.413428
INFO: iteration 33, average log likelihood -1.413415
INFO: iteration 34, average log likelihood -1.413403
INFO: iteration 35, average log likelihood -1.413390
INFO: iteration 36, average log likelihood -1.413378
INFO: iteration 37, average log likelihood -1.413366
INFO: iteration 38, average log likelihood -1.413355
INFO: iteration 39, average log likelihood -1.413343
INFO: iteration 40, average log likelihood -1.413332
INFO: iteration 41, average log likelihood -1.413321
INFO: iteration 42, average log likelihood -1.413310
INFO: iteration 43, average log likelihood -1.413299
INFO: iteration 44, average log likelihood -1.413289
INFO: iteration 45, average log likelihood -1.413278
INFO: iteration 46, average log likelihood -1.413268
INFO: iteration 47, average log likelihood -1.413259
INFO: iteration 48, average log likelihood -1.413249
INFO: iteration 49, average log likelihood -1.413240
INFO: iteration 50, average log likelihood -1.413231
INFO: EM with 100000 data points 50 iterations avll -1.413231
118.1 data points per parameter
4: avll = [-1.41509,-1.41503,-1.41497,-1.4149,-1.41481,-1.41472,-1.4146,-1.41448,-1.41436,-1.41425,-1.41414,-1.41405,-1.41397,-1.4139,-1.41383,-1.41378,-1.41373,-1.4137,-1.41366,-1.41363,-1.41361,-1.41359,-1.41357,-1.41355,-1.41353,-1.41351,-1.4135,-1.41348,-1.41347,-1.41345,-1.41344,-1.41343,-1.41342,-1.4134,-1.41339,-1.41338,-1.41337,-1.41335,-1.41334,-1.41333,-1.41332,-1.41331,-1.4133,-1.41329,-1.41328,-1.41327,-1.41326,-1.41325,-1.41324,-1.41323]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.413231
INFO: iteration 2, average log likelihood -1.413168
INFO: iteration 3, average log likelihood -1.413109
INFO: iteration 4, average log likelihood -1.413042
INFO: iteration 5, average log likelihood -1.412961
INFO: iteration 6, average log likelihood -1.412862
INFO: iteration 7, average log likelihood -1.412744
INFO: iteration 8, average log likelihood -1.412609
INFO: iteration 9, average log likelihood -1.412461
INFO: iteration 10, average log likelihood -1.412307
INFO: iteration 11, average log likelihood -1.412154
INFO: iteration 12, average log likelihood -1.412007
INFO: iteration 13, average log likelihood -1.411871
INFO: iteration 14, average log likelihood -1.411748
INFO: iteration 15, average log likelihood -1.411639
INFO: iteration 16, average log likelihood -1.411544
INFO: iteration 17, average log likelihood -1.411461
INFO: iteration 18, average log likelihood -1.411388
INFO: iteration 19, average log likelihood -1.411324
INFO: iteration 20, average log likelihood -1.411268
INFO: iteration 21, average log likelihood -1.411218
INFO: iteration 22, average log likelihood -1.411173
INFO: iteration 23, average log likelihood -1.411132
INFO: iteration 24, average log likelihood -1.411095
INFO: iteration 25, average log likelihood -1.411061
INFO: iteration 26, average log likelihood -1.411030
INFO: iteration 27, average log likelihood -1.411001
INFO: iteration 28, average log likelihood -1.410974
INFO: iteration 29, average log likelihood -1.410949
INFO: iteration 30, average log likelihood -1.410925
INFO: iteration 31, average log likelihood -1.410903
INFO: iteration 32, average log likelihood -1.410881
INFO: iteration 33, average log likelihood -1.410860
INFO: iteration 34, average log likelihood -1.410841
INFO: iteration 35, average log likelihood -1.410821
INFO: iteration 36, average log likelihood -1.410803
INFO: iteration 37, average log likelihood -1.410785
INFO: iteration 38, average log likelihood -1.410768
INFO: iteration 39, average log likelihood -1.410751
INFO: iteration 40, average log likelihood -1.410734
INFO: iteration 41, average log likelihood -1.410718
INFO: iteration 42, average log likelihood -1.410702
INFO: iteration 43, average log likelihood -1.410686
INFO: iteration 44, average log likelihood -1.410671
INFO: iteration 45, average log likelihood -1.410656
INFO: iteration 46, average log likelihood -1.410641
INFO: iteration 47, average log likelihood -1.410627
INFO: iteration 48, average log likelihood -1.410612
INFO: iteration 49, average log likelihood -1.410598
INFO: iteration 50, average log likelihood -1.410584
INFO: EM with 100000 data points 50 iterations avll -1.410584
59.0 data points per parameter
5: avll = [-1.41323,-1.41317,-1.41311,-1.41304,-1.41296,-1.41286,-1.41274,-1.41261,-1.41246,-1.41231,-1.41215,-1.41201,-1.41187,-1.41175,-1.41164,-1.41154,-1.41146,-1.41139,-1.41132,-1.41127,-1.41122,-1.41117,-1.41113,-1.4111,-1.41106,-1.41103,-1.411,-1.41097,-1.41095,-1.41093,-1.4109,-1.41088,-1.41086,-1.41084,-1.41082,-1.4108,-1.41079,-1.41077,-1.41075,-1.41073,-1.41072,-1.4107,-1.41069,-1.41067,-1.41066,-1.41064,-1.41063,-1.41061,-1.4106,-1.41058]
[-1.4231,-1.42312,-1.42306,-1.42301,-1.42296,-1.4229,-1.42283,-1.42275,-1.42267,-1.42259,-1.42251,-1.42243,-1.42231,-1.42211,-1.42174,-1.42109,-1.42016,-1.41919,-1.41846,-1.41807,-1.4179,-1.41782,-1.41779,-1.41778,-1.41778,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41777,-1.41776,-1.41776,-1.41776,-1.41776,-1.41776,-1.41776,-1.41776,-1.41778,-1.41772,-1.41768,-1.41763,-1.41756,-1.41749,-1.41741,-1.41732,-1.41725,-1.41719,-1.41715,-1.41711,-1.41708,-1.41706,-1.41704,-1.41703,-1.41701,-1.41699,-1.41698,-1.41697,-1.41695,-1.41694,-1.41692,-1.4169,-1.41689,-1.41687,-1.41686,-1.41684,-1.41683,-1.41681,-1.4168,-1.41679,-1.41677,-1.41676,-1.41676,-1.41675,-1.41674,-1.41673,-1.41673,-1.41672,-1.41672,-1.41671,-1.41671,-1.41671,-1.4167,-1.4167,-1.4167,-1.4167,-1.41669,-1.41669,-1.4167,-1.41665,-1.41661,-1.41656,-1.41651,-1.41645,-1.41637,-1.41629,-1.41621,-1.41613,-1.41605,-1.41598,-1.41592,-1.41586,-1.41581,-1.41576,-1.41571,-1.41566,-1.41562,-1.41558,-1.41554,-1.4155,-1.41546,-1.41543,-1.41539,-1.41536,-1.41534,-1.41531,-1.41529,-1.41527,-1.41525,-1.41524,-1.41522,-1.41521,-1.4152,-1.41519,-1.41518,-1.41517,-1.41516,-1.41515,-1.41514,-1.41513,-1.41513,-1.41512,-1.41511,-1.41511,-1.4151,-1.4151,-1.41509,-1.41509,-1.41509,-1.41503,-1.41497,-1.4149,-1.41481,-1.41472,-1.4146,-1.41448,-1.41436,-1.41425,-1.41414,-1.41405,-1.41397,-1.4139,-1.41383,-1.41378,-1.41373,-1.4137,-1.41366,-1.41363,-1.41361,-1.41359,-1.41357,-1.41355,-1.41353,-1.41351,-1.4135,-1.41348,-1.41347,-1.41345,-1.41344,-1.41343,-1.41342,-1.4134,-1.41339,-1.41338,-1.41337,-1.41335,-1.41334,-1.41333,-1.41332,-1.41331,-1.4133,-1.41329,-1.41328,-1.41327,-1.41326,-1.41325,-1.41324,-1.41323,-1.41323,-1.41317,-1.41311,-1.41304,-1.41296,-1.41286,-1.41274,-1.41261,-1.41246,-1.41231,-1.41215,-1.41201,-1.41187,-1.41175,-1.41164,-1.41154,-1.41146,-1.41139,-1.41132,-1.41127,-1.41122,-1.41117,-1.41113,-1.4111,-1.41106,-1.41103,-1.411,-1.41097,-1.41095,-1.41093,-1.4109,-1.41088,-1.41086,-1.41084,-1.41082,-1.4108,-1.41079,-1.41077,-1.41075,-1.41073,-1.41072,-1.4107,-1.41069,-1.41067,-1.41066,-1.41064,-1.41063,-1.41061,-1.4106,-1.41058]
32×26 Array{Float64,2}:
  0.726642    -0.0818383    0.550146    -0.246265    -0.106442   -0.374552    -0.401575    0.171315   -0.621594   -0.59435      -0.14026     0.121482   -0.364452    0.424815    0.544941      0.145949    -0.829872     0.209164    0.360273    -0.248287      0.114438   -0.323388     0.22222    -0.0873887   0.163562     0.0614169 
  0.0672651   -0.65112      0.439676     0.116774     0.0741648   0.349017     0.308803    0.809728    0.424598   -0.314118     -0.19209     0.680779    0.191544    0.504514    0.0535281     0.10412     -0.490634    -0.191748    0.0829189   -0.612966      0.454685    0.177329     0.166223   -0.0748567  -0.317879    -0.0562665 
  0.189538    -0.0353722    0.297418     0.287394     0.548837    0.00626657  -0.229562   -0.378986   -0.614712   -0.206094      0.0876388   0.735036    0.245982    0.154666    0.210193     -0.0242704   -0.56801      0.10534    -0.292641     0.296572     -0.348979    0.421887     0.0930591  -0.699626   -0.0289033   -0.410666  
  0.193077    -0.13591      0.0716832    0.291028     0.842592   -0.524887    -0.290472   -0.433219    0.667266    0.110618     -0.279735    0.269412    0.288933    0.657051   -0.126534     -0.0978765   -0.338976     0.0500474  -0.135943     0.62459      -0.0154129   0.266161     0.357119   -0.18893    -0.100301    -0.461304  
 -0.418793     0.164867     0.178387    -0.268789     0.482837   -0.182509     0.110874    0.162954   -0.248757    0.548914     -0.610684    0.841686    0.134857   -0.080487   -0.625483     -0.400884    -0.392927     0.558359    0.21634      0.188481      0.248089   -0.220352     0.171205   -0.498351   -0.108651    -0.543163  
 -0.100958    -0.351407     0.245157    -0.667599     0.0946708   0.139171     0.431663   -0.572355   -0.100677    0.612263     -0.580339    0.476298   -0.251027   -0.0229248  -0.157856      0.0664113    0.516962     0.342286   -0.170277    -0.28983      -0.428149    0.216903    -0.337385    0.88829     0.0979647   -0.0795721 
  0.14965     -0.886666     0.168409    -0.763259     0.136634    0.248068    -0.0216472  -0.0505049  -0.15787     0.0940763     0.717739    0.504944    0.826821   -0.757874   -0.303773     -0.260928    -0.341956    -0.0663203  -0.383949    -0.820907      0.102042    0.622457     0.130328    0.121184   -0.480178     0.505092  
 -0.203617     0.0816375    0.55269     -0.357836     0.422526    0.551432     0.714833    0.313737   -0.0629369   0.220248      1.05012     0.0488182   0.629314   -0.190233   -0.162689     -0.282289     0.0218095   -0.364914   -0.887106    -0.299635      0.0324559   0.0131267    0.142974   -0.0737345   0.387015     0.302747  
 -0.125477     0.275495     0.260412    -0.0555085    0.0511244  -0.107057     0.260342   -0.534342   -0.0251743  -0.308154      0.671675    0.0656235  -0.0263219   0.0525435   0.0629953     0.238331     0.118948     0.173313   -0.2971       0.100748     -0.554597    0.0677436    0.275303   -0.421321   -0.796001     0.121217  
  0.389151    -0.331281    -0.0924523    0.327196     0.124617    0.109763    -0.434743   -0.492235    0.0552928  -0.494967      0.142127   -0.325334   -0.411159    0.288572    0.660433      0.319707     0.444535     0.137711   -0.0321157    0.0864262     0.146538    0.118693    -0.0430715   0.65789    -0.463537     0.188741  
  0.246126     0.407628    -0.822985     0.258995    -0.177107   -0.437028    -0.439689   -0.380291   -0.303532   -0.000785696  -0.0416066  -0.705501   -0.319511   -0.204574    0.00555497    0.202219     0.587394     0.0251001   0.00210277   0.43808      -0.397446   -0.315085     0.0823482   0.0584432   0.281267     0.11966   
 -0.218117    -0.252007     0.251221    -0.160115     0.284546   -0.487932    -0.101319   -0.722897    0.17836    -0.559421     -0.353345   -0.251534   -0.143124    0.0780548  -0.116538      0.737712     0.298315     0.155831   -0.358733     0.322695     -0.125051   -0.307329     0.0655648   0.118863   -0.191205     0.298415  
 -0.214536     0.271566    -0.136951    -0.0854927   -0.64959    -0.181249     0.0172103   0.673205    0.32601     0.00476301    0.127262    0.0650082  -0.308531    0.079955   -0.0208169    -0.159191    -0.00161129  -0.317344    0.719372     0.430943      0.180069   -0.0956987   -0.229641    0.182501   -0.341805    -0.0532115 
  0.40895      0.00813873   0.0195635    0.18643      0.335173    0.0415249    0.0492567   0.556192   -0.0635811   0.18124      -0.326661   -0.111994   -0.845919   -0.215821    0.0114196    -0.192846     0.218819    -0.108646   -0.164753     0.157438      0.841549    0.039609     0.279402   -0.0963004   0.54641     -0.356763  
 -0.00371473  -0.136754    -0.507363     0.240192    -0.231706    0.227523     0.0440361   0.325707    0.0578352   0.287007     -0.190741   -0.205171    0.526304   -0.34125     0.000667779   0.0737345   -0.29948      0.0774934   0.0769679   -0.223107     -0.327087   -0.173386    -0.0385307  -0.355924    0.875862     0.388707  
 -0.212526     0.346563    -0.417631     0.326979     0.533294   -0.335658     0.181482    0.403618    0.227671   -0.299271      0.13405    -0.706986    0.851867    0.438787    0.240034     -0.439613    -0.0260352   -0.170229   -0.0123907   -0.2285       -0.33656    -0.511932     0.106012    0.940266    0.168008     0.0800717 
  0.0557005    0.313499    -0.082534     0.527789    -0.248598    0.0339658   -0.283762    0.472858    0.0375382  -0.137447      0.401545   -0.531921    0.515333    0.1206      0.107861      0.106017    -0.402564    -0.797785   -0.195794     0.295331      0.116527   -0.0828585   -0.0380916  -0.327455    0.0768785   -0.166614  
  0.452103     0.580454     0.0289845    0.128063    -0.0475144   0.0146298    0.388834    0.316203    0.197059   -0.0378013     0.479844   -0.235728    0.0225189   0.146493   -0.160358      0.217384     0.0328639   -0.0174043  -0.177364     0.257964     -0.0219816  -0.449823     0.483869   -0.218694   -0.173062    -0.196775  
  0.116634    -0.0326018    0.355923     0.284668     0.388403   -0.296676     0.650602    0.0645476  -0.183286    0.400816     -0.131556    0.454533   -0.05907    -0.225137   -0.186874     -0.203133    -0.340976     0.262516   -0.117517    -0.163353      0.0186899   0.0715348    0.636391   -0.310175    3.92512e-5  -0.00892783
 -0.066031     0.0797429   -0.221913     0.271592    -0.115254    0.124256     0.143009   -0.116903   -0.068174    0.0295327    -0.276752   -0.155534    0.38419    -0.491583    0.187777     -0.317849     0.123468     0.403563   -0.0149214   -0.619334     -0.606057   -0.269351     0.262335   -0.167861    0.685351     0.443587  
 -0.0530374    0.0552677    0.144073     0.163133     0.271519   -0.197518    -0.150539   -0.0407865  -0.115913   -0.634764      0.744155    0.461456   -0.0706615  -0.167513    0.324016     -0.314596     0.108501    -0.100616   -0.196849     0.193485     -0.419037   -0.170501     0.147863    0.172528   -0.529116    -0.0680584 
 -0.348653     0.696248    -0.229843    -0.601013     0.0201916  -0.041635     0.0277934   0.0050885   0.0162818   0.71113       0.315332    0.177833    0.0839228  -0.224075    0.0909453    -0.506848     0.367509    -0.247169    0.0941879    0.54555      -0.158794    0.0682329   -0.48289     0.318502    0.0567346   -0.460169  
  0.101998    -0.0642287   -0.05923      0.00972179   0.147764   -0.120107    -0.281407   -0.125211   -0.0476044  -0.0814987    -0.283478   -0.0240872  -0.306147    0.218178    0.123244      0.0525594    0.0737057    0.279669    0.119257     0.000557785   0.110118   -0.117639     0.0351278   0.148405   -0.00525006  -0.0544377 
 -0.0803139   -0.0129282   -0.00802127   0.00426467  -0.0374541   0.0618105    0.099943    0.125829    0.0248236  -0.0266009     0.187597    0.0296593   0.206482   -0.0496608  -0.0603206     0.0560906   -0.0598347   -0.110279   -0.0690716    0.0313546    -0.0698541   0.0521767   -0.0464664  -0.0763272  -0.0608998    0.0295727 
 -0.400066     0.3149       0.0231946   -0.325528    -0.397428    0.250575    -0.268079   -0.369814   -0.759342   -0.489857      0.294082   -0.144458   -0.181891   -0.501788   -0.0431619     0.204303    -0.289486     0.176236    0.231804    -0.592537      0.186734    0.0834018    0.274717   -0.512792   -0.0063979    0.410435  
 -0.396327    -0.168606    -0.119322    -0.240521    -0.876438    0.451051     0.300256    0.568896   -0.153299    0.0698864     0.106621   -0.395422   -0.157233   -0.549715   -0.159516      0.134762     0.561427    -0.242406    0.180985    -0.336634     -0.0776534  -0.271466    -0.561287    0.275376    0.140375     0.341031  
 -0.186992    -0.182506     0.0169906   -0.0368627   -0.221088   -0.0941064    0.270822    0.0781735   0.156212    0.236853      0.0337884   0.23619    -0.436128   -0.188957    0.226187     -0.0592525    0.584146     0.0295141  -0.0882449    0.234501      0.111845    0.852257    -0.353651   -0.102237   -0.453991     0.127088  
 -0.511534    -0.637002    -0.0410016   -0.0395157   -0.074401    0.387307    -0.315488   -0.49223     0.376581   -0.187264     -0.135196    0.136409    0.373075    0.0230177   0.212606     -0.110907     0.142615     0.221313    0.160255    -0.230859     -0.20082     0.573599    -0.328896   -0.0800402  -0.272677     0.119435  
 -0.0780926    0.144617     0.0283548    0.00674776  -0.464024    0.0518451   -0.0156491   0.166737    0.271821    0.109688     -0.656257   -0.646552   -0.182585    0.550205   -0.295354      0.598867    -0.0550637    0.264464   -0.00424026  -0.137382      0.676205    0.213361    -0.094902   -0.329488    0.45872      0.0415446 
  0.217257    -0.343095    -0.137468    -0.209416     0.0601816   0.280659     0.119955    0.0825116  -0.291566    0.649938     -0.413686    0.202081    0.162672   -0.133004   -0.547849      0.00424864  -0.51137     -0.225181    0.27834      0.0557462    -0.127804    0.108715    -0.309749    0.0697314   0.356117    -0.505575  
 -0.101323    -0.268101    -0.486822    -0.386471     0.0762733   0.170739    -0.64925     0.170896    0.169178    0.448205     -0.269375    0.209752   -0.104719   -0.0872867  -0.124665     -0.0402614   -0.489687    -0.0492283   0.831656     0.605921      0.627933   -0.684104     0.145864    0.713064   -0.169929     0.308411  
 -0.186691    -0.106966     0.0939373   -0.509372     0.261517   -0.152817     0.336643    0.194729    0.024219    0.128519      0.142888   -0.452107   -0.34262     0.174601   -0.0367699    -0.143184     0.164392    -0.191586    0.483865    -0.254877      0.281574   -0.00135977  -0.269202    1.11292    -0.157719     0.349553  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.410571
INFO: iteration 2, average log likelihood -1.410557
INFO: iteration 3, average log likelihood -1.410544
INFO: iteration 4, average log likelihood -1.410531
INFO: iteration 5, average log likelihood -1.410518
INFO: iteration 6, average log likelihood -1.410505
INFO: iteration 7, average log likelihood -1.410493
INFO: iteration 8, average log likelihood -1.410480
INFO: iteration 9, average log likelihood -1.410467
INFO: iteration 10, average log likelihood -1.410455
INFO: EM with 100000 data points 10 iterations avll -1.410455
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.281353e+05
      1       7.032869e+05      -2.248484e+05 |       32
      2       6.913862e+05      -1.190067e+04 |       32
      3       6.866572e+05      -4.729017e+03 |       32
      4       6.841184e+05      -2.538756e+03 |       32
      5       6.825023e+05      -1.616098e+03 |       32
      6       6.812740e+05      -1.228302e+03 |       32
      7       6.802570e+05      -1.017066e+03 |       32
      8       6.794426e+05      -8.143817e+02 |       32
      9       6.787627e+05      -6.799197e+02 |       32
     10       6.781990e+05      -5.636760e+02 |       32
     11       6.777452e+05      -4.537523e+02 |       32
     12       6.773616e+05      -3.836483e+02 |       32
     13       6.770270e+05      -3.346227e+02 |       32
     14       6.767136e+05      -3.133877e+02 |       32
     15       6.764394e+05      -2.741370e+02 |       32
     16       6.761908e+05      -2.486104e+02 |       32
     17       6.759590e+05      -2.318331e+02 |       32
     18       6.757437e+05      -2.153062e+02 |       32
     19       6.755579e+05      -1.858227e+02 |       32
     20       6.753947e+05      -1.631315e+02 |       32
     21       6.752428e+05      -1.519545e+02 |       32
     22       6.751088e+05      -1.340272e+02 |       32
     23       6.749961e+05      -1.127111e+02 |       32
     24       6.748810e+05      -1.150109e+02 |       32
     25       6.747604e+05      -1.206833e+02 |       32
     26       6.746487e+05      -1.116663e+02 |       32
     27       6.745482e+05      -1.004752e+02 |       32
     28       6.744530e+05      -9.525917e+01 |       32
     29       6.743771e+05      -7.587969e+01 |       32
     30       6.743168e+05      -6.025556e+01 |       32
     31       6.742604e+05      -5.645202e+01 |       32
     32       6.742085e+05      -5.190904e+01 |       32
     33       6.741572e+05      -5.128797e+01 |       32
     34       6.741065e+05      -5.068742e+01 |       32
     35       6.740558e+05      -5.063508e+01 |       32
     36       6.740050e+05      -5.084654e+01 |       32
     37       6.739545e+05      -5.049969e+01 |       32
     38       6.739059e+05      -4.861981e+01 |       32
     39       6.738634e+05      -4.249337e+01 |       32
     40       6.738239e+05      -3.953661e+01 |       32
     41       6.737851e+05      -3.875516e+01 |       32
     42       6.737479e+05      -3.715625e+01 |       32
     43       6.737126e+05      -3.533737e+01 |       32
     44       6.736826e+05      -3.004046e+01 |       32
     45       6.736567e+05      -2.585270e+01 |       32
     46       6.736337e+05      -2.301261e+01 |       32
     47       6.736146e+05      -1.908419e+01 |       32
     48       6.735970e+05      -1.757964e+01 |       32
     49       6.735806e+05      -1.647817e+01 |       32
     50       6.735672e+05      -1.334433e+01 |       31
K-means terminated without convergence after 50 iterations (objv = 673567.2127067728)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.422695
INFO: iteration 2, average log likelihood -1.417639
INFO: iteration 3, average log likelihood -1.416324
INFO: iteration 4, average log likelihood -1.415408
INFO: iteration 5, average log likelihood -1.414415
INFO: iteration 6, average log likelihood -1.413383
INFO: iteration 7, average log likelihood -1.412573
INFO: iteration 8, average log likelihood -1.412088
INFO: iteration 9, average log likelihood -1.411822
INFO: iteration 10, average log likelihood -1.411661
INFO: iteration 11, average log likelihood -1.411547
INFO: iteration 12, average log likelihood -1.411456
INFO: iteration 13, average log likelihood -1.411380
INFO: iteration 14, average log likelihood -1.411312
INFO: iteration 15, average log likelihood -1.411252
INFO: iteration 16, average log likelihood -1.411197
INFO: iteration 17, average log likelihood -1.411146
INFO: iteration 18, average log likelihood -1.411099
INFO: iteration 19, average log likelihood -1.411054
INFO: iteration 20, average log likelihood -1.411013
INFO: iteration 21, average log likelihood -1.410974
INFO: iteration 22, average log likelihood -1.410937
INFO: iteration 23, average log likelihood -1.410902
INFO: iteration 24, average log likelihood -1.410869
INFO: iteration 25, average log likelihood -1.410838
INFO: iteration 26, average log likelihood -1.410808
INFO: iteration 27, average log likelihood -1.410779
INFO: iteration 28, average log likelihood -1.410752
INFO: iteration 29, average log likelihood -1.410726
INFO: iteration 30, average log likelihood -1.410701
INFO: iteration 31, average log likelihood -1.410677
INFO: iteration 32, average log likelihood -1.410654
INFO: iteration 33, average log likelihood -1.410632
INFO: iteration 34, average log likelihood -1.410611
INFO: iteration 35, average log likelihood -1.410590
INFO: iteration 36, average log likelihood -1.410571
INFO: iteration 37, average log likelihood -1.410552
INFO: iteration 38, average log likelihood -1.410533
INFO: iteration 39, average log likelihood -1.410515
INFO: iteration 40, average log likelihood -1.410498
INFO: iteration 41, average log likelihood -1.410481
INFO: iteration 42, average log likelihood -1.410465
INFO: iteration 43, average log likelihood -1.410450
INFO: iteration 44, average log likelihood -1.410434
INFO: iteration 45, average log likelihood -1.410420
INFO: iteration 46, average log likelihood -1.410405
INFO: iteration 47, average log likelihood -1.410391
INFO: iteration 48, average log likelihood -1.410378
INFO: iteration 49, average log likelihood -1.410365
INFO: iteration 50, average log likelihood -1.410352
INFO: EM with 100000 data points 50 iterations avll -1.410352
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.00806319  -0.0403562  -0.0896829   -0.0688157  -0.0146331   -0.0189495  -0.0449005    0.00818886  -0.0316653   -0.0055038  -0.0359205    0.00366551  -0.0434769  -0.0187847    0.00808201   0.006484    0.0379959    0.0522387    0.136942    0.0270952  -0.0376456  -0.0317218   -0.0315564   0.119158   -0.0196875   0.00207488
 -0.392893     0.100829   -0.20878     -0.252085   -1.04203      0.295349    0.146489     0.654802    -0.216303     0.0330401   0.214109    -0.227368    -0.216461   -0.700108    -0.177267    -0.0373855   0.19008     -0.335478     0.47112    -0.268092    0.288112   -0.0334729   -0.317336    0.0428384   0.0934112   0.356478  
 -0.0865991   -0.177126   -0.495133    -0.389321    0.0955188    0.143491   -0.520074     0.209699     0.0753281    0.527254   -0.228662     0.202297    -0.0394869  -0.0787825   -0.212865    -0.143978   -0.538568    -0.0419332    0.871468    0.516922    0.54969    -0.617434     0.102514    0.557998   -0.103233    0.158425  
 -0.133204    -0.147052   -0.521747     0.209578   -0.170242     0.185481    0.0548122    0.143558    -0.0478192    0.327576   -0.419993    -0.143195     0.589123   -0.380588     0.169702    -0.0915435  -0.145657     0.337901     0.0796014  -0.39275    -0.627236   -0.223121    -0.155349   -0.217359    0.88439     0.607416  
  0.222        0.244601   -0.451494    -0.0635307   0.159676     0.134926   -0.262863    -0.569264    -0.246598    -0.193084    0.138824    -0.65436     -0.19858    -0.410566    -0.432252     0.393719    0.119963    -0.313491    -0.0188117  -0.0881602  -0.152098   -0.798586     0.526392   -0.169125    0.059547    0.201738  
  0.135822    -0.164753    0.350565    -0.339993    0.157372     0.452931    0.350415     0.217365    -0.0507835    0.107237    0.821962     0.056698     0.622878   -0.371732    -0.141259    -0.192479   -0.208502    -0.265025    -0.546251   -0.825216    0.114847    0.290156     0.499991   -0.244018    0.0384208   0.338493  
  0.166382    -0.0406058   0.116957     0.178911    0.624248    -0.438275   -0.259142    -0.422056     0.387684    -0.135632   -0.194778     0.168529     0.0570362   0.531519     0.047822     0.0242527  -0.173697     0.171362    -0.134073    0.370128   -0.0400272   0.0475487    0.289166   -0.0868877  -0.158232   -0.239768  
  0.36294      0.654485    0.192624     0.25838    -0.230768    -0.354558    0.352496    -0.056559     0.0859921   -0.12663     0.69229     -0.0307274   -0.359759   -0.0719665    0.0602429   -0.0815511   0.546834     0.168849    -0.230143    0.495541   -0.0718119  -0.0464148    0.672992   -0.12831    -0.801121   -0.0190841 
 -0.15063      0.706844   -0.0734321   -0.644873    0.320078     0.106734    0.449504     0.101316     0.0901042    0.563758    0.342201     0.131443     0.137828    0.0088939   -0.0452381   -0.729247    0.434625    -0.500693     0.0359362   0.331924   -0.141841   -0.157276    -0.246079    0.606503    0.159053   -0.555575  
 -0.240802    -0.638371   -0.0528998    0.321749   -0.146384     0.238987   -0.232095    -0.00200663   0.0294696   -0.289924   -0.469691     0.047608    -0.140446    0.334486     0.263119     0.317801   -0.134368     0.236516     0.316142   -0.371042    0.251552    0.404431    -0.103721   -0.111287   -0.112036    0.230964  
 -0.161566    -0.0333059   0.208915     0.06797     0.144112     0.15284     0.214096     0.0788486   -0.167473    -0.193249    0.626413     0.262837     0.169997   -0.337802    -0.0394568   -0.261013   -0.0911167    0.0171669   -0.320954   -0.196885   -0.194472    0.0415214    0.179472   -0.229134   -0.111213    0.263625  
 -0.176123     0.228844   -0.231195    -0.0592149  -0.0789929   -0.177062   -0.0624495    0.287943     0.206845     0.0861681  -0.0703959    0.0309721   -0.282664    0.211996    -0.0259384   -0.0629601   0.0304762   -0.0652818    0.312107    0.547777    0.176106   -0.175074    -0.280933    0.126303   -0.116641   -0.213373  
  0.501477     0.152248   -0.515532     0.482405   -0.652879    -0.113634   -0.794053    -0.17175      0.439801    -0.335275    0.0354941   -0.697482    -0.353342    0.543565     0.263061     0.342773    0.318487    -0.155993     0.180381    0.564162    0.384283    0.0757258   -0.19002     0.0669648   0.0484134   0.0782237 
  0.496627     0.234709    0.497934    -0.740183   -0.217439    -0.424851   -0.449253    -0.0149372   -0.730399    -0.609625   -0.0285897    0.421165    -0.372995    0.200454     0.5652       0.0456689  -0.721235     0.0974266    0.400929   -0.18176    -0.0506267  -0.210181     0.174292   -0.180735    0.0324988  -0.120828  
  0.121451     0.536243    0.215661     0.11053    -0.232467     0.180941    0.231353     0.288118    -0.0214382    0.0858989   0.303919    -0.425615     0.133649    0.254621    -0.119874     0.39799    -0.103292    -0.235712    -0.351755    0.0921394   0.184545   -0.107139    -0.0435139  -0.191925    0.106705   -0.166796  
 -0.20916      0.420586   -0.450813     0.127023   -0.180592    -0.311871   -0.132375    -0.0197412   -0.163234    -0.238045   -0.0524652   -0.74305      0.0874782  -0.111509     0.354224     0.0333671   0.354655     0.371791     0.224747   -0.142211   -0.439312   -0.395769    -0.0873779   0.425292    0.421331    0.281313  
  0.18403     -0.450513    0.336623     0.0193869   0.0439466    0.129335    0.190622     0.808288     0.246519     0.0366595  -0.0628098    0.765534     0.116699    0.249417     0.0720622   -0.211451   -0.431183    -0.395508    -0.0269062  -0.104011    0.706891    0.58143     -0.0830011  -0.116487   -0.318593   -0.38767   
 -0.406014    -0.202416    0.271107    -0.596271    0.339294    -0.692953   -0.20979     -0.775686     0.102682    -0.498288   -0.0704436    0.0299659   -0.298863   -0.00798835  -0.0279255    0.597952    0.518032     0.0781014   -0.325964    0.663968   -0.209455   -0.238232    -0.0716604   0.420206   -0.436678    0.458691  
 -0.0553568   -0.323511    0.246915    -0.748957   -0.405221     0.90605    -0.0136076   -0.368052    -0.00770409   1.04138    -0.387636     0.131932    -0.258358    0.327133    -0.504892     0.18021    -0.256332    -0.187642    -0.0954638  -0.0527785  -0.280864    0.870712    -0.553914    0.0516235   0.0901833  -0.663113  
  0.296454     0.063835    0.00595644   0.0575399   0.307236    -0.0514033   0.0506737    0.623085     0.137039     0.0323331  -0.526725    -0.641349    -0.162595    0.624643    -0.183075    -0.0167094  -0.373573    -0.0148023    0.157705   -0.246198    0.57438    -0.495255     0.254123    0.0827924   0.450075   -0.244372  
  0.231197    -0.289782   -0.170021     0.463365    0.563955    -0.0204847  -0.503349    -0.242128    -0.506056    -0.258098    0.458656     0.293553    -0.0560605  -0.357153     0.780901    -0.165262    0.279714    -0.00135815  -0.137654    0.389022   -0.771818   -0.0237664   -0.379017    0.324689   -0.643491   -0.290483  
 -0.0421152   -0.0174925   0.319431     0.178672    0.756531    -0.150966   -0.0895553   -0.495371    -0.322334    -0.091239    0.0665988    0.595196     0.192966    0.136179    -0.209495    -0.0680656  -0.622918     0.277465    -0.149357    0.337526   -0.100136    0.168675     0.449453   -0.722483   -0.114443   -0.396696  
 -0.269519     0.395442    0.00966881   0.560663   -0.00631067  -0.353356   -0.15101      0.992262     0.0158698    0.11249     0.252772    -0.316807     0.861949    0.164611     0.189928    -0.405162   -0.283955    -1.14939     -0.192202    0.437629   -0.157473   -0.0467635    0.0833625  -0.150362    0.0449653  -0.155615  
 -0.221649    -0.580829    0.338174    -0.193499   -0.211526     0.0907799   0.850515     0.28132      0.468118     0.371913   -0.265252    -0.268785    -0.0657979  -0.0186904   -0.382291     0.483591    0.579054    -0.265231    -0.0917615   0.158334    0.113538   -0.340494    -0.382945    0.754386    0.0301874   0.334841  
  0.70225     -0.0458346  -0.148524     0.603521    0.0379771   -0.160284    0.510353     0.69967      0.0980678   -0.116819    0.596508     0.267966     0.15308    -0.0472258   -0.0166784    0.260769   -0.983902    -0.316796     0.151223   -0.179054   -0.253185   -0.692849     0.746996   -0.153091    0.0601425  -0.0862888 
 -0.263196    -0.103355    0.380205    -0.236097    0.312005    -0.238762    0.461823     0.016632    -0.0930436    0.589644   -0.724474     0.722075     0.137685   -0.123984    -0.349321    -0.225905   -0.0177763    0.822825     0.0409288  -0.264863   -0.0775745  -0.00911677   0.278662   -0.137753   -0.0356484  -0.114702  
 -0.178378     0.218157   -0.753211     0.441421    0.00514199  -0.0933367   0.00022798  -0.185597     0.0876959    0.343081   -0.100758    -0.0770554    0.501734   -0.20203     -0.355239     0.123087    0.307577    -0.016364    -0.526662    0.283232   -0.704582    0.157669    -0.0526137  -0.408521    0.117618   -0.599972  
  0.246478    -0.375918    0.183705    -0.203271    0.0927146    0.274672    0.34922     -0.36358     -0.0886353   -0.223951    0.00752588   0.00337241  -0.495326    0.17391      0.431422     0.242468    0.343692     0.269639     0.188959   -0.680574   -0.0166893   0.148184     0.0749616   0.865091   -0.478692    0.292732  
  0.10546     -0.124875    0.555521     0.597344   -0.0545252   -0.199512   -0.0602013   -0.248372    -0.0173981   -0.858227   -0.148402    -0.35489      0.0390881   0.394058     0.375847     0.630351    0.135138     0.29062     -0.73671    -0.221956   -0.116196    0.296517     0.134748   -0.381354   -0.0298151   0.234059  
 -0.390201     0.084613   -0.164636    -0.260729   -0.0913074   -0.120853    0.0294508   -0.0775159    0.116335     0.394447   -0.00678734  -0.0620569   -0.514626   -0.443598     0.310198    -0.312015    0.687392     0.0979989   -0.0038807   0.315298    0.115249    0.791597    -0.521384   -0.0350535  -0.104298    0.152507  
  0.538285    -0.203106   -0.0593491    0.0323086   0.189104    -0.0148373   0.0778498   -0.0327466   -0.442739     0.300654   -0.621407     0.182704    -0.618713   -0.60233     -0.265401    -0.042786    0.00371692   0.214647     0.0950549   0.145257    0.334385    0.205444     0.022753    0.0332432   0.883824   -0.209715  
 -0.684542    -0.546895    0.0344278   -0.451161   -0.030599     0.332541   -0.149393    -0.391403     0.34615     -0.18617     0.257558     0.239354     0.681027   -0.0723976   -0.0829454   -0.179452    0.114624    -0.0399165    0.113304   -0.22655    -0.295257    0.319019    -0.433209    0.0902548  -0.488306    0.234743  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.410339
INFO: iteration 2, average log likelihood -1.410327
INFO: iteration 3, average log likelihood -1.410315
INFO: iteration 4, average log likelihood -1.410304
INFO: iteration 5, average log likelihood -1.410293
INFO: iteration 6, average log likelihood -1.410282
INFO: iteration 7, average log likelihood -1.410271
INFO: iteration 8, average log likelihood -1.410260
INFO: iteration 9, average log likelihood -1.410250
INFO: iteration 10, average log likelihood -1.410240
INFO: EM with 100000 data points 10 iterations avll -1.410240
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
