>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.7.0
INFO: Installing JLD v0.6.6
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.1
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.3.0
INFO: Installing ScikitLearnBase v0.2.1
INFO: Installing StaticArrays v0.1.0
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
WARNING: takebuf_string(b) is deprecated, use String(take!(b)) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in takebuf_string(::Base.AbstractIOBuffer{Array{UInt8,1}}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/BinDeps/src/dependencies.jl:887 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::SubString{String}) at ./sysimg.jl:14
 in evalfile(::SubString{String}, ::Array{String,1}) at ./loading.jl:572 (repeats 2 times)
 in cd(::##2#4, ::String) at ./file.jl:69
 in (::##1#3)(::IOStream) at ./none:13
 in open(::##1#3, ::String, ::String) at ./iostream.jl:152
 in eval(::Module, ::Any) at ./boot.jl:236
 in process_options(::Base.JLOptions) at ./client.jl:248
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/Rmath/deps/build.jl, in expression starting on line 39
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1320
Commit 8dd8ea9 (2016-11-26 22:03 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-101-generic #148-Ubuntu SMP Thu Oct 20 22:08:32 UTC 2016 x86_64 x86_64
Memory: 2.939281463623047 GB (677.5703125 MB free)
Uptime: 27738.0 sec
Load Avg:  1.19580078125  1.025390625  1.02392578125
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3500 MHz    1637772 s       6529 s     159996 s     678755 s         51 s
#2  3500 MHz     788943 s         80 s      86268 s    1792987 s          1 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.4
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.7.0
 - JLD                           0.6.6
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.1
 - Rmath                         0.1.4
 - SHA                           0.3.0
 - ScikitLearnBase               0.2.1
 - StaticArrays                  0.1.0
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: takebuf_string(b) is deprecated, use String(take!(b)) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in takebuf_string(::Base.AbstractIOBuffer{Array{UInt8,1}}) at ./deprecated.jl:50
 in #_write#17(::Array{Any,1}, ::Function, ::JLD.JldFile, ::String, ::Array{Any,1}, ::JLD.JldWriteSession) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:587
 in #write#14(::Array{Any,1}, ::Function, ::JLD.JldFile, ::String, ::Array{Any,1}, ::JLD.JldWriteSession) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:529
 in #jldopen#9(::Bool, ::Bool, ::Bool, ::Function, ::String, ::Bool, ::Bool, ::Bool, ::Bool, ::Bool) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:198
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::String, ::Bool, ::Bool, ::Bool, ::Bool, ::Bool) at ./<missing>:0
 in #jldopen#10(::Bool, ::Bool, ::Bool, ::Function, ::String, ::String) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:253
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::String, ::String) at ./<missing>:0
 in #jldopen#11(::Array{Any,1}, ::Function, ::JLD.##34#35{String,Array{Float64,2},Tuple{}}, ::String, ::Vararg{String,N}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:263
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::Function, ::String, ::String) at ./<missing>:0
 in #save#33(::Bool, ::Bool, ::Function, ::FileIO.File{FileIO.DataFormat{:JLD}}, ::String, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:1217
 in save(::FileIO.File{FileIO.DataFormat{:JLD}}, ::String, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:1214
 in #save#14(::Array{Any,1}, ::Function, ::String, ::String, ::Vararg{Any,N}) at /home/vagrant/.julia/v0.6/FileIO/src/loadsave.jl:54
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:8 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-7.164212059533123e6,[36951.2,63048.8],
[6592.2 1630.27 12370.5; -6726.85 -1867.91 -11993.4],

Array{Float64,2}[
[25366.5 9989.59 -11156.7; 9989.59 30272.4 9001.56; -11156.7 9001.56 26221.6],

[74927.8 -10306.4 11026.4; -10306.4 71033.9 -8918.22; 11026.4 -8918.22 73664.2]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.330617e+03
      1       1.030151e+03      -3.004661e+02 |        5
      2       9.762975e+02      -5.385325e+01 |        0
      3       9.762975e+02       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 976.2975247799964)
INFO: K-means with 272 data points using 3 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.064615
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.686765
INFO: iteration 2, lowerbound -3.548112
INFO: iteration 3, lowerbound -3.417689
INFO: iteration 4, lowerbound -3.288804
INFO: dropping number of Gaussions to 7
INFO: iteration 5, lowerbound -3.166821
INFO: iteration 6, lowerbound -3.058780
INFO: dropping number of Gaussions to 6
INFO: iteration 7, lowerbound -2.968071
INFO: dropping number of Gaussions to 5
INFO: iteration 8, lowerbound -2.866826
INFO: iteration 9, lowerbound -2.756377
INFO: dropping number of Gaussions to 4
INFO: iteration 10, lowerbound -2.633233
INFO: iteration 11, lowerbound -2.517264
INFO: iteration 12, lowerbound -2.431919
INFO: iteration 13, lowerbound -2.378507
INFO: dropping number of Gaussions to 3
INFO: iteration 14, lowerbound -2.341802
INFO: iteration 15, lowerbound -2.316185
INFO: iteration 16, lowerbound -2.307487
INFO: dropping number of Gaussions to 2
INFO: iteration 17, lowerbound -2.302973
INFO: iteration 18, lowerbound -2.299262
INFO: iteration 19, lowerbound -2.299257
INFO: iteration 20, lowerbound -2.299255
INFO: iteration 21, lowerbound -2.299254
INFO: iteration 22, lowerbound -2.299253
INFO: iteration 23, lowerbound -2.299253
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Sun 27 Nov 2016 01:12:07 PM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Sun 27 Nov 2016 01:12:09 PM UTC: K-means with 272 data points using 3 iterations
11.3 data points per parameter
,Sun 27 Nov 2016 01:12:10 PM UTC: EM with 272 data points 0 iterations avll -2.064615
5.8 data points per parameter
,Sun 27 Nov 2016 01:12:11 PM UTC: GMM converted to Variational GMM
,Sun 27 Nov 2016 01:12:12 PM UTC: iteration 1, lowerbound -3.686765
,Sun 27 Nov 2016 01:12:13 PM UTC: iteration 2, lowerbound -3.548112
,Sun 27 Nov 2016 01:12:13 PM UTC: iteration 3, lowerbound -3.417689
,Sun 27 Nov 2016 01:12:13 PM UTC: iteration 4, lowerbound -3.288804
,Sun 27 Nov 2016 01:12:13 PM UTC: dropping number of Gaussions to 7
,Sun 27 Nov 2016 01:12:13 PM UTC: iteration 5, lowerbound -3.166821
,Sun 27 Nov 2016 01:12:13 PM UTC: iteration 6, lowerbound -3.058780
,Sun 27 Nov 2016 01:12:13 PM UTC: dropping number of Gaussions to 6
,Sun 27 Nov 2016 01:12:13 PM UTC: iteration 7, lowerbound -2.968071
,Sun 27 Nov 2016 01:12:13 PM UTC: dropping number of Gaussions to 5
,Sun 27 Nov 2016 01:12:13 PM UTC: iteration 8, lowerbound -2.866826
,Sun 27 Nov 2016 01:12:13 PM UTC: iteration 9, lowerbound -2.756377
,Sun 27 Nov 2016 01:12:13 PM UTC: dropping number of Gaussions to 4
,Sun 27 Nov 2016 01:12:13 PM UTC: iteration 10, lowerbound -2.633233
,Sun 27 Nov 2016 01:12:13 PM UTC: iteration 11, lowerbound -2.517264
,Sun 27 Nov 2016 01:12:14 PM UTC: iteration 12, lowerbound -2.431919
,Sun 27 Nov 2016 01:12:14 PM UTC: iteration 13, lowerbound -2.378507
,Sun 27 Nov 2016 01:12:14 PM UTC: dropping number of Gaussions to 3
,Sun 27 Nov 2016 01:12:14 PM UTC: iteration 14, lowerbound -2.341802
,Sun 27 Nov 2016 01:12:14 PM UTC: iteration 15, lowerbound -2.316185
,Sun 27 Nov 2016 01:12:14 PM UTC: iteration 16, lowerbound -2.307487
,Sun 27 Nov 2016 01:12:14 PM UTC: dropping number of Gaussions to 2
,Sun 27 Nov 2016 01:12:14 PM UTC: iteration 17, lowerbound -2.302973
,Sun 27 Nov 2016 01:12:14 PM UTC: iteration 18, lowerbound -2.299262
,Sun 27 Nov 2016 01:12:14 PM UTC: iteration 19, lowerbound -2.299257
,Sun 27 Nov 2016 01:12:14 PM UTC: iteration 20, lowerbound -2.299255
,Sun 27 Nov 2016 01:12:14 PM UTC: iteration 21, lowerbound -2.299254
,Sun 27 Nov 2016 01:12:14 PM UTC: iteration 22, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:14 PM UTC: iteration 23, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:14 PM UTC: iteration 24, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:14 PM UTC: iteration 25, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:14 PM UTC: iteration 26, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:15 PM UTC: iteration 27, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:15 PM UTC: iteration 28, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:15 PM UTC: iteration 29, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:15 PM UTC: iteration 30, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:15 PM UTC: iteration 31, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:15 PM UTC: iteration 32, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:15 PM UTC: iteration 33, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:15 PM UTC: iteration 34, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:15 PM UTC: iteration 35, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:15 PM UTC: iteration 36, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:15 PM UTC: iteration 37, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:15 PM UTC: iteration 38, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:16 PM UTC: iteration 39, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:16 PM UTC: iteration 40, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:16 PM UTC: iteration 41, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:16 PM UTC: iteration 42, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:16 PM UTC: iteration 43, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:16 PM UTC: iteration 44, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:16 PM UTC: iteration 45, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:16 PM UTC: iteration 46, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:16 PM UTC: iteration 47, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:16 PM UTC: iteration 48, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:16 PM UTC: iteration 49, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:16 PM UTC: iteration 50, lowerbound -2.299253
,Sun 27 Nov 2016 01:12:16 PM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.045,95.9549]
β = [178.045,95.9549]
m = [4.2503 79.2869; 2.00023 53.852]
ν = [180.045,97.9549]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.184042 -0.00764405; 0.0 0.00858171],

[0.375876 -0.00895312; 0.0 0.0127487]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 99999.99999999999
avll from stats: -0.9884256917595202
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9884256917595131
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9884256917595131
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -0.9980699953592759
avll from llpg:  -0.9980699953592759
avll direct:     -0.9980699953592758
sum posterior: 100000.0
32×26 Array{Float64,2}:
 -0.1447      -0.093655   -0.0494136   -0.0343175    0.107748      0.0718535    0.0374867    -0.059811    -0.0619389    -0.0521548    0.089661      0.190746    -0.1376     -0.193247     0.0314678   -0.207145     -0.018629     -0.035421    0.0844819   -0.00738453   0.141624     0.0671117   -0.0117141   -0.0730676   -0.0249414    0.00518829
 -0.135567     0.303764    0.13024     -0.118299    -0.163258     -0.0297765   -0.229448     -0.0598488   -0.0747229     0.190496    -0.15233      -0.00894887  -0.104515    0.063645     0.0646138    0.0120436     0.0857209    -0.132846   -0.100742     0.115018     0.118355     0.175669    -0.00132733  -0.0632707   -0.0602735    0.170811  
  0.0919848   -0.0723877  -0.0156845   -0.172382    -0.0920522     0.0441631   -0.0173855    -0.0937026   -0.00975414   -0.0866118   -0.0567234    -0.026628     0.0656977  -0.166293     0.0431937   -0.000904231   0.089893      0.132593   -0.020194    -0.0271865    0.0203344    0.0701026    0.0209704   -0.0325768   -0.0583563    0.0581969 
 -0.00718045   0.0307202   0.163737     0.00345828  -0.196382      0.0598038    0.112075     -0.132423     0.0406492    -0.171012     0.0420988     0.0855541   -0.0564065  -0.11129      0.0899116   -0.124002      0.04505       0.0959833   0.0179647   -0.0502899   -0.0463614   -0.12404     -0.0467886    0.117458     0.114939     0.0386355 
  0.00658459  -0.162404   -0.057652     0.164173    -0.0103844     0.188113     0.057586      0.0888649   -0.044708      0.0593016    0.255106     -0.103589    -0.0674152  -0.0239678    0.0795983   -0.133244      0.087453     -0.0809928  -0.0408483   -0.0135763   -0.102703     0.102578    -0.0315191    0.132109    -0.0604406    0.152739  
  0.0131139   -0.0665734  -0.12287     -0.0386412    0.111354     -0.241245    -0.0352678     0.107582     0.0809884     0.126889    -0.0503566    -0.045522     0.155018   -0.081703     0.0142889    0.115832     -0.0536787    -0.0505112   0.152551     0.0780146   -0.00428055  -0.191099    -0.0226551    0.187356     0.0664666   -0.00973168
  0.175527     0.133219    0.148348     0.044193    -0.294823      0.0122578   -0.0417803    -0.0374535    0.0458358    -0.0824728    0.165786      0.0933295   -0.0723389   0.0703989    0.077334    -0.0660209    -0.0743924    -0.0376886  -0.0161224    0.0177316   -0.0358225   -0.0169754   -0.09267      0.017139    -0.00589569   0.0508158 
 -0.0844911    0.0253061   0.0871049   -0.0338375    0.0578133     0.0486746   -0.0278945    -0.0235672    0.0743962    -0.138009    -0.00029434    0.0461501   -0.157746    0.016121     0.0582768    0.0225115    -0.0392737    -0.10993     0.0115394   -0.118502     0.0236895   -0.237127     0.0542985    0.143249     0.0628344    0.129276  
 -0.00717743  -0.0770916  -0.173103    -0.0946049    0.0210252    -0.246299     0.133876     -0.0528467   -0.147863      0.185164    -0.000428954   0.129326     0.150479    0.116668     0.0460822    0.000422029   0.000267559   0.114221    0.00523023   0.0940054   -0.0684847   -0.0480888    0.0425343   -0.0616322   -0.201055     0.00994643
 -0.231522    -0.0541656   0.0119146   -0.0176278    0.0360175     0.00612901  -0.122736     -0.0410379   -0.198789      0.0736032   -0.0721375     0.106547    -0.0457688  -0.121459    -0.0364445    0.050433      0.0632989     0.0877459   0.025093     0.132631    -0.00430178   0.00114143  -0.0213334    0.0833898   -0.0248973   -0.0580048 
  0.0105812    0.0832498   0.128525    -0.254416     0.176009     -0.0190338    0.0406458     0.0367421    0.102943      0.0889902   -0.125268     -0.200865    -0.0147817  -0.13555     -0.0231401    0.0241522     0.185222      0.0210045  -0.00271611  -0.0110207    0.0434861    0.0187402   -0.111179    -0.141812    -0.0213164   -0.0375896 
 -0.044569     0.100803   -0.072605     0.0233935    0.0326118     0.0636532   -0.026499     -0.0501988   -0.0431294    -0.123183     0.174976      0.164886    -0.0607871   0.102282     0.0261914    0.118371     -0.185448      0.0936166   0.133317     0.18908      0.0463736   -0.122783    -0.0745971   -0.0665484    0.0807199    0.00706569
 -0.00715452   0.0519196   0.0245718    0.0979969   -0.0153457     0.068078     0.00177339   -0.163894    -0.00911812   -0.204731    -0.135694     -0.00672809   0.0245468   0.0183351   -0.227915    -0.130132      0.0411594     0.122579    0.0209004    0.189854    -0.00532648   0.064892    -0.0585621    0.0983846   -0.0202416   -0.0982593 
  0.0488934    0.0868949   0.0218284    0.0627762   -0.044216     -0.158421     0.0511066     0.132953    -0.0386876     0.00161241   0.343764     -0.10748      0.144963   -0.131205     0.0442395    0.105242      0.00538208    0.149225    0.111639    -0.0612074    0.069463     0.077678    -0.00717827   0.0528273   -0.0279418   -0.131905  
 -0.0663913   -0.0338683   0.180274    -0.0338156    0.0584994    -0.108993     0.110535     -0.0988843    0.000100485   0.0761592   -0.0970772    -0.0843797    0.173653    0.104402     0.0718398    0.169396      0.154134     -0.0253877  -0.0162407    0.086516     0.0664832    0.0562285    0.184113    -0.053571    -0.030334     0.00450736
  0.0391822    0.18511    -0.15456      0.157778     0.0586974     0.0724325   -0.0113797     0.0205451    0.0306593     0.106291    -0.0582179     0.0976343    0.0576646  -0.0412164   -0.00472765  -0.109379      0.126178      0.031718    0.14455      0.0534442    0.0130055   -0.114998     0.14332     -0.00354118   0.0780406   -0.0355908 
  0.0103425   -0.0114235   0.104549     0.118192     0.101924     -0.233687     0.0685936    -0.0430348    0.132433     -0.126206     0.00723014    0.107678     0.319105   -0.0403812    0.0331018    0.0803858    -0.15554      -0.133423   -0.192942    -0.0193754    0.0476944   -0.0922621   -0.21724     -0.0396772    0.103974    -0.0263513 
  0.0830204   -0.0209812   0.0613866    0.0223641   -0.0240838     0.0737329   -0.183447      0.0452326   -0.117252      0.0666369    0.0405246     0.0868577   -0.0516034  -0.00157792  -0.063248    -0.153088     -0.131103      0.147306    0.0414268   -0.00582377   0.114441    -0.132712     0.0642886   -0.177067    -0.0968236    0.00911794
 -0.0834505    0.152313   -0.275069    -0.0950439    0.000425814   0.0174279    0.0918439    -0.0698348   -0.0521252    -0.0759295    0.104543      0.0253816   -0.0690312  -0.099059     0.00584475   0.0357183    -0.0519641     0.0504491  -0.0334705   -0.0236573   -0.0788707    0.128527    -0.00519053   0.214258    -0.0367787   -0.115155  
  0.0672786   -0.0873105   0.052271    -0.137888     0.0323446    -0.142908     0.0748787    -0.108981     0.181196      0.0898521    0.108953      0.0902622   -0.0238704   0.0814606   -0.164691     0.0021599     0.0511357     0.0967884  -0.0526003   -0.0257651   -0.0357254    0.0126406    0.0398973   -0.00485043   0.051467    -0.0664607 
  0.114743    -0.164813   -0.0884926   -0.086694     0.14204      -0.106789     0.0818624     0.0719861   -0.140066      0.0542208    0.13921       0.258357    -0.039047   -0.126131     0.0452197   -0.0707914     0.0984652     0.0414666   0.0264275   -0.134224     0.0272081   -0.0193163   -0.0539258   -0.137321    -0.0779166   -0.0663073 
 -0.0196646    0.121661    0.0241814   -0.155037     0.0721311     0.119872     0.210285      0.00177921  -0.00369672   -0.150332    -0.0972664    -0.10304      0.03469    -0.0884223    0.0339857    0.0944715     0.0939511    -0.136238   -0.121204    -0.130996     0.0210818    0.0263639    0.0379713   -0.108152     0.032222     0.0166062 
 -0.0121996    0.0473353  -0.00940201   0.0164498   -0.04821      -0.00745562  -0.000169422  -0.0693752   -0.169587      0.025075     0.0629609    -0.161532    -0.0961868   0.077064     0.0385024   -0.010741      0.0189158     0.156524   -0.0201665   -0.0147126    0.0275726    0.00328422  -0.0334002   -0.166586     0.12244      0.0139463 
  0.0685553   -0.0351326   0.0314315   -0.127918     0.0759235    -0.0474604   -0.00783976    0.0219434    0.0907105    -0.0766557    0.14799      -0.162024     0.0349089  -0.0940048    0.0405198    0.0814558    -0.0682833    -0.100319    0.0518119    0.0428517   -0.194977     0.00728426   0.0389309    0.0712521    0.0166119    0.0722881 
  0.18062      0.178492    0.0875265    0.0470099   -0.0660519    -0.0316735   -0.0356       -0.091504    -0.112455      0.070503    -0.206503     -0.0407611   -0.0600746  -0.123954    -0.0313658    0.0349114     0.0827027     0.103024    0.183742    -0.013451     0.0972308   -0.0552614    0.0643692    0.110148     0.0341374   -0.139262  
 -0.109526     0.0217394  -0.0145389   -0.0415734   -0.0969177     0.0603202   -0.0222199     0.0566613    0.0664302    -0.163143     0.0277012     0.0852294   -0.0853333   0.0449613   -0.11089      0.0421142    -0.0289691    -0.189647   -0.0684428    0.00370554  -0.0512846    0.103328     0.0403887    0.0734577   -0.0548434   -0.0226088 
  0.0844963    0.163346   -0.00693461  -0.0966746   -0.127864     -0.0367527   -0.0877506     0.0382534   -0.03585      -0.0542855   -0.0980853     0.055449    -0.0249467   0.0157705   -0.0144489   -0.0270814    -0.0410616    -0.0819707   0.161926     0.0108507    0.0674638    0.0148422    0.0746715   -0.0409983   -0.12529      0.127188  
  0.0874493   -0.165154   -0.217855    -0.0427464    0.0510738    -0.147638     0.0273595    -0.0370825    0.18572       0.0105717   -0.0885637     0.113364     0.0412655  -0.0639064   -0.130316     0.0413498    -0.0613105     0.0700828   0.0458001   -0.175674    -0.0190689    0.105697     0.281645     0.0687455   -0.24983      0.176317  
 -0.100373     0.0057674   0.129373     0.0596874   -0.0227945     0.109977     0.120023     -0.054004    -0.159335      0.00584397  -0.100941      0.152706    -0.129113    0.146631    -0.182498     0.162149      0.116262     -0.0889284   0.0466368    0.16747     -0.122484    -0.136226     0.0800252    0.328621     0.106838    -0.171777  
  0.0405175   -0.016209    0.0208423    0.0278207    0.0114847    -0.0124449   -0.00854372   -0.0512334    0.0323738    -0.0255565    0.119887     -0.110968     0.0346945   0.059098     0.218718    -0.113147     -0.118126     -0.0958767  -0.0478089    0.155092     0.0144102   -0.0496432    0.0600953    0.0413198   -0.0415438   -0.0135206 
  0.272572    -0.0936879  -0.0111478    0.0469428    0.109243     -0.00799487   0.0639137     0.133062     0.106079     -0.030804     0.0533458    -0.0204075   -0.0135505   0.00609588   0.134174    -0.160939      0.220547      0.0673658  -0.0103234    0.00545311   0.116851    -0.1551      -0.165792     0.11438     -0.0352875   -0.0703127 
 -0.0979659   -0.106387    0.114694    -0.200461    -0.0670039     0.054482     0.0886126    -0.0395837   -0.0156404     0.0133951   -0.0818202    -0.023058    -0.033713   -0.163622    -0.0376795   -0.102391     -0.000185982  -0.0108854   0.0215026    0.0859239    0.00047331   0.0665393    0.0146702    0.0378995   -0.0466554    0.153463  kind diag, method split
0: avll = -1.3702178866539907
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.370288
INFO: iteration 2, average log likelihood -1.370215
INFO: iteration 3, average log likelihood -1.369780
INFO: iteration 4, average log likelihood -1.364796
INFO: iteration 5, average log likelihood -1.350755
INFO: iteration 6, average log likelihood -1.343772
INFO: iteration 7, average log likelihood -1.342085
INFO: iteration 8, average log likelihood -1.341127
INFO: iteration 9, average log likelihood -1.340372
INFO: iteration 10, average log likelihood -1.339730
INFO: iteration 11, average log likelihood -1.339204
INFO: iteration 12, average log likelihood -1.338815
INFO: iteration 13, average log likelihood -1.338528
INFO: iteration 14, average log likelihood -1.338304
INFO: iteration 15, average log likelihood -1.338118
INFO: iteration 16, average log likelihood -1.337953
INFO: iteration 17, average log likelihood -1.337800
INFO: iteration 18, average log likelihood -1.337649
INFO: iteration 19, average log likelihood -1.337493
INFO: iteration 20, average log likelihood -1.337314
INFO: iteration 21, average log likelihood -1.337059
INFO: iteration 22, average log likelihood -1.336389
INFO: iteration 23, average log likelihood -1.334844
INFO: iteration 24, average log likelihood -1.333891
INFO: iteration 25, average log likelihood -1.333297
INFO: iteration 26, average log likelihood -1.332871
INFO: iteration 27, average log likelihood -1.332542
INFO: iteration 28, average log likelihood -1.332280
INFO: iteration 29, average log likelihood -1.332070
INFO: iteration 30, average log likelihood -1.331887
INFO: iteration 31, average log likelihood -1.331722
INFO: iteration 32, average log likelihood -1.331577
INFO: iteration 33, average log likelihood -1.331451
INFO: iteration 34, average log likelihood -1.331345
INFO: iteration 35, average log likelihood -1.331256
INFO: iteration 36, average log likelihood -1.331184
INFO: iteration 37, average log likelihood -1.331127
INFO: iteration 38, average log likelihood -1.331082
INFO: iteration 39, average log likelihood -1.331046
INFO: iteration 40, average log likelihood -1.331017
INFO: iteration 41, average log likelihood -1.330993
INFO: iteration 42, average log likelihood -1.330974
INFO: iteration 43, average log likelihood -1.330959
INFO: iteration 44, average log likelihood -1.330946
INFO: iteration 45, average log likelihood -1.330935
INFO: iteration 46, average log likelihood -1.330927
INFO: iteration 47, average log likelihood -1.330919
INFO: iteration 48, average log likelihood -1.330913
INFO: iteration 49, average log likelihood -1.330908
INFO: iteration 50, average log likelihood -1.330904
INFO: EM with 100000 data points 50 iterations avll -1.330904
952.4 data points per parameter
1: avll = [-1.37029,-1.37022,-1.36978,-1.3648,-1.35075,-1.34377,-1.34209,-1.34113,-1.34037,-1.33973,-1.3392,-1.33881,-1.33853,-1.3383,-1.33812,-1.33795,-1.3378,-1.33765,-1.33749,-1.33731,-1.33706,-1.33639,-1.33484,-1.33389,-1.3333,-1.33287,-1.33254,-1.33228,-1.33207,-1.33189,-1.33172,-1.33158,-1.33145,-1.33134,-1.33126,-1.33118,-1.33113,-1.33108,-1.33105,-1.33102,-1.33099,-1.33097,-1.33096,-1.33095,-1.33094,-1.33093,-1.33092,-1.33091,-1.33091,-1.3309]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.331011
INFO: iteration 2, average log likelihood -1.330891
INFO: iteration 3, average log likelihood -1.330311
INFO: iteration 4, average log likelihood -1.325739
INFO: iteration 5, average log likelihood -1.311798
INFO: iteration 6, average log likelihood -1.300244
INFO: iteration 7, average log likelihood -1.295883
INFO: iteration 8, average log likelihood -1.293692
INFO: iteration 9, average log likelihood -1.292090
INFO: iteration 10, average log likelihood -1.290779
INFO: iteration 11, average log likelihood -1.289732
INFO: iteration 12, average log likelihood -1.288888
INFO: iteration 13, average log likelihood -1.288185
INFO: iteration 14, average log likelihood -1.287598
INFO: iteration 15, average log likelihood -1.287117
INFO: iteration 16, average log likelihood -1.286730
INFO: iteration 17, average log likelihood -1.286418
INFO: iteration 18, average log likelihood -1.286173
INFO: iteration 19, average log likelihood -1.285988
INFO: iteration 20, average log likelihood -1.285853
INFO: iteration 21, average log likelihood -1.285756
INFO: iteration 22, average log likelihood -1.285689
INFO: iteration 23, average log likelihood -1.285642
INFO: iteration 24, average log likelihood -1.285608
INFO: iteration 25, average log likelihood -1.285584
INFO: iteration 26, average log likelihood -1.285566
INFO: iteration 27, average log likelihood -1.285553
INFO: iteration 28, average log likelihood -1.285542
INFO: iteration 29, average log likelihood -1.285534
INFO: iteration 30, average log likelihood -1.285528
INFO: iteration 31, average log likelihood -1.285522
INFO: iteration 32, average log likelihood -1.285518
INFO: iteration 33, average log likelihood -1.285515
INFO: iteration 34, average log likelihood -1.285512
INFO: iteration 35, average log likelihood -1.285509
INFO: iteration 36, average log likelihood -1.285508
INFO: iteration 37, average log likelihood -1.285506
INFO: iteration 38, average log likelihood -1.285505
INFO: iteration 39, average log likelihood -1.285504
INFO: iteration 40, average log likelihood -1.285503
INFO: iteration 41, average log likelihood -1.285502
INFO: iteration 42, average log likelihood -1.285502
INFO: iteration 43, average log likelihood -1.285501
INFO: iteration 44, average log likelihood -1.285501
INFO: iteration 45, average log likelihood -1.285501
INFO: iteration 46, average log likelihood -1.285500
INFO: iteration 47, average log likelihood -1.285500
INFO: iteration 48, average log likelihood -1.285500
INFO: iteration 49, average log likelihood -1.285500
INFO: iteration 50, average log likelihood -1.285500
INFO: EM with 100000 data points 50 iterations avll -1.285500
473.9 data points per parameter
2: avll = [-1.33101,-1.33089,-1.33031,-1.32574,-1.3118,-1.30024,-1.29588,-1.29369,-1.29209,-1.29078,-1.28973,-1.28889,-1.28819,-1.2876,-1.28712,-1.28673,-1.28642,-1.28617,-1.28599,-1.28585,-1.28576,-1.28569,-1.28564,-1.28561,-1.28558,-1.28557,-1.28555,-1.28554,-1.28553,-1.28553,-1.28552,-1.28552,-1.28551,-1.28551,-1.28551,-1.28551,-1.28551,-1.2855,-1.2855,-1.2855,-1.2855,-1.2855,-1.2855,-1.2855,-1.2855,-1.2855,-1.2855,-1.2855,-1.2855,-1.2855]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.285647
INFO: iteration 2, average log likelihood -1.285489
INFO: iteration 3, average log likelihood -1.284839
INFO: iteration 4, average log likelihood -1.278708
INFO: iteration 5, average log likelihood -1.260232
INFO: iteration 6, average log likelihood -1.244666
INFO: iteration 7, average log likelihood -1.238004
INFO: iteration 8, average log likelihood -1.234319
INFO: iteration 9, average log likelihood -1.231751
INFO: iteration 10, average log likelihood -1.229485
INFO: iteration 11, average log likelihood -1.227121
INFO: iteration 12, average log likelihood -1.224896
INFO: iteration 13, average log likelihood -1.223127
INFO: iteration 14, average log likelihood -1.221877
INFO: iteration 15, average log likelihood -1.220955
INFO: iteration 16, average log likelihood -1.220173
INFO: iteration 17, average log likelihood -1.219489
INFO: iteration 18, average log likelihood -1.218959
INFO: iteration 19, average log likelihood -1.218606
INFO: iteration 20, average log likelihood -1.218390
INFO: iteration 21, average log likelihood -1.218274
INFO: iteration 22, average log likelihood -1.218214
INFO: iteration 23, average log likelihood -1.218181
INFO: iteration 24, average log likelihood -1.218161
INFO: iteration 25, average log likelihood -1.218147
INFO: iteration 26, average log likelihood -1.218136
INFO: iteration 27, average log likelihood -1.218126
INFO: iteration 28, average log likelihood -1.218116
INFO: iteration 29, average log likelihood -1.218106
INFO: iteration 30, average log likelihood -1.218094
INFO: iteration 31, average log likelihood -1.218081
INFO: iteration 32, average log likelihood -1.218065
INFO: iteration 33, average log likelihood -1.218045
INFO: iteration 34, average log likelihood -1.218018
INFO: iteration 35, average log likelihood -1.217976
INFO: iteration 36, average log likelihood -1.217908
INFO: iteration 37, average log likelihood -1.217791
INFO: iteration 38, average log likelihood -1.217595
INFO: iteration 39, average log likelihood -1.217297
INFO: iteration 40, average log likelihood -1.217023
INFO: iteration 41, average log likelihood -1.216889
INFO: iteration 42, average log likelihood -1.216833
INFO: iteration 43, average log likelihood -1.216805
INFO: iteration 44, average log likelihood -1.216786
INFO: iteration 45, average log likelihood -1.216773
INFO: iteration 46, average log likelihood -1.216764
INFO: iteration 47, average log likelihood -1.216757
INFO: iteration 48, average log likelihood -1.216753
INFO: iteration 49, average log likelihood -1.216750
INFO: iteration 50, average log likelihood -1.216748
INFO: EM with 100000 data points 50 iterations avll -1.216748
236.4 data points per parameter
3: avll = [-1.28565,-1.28549,-1.28484,-1.27871,-1.26023,-1.24467,-1.238,-1.23432,-1.23175,-1.22949,-1.22712,-1.2249,-1.22313,-1.22188,-1.22095,-1.22017,-1.21949,-1.21896,-1.21861,-1.21839,-1.21827,-1.21821,-1.21818,-1.21816,-1.21815,-1.21814,-1.21813,-1.21812,-1.21811,-1.21809,-1.21808,-1.21807,-1.21805,-1.21802,-1.21798,-1.21791,-1.21779,-1.2176,-1.2173,-1.21702,-1.21689,-1.21683,-1.2168,-1.21679,-1.21677,-1.21676,-1.21676,-1.21675,-1.21675,-1.21675]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.216925
INFO: iteration 2, average log likelihood -1.216651
INFO: iteration 3, average log likelihood -1.214697
WARNING: Variances had to be floored 2
INFO: iteration 4, average log likelihood -1.196615
WARNING: Variances had to be floored 2
INFO: iteration 5, average log likelihood -1.163503
WARNING: Variances had to be floored 2 3 9
INFO: iteration 6, average log likelihood -1.142877
WARNING: Variances had to be floored 2
INFO: iteration 7, average log likelihood -1.154958
WARNING: Variances had to be floored 2
INFO: iteration 8, average log likelihood -1.135951
WARNING: Variances had to be floored 2 3 9
INFO: iteration 9, average log likelihood -1.123470
WARNING: Variances had to be floored 2
INFO: iteration 10, average log likelihood -1.142458
WARNING: Variances had to be floored 2 9
INFO: iteration 11, average log likelihood -1.126937
WARNING: Variances had to be floored 2 3
INFO: iteration 12, average log likelihood -1.125519
WARNING: Variances had to be floored 9
INFO: iteration 13, average log likelihood -1.133691
WARNING: Variances had to be floored 2
INFO: iteration 14, average log likelihood -1.125654
WARNING: Variances had to be floored 3 9
INFO: iteration 15, average log likelihood -1.119202
WARNING: Variances had to be floored 2
INFO: iteration 16, average log likelihood -1.133984
WARNING: Variances had to be floored 9
INFO: iteration 17, average log likelihood -1.124533
WARNING: Variances had to be floored 2 3
INFO: iteration 18, average log likelihood -1.119505
WARNING: Variances had to be floored 9
INFO: iteration 19, average log likelihood -1.132961
WARNING: Variances had to be floored 2
INFO: iteration 20, average log likelihood -1.124864
WARNING: Variances had to be floored 3 9
INFO: iteration 21, average log likelihood -1.118316
WARNING: Variances had to be floored 2
INFO: iteration 22, average log likelihood -1.133242
WARNING: Variances had to be floored 2 9
INFO: iteration 23, average log likelihood -1.123713
WARNING: Variances had to be floored 2 3
INFO: iteration 24, average log likelihood -1.123059
WARNING: Variances had to be floored 2 9
INFO: iteration 25, average log likelihood -1.132299
WARNING: Variances had to be floored 2
INFO: iteration 26, average log likelihood -1.128658
WARNING: Variances had to be floored 3 9
INFO: iteration 27, average log likelihood -1.117788
WARNING: Variances had to be floored 2
INFO: iteration 28, average log likelihood -1.132899
WARNING: Variances had to be floored 2 9
INFO: iteration 29, average log likelihood -1.123454
WARNING: Variances had to be floored 2 3
INFO: iteration 30, average log likelihood -1.122861
WARNING: Variances had to be floored 2 9
INFO: iteration 31, average log likelihood -1.132145
WARNING: Variances had to be floored 2
INFO: iteration 32, average log likelihood -1.128566
WARNING: Variances had to be floored 2 3 9
INFO: iteration 33, average log likelihood -1.117741
WARNING: Variances had to be floored 2
INFO: iteration 34, average log likelihood -1.137263
WARNING: Variances had to be floored 2 9
INFO: iteration 35, average log likelihood -1.123429
WARNING: Variances had to be floored 2 3
INFO: iteration 36, average log likelihood -1.122898
WARNING: Variances had to be floored 2 9
INFO: iteration 37, average log likelihood -1.132126
WARNING: Variances had to be floored 2
INFO: iteration 38, average log likelihood -1.128568
WARNING: Variances had to be floored 2 3 9
INFO: iteration 39, average log likelihood -1.117753
WARNING: Variances had to be floored 2
INFO: iteration 40, average log likelihood -1.137260
WARNING: Variances had to be floored 2 9
INFO: iteration 41, average log likelihood -1.123428
WARNING: Variances had to be floored 2 3
INFO: iteration 42, average log likelihood -1.122906
WARNING: Variances had to be floored 2 9
INFO: iteration 43, average log likelihood -1.132123
WARNING: Variances had to be floored 2
INFO: iteration 44, average log likelihood -1.128569
WARNING: Variances had to be floored 2 3 9
INFO: iteration 45, average log likelihood -1.117755
WARNING: Variances had to be floored 2
INFO: iteration 46, average log likelihood -1.137260
WARNING: Variances had to be floored 2 9
INFO: iteration 47, average log likelihood -1.123428
WARNING: Variances had to be floored 2 3
INFO: iteration 48, average log likelihood -1.122907
WARNING: Variances had to be floored 2 9
INFO: iteration 49, average log likelihood -1.132123
WARNING: Variances had to be floored 2
INFO: iteration 50, average log likelihood -1.128569
INFO: EM with 100000 data points 50 iterations avll -1.128569
118.1 data points per parameter
4: avll = [-1.21693,-1.21665,-1.2147,-1.19662,-1.1635,-1.14288,-1.15496,-1.13595,-1.12347,-1.14246,-1.12694,-1.12552,-1.13369,-1.12565,-1.1192,-1.13398,-1.12453,-1.1195,-1.13296,-1.12486,-1.11832,-1.13324,-1.12371,-1.12306,-1.1323,-1.12866,-1.11779,-1.1329,-1.12345,-1.12286,-1.13215,-1.12857,-1.11774,-1.13726,-1.12343,-1.1229,-1.13213,-1.12857,-1.11775,-1.13726,-1.12343,-1.12291,-1.13212,-1.12857,-1.11776,-1.13726,-1.12343,-1.12291,-1.13212,-1.12857]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 4 5 6 17 18
INFO: iteration 1, average log likelihood -1.117997
WARNING: Variances had to be floored 3 5 6 17 18
INFO: iteration 2, average log likelihood -1.114093
WARNING: Variances had to be floored 4 5 6 17 18
INFO: iteration 3, average log likelihood -1.114297
WARNING: Variances had to be floored 3 4 5 6 17 18 30
INFO: iteration 4, average log likelihood -1.079512
WARNING: Variances had to be floored 3 4 5 6 8 17 18 23 29 30 32
INFO: iteration 5, average log likelihood -1.013022
WARNING: Variances had to be floored 3 4 5 6 17 18 24 28
INFO: iteration 6, average log likelihood -1.030946
WARNING: Variances had to be floored 3 4 5 6 8 9 17 18 29 30 32
INFO: iteration 7, average log likelihood -1.005342
WARNING: Variances had to be floored 3 4 5 6 17 18 23
INFO: iteration 8, average log likelihood -1.028356
WARNING: Variances had to be floored 3 4 5 6 8 17 18 24 28 29 30 32
INFO: iteration 9, average log likelihood -0.997849
WARNING: Variances had to be floored 3 4 5 6 17 18 30
INFO: iteration 10, average log likelihood -1.035608
WARNING: Variances had to be floored 3 4 5 6 8 9 17 18 23 29 30 32
INFO: iteration 11, average log likelihood -0.989772
WARNING: Variances had to be floored 3 4 5 6 17 18 24 28
INFO: iteration 12, average log likelihood -1.034424
WARNING: Variances had to be floored 3 4 5 6 8 17 18 29 30 32
INFO: iteration 13, average log likelihood -1.009031
WARNING: Variances had to be floored 3 4 5 6 17 18 23 30
INFO: iteration 14, average log likelihood -1.018437
WARNING: Variances had to be floored 3 4 5 6 8 9 17 18 24 28 29 30 32
INFO: iteration 15, average log likelihood -0.995944
WARNING: Variances had to be floored 3 4 5 6 17 18
INFO: iteration 16, average log likelihood -1.045437
WARNING: Variances had to be floored 3 4 5 6 8 17 18 23 29 30 32
INFO: iteration 17, average log likelihood -0.991881
WARNING: Variances had to be floored 3 4 5 6 17 18 24 28 30
INFO: iteration 18, average log likelihood -1.024707
WARNING: Variances had to be floored 3 4 5 6 8 9 17 18 29 30 32
INFO: iteration 19, average log likelihood -1.007058
WARNING: Variances had to be floored 3 4 5 6 17 18 23
INFO: iteration 20, average log likelihood -1.028062
WARNING: Variances had to be floored 3 4 5 6 8 17 18 24 28 29 30 32
INFO: iteration 21, average log likelihood -0.997711
WARNING: Variances had to be floored 3 4 5 6 17 18 30
INFO: iteration 22, average log likelihood -1.035663
WARNING: Variances had to be floored 3 4 5 6 8 9 17 18 23 29 30 32
INFO: iteration 23, average log likelihood -0.989789
WARNING: Variances had to be floored 3 4 5 6 17 18 24 28
INFO: iteration 24, average log likelihood -1.034316
WARNING: Variances had to be floored 3 4 5 6 8 17 18 29 30 32
INFO: iteration 25, average log likelihood -1.009010
WARNING: Variances had to be floored 3 4 5 6 17 18 23 30
INFO: iteration 26, average log likelihood -1.018509
WARNING: Variances had to be floored 3 4 5 6 8 9 17 18 24 28 29 30 32
INFO: iteration 27, average log likelihood -0.995937
WARNING: Variances had to be floored 3 4 5 6 17 18
INFO: iteration 28, average log likelihood -1.045386
WARNING: Variances had to be floored 3 4 5 6 8 17 18 23 29 30 32
INFO: iteration 29, average log likelihood -0.991919
WARNING: Variances had to be floored 3 4 5 6 17 18 24 28 30
INFO: iteration 30, average log likelihood -1.024669
WARNING: Variances had to be floored 3 4 5 6 8 9 17 18 29 30 32
INFO: iteration 31, average log likelihood -1.006967
WARNING: Variances had to be floored 3 4 5 6 17 18 23
INFO: iteration 32, average log likelihood -1.027877
WARNING: Variances had to be floored 3 4 5 6 8 17 18 24 28 29 30 32
INFO: iteration 33, average log likelihood -0.997243
WARNING: Variances had to be floored 3 4 5 6 17 18 30
INFO: iteration 34, average log likelihood -1.033559
WARNING: Variances had to be floored 3 4 5 6 8 9 17 18 23 29 30 32
INFO: iteration 35, average log likelihood -0.982995
WARNING: Variances had to be floored 3 4 5 6 17 18 24 28
INFO: iteration 36, average log likelihood -1.024974
WARNING: Variances had to be floored 3 4 5 6 8 17 18 29 30 32
INFO: iteration 37, average log likelihood -0.998214
WARNING: Variances had to be floored 3 4 5 6 9 17 18 23 30
INFO: iteration 38, average log likelihood -1.007056
WARNING: Variances had to be floored 3 4 5 6 8 17 18 24 28 29 30 32
INFO: iteration 39, average log likelihood -0.997312
WARNING: Variances had to be floored 3 4 5 6 17 18
INFO: iteration 40, average log likelihood -1.028221
WARNING: Variances had to be floored 3 4 5 6 8 9 17 18 23 29 30 32
INFO: iteration 41, average log likelihood -0.975613
WARNING: Variances had to be floored 3 4 5 6 17 18 24 28
INFO: iteration 42, average log likelihood -1.024655
WARNING: Variances had to be floored 3 4 5 6 8 17 18 29 30 32
INFO: iteration 43, average log likelihood -0.998243
WARNING: Variances had to be floored 3 4 5 6 9 17 18 23 30
INFO: iteration 44, average log likelihood -1.007082
WARNING: Variances had to be floored 3 4 5 6 8 17 18 24 28 29 30 32
INFO: iteration 45, average log likelihood -0.997309
WARNING: Variances had to be floored 3 4 5 6 17 18
INFO: iteration 46, average log likelihood -1.028223
WARNING: Variances had to be floored 3 4 5 6 8 9 17 18 23 29 30 32
INFO: iteration 47, average log likelihood -0.975613
WARNING: Variances had to be floored 3 4 5 6 17 18 24 28
INFO: iteration 48, average log likelihood -1.024655
WARNING: Variances had to be floored 3 4 5 6 8 17 18 29 30 32
INFO: iteration 49, average log likelihood -0.998242
WARNING: Variances had to be floored 3 4 5 6 9 17 18 23 30
INFO: iteration 50, average log likelihood -1.007082
INFO: EM with 100000 data points 50 iterations avll -1.007082
59.0 data points per parameter
5: avll = [-1.118,-1.11409,-1.1143,-1.07951,-1.01302,-1.03095,-1.00534,-1.02836,-0.997849,-1.03561,-0.989772,-1.03442,-1.00903,-1.01844,-0.995944,-1.04544,-0.991881,-1.02471,-1.00706,-1.02806,-0.997711,-1.03566,-0.989789,-1.03432,-1.00901,-1.01851,-0.995937,-1.04539,-0.991919,-1.02467,-1.00697,-1.02788,-0.997243,-1.03356,-0.982995,-1.02497,-0.998214,-1.00706,-0.997312,-1.02822,-0.975613,-1.02466,-0.998243,-1.00708,-0.997309,-1.02822,-0.975613,-1.02465,-0.998242,-1.00708]
[-1.37022,-1.37029,-1.37022,-1.36978,-1.3648,-1.35075,-1.34377,-1.34209,-1.34113,-1.34037,-1.33973,-1.3392,-1.33881,-1.33853,-1.3383,-1.33812,-1.33795,-1.3378,-1.33765,-1.33749,-1.33731,-1.33706,-1.33639,-1.33484,-1.33389,-1.3333,-1.33287,-1.33254,-1.33228,-1.33207,-1.33189,-1.33172,-1.33158,-1.33145,-1.33134,-1.33126,-1.33118,-1.33113,-1.33108,-1.33105,-1.33102,-1.33099,-1.33097,-1.33096,-1.33095,-1.33094,-1.33093,-1.33092,-1.33091,-1.33091,-1.3309,-1.33101,-1.33089,-1.33031,-1.32574,-1.3118,-1.30024,-1.29588,-1.29369,-1.29209,-1.29078,-1.28973,-1.28889,-1.28819,-1.2876,-1.28712,-1.28673,-1.28642,-1.28617,-1.28599,-1.28585,-1.28576,-1.28569,-1.28564,-1.28561,-1.28558,-1.28557,-1.28555,-1.28554,-1.28553,-1.28553,-1.28552,-1.28552,-1.28551,-1.28551,-1.28551,-1.28551,-1.28551,-1.2855,-1.2855,-1.2855,-1.2855,-1.2855,-1.2855,-1.2855,-1.2855,-1.2855,-1.2855,-1.2855,-1.2855,-1.2855,-1.28565,-1.28549,-1.28484,-1.27871,-1.26023,-1.24467,-1.238,-1.23432,-1.23175,-1.22949,-1.22712,-1.2249,-1.22313,-1.22188,-1.22095,-1.22017,-1.21949,-1.21896,-1.21861,-1.21839,-1.21827,-1.21821,-1.21818,-1.21816,-1.21815,-1.21814,-1.21813,-1.21812,-1.21811,-1.21809,-1.21808,-1.21807,-1.21805,-1.21802,-1.21798,-1.21791,-1.21779,-1.2176,-1.2173,-1.21702,-1.21689,-1.21683,-1.2168,-1.21679,-1.21677,-1.21676,-1.21676,-1.21675,-1.21675,-1.21675,-1.21693,-1.21665,-1.2147,-1.19662,-1.1635,-1.14288,-1.15496,-1.13595,-1.12347,-1.14246,-1.12694,-1.12552,-1.13369,-1.12565,-1.1192,-1.13398,-1.12453,-1.1195,-1.13296,-1.12486,-1.11832,-1.13324,-1.12371,-1.12306,-1.1323,-1.12866,-1.11779,-1.1329,-1.12345,-1.12286,-1.13215,-1.12857,-1.11774,-1.13726,-1.12343,-1.1229,-1.13213,-1.12857,-1.11775,-1.13726,-1.12343,-1.12291,-1.13212,-1.12857,-1.11776,-1.13726,-1.12343,-1.12291,-1.13212,-1.12857,-1.118,-1.11409,-1.1143,-1.07951,-1.01302,-1.03095,-1.00534,-1.02836,-0.997849,-1.03561,-0.989772,-1.03442,-1.00903,-1.01844,-0.995944,-1.04544,-0.991881,-1.02471,-1.00706,-1.02806,-0.997711,-1.03566,-0.989789,-1.03432,-1.00901,-1.01851,-0.995937,-1.04539,-0.991919,-1.02467,-1.00697,-1.02788,-0.997243,-1.03356,-0.982995,-1.02497,-0.998214,-1.00706,-0.997312,-1.02822,-0.975613,-1.02466,-0.998243,-1.00708,-0.997309,-1.02822,-0.975613,-1.02465,-0.998242,-1.00708]
32×26 Array{Float64,2}:
  0.127667     0.0591945   0.0999188    0.0425197  -0.273794     0.0245488   -0.490547    -0.0449209    0.0158922   -0.0466934    0.161864     0.149268    -0.0196392    0.0306525    0.0657593   -0.060532     -0.0161046    0.0755031    -0.0209741   -0.0563114    0.0607607    -0.0505957    -0.0741979   -0.0247601   -0.0160816     0.0532923  
  0.171563     0.13542     0.279427     0.0437057  -0.324219    -0.0347117    0.493043    -0.0315802    0.109374    -0.146016     0.162016     0.0191772   -0.105745     0.089816     0.0939885   -0.0705275    -0.135645    -0.15216      -0.0108879    0.141073    -0.0889867     0.000884667  -0.105556     0.0250214   -0.0014155     0.0249904  
 -0.10328      0.24687    -0.051778    -0.136982   -0.10275     -0.0169411   -0.065226    -0.0646157   -0.0722133    0.0740939   -0.295737     0.00435321  -0.0750954    0.00540382   0.0371522   -0.00615071    0.041208    -0.0694762    -0.071048     0.0434938    0.0248947     0.151552     -0.00341335   0.0546937   -0.0477282     0.0399582  
 -0.040648     0.0892214  -0.306598     0.264842   -0.0270927    0.00311088   0.132228    -0.0702397   -0.0400538   -0.126105     3.07196      0.0468405   -0.0763202   -0.248291     0.00312387   0.0681116    -0.0710352    0.0962455    -0.00590124  -0.0294138   -0.118322      0.129348      0.0658722    0.259381    -0.0438793    -0.1857     
  0.0261864    0.116769    0.12566     -0.256827    0.171728    -0.020766     0.0874852    0.127177     0.112305     0.0726401   -0.117686    -0.200807    -0.015055    -0.137767    -0.458577    -0.0522456     0.181666    -0.0840291     0.0221037   -0.00945608   0.0362278    -0.101873     -0.112469    -0.159596    -0.0182073    -0.144659   
 -0.00538231   0.0367567   0.21585     -0.256434    0.174548    -0.020863     0.0199336   -0.0481791    0.0917551    0.0697455   -0.122173    -0.200702    -0.0177879   -0.138948     0.622734     0.0794921     0.188097     0.13578      -0.0253854   -0.0118383    0.0590629     0.159792     -0.111651    -0.113823    -0.000862319   0.0658054  
  0.0906579    0.155706   -0.00850399  -0.111647   -0.132967    -0.00961083  -0.0909147    0.0388215   -0.0372327   -0.0460391   -0.0268215    0.0623635   -0.00552826  -0.0146019   -0.018863    -0.0556781    -0.0504778   -0.108163      0.16212      0.00889811   0.070977      0.0335236     0.0699552   -0.0249985   -0.117786      0.124959   
  0.0339338   -0.0146093   0.013929     0.0397217  -0.00186718   0.0455452    0.030292    -0.0671009    0.031801    -0.0278596    0.0997298   -0.107784     0.0309957    0.0601096    0.246708    -0.107628     -0.112049    -0.082459     -0.0482786    0.153271     0.0255572    -0.0503378     0.0623047    0.0141876   -0.0405349    -0.0111398  
  0.0631128    0.0205377   0.00438199  -0.0742351  -0.0757604   -0.0692808    0.00459358   0.0358255   -0.0584797   -0.0449528    0.154069    -0.0824088    0.117071    -0.139617     0.0288161    0.0643353     0.0542439    0.142737      0.0326974   -0.0324656    0.0551801     0.0732902     0.0117972    0.0145177   -0.043047     -0.0435017  
  0.0194268    0.0095847   0.135698     0.0473943  -0.0581144   -0.0850289    0.102214    -0.0857778    0.0880377   -0.116145     0.0298196    0.12988      0.148875    -0.0763339    0.0618215   -0.0219329    -0.0553116   -0.0293311    -0.0926625   -0.0273007    0.0106783    -0.105372     -0.122272     0.038226     0.108834     -0.0033197  
 -0.0828406    0.030413    0.0764634   -0.0421456  -0.0120498    0.0479252   -0.0284759   -0.0233389    0.0574715   -0.105975    -0.00481827   0.0462147   -0.161317     0.0214965    0.0602881    0.0248424    -0.0366276   -0.126283      0.022391    -0.108399     0.0330185    -0.235762      0.0454847    0.179196     0.0553336     0.129772   
 -0.0317441    0.115845    0.019778    -0.159082    0.0927911    0.118999     0.201436    -0.00416615  -0.00990185  -0.126262    -0.109957    -0.107858     0.00538155  -0.108649     0.0391169    0.101531      0.0908522   -0.121443     -0.128764    -0.122719     0.018402      0.0157364     0.0528658   -0.137698     0.0351062    -0.000407976
  0.082516    -0.0229837   0.0603792   -0.0236143  -0.0231221    0.0780045   -0.17408      0.0785931   -0.111361     0.00982319   0.055786     0.0981394   -0.0366225    0.0111045   -0.0654261   -0.150753     -0.131774     0.121063      0.0491529    0.00113401   0.121494     -0.122804      0.0541713   -0.207181    -0.096274     -0.00524208 
  0.182509     0.16085     0.0936718    0.0571092  -0.0933462   -0.0206493   -0.0442329   -0.0806381   -0.114322     0.0181535   -0.204035    -0.0614954   -0.0610716   -0.12526     -0.0305996    0.0601346     0.0778734    0.0822947     0.22466     -0.0107586    0.0995365     0.0275222     0.0413678    0.123742     0.0689044    -0.0890697  
 -0.00963745   0.0313269  -0.011911     0.0324643  -0.0134598    0.0114191    0.00251384  -0.0698046   -0.171535     0.0767418    0.0671814   -0.151328    -0.10293      0.0869583    0.041799    -0.0157157     0.0141012    0.140741     -0.0333023   -0.0197498    0.0403939     0.00233652   -0.0415381   -0.16688      0.151192      0.0228242  
  0.0182028   -0.0629584  -0.0879365   -0.0431283   0.0968516   -0.240488    -0.0350045    0.105444     0.0782024    0.147514    -0.0397149   -0.0432713    0.163015    -0.0943971    0.0200864    0.0796133    -0.050917    -0.0464128     0.136681     0.0767252    0.00462698   -0.184666     -0.0554475    0.184234     0.0474561     0.000236144
 -0.618931     0.35591    -0.0117718    0.15149     0.0630913    0.0779037    0.045792    -0.156245     0.0109065   -0.202795    -0.113314     0.0507835    0.0366722   -0.00889382  -0.114196     0.120048      0.00921572   0.12207      -0.00887298   0.252185     0.0124722     0.0737222    -0.0460417    0.0982169   -0.095475     -0.0957806  
  0.625816    -0.227198    0.0437942    0.103412   -0.11325      0.0579193   -0.0766501   -0.173749     0.00292873  -0.203168    -0.150739    -0.0459196    0.0214851    0.038094    -0.315552    -0.338054     -0.00212184   0.122629      0.0500411    0.0997568   -0.0011306     0.0926922    -0.151039     0.0980562    0.0505862    -0.0949638  
  0.0150838   -0.0418493   0.0385183   -0.186349    0.073325    -0.167664     0.070388    -0.230679    -0.479246     0.0193495    0.0780599    0.0804272   -0.0590059    0.077077    -0.17365      0.00273298    0.0580594    0.0980924    -0.0471244   -0.0441858    0.131537     -0.000311806  -0.0173358   -0.00184113   0.11952       0.0306143  
  0.149323    -0.231411    0.0713992   -0.0550096   0.0232799   -0.125264     0.0701979   -0.0200449    0.890008     0.0696696    0.0468818    0.0955671    0.0275454    0.0781214   -0.165287     0.00294611    0.0355116    0.110203     -0.0340207    0.0619455   -0.187172     -0.0232826     0.0450114   -0.00866358   0.0926182    -0.144188   
  0.116593    -0.161172   -0.133693    -0.089182    0.139218    -0.106528     0.0655622    0.0665666   -0.178856     0.0512069    0.140897     0.26262     -0.0153866   -0.12246      0.0473014   -0.0637383     0.0992775    0.0469374     0.0261467   -0.130392     0.0306319    -0.0161927    -0.0561935   -0.216372    -0.0746279    -0.0664575  
 -0.039819    -0.0730297  -0.170729    -0.0918266   0.0132636   -0.249711     0.130503    -0.0468761   -0.141551     0.207103     0.00527858   0.110925     0.155145     0.117868     0.0472478    0.000533028  -0.0199934    0.109718      0.00553835   0.0935788   -0.065703     -0.0483416     0.0104529   -0.0856483   -0.192091      0.0113892  
 -0.204309    -0.0535181   0.0196639   -0.0270686   0.0345299    0.00721336  -0.124915    -0.0438602   -0.124509     0.0918874   -0.0435406    0.10521     -0.0448466   -0.123645    -0.0381672    0.072155      0.0639977    0.0896143     0.0693725    0.148005     0.000612879  -0.00235842   -0.0212707    0.0946873   -0.012643     -0.0579964  
  0.0142147    0.185723   -0.152756     0.159559    0.0613512    0.0745447   -0.0362123    0.0131317    0.0771957    0.104566    -0.049417     0.0967626    0.0730844   -0.0440323   -0.00602285  -0.110556      0.124763     0.0314624     0.142895     0.061222     0.0130476    -0.116476      0.155299     0.00452771   0.0787252    -0.0313989  
  0.0420798   -0.162683   -0.207893    -0.0374064   0.0473404   -0.143053     0.0266902   -0.0260442    0.182295     0.00784526  -0.0801282    0.123328     0.115025    -0.0854865   -0.120966     0.0631621    -0.0595453    0.0477639     0.0411836   -0.171435    -0.060592      0.0782717     0.279428     0.0571624   -0.247105      0.165641   
  0.0101413   -0.0619292   0.0214868    0.025593    0.065178     0.0672409    0.0717826    0.0190817   -0.0360084   -0.0381181    0.0115509    0.103868    -0.0954296   -0.0106522    0.00305728  -0.0593194     0.106093    -0.000473082   0.0351439    0.0563147    0.0280275    -0.0799979    -0.0399157    0.134196     0.0248176    -0.0762823  
  0.0685637   -0.020451    0.0658781   -0.140515    0.0751111   -0.022198    -0.0120577   -0.0097393    0.112694    -0.0830753    0.139104    -0.161407     0.0313437   -0.0817423    0.0382719    0.0874286    -0.039289    -0.10072       0.0417487    0.037761    -0.189885      0.0106104     0.0240209    0.0391468   -0.0136846     0.0580219  
 -0.0674399   -0.0712622   0.225662    -0.0496217   0.0581532   -0.0911138    0.110852    -0.0156047   -0.0264651    0.0578602   -0.0844367   -0.0758614    0.143173     0.113707     0.0710828    0.168458      0.119865    -0.0267299    -0.016726     0.0886526    0.0505137     0.0658677     0.181487    -0.0547466   -0.0288848    -0.00237389 
 -0.114859     0.0279481  -0.00891883  -0.0463758  -0.0961548    0.0300879   -0.0281295    0.07535      0.0482753   -0.173489     0.00708858   0.0467082   -0.0912063    0.0444815   -0.116185     0.0541856    -0.0301645   -0.139088     -0.0885839    0.00977249  -0.033729      0.083903      0.0395173    0.0879269   -0.0566727    -0.00971293 
  0.0235889   -0.182009   -0.058138     0.159875   -0.0105205    0.203874     0.0669695    0.0784452   -0.0273371    0.043908     0.294248    -0.109486    -0.0636512   -0.0231949    0.0754951   -0.138482      0.103791    -0.0509871    -0.0423669   -0.0480741   -0.109042      0.100485      0.0115423    0.144147    -0.0110136     0.140435   
 -0.0505973   -0.107989    0.103201    -0.219946   -0.0762594    0.0754484    0.0909687   -0.0390999   -0.029712     0.00780512  -0.0878525   -0.0194462   -0.0339103   -0.174072    -0.0117834   -0.125726     -0.00217209  -0.0309945    -0.00790705   0.0515142    0.00447766    0.0957995     0.0410415    0.0251408   -0.0424858     0.155798   
 -0.0426023    0.0948666  -0.066205     0.0339      0.0416307    0.0500839   -0.024635    -0.0725928   -0.0425149   -0.11547      0.196654     0.171591    -0.0630396    0.0960803    0.0262034    0.105453     -0.187485     0.0741578     0.199665     0.192983     0.040757     -0.134963     -0.0815564   -0.0657816    0.0776584     0.0100946  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 3 4 5 6 8 17 18 24 28 29 30 32
INFO: iteration 1, average log likelihood -0.997309
WARNING: Variances had to be floored 3 4 5 6 8 17 18 24 28 29 30 32
INFO: iteration 2, average log likelihood -0.982647
WARNING: Variances had to be floored 3 4 5 6 8 9 17 18 23 24 28 29 30 32
INFO: iteration 3, average log likelihood -0.974566
WARNING: Variances had to be floored 3 4 5 6 8 17 18 24 28 29 30 32
INFO: iteration 4, average log likelihood -0.997206
WARNING: Variances had to be floored 3 4 5 6 8 17 18 24 28 29 30 32
INFO: iteration 5, average log likelihood -0.982645
WARNING: Variances had to be floored 3 4 5 6 8 9 17 18 23 24 28 29 30 32
INFO: iteration 6, average log likelihood -0.974565
WARNING: Variances had to be floored 3 4 5 6 8 17 18 24 28 29 30 32
INFO: iteration 7, average log likelihood -0.997205
WARNING: Variances had to be floored 3 4 5 6 8 17 18 24 28 29 30 32
INFO: iteration 8, average log likelihood -0.982645
WARNING: Variances had to be floored 3 4 5 6 8 9 17 18 23 24 28 29 30 32
INFO: iteration 9, average log likelihood -0.974564
WARNING: Variances had to be floored 3 4 5 6 8 17 18 24 28 29 30 32
INFO: iteration 10, average log likelihood -0.997205
INFO: EM with 100000 data points 10 iterations avll -0.997205
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.280844e+05
      1       6.278943e+05      -2.001901e+05 |       32
      2       5.991784e+05      -2.871594e+04 |       32
      3       5.854699e+05      -1.370847e+04 |       32
      4       5.781678e+05      -7.302091e+03 |       32
      5       5.729448e+05      -5.222982e+03 |       32
      6       5.695213e+05      -3.423501e+03 |       32
      7       5.675240e+05      -1.997329e+03 |       32
      8       5.662712e+05      -1.252837e+03 |       32
      9       5.653057e+05      -9.654390e+02 |       32
     10       5.644863e+05      -8.194644e+02 |       32
     11       5.637864e+05      -6.999025e+02 |       32
     12       5.632345e+05      -5.518989e+02 |       32
     13       5.628380e+05      -3.964603e+02 |       32
     14       5.624889e+05      -3.491326e+02 |       32
     15       5.620469e+05      -4.419930e+02 |       32
     16       5.614252e+05      -6.217347e+02 |       32
     17       5.607195e+05      -7.056839e+02 |       32
     18       5.600758e+05      -6.436664e+02 |       32
     19       5.596112e+05      -4.645698e+02 |       32
     20       5.592765e+05      -3.347106e+02 |       31
     21       5.590006e+05      -2.759173e+02 |       32
     22       5.587844e+05      -2.161790e+02 |       32
     23       5.586396e+05      -1.447901e+02 |       29
     24       5.585837e+05      -5.594206e+01 |       31
     25       5.585632e+05      -2.054381e+01 |       31
     26       5.585528e+05      -1.036472e+01 |       31
     27       5.585445e+05      -8.277484e+00 |       27
     28       5.585387e+05      -5.840052e+00 |       25
     29       5.585335e+05      -5.123136e+00 |       25
     30       5.585306e+05      -2.900694e+00 |       22
     31       5.585281e+05      -2.572722e+00 |       23
     32       5.585252e+05      -2.863874e+00 |       23
     33       5.585231e+05      -2.082375e+00 |       20
     34       5.585220e+05      -1.093502e+00 |       14
     35       5.585214e+05      -6.602695e-01 |       12
     36       5.585210e+05      -4.021726e-01 |        9
     37       5.585204e+05      -5.622355e-01 |       16
     38       5.585195e+05      -8.656238e-01 |       12
     39       5.585187e+05      -8.902335e-01 |       13
     40       5.585178e+05      -8.692489e-01 |       11
     41       5.585170e+05      -7.396792e-01 |       13
     42       5.585163e+05      -7.649007e-01 |       12
     43       5.585155e+05      -8.013448e-01 |       14
     44       5.585148e+05      -7.229808e-01 |       14
     45       5.585141e+05      -6.963302e-01 |       14
     46       5.585131e+05      -9.364693e-01 |       13
     47       5.585119e+05      -1.198783e+00 |       14
     48       5.585113e+05      -6.453189e-01 |        9
     49       5.585101e+05      -1.131913e+00 |       10
     50       5.585088e+05      -1.382237e+00 |       18
K-means terminated without convergence after 50 iterations (objv = 558508.7644965709)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.283370
INFO: iteration 2, average log likelihood -1.247974
INFO: iteration 3, average log likelihood -1.203430
INFO: iteration 4, average log likelihood -1.155976
INFO: iteration 5, average log likelihood -1.108399
WARNING: Variances had to be floored 30
INFO: iteration 6, average log likelihood -1.053325
WARNING: Variances had to be floored 4 9 13 15 17 22 25 26 32
INFO: iteration 7, average log likelihood -1.007933
WARNING: Variances had to be floored 8 12 19
INFO: iteration 8, average log likelihood -1.082491
WARNING: Variances had to be floored 11 27
INFO: iteration 9, average log likelihood -1.059411
WARNING: Variances had to be floored 20
INFO: iteration 10, average log likelihood -1.040047
WARNING: Variances had to be floored 17 22 25 26 30 31
INFO: iteration 11, average log likelihood -0.983950
WARNING: Variances had to be floored 8 13 15 19 27 32
INFO: iteration 12, average log likelihood -1.028340
WARNING: Variances had to be floored 11
INFO: iteration 13, average log likelihood -1.072887
WARNING: Variances had to be floored 4 12 20
INFO: iteration 14, average log likelihood -1.025202
WARNING: Variances had to be floored 9 25 26 30 31
INFO: iteration 15, average log likelihood -0.983980
WARNING: Variances had to be floored 8 13 17 19 22 27 32
INFO: iteration 16, average log likelihood -1.005606
WARNING: Variances had to be floored 11
INFO: iteration 17, average log likelihood -1.080310
INFO: iteration 18, average log likelihood -1.037384
WARNING: Variances had to be floored 4 15 19 20 22 25 26 27 30 31
INFO: iteration 19, average log likelihood -0.966214
WARNING: Variances had to be floored 8 11 13 17 32
INFO: iteration 20, average log likelihood -1.061182
WARNING: Variances had to be floored 9
INFO: iteration 21, average log likelihood -1.087136
INFO: iteration 22, average log likelihood -1.048832
WARNING: Variances had to be floored 11 13 19 22 25 26 27 30
INFO: iteration 23, average log likelihood -0.979337
WARNING: Variances had to be floored 8 17 20 31 32
INFO: iteration 24, average log likelihood -1.045174
WARNING: Variances had to be floored 4 9 12 15
INFO: iteration 25, average log likelihood -1.063315
INFO: iteration 26, average log likelihood -1.048427
WARNING: Variances had to be floored 17 19 26 27 30
INFO: iteration 27, average log likelihood -0.990754
WARNING: Variances had to be floored 8 13 22 25 31 32
INFO: iteration 28, average log likelihood -1.024227
WARNING: Variances had to be floored 11 20
INFO: iteration 29, average log likelihood -1.042495
WARNING: Variances had to be floored 4 9 15 19 27
INFO: iteration 30, average log likelihood -1.009915
WARNING: Variances had to be floored 17 26 30
INFO: iteration 31, average log likelihood -1.026326
WARNING: Variances had to be floored 8 12 13 22 31 32
INFO: iteration 32, average log likelihood -1.019738
WARNING: Variances had to be floored 11 20 25
INFO: iteration 33, average log likelihood -1.048002
WARNING: Variances had to be floored 4 19 27
INFO: iteration 34, average log likelihood -1.020139
WARNING: Variances had to be floored 15 17 26 30
INFO: iteration 35, average log likelihood -1.008557
WARNING: Variances had to be floored 8 12 13 22 25 31 32
INFO: iteration 36, average log likelihood -1.019072
WARNING: Variances had to be floored 19 20
INFO: iteration 37, average log likelihood -1.054620
WARNING: Variances had to be floored 4 9 11 27
INFO: iteration 38, average log likelihood -1.010857
WARNING: Variances had to be floored 15 17 26 30
INFO: iteration 39, average log likelihood -1.018561
WARNING: Variances had to be floored 8 12 13 19 22 31
INFO: iteration 40, average log likelihood -1.039157
INFO: iteration 41, average log likelihood -1.061617
WARNING: Variances had to be floored 4 20 25 27 32
INFO: iteration 42, average log likelihood -0.996123
WARNING: Variances had to be floored 15 17 26 30
INFO: iteration 43, average log likelihood -1.010785
WARNING: Variances had to be floored 8 11 12 13 19 22 31
INFO: iteration 44, average log likelihood -1.017950
WARNING: Variances had to be floored 9
INFO: iteration 45, average log likelihood -1.064297
WARNING: Variances had to be floored 4 27
INFO: iteration 46, average log likelihood -1.024122
WARNING: Variances had to be floored 15 17 20 25 26 30
INFO: iteration 47, average log likelihood -0.995221
WARNING: Variances had to be floored 8 13 19 22 31
INFO: iteration 48, average log likelihood -1.032785
WARNING: Variances had to be floored 12 27 32
INFO: iteration 49, average log likelihood -1.047907
WARNING: Variances had to be floored 4 11
INFO: iteration 50, average log likelihood -1.029802
INFO: EM with 100000 data points 50 iterations avll -1.029802
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0537632    -0.159925    -0.204586    -0.0370198    0.0446244  -0.141107    0.0261423    -0.0315743    0.181181     0.00487956  -0.081256     0.126477     0.113606   -0.0772494   -0.126013     0.0706294   -0.0616652    0.0507497    0.0416079   -0.169582     -0.0575516    0.0775359    0.28051      0.0585641   -0.250898     0.165932   
 -0.0406568    -0.0725032   -0.171192    -0.0915072    0.0143924  -0.249569    0.131131     -0.0469279   -0.142296     0.207525     0.00600642   0.111841     0.155034    0.118425     0.0473744    0.00120415  -0.0223277    0.110688     0.00559719   0.0937484    -0.0657928   -0.0485147    0.0101052   -0.0859052   -0.191916     0.0116866  
  0.0818442    -0.0222339    0.0602496   -0.0245779   -0.0239385   0.07579    -0.17372       0.0789057   -0.110833     0.0125325    0.0598771    0.0985199   -0.0377035   0.0101142   -0.0674266   -0.149801    -0.132306     0.121353     0.0456041    0.000584719   0.114117    -0.121947     0.0531251   -0.205854    -0.0960869   -0.00486231 
  0.0834163    -0.0678772   -0.0157297   -0.172258    -0.0853337   0.0247851  -0.00863582   -0.093083    -0.0610883   -0.0660493   -0.0483472   -0.0267604    0.0707852  -0.165542     0.0223574    0.00287024   0.0990109    0.131775    -0.0232467   -0.0306925     0.025927     0.0698719    0.0242546   -0.0267111   -0.0586418    0.050889   
  0.118478     -0.16157     -0.137446    -0.0892401    0.140592   -0.104246    0.0668241     0.0671609   -0.177808     0.0499044    0.140822     0.263015    -0.0148964  -0.121946     0.0472215   -0.0628116    0.0993641    0.0475425    0.0265888   -0.130953      0.0296386   -0.0166856   -0.0561787   -0.215746    -0.0747733   -0.0661615  
 -0.0286673     0.11672      0.0158128   -0.144161     0.0888572   0.11874     0.201754     -0.00551712  -0.001729    -0.113023    -0.106415    -0.110794     0.0116125  -0.106645     0.0382922    0.09859      0.0935428   -0.114069    -0.123849    -0.121357      0.0126921    0.0156788    0.0574073   -0.129402     0.0346832   -0.000384055
  0.0201895    -0.062275    -0.0941044   -0.0424822    0.102677   -0.23988    -0.0353886     0.106488     0.0818585    0.151537    -0.0407452   -0.0441063    0.163472   -0.0935515    0.021943     0.0849386   -0.0525173   -0.0462906    0.137887     0.0782666     0.00359621  -0.184983    -0.0544665    0.183844     0.0519383    0.00172401 
 -0.151439     -0.0806316   -0.062949    -0.0343783    0.0937043   0.0865183   0.029527     -0.0648419   -0.0626533   -0.0822939    0.0829493    0.241099    -0.135096   -0.179344     0.0180486   -0.234588    -0.0221881   -0.0167715    0.0387208    0.00163506    0.116809     0.0704802   -0.00671331  -0.0649234   -0.0248341   -0.0107111  
 -0.107904      0.0233292    0.0247259   -0.0423931   -0.0969434   0.02532    -0.0297904    -0.0840995    0.08492     -0.160744    -0.00990225   0.00202551  -0.0847765   0.0125466   -0.102128     0.0766828   -0.0571157   -0.115771    -0.056888     0.00665782   -0.0270928    0.0141621    0.0431784    0.0695483   -0.056985    -0.0170129  
  0.0154733     2.4343e-5    0.105323     0.103809     0.0927318  -0.230951    0.0762294    -0.0220832    0.170413    -0.081174     0.0192513    0.131397     0.347505   -0.0396364    0.0419354    0.0893021   -0.156428    -0.126597    -0.182824    -0.00817662    0.0482591   -0.0894448   -0.195157    -0.0379942    0.106017    -0.0686245  
  0.257347     -0.0948762   -0.00364996   0.044816     0.109287    0.0241523   0.0582725     0.148725     0.111128    -0.0261756    0.0210084   -0.0389879   -0.0305205   0.00312305   0.134172    -0.160292     0.26239      0.0829231   -0.00967893  -0.010477      0.0998116   -0.151603    -0.174566     0.166822    -0.0077706   -0.0538805  
  0.0249559     0.257596    -0.195366     0.172534     0.0554965   0.0727814  -0.0837761    -0.0302999    0.0949246    0.151338    -0.070697     0.0940188    0.0935048  -0.0300811    0.00416478  -0.105511     0.165397     0.0609958    0.107067     0.0669065     0.00847642  -0.22344      0.250681     0.0260929    0.0729611   -0.0394131  
  0.0203855     0.101211     0.00634371  -0.00135797  -0.0498211  -0.121479    0.00139936    0.130234    -0.0479563   -0.0496675    0.303508    -0.0748196    0.109494   -0.0751843    0.0371353    0.111173    -0.0346091    0.130797     0.0987793    0.0111836     0.0729611    0.0402024   -0.0135444    0.024001    -0.0134638   -0.0949865  
  0.0686486     0.0616405    0.00120569  -0.0315572   -0.0620967   0.0177907  -0.0292049    -0.0108192    0.0016825   -0.0328187    0.0462845   -0.0354249    0.0167084   0.0279378    0.124897    -0.0916813   -0.078247    -0.0940192    0.0499884    0.0860902     0.0466959   -0.00961271   0.0669115   -0.0129349   -0.0813464    0.0525421  
 -0.00263634   -0.15898     -0.0590091    0.140185    -0.0193944   0.222468    0.0536147     0.0918941   -0.0326837    0.00605365   0.290789    -0.12682     -0.0653239  -0.0105882    0.0503514   -0.133363     0.0947291   -0.0877788   -0.0738127   -0.0447397    -0.113797     0.107643     0.0237782    0.135592    -0.0261257    0.13204    
  0.0567463    -0.0216864    0.0598085   -0.136624     0.0630385  -0.0304339  -0.0127608    -0.00196763   0.111608    -0.0886649    0.123442    -0.160733     0.0216674  -0.0744459    0.0295943    0.0971287   -0.050156    -0.0978665    0.0450596    0.0362281    -0.182637     0.0223725    0.0295739    0.0272951   -0.01497      0.0562561  
  0.00867312    0.0794657    0.165419    -0.251695     0.166488   -0.0201939   0.0486204     0.0413661    0.098588     0.0633774   -0.115323    -0.197352    -0.0174169  -0.13359      0.0513311    0.015467     0.177244     0.0223774   -8.16051e-5  -0.00593517    0.0508294    0.0207627   -0.110691    -0.135398    -0.00833478  -0.0430192  
 -0.000825995   0.2063       0.137188    -0.0353262   -0.224464   -0.0229049  -0.126697     -0.0505766   -0.0104132    0.0454459    0.0101779    0.0372525   -0.0772128   0.0492526    0.0668438   -0.0354973    0.0211749   -0.0695139   -0.0558688    0.0632435     0.0506022    0.0764968   -0.0429625   -0.020418    -0.0288067    0.0864453  
 -0.203757     -0.0531843    0.0108537   -0.0182695    0.0652736   0.0102221  -0.133955     -0.0393557   -0.106467     0.0976519   -0.0470897    0.108124    -0.0343981  -0.116196    -0.0365098    0.0619393    0.0640778    0.0859468    0.0740871    0.141624      0.00196108  -0.003684    -0.0164057    0.0976148   -0.00801157  -0.0574684  
 -0.113599      0.00317701   0.111565     0.0527668   -0.0249025   0.109027    0.110138     -0.0565761   -0.16228     -0.0161081   -0.052749     0.16066     -0.128828    0.123715    -0.169728     0.155533     0.0664128   -0.0969073    0.0561909    0.15788      -0.145157    -0.131181     0.0752276    0.286009     0.0974847   -0.134448   
 -0.00855012    0.0323882   -0.0124692    0.0309942   -0.0136383   0.0147385   0.000437912  -0.0700759   -0.170691     0.0773525    0.0668399   -0.154213    -0.102407    0.085182     0.0499112   -0.0177336    0.0117867    0.140127    -0.0347229   -0.0208676     0.0439518    0.00519967  -0.0438602   -0.179016     0.149161     0.0214707  
 -0.0661117     0.0844969   -0.0790292    0.0390105    0.0393639   0.0585487  -0.0225834    -0.140894    -0.0373179   -0.125044     0.207234     0.236719    -0.0839726   0.0942654    0.0586524    0.141667    -0.18271      0.083243     0.246032     0.174911      0.0582548   -0.130978    -0.068672    -0.0653005    0.061592     0.00588094 
  0.182649      0.162959     0.0963724    0.0577773   -0.0929561  -0.0255247  -0.0458749    -0.0810279   -0.116851     0.0226531   -0.204486    -0.0610373   -0.0615586  -0.124429    -0.0317433    0.066415     0.0770366    0.0881284    0.222762    -0.0112388     0.100057     0.0285691    0.0420924    0.122311     0.0687572   -0.0939185  
  0.0813046    -0.132325     0.0529512   -0.116214     0.0462452  -0.145526    0.070509     -0.127692     0.199915     0.0476526    0.0623959    0.0882324   -0.0188829   0.0773449   -0.170133     0.00283251   0.0481115    0.10176     -0.03844      0.00735446   -0.0252658   -0.0145337    0.0186348   -0.00566917   0.110246    -0.0566376  
 -0.0157528    -0.0486319   -0.0192846    0.0541226   -0.0556771   0.102618    0.0126366     0.245402     0.0755765   -0.060436     0.0457256   -0.0548074   -0.0765659   0.00622137  -0.0290664   -0.0847602    0.0743385    0.023414    -0.0441825    0.0135382    -0.0092297    0.0721927   -0.0261457    0.231463    -0.0119648    0.0438801  
  0.0304715     0.0216773    0.164706     0.0036089   -0.212584    0.0614462   0.137349     -0.144511     0.0102572   -0.141241     0.039125     0.12858     -0.0502473  -0.107736     0.0800858   -0.134222     0.0487821    0.0617614   -0.00182929  -0.040962     -0.0114437   -0.121231    -0.0506926    0.112772     0.111511     0.0533181  
  0.043241      0.0796708    0.00458569   0.141111    -0.03194     0.069321   -0.0260862    -0.117625     0.00435344  -0.180111    -0.126972     0.0158673    0.0469272   0.00164882  -0.202455    -0.13323      0.0135017    0.0930841    0.0615246    0.162192      0.00711641   0.0814606   -0.101965     0.0724635   -0.0027514   -0.0794546  
 -0.0468697    -0.106978     0.100925    -0.217513    -0.068025    0.0756584   0.0879085    -0.0408496   -0.028545     0.00559909  -0.085907    -0.0185964   -0.0340043  -0.16908     -0.0109454   -0.123688    -0.00539227  -0.0284101   -0.00311464   0.056064      0.00235387   0.0922562    0.0404774    0.0175793   -0.0408216    0.152452   
 -0.0829885     0.0305811    0.0747364   -0.0415763   -0.0124091   0.0478929  -0.0264037    -0.0232308    0.0584924   -0.105768    -0.00479648   0.0457144   -0.160697    0.0218615    0.0602202    0.0247293   -0.0363332   -0.126588     0.0228697   -0.109251      0.0325358   -0.236945     0.0457551    0.179895     0.0573463    0.130067   
 -0.0677847    -0.0671812    0.219007    -0.0470606    0.0579989  -0.0985421   0.109896     -0.0122632   -0.0265328    0.0476002   -0.0767759   -0.0756703    0.133737    0.113214     0.0680302    0.165286     0.110434    -0.0254393   -0.0125588    0.0833489     0.0507919    0.0665776    0.175047    -0.0544214   -0.028748    -0.00332809 
 -0.066149      0.166817    -0.315027    -0.081114    -0.0260307   0.0136109   0.168839     -0.064954    -0.0628264   -0.0774052    0.132785     0.03351     -0.0601681  -0.081026     0.00437932   0.0158508   -0.0699521    0.00803525  -0.0192875   -0.0244646    -0.0904259    0.122529     0.0274065    0.212722    -0.0414936   -0.108585   
 -0.0995462     0.0358643    0.00432211  -0.040974    -0.0938955  -0.0228577  -0.0336343     0.175925     0.0530751   -0.145881    -0.0213898    0.0726352   -0.0916906   0.0309673   -0.0956735    0.0901056   -0.0531986   -0.237446    -0.0647887    0.015293     -0.0691055    0.130552     0.0470684    0.101421    -0.0247015   -0.00832692 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 8 9 15 17 20 22 26 30
INFO: iteration 1, average log likelihood -0.985580
WARNING: Variances had to be floored 8 9 13 15 17 19 20 22 26 27 30 31
INFO: iteration 2, average log likelihood -0.957121
WARNING: Variances had to be floored 8 9 11 12 13 15 17 19 20 22 26 30 31 32
INFO: iteration 3, average log likelihood -0.953498
WARNING: Variances had to be floored 4 8 9 15 17 20 22 25 26 27 30
INFO: iteration 4, average log likelihood -0.961682
WARNING: Variances had to be floored 8 9 13 15 17 19 20 22 26 30 31
INFO: iteration 5, average log likelihood -0.958834
WARNING: Variances had to be floored 8 9 11 12 13 15 17 19 20 22 26 27 30 31
INFO: iteration 6, average log likelihood -0.948407
WARNING: Variances had to be floored 4 8 9 15 17 20 22 26 30
INFO: iteration 7, average log likelihood -0.970981
WARNING: Variances had to be floored 8 9 13 15 17 19 20 22 26 27 30 31 32
INFO: iteration 8, average log likelihood -0.951839
WARNING: Variances had to be floored 8 9 11 12 13 15 17 19 20 22 26 30 31
INFO: iteration 9, average log likelihood -0.955201
WARNING: Variances had to be floored 4 8 9 15 17 20 22 26 27 30
INFO: iteration 10, average log likelihood -0.964382
INFO: EM with 100000 data points 10 iterations avll -0.964382
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0897199   -0.00513409  -0.100873      0.0560633   0.00324924   0.0437745    0.191159    -0.0307071    -0.0137438    -0.0737075   0.0188448    0.0146696    0.125935    0.149851    -0.0397708     0.0639319    0.140553    -0.0884683   -0.0221272    -0.276198    -0.204092     0.168923    0.122214     0.0740844    0.061213      0.0532304 
  0.0584062    0.0585563    0.0394871     0.152584   -0.113984    -0.0620974   -0.100277     0.0180028     0.0350728     0.0600978   0.0335875    0.0378421   -0.0442782  -0.144747     0.0259162     0.0247632   -0.120743    -0.114121    -0.0317228    -0.191381    -0.0926445    0.190425   -0.128169    -0.130233     0.126463      0.00196974
 -0.105006     0.0443366    0.192952     -0.139934   -0.0374292   -0.173051     0.0890046   -0.0522262    -0.0503559     0.0609881   0.244898     0.10468     -0.057257    0.0781558   -0.000758994  -0.0142804   -0.126527    -0.0996576    0.00956605   -0.107212     0.0616408    0.101448   -0.0136559   -0.14654     -0.0108906    -0.0442284 
  0.0598979    0.184323    -0.232599      0.0148432   0.0244383   -0.167126    -0.0646311    0.15104       0.0918897     0.0172399  -0.00569859  -0.0774826    0.0253243  -0.149512     0.15765       0.126815     0.0201726    0.0279916    0.024292     -0.0755029    0.0997333    0.0220958  -0.0727831    0.0339945   -0.0581924    -0.028668  
  0.11337      0.0672812   -0.145499     -0.106223    0.0990923   -0.00513508  -0.00405856  -0.0109706     0.038212     -0.275441    0.0614488    0.122493    -0.0484953   0.0951834   -0.15511       0.0317939   -0.0737617    0.00700633  -0.170722     -0.151916    -0.164473     0.114091    0.110915     0.0350591   -0.0333787    -0.00655636
  0.155659    -0.295605    -0.0196536    -0.157437    0.136325     0.0772101    0.0231068   -0.0315913     0.0903567    -0.0828041  -0.0670412    0.125436    -0.151785    0.0573328   -0.0785525    -0.108421    -0.126101    -0.0303114    0.0562106     0.0626588   -0.0738236    0.0472468   0.0768065    0.202868     0.152308     -0.0512364 
 -0.0560264    0.0579988   -0.149938      0.144408   -0.0527999    0.122682    -0.00945323   0.190166     -0.0240057     0.104286   -0.242025     0.138176     0.0505064   0.0703439    0.00293784   -0.109186    -0.0391584    0.103699     0.0796219     0.0837162    0.206956     0.0356628   0.0128186   -0.0763232    0.0148842    -0.0433548 
  0.104868     0.0254817   -0.201797      0.070281   -0.0832163    0.0278177   -0.108764     0.00121168   -0.185299      0.113807    0.0798147    0.0749198    0.13386     0.0448724   -0.0808013     0.0404219    0.00553997   0.0766738    0.15165      -0.216669    -0.182335    -0.107792   -0.0840476    0.0126358   -0.042243      0.107496  
  0.0671782    0.0100015   -0.0379222    -0.0508944   0.0988335    0.158016    -0.190905    -0.0376288    -0.0440803    -0.222655    0.0651043    0.136592    -0.0396422   0.0603109   -0.105612     -0.15845      0.00827492   0.0928579    0.000434636   0.140169    -0.128552    -0.0373976  -0.0998526   -0.0214076   -0.131187      0.0186821 
  0.251443     0.00403317  -0.0377027     0.0807337   0.0389747    0.112839     0.133153    -0.0409693    -0.0687555     0.0536379  -0.0970831   -0.125315     0.0288674   0.0680098   -0.143305      0.133306    -0.138077    -0.0567047    0.0567587     0.0989741   -0.00200503   0.207755    0.0421596   -0.0447202    0.0266967     0.113611  
  0.158487    -0.129935     0.0302477    -0.0843007   0.03403     -0.03132      0.0181562    0.11713       0.0779731    -0.0106039   0.130839     0.0236162    0.176464   -0.178228    -0.100667      0.0911272   -0.00259101   0.059019     0.0340652    -0.0531287    0.135223    -0.166301    0.11849      0.117159    -0.0147048     0.0781947 
  0.00533642  -0.191549     0.0772466     0.195051    0.0415448   -0.0672769   -0.0250808   -0.0140388    -0.102919      0.0647847   0.0711978   -0.0749986   -0.108027    0.0870517    0.0186801     0.164865    -0.109027     0.035776    -0.0341511    -0.106656    -0.00444296   0.11672     0.0552553   -0.145435     0.151948      0.255616  
  0.0953965    0.127118     0.10869       0.0662457   0.092241     0.0678264   -0.0388373    0.042423      0.0917592    -0.0341776  -0.0150792   -0.00206814   0.10103    -0.0126811   -0.0203204     0.00606172  -0.0885309   -0.0291247    0.0340559    -0.0696222    0.126577    -0.0868111  -0.0156703   -0.0778997    0.0748391    -0.281142  
  0.0358484   -0.152925    -0.00723026   -0.104412   -0.0154111    0.0281763    0.161027     0.0111368    -0.0029409    -0.0262095   0.102293    -0.136283    -0.056395    0.0036196    0.147234     -0.122174     0.071901     0.0814338    0.00712524   -0.0519269    0.045323     0.137949   -0.0295227    0.0461341   -0.0297601     0.199372  
  0.105972     0.186488    -0.0242162     0.0207851   0.132008     0.107786    -0.00895997   0.140064      0.109248      0.0707807  -0.0991961   -0.0213861   -0.0947464   0.0658189   -0.0971107    -0.0103062   -0.145464     0.070686     0.0377745     0.0988609   -0.042875     0.0619603   0.11639     -0.194974    -0.139199     -0.0421605 
  0.0565927    0.117836    -0.0788226    -0.073286   -0.0809601    0.0886838   -0.00307537  -0.104219     -0.0258158    -0.0891057  -0.0279459   -0.0662772   -0.0536397  -0.118828     0.0417004     0.03276     -0.00787934  -0.0252228    0.0319026     0.138997     0.137918     0.102507   -0.135422    -0.119951     0.171374      0.0480693 
 -0.121658     0.0951625   -0.124596      0.169122   -0.0651015   -0.0795273    0.0395641   -0.101218      0.0924884    -0.0170195  -0.031205    -0.00862499  -0.0813309  -0.0140943   -0.04221       0.0221134   -0.0335443   -0.059111    -0.0684817     0.38324     -0.0298592    0.17435     0.0211417    0.00376414  -0.048826     -0.0774807 
  0.0195543    0.0964892    0.0302617     0.152035    0.0968207   -0.175402    -0.0936929    0.0430456     0.124755      0.131596    0.0753593   -0.016706    -0.143276    0.0285416    0.00986549   -0.126464    -0.103509    -0.15428      0.034099     -0.110099    -0.0343469    0.0295627   0.0482778    0.140965     0.0546992     0.00921197
 -0.0660345   -0.300887    -0.0105059    -0.0114411   0.0343641    0.037265     0.127421     0.0238671    -0.00591597    0.146357    0.0269553    0.0936891    0.0772852   0.00994669  -0.0282927    -0.137922     0.0237212    0.0387229   -0.10867      -0.115916    -0.0426378   -0.0401281   0.0545131    0.0966131    0.0617287    -0.0199625 
  0.110704     0.0486249   -0.0531285     0.0287628  -0.0486681    0.153673     0.0852283   -0.0538362     0.0610155     0.0604376  -0.10613      0.0172962    0.033764   -0.125947    -0.0785319    -0.0222301   -0.0740209   -0.271307    -0.00123793    0.116628    -0.0232978   -0.0890791  -0.0638209    0.0440825   -0.0362688    -0.0233477 
  0.201147     0.202204    -0.17582       0.0428191  -0.0780557   -0.11861      0.0157142   -0.0142373     0.0993141     0.0441833  -0.0491745   -0.041379    -0.140956   -0.0401434   -0.0713776     0.0953295   -0.101438     0.0280426    0.0615395    -0.0197363    0.177669     0.0882201   0.0136575    0.0528067    0.00575003   -0.167705  
 -0.0017306    0.114285    -0.0297531     0.0192691  -0.0136755   -0.0116879   -0.0144523   -0.0442081     0.115484      0.0227905   0.022969    -0.0121841    0.023861   -0.00307055  -0.260663      0.143169     0.0391932    0.0911966    0.049012     -0.025956    -0.239356     0.14671    -0.0112067    0.0670541   -0.217601     -0.0771692 
 -0.052042     0.011071     0.0632873    -0.183395   -0.0187119    0.0702754    0.179125    -0.00296085    0.0797537     0.0654782  -0.0623693   -0.130326    -0.082046   -0.0857641   -0.0573794    -0.0256894   -0.147984    -0.00942126   0.0110308    -0.0044929    0.0965124    0.0977863  -0.0442733    0.170742     0.0233585    -0.0664789 
  0.0166307   -0.172373    -0.0114152     0.390558    0.151774    -0.0995572    0.0873366   -0.103996      0.177394      0.0796191  -0.0283828   -0.246279    -0.240435    0.0701222    0.088908     -0.160335    -0.13774     -0.0387966    0.0640941    -0.00263415  -0.0789503    0.0335843   0.0443747    0.0194304    0.172544      0.0621224 
  0.169156    -0.00273085   0.111201     -0.160165    0.00237241   0.021208     0.145321    -0.140674     -0.0229449     0.0496323   0.0361536   -0.0309591   -0.0515284  -0.109969    -0.119844      0.0380027   -0.168629     0.13622      0.212662      0.0493621    0.0874648   -0.12504     0.127658     0.0308175    0.123746      0.0327316 
  0.0817903    0.00316509   0.0320665     0.0454493  -0.0381214    0.182508    -0.155259    -0.010585     -0.0514496     0.0178809  -0.162728     0.0480909    0.0135027  -0.0348581    0.0130084    -0.0367476   -0.223549    -0.0392817    0.0314697    -0.0405232   -0.0422394    0.0491314   0.0353001    0.00989033   0.114242     -0.155215  
 -0.0254535   -0.0848378    0.046088     -0.089268    0.213742    -0.0201869    0.0732648   -0.0512921     0.0306037     0.049501    0.133121    -0.0466235    0.137434    0.146296     0.0984348     0.130464    -0.0903076    0.0228505   -0.0977868    -0.0106383   -0.110986    -0.038494    0.118975     0.168405    -0.00820037   -0.118176  
  0.0512429   -0.0309424   -0.150238     -0.0405768   0.0842058    0.205178    -0.00591715  -0.0545703     0.039281     -0.0815845   0.0974946   -0.0306235    0.0234627  -0.0260859    0.0996081     0.0181577   -0.00538173  -0.120633     0.0891064    -0.169049    -0.053482     0.0890054  -0.0720749    0.099591    -0.177487     -0.168162  
  0.123831     0.0761058   -0.0118163    -0.0942513   0.0741311   -0.0530359    0.0523907    0.000529278   0.0018991    -0.0595526   0.0124121   -0.0701681   -0.300674   -0.0440011    0.089503     -0.11325      0.122164     0.0301188   -0.177074     -0.0868666    0.082618     0.0961279   0.090429     0.0150684    0.120748      0.0583996 
 -0.0674378    0.0597683    0.0149828    -0.0237516   0.0409502   -0.0935335    0.0165116   -0.0267099     0.000919567   0.145387    0.139057    -0.0341378    0.0795583  -0.058718     0.127591      0.0740074   -0.181299     0.0500227   -0.0750925    -0.0209339    0.127011    -0.166152    0.00790094  -0.105664    -0.02323      -0.00710258
  0.126326     0.0901894   -0.0128559    -0.0714281  -0.0397749   -0.0425878   -0.00140814   0.0871533     0.17528       0.108667    0.127559     0.0367395    0.0238009  -0.0477156    0.0262034     0.245698    -0.0580945    0.0280214    0.0977615    -0.041775    -0.0345269   -0.14622    -0.00210104   0.0101864   -0.000423027  -0.18191   
  0.0924167    0.0145133   -0.000307099   0.0103488   0.236621     0.0141509    0.0657971    0.128728     -0.0127379    -0.0889284   0.0426323   -0.0644283   -0.066066   -0.0739487    0.0172504     0.0437709    0.180173     0.114404     0.0817323    -0.0225558    0.143213     0.167799   -0.1818      -0.0454497    0.0468051     0.187909  kind full, method split
0: avll = -1.4243579650180913
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.424376
INFO: iteration 2, average log likelihood -1.424315
INFO: iteration 3, average log likelihood -1.424267
INFO: iteration 4, average log likelihood -1.424207
INFO: iteration 5, average log likelihood -1.424132
INFO: iteration 6, average log likelihood -1.424041
INFO: iteration 7, average log likelihood -1.423937
INFO: iteration 8, average log likelihood -1.423818
INFO: iteration 9, average log likelihood -1.423672
INFO: iteration 10, average log likelihood -1.423460
INFO: iteration 11, average log likelihood -1.423101
INFO: iteration 12, average log likelihood -1.422495
INFO: iteration 13, average log likelihood -1.421614
INFO: iteration 14, average log likelihood -1.420635
INFO: iteration 15, average log likelihood -1.419863
INFO: iteration 16, average log likelihood -1.419421
INFO: iteration 17, average log likelihood -1.419218
INFO: iteration 18, average log likelihood -1.419133
INFO: iteration 19, average log likelihood -1.419098
INFO: iteration 20, average log likelihood -1.419083
INFO: iteration 21, average log likelihood -1.419077
INFO: iteration 22, average log likelihood -1.419074
INFO: iteration 23, average log likelihood -1.419073
INFO: iteration 24, average log likelihood -1.419072
INFO: iteration 25, average log likelihood -1.419072
INFO: iteration 26, average log likelihood -1.419071
INFO: iteration 27, average log likelihood -1.419071
INFO: iteration 28, average log likelihood -1.419071
INFO: iteration 29, average log likelihood -1.419071
INFO: iteration 30, average log likelihood -1.419071
INFO: iteration 31, average log likelihood -1.419071
INFO: iteration 32, average log likelihood -1.419070
INFO: iteration 33, average log likelihood -1.419070
INFO: iteration 34, average log likelihood -1.419070
INFO: iteration 35, average log likelihood -1.419070
INFO: iteration 36, average log likelihood -1.419070
INFO: iteration 37, average log likelihood -1.419070
INFO: iteration 38, average log likelihood -1.419070
INFO: iteration 39, average log likelihood -1.419070
INFO: iteration 40, average log likelihood -1.419070
INFO: iteration 41, average log likelihood -1.419070
INFO: iteration 42, average log likelihood -1.419070
INFO: iteration 43, average log likelihood -1.419070
INFO: iteration 44, average log likelihood -1.419070
INFO: iteration 45, average log likelihood -1.419069
INFO: iteration 46, average log likelihood -1.419069
INFO: iteration 47, average log likelihood -1.419069
INFO: iteration 48, average log likelihood -1.419069
INFO: iteration 49, average log likelihood -1.419069
INFO: iteration 50, average log likelihood -1.419069
INFO: EM with 100000 data points 50 iterations avll -1.419069
952.4 data points per parameter
1: avll = [-1.42438,-1.42432,-1.42427,-1.42421,-1.42413,-1.42404,-1.42394,-1.42382,-1.42367,-1.42346,-1.4231,-1.4225,-1.42161,-1.42064,-1.41986,-1.41942,-1.41922,-1.41913,-1.4191,-1.41908,-1.41908,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.419084
INFO: iteration 2, average log likelihood -1.419019
INFO: iteration 3, average log likelihood -1.418958
INFO: iteration 4, average log likelihood -1.418879
INFO: iteration 5, average log likelihood -1.418777
INFO: iteration 6, average log likelihood -1.418652
INFO: iteration 7, average log likelihood -1.418512
INFO: iteration 8, average log likelihood -1.418373
INFO: iteration 9, average log likelihood -1.418249
INFO: iteration 10, average log likelihood -1.418151
INFO: iteration 11, average log likelihood -1.418078
INFO: iteration 12, average log likelihood -1.418027
INFO: iteration 13, average log likelihood -1.417991
INFO: iteration 14, average log likelihood -1.417966
INFO: iteration 15, average log likelihood -1.417947
INFO: iteration 16, average log likelihood -1.417933
INFO: iteration 17, average log likelihood -1.417922
INFO: iteration 18, average log likelihood -1.417913
INFO: iteration 19, average log likelihood -1.417905
INFO: iteration 20, average log likelihood -1.417898
INFO: iteration 21, average log likelihood -1.417893
INFO: iteration 22, average log likelihood -1.417887
INFO: iteration 23, average log likelihood -1.417883
INFO: iteration 24, average log likelihood -1.417879
INFO: iteration 25, average log likelihood -1.417875
INFO: iteration 26, average log likelihood -1.417872
INFO: iteration 27, average log likelihood -1.417869
INFO: iteration 28, average log likelihood -1.417866
INFO: iteration 29, average log likelihood -1.417863
INFO: iteration 30, average log likelihood -1.417861
INFO: iteration 31, average log likelihood -1.417859
INFO: iteration 32, average log likelihood -1.417857
INFO: iteration 33, average log likelihood -1.417855
INFO: iteration 34, average log likelihood -1.417854
INFO: iteration 35, average log likelihood -1.417852
INFO: iteration 36, average log likelihood -1.417850
INFO: iteration 37, average log likelihood -1.417849
INFO: iteration 38, average log likelihood -1.417848
INFO: iteration 39, average log likelihood -1.417846
INFO: iteration 40, average log likelihood -1.417845
INFO: iteration 41, average log likelihood -1.417844
INFO: iteration 42, average log likelihood -1.417843
INFO: iteration 43, average log likelihood -1.417842
INFO: iteration 44, average log likelihood -1.417841
INFO: iteration 45, average log likelihood -1.417840
INFO: iteration 46, average log likelihood -1.417839
INFO: iteration 47, average log likelihood -1.417838
INFO: iteration 48, average log likelihood -1.417837
INFO: iteration 49, average log likelihood -1.417836
INFO: iteration 50, average log likelihood -1.417836
INFO: EM with 100000 data points 50 iterations avll -1.417836
473.9 data points per parameter
2: avll = [-1.41908,-1.41902,-1.41896,-1.41888,-1.41878,-1.41865,-1.41851,-1.41837,-1.41825,-1.41815,-1.41808,-1.41803,-1.41799,-1.41797,-1.41795,-1.41793,-1.41792,-1.41791,-1.41791,-1.4179,-1.41789,-1.41789,-1.41788,-1.41788,-1.41788,-1.41787,-1.41787,-1.41787,-1.41786,-1.41786,-1.41786,-1.41786,-1.41786,-1.41785,-1.41785,-1.41785,-1.41785,-1.41785,-1.41785,-1.41785,-1.41784,-1.41784,-1.41784,-1.41784,-1.41784,-1.41784,-1.41784,-1.41784,-1.41784,-1.41784]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.417846
INFO: iteration 2, average log likelihood -1.417797
INFO: iteration 3, average log likelihood -1.417756
INFO: iteration 4, average log likelihood -1.417708
INFO: iteration 5, average log likelihood -1.417651
INFO: iteration 6, average log likelihood -1.417582
INFO: iteration 7, average log likelihood -1.417501
INFO: iteration 8, average log likelihood -1.417412
INFO: iteration 9, average log likelihood -1.417321
INFO: iteration 10, average log likelihood -1.417232
INFO: iteration 11, average log likelihood -1.417151
INFO: iteration 12, average log likelihood -1.417080
INFO: iteration 13, average log likelihood -1.417020
INFO: iteration 14, average log likelihood -1.416969
INFO: iteration 15, average log likelihood -1.416926
INFO: iteration 16, average log likelihood -1.416889
INFO: iteration 17, average log likelihood -1.416857
INFO: iteration 18, average log likelihood -1.416827
INFO: iteration 19, average log likelihood -1.416800
INFO: iteration 20, average log likelihood -1.416775
INFO: iteration 21, average log likelihood -1.416752
INFO: iteration 22, average log likelihood -1.416730
INFO: iteration 23, average log likelihood -1.416709
INFO: iteration 24, average log likelihood -1.416690
INFO: iteration 25, average log likelihood -1.416671
INFO: iteration 26, average log likelihood -1.416653
INFO: iteration 27, average log likelihood -1.416637
INFO: iteration 28, average log likelihood -1.416620
INFO: iteration 29, average log likelihood -1.416605
INFO: iteration 30, average log likelihood -1.416590
INFO: iteration 31, average log likelihood -1.416575
INFO: iteration 32, average log likelihood -1.416562
INFO: iteration 33, average log likelihood -1.416548
INFO: iteration 34, average log likelihood -1.416535
INFO: iteration 35, average log likelihood -1.416522
INFO: iteration 36, average log likelihood -1.416510
INFO: iteration 37, average log likelihood -1.416498
INFO: iteration 38, average log likelihood -1.416487
INFO: iteration 39, average log likelihood -1.416475
INFO: iteration 40, average log likelihood -1.416465
INFO: iteration 41, average log likelihood -1.416454
INFO: iteration 42, average log likelihood -1.416444
INFO: iteration 43, average log likelihood -1.416434
INFO: iteration 44, average log likelihood -1.416425
INFO: iteration 45, average log likelihood -1.416415
INFO: iteration 46, average log likelihood -1.416406
INFO: iteration 47, average log likelihood -1.416398
INFO: iteration 48, average log likelihood -1.416389
INFO: iteration 49, average log likelihood -1.416381
INFO: iteration 50, average log likelihood -1.416372
INFO: EM with 100000 data points 50 iterations avll -1.416372
236.4 data points per parameter
3: avll = [-1.41785,-1.4178,-1.41776,-1.41771,-1.41765,-1.41758,-1.4175,-1.41741,-1.41732,-1.41723,-1.41715,-1.41708,-1.41702,-1.41697,-1.41693,-1.41689,-1.41686,-1.41683,-1.4168,-1.41678,-1.41675,-1.41673,-1.41671,-1.41669,-1.41667,-1.41665,-1.41664,-1.41662,-1.4166,-1.41659,-1.41658,-1.41656,-1.41655,-1.41653,-1.41652,-1.41651,-1.4165,-1.41649,-1.41648,-1.41646,-1.41645,-1.41644,-1.41643,-1.41642,-1.41642,-1.41641,-1.4164,-1.41639,-1.41638,-1.41637]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.416372
INFO: iteration 2, average log likelihood -1.416313
INFO: iteration 3, average log likelihood -1.416255
INFO: iteration 4, average log likelihood -1.416185
INFO: iteration 5, average log likelihood -1.416097
INFO: iteration 6, average log likelihood -1.415988
INFO: iteration 7, average log likelihood -1.415859
INFO: iteration 8, average log likelihood -1.415715
INFO: iteration 9, average log likelihood -1.415565
INFO: iteration 10, average log likelihood -1.415421
INFO: iteration 11, average log likelihood -1.415288
INFO: iteration 12, average log likelihood -1.415170
INFO: iteration 13, average log likelihood -1.415068
INFO: iteration 14, average log likelihood -1.414981
INFO: iteration 15, average log likelihood -1.414907
INFO: iteration 16, average log likelihood -1.414844
INFO: iteration 17, average log likelihood -1.414791
INFO: iteration 18, average log likelihood -1.414746
INFO: iteration 19, average log likelihood -1.414708
INFO: iteration 20, average log likelihood -1.414675
INFO: iteration 21, average log likelihood -1.414647
INFO: iteration 22, average log likelihood -1.414622
INFO: iteration 23, average log likelihood -1.414600
INFO: iteration 24, average log likelihood -1.414580
INFO: iteration 25, average log likelihood -1.414563
INFO: iteration 26, average log likelihood -1.414546
INFO: iteration 27, average log likelihood -1.414531
INFO: iteration 28, average log likelihood -1.414517
INFO: iteration 29, average log likelihood -1.414504
INFO: iteration 30, average log likelihood -1.414492
INFO: iteration 31, average log likelihood -1.414480
INFO: iteration 32, average log likelihood -1.414469
INFO: iteration 33, average log likelihood -1.414458
INFO: iteration 34, average log likelihood -1.414448
INFO: iteration 35, average log likelihood -1.414439
INFO: iteration 36, average log likelihood -1.414429
INFO: iteration 37, average log likelihood -1.414421
INFO: iteration 38, average log likelihood -1.414412
INFO: iteration 39, average log likelihood -1.414404
INFO: iteration 40, average log likelihood -1.414397
INFO: iteration 41, average log likelihood -1.414390
INFO: iteration 42, average log likelihood -1.414383
INFO: iteration 43, average log likelihood -1.414376
INFO: iteration 44, average log likelihood -1.414370
INFO: iteration 45, average log likelihood -1.414364
INFO: iteration 46, average log likelihood -1.414358
INFO: iteration 47, average log likelihood -1.414352
INFO: iteration 48, average log likelihood -1.414347
INFO: iteration 49, average log likelihood -1.414342
INFO: iteration 50, average log likelihood -1.414337
INFO: EM with 100000 data points 50 iterations avll -1.414337
118.1 data points per parameter
4: avll = [-1.41637,-1.41631,-1.41625,-1.41618,-1.4161,-1.41599,-1.41586,-1.41571,-1.41557,-1.41542,-1.41529,-1.41517,-1.41507,-1.41498,-1.41491,-1.41484,-1.41479,-1.41475,-1.41471,-1.41467,-1.41465,-1.41462,-1.4146,-1.41458,-1.41456,-1.41455,-1.41453,-1.41452,-1.4145,-1.41449,-1.41448,-1.41447,-1.41446,-1.41445,-1.41444,-1.41443,-1.41442,-1.41441,-1.4144,-1.4144,-1.41439,-1.41438,-1.41438,-1.41437,-1.41436,-1.41436,-1.41435,-1.41435,-1.41434,-1.41434]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.414340
INFO: iteration 2, average log likelihood -1.414285
INFO: iteration 3, average log likelihood -1.414232
INFO: iteration 4, average log likelihood -1.414169
INFO: iteration 5, average log likelihood -1.414090
INFO: iteration 6, average log likelihood -1.413992
INFO: iteration 7, average log likelihood -1.413874
INFO: iteration 8, average log likelihood -1.413736
INFO: iteration 9, average log likelihood -1.413587
INFO: iteration 10, average log likelihood -1.413433
INFO: iteration 11, average log likelihood -1.413285
INFO: iteration 12, average log likelihood -1.413150
INFO: iteration 13, average log likelihood -1.413029
INFO: iteration 14, average log likelihood -1.412924
INFO: iteration 15, average log likelihood -1.412835
INFO: iteration 16, average log likelihood -1.412757
INFO: iteration 17, average log likelihood -1.412690
INFO: iteration 18, average log likelihood -1.412631
INFO: iteration 19, average log likelihood -1.412579
INFO: iteration 20, average log likelihood -1.412532
INFO: iteration 21, average log likelihood -1.412489
INFO: iteration 22, average log likelihood -1.412449
INFO: iteration 23, average log likelihood -1.412412
INFO: iteration 24, average log likelihood -1.412378
INFO: iteration 25, average log likelihood -1.412346
INFO: iteration 26, average log likelihood -1.412316
INFO: iteration 27, average log likelihood -1.412287
INFO: iteration 28, average log likelihood -1.412260
INFO: iteration 29, average log likelihood -1.412234
INFO: iteration 30, average log likelihood -1.412209
INFO: iteration 31, average log likelihood -1.412185
INFO: iteration 32, average log likelihood -1.412163
INFO: iteration 33, average log likelihood -1.412141
INFO: iteration 34, average log likelihood -1.412120
INFO: iteration 35, average log likelihood -1.412100
INFO: iteration 36, average log likelihood -1.412081
INFO: iteration 37, average log likelihood -1.412062
INFO: iteration 38, average log likelihood -1.412044
INFO: iteration 39, average log likelihood -1.412027
INFO: iteration 40, average log likelihood -1.412010
INFO: iteration 41, average log likelihood -1.411994
INFO: iteration 42, average log likelihood -1.411978
INFO: iteration 43, average log likelihood -1.411963
INFO: iteration 44, average log likelihood -1.411948
INFO: iteration 45, average log likelihood -1.411934
INFO: iteration 46, average log likelihood -1.411919
INFO: iteration 47, average log likelihood -1.411906
INFO: iteration 48, average log likelihood -1.411892
INFO: iteration 49, average log likelihood -1.411879
INFO: iteration 50, average log likelihood -1.411865
INFO: EM with 100000 data points 50 iterations avll -1.411865
59.0 data points per parameter
5: avll = [-1.41434,-1.41429,-1.41423,-1.41417,-1.41409,-1.41399,-1.41387,-1.41374,-1.41359,-1.41343,-1.41329,-1.41315,-1.41303,-1.41292,-1.41283,-1.41276,-1.41269,-1.41263,-1.41258,-1.41253,-1.41249,-1.41245,-1.41241,-1.41238,-1.41235,-1.41232,-1.41229,-1.41226,-1.41223,-1.41221,-1.41219,-1.41216,-1.41214,-1.41212,-1.4121,-1.41208,-1.41206,-1.41204,-1.41203,-1.41201,-1.41199,-1.41198,-1.41196,-1.41195,-1.41193,-1.41192,-1.41191,-1.41189,-1.41188,-1.41187]
[-1.42436,-1.42438,-1.42432,-1.42427,-1.42421,-1.42413,-1.42404,-1.42394,-1.42382,-1.42367,-1.42346,-1.4231,-1.4225,-1.42161,-1.42064,-1.41986,-1.41942,-1.41922,-1.41913,-1.4191,-1.41908,-1.41908,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41907,-1.41908,-1.41902,-1.41896,-1.41888,-1.41878,-1.41865,-1.41851,-1.41837,-1.41825,-1.41815,-1.41808,-1.41803,-1.41799,-1.41797,-1.41795,-1.41793,-1.41792,-1.41791,-1.41791,-1.4179,-1.41789,-1.41789,-1.41788,-1.41788,-1.41788,-1.41787,-1.41787,-1.41787,-1.41786,-1.41786,-1.41786,-1.41786,-1.41786,-1.41785,-1.41785,-1.41785,-1.41785,-1.41785,-1.41785,-1.41785,-1.41784,-1.41784,-1.41784,-1.41784,-1.41784,-1.41784,-1.41784,-1.41784,-1.41784,-1.41784,-1.41785,-1.4178,-1.41776,-1.41771,-1.41765,-1.41758,-1.4175,-1.41741,-1.41732,-1.41723,-1.41715,-1.41708,-1.41702,-1.41697,-1.41693,-1.41689,-1.41686,-1.41683,-1.4168,-1.41678,-1.41675,-1.41673,-1.41671,-1.41669,-1.41667,-1.41665,-1.41664,-1.41662,-1.4166,-1.41659,-1.41658,-1.41656,-1.41655,-1.41653,-1.41652,-1.41651,-1.4165,-1.41649,-1.41648,-1.41646,-1.41645,-1.41644,-1.41643,-1.41642,-1.41642,-1.41641,-1.4164,-1.41639,-1.41638,-1.41637,-1.41637,-1.41631,-1.41625,-1.41618,-1.4161,-1.41599,-1.41586,-1.41571,-1.41557,-1.41542,-1.41529,-1.41517,-1.41507,-1.41498,-1.41491,-1.41484,-1.41479,-1.41475,-1.41471,-1.41467,-1.41465,-1.41462,-1.4146,-1.41458,-1.41456,-1.41455,-1.41453,-1.41452,-1.4145,-1.41449,-1.41448,-1.41447,-1.41446,-1.41445,-1.41444,-1.41443,-1.41442,-1.41441,-1.4144,-1.4144,-1.41439,-1.41438,-1.41438,-1.41437,-1.41436,-1.41436,-1.41435,-1.41435,-1.41434,-1.41434,-1.41434,-1.41429,-1.41423,-1.41417,-1.41409,-1.41399,-1.41387,-1.41374,-1.41359,-1.41343,-1.41329,-1.41315,-1.41303,-1.41292,-1.41283,-1.41276,-1.41269,-1.41263,-1.41258,-1.41253,-1.41249,-1.41245,-1.41241,-1.41238,-1.41235,-1.41232,-1.41229,-1.41226,-1.41223,-1.41221,-1.41219,-1.41216,-1.41214,-1.41212,-1.4121,-1.41208,-1.41206,-1.41204,-1.41203,-1.41201,-1.41199,-1.41198,-1.41196,-1.41195,-1.41193,-1.41192,-1.41191,-1.41189,-1.41188,-1.41187]
32×26 Array{Float64,2}:
 -0.775466    -0.404313      0.462796    0.0684557   0.360431   -0.402772    -0.339499   -0.618401     0.132213   -0.0657608    0.333358    -0.191722    0.0133291   0.0120724   -0.356409   -0.536546    -0.912563     0.0844701   -0.0905426   -0.12099     -0.203828    0.0794802  -0.107266    -0.214422     -0.0690263    0.471234  
 -0.629783    -0.408036      0.401565    0.395932   -0.377191   -0.189096    -0.507244   -0.399623     0.231472    0.328558     0.432027     0.0620914   0.475428   -0.809171     0.440429    0.133299    -0.211634     0.00109171   0.108134    -0.567559    -0.222582   -0.23848     0.579255     0.285403      0.156664     0.335345  
 -0.0702299   -0.104806     -0.146736   -0.570673    0.250765   -0.110754     0.754549   -0.121693     0.0986539   0.157221     0.238692     0.0944045   0.302986    0.243005     0.0901037  -0.633523    -0.307213     0.226248    -0.0310627   -0.465628    -0.119064   -0.765935    0.459784    -0.310035      0.137972     0.00352525
 -0.377246     0.148913     -0.0710617  -0.69505    -0.248592    0.497913     0.512952    0.327348     0.0128834   0.247908     0.417119    -0.196219    0.0077075  -0.0469276   -0.246285    0.223533     0.223255     0.592456    -0.373835     0.167807    -0.0855873  -0.0563033   0.344381    -0.18962       0.251241     0.0917919 
  0.00140628  -0.769599     -0.267678   -0.416534    0.0739963   0.0329507    0.165332   -0.290461     0.210528   -0.614103     0.585867    -0.0171148  -0.20515    -0.365734     0.251208    0.346501     0.218074     0.0684377   -0.908635     0.301147     0.156656   -0.174904   -0.812936     0.326916     -0.433265    -0.432541  
 -0.395101     0.394649      0.317859   -0.124026    0.100397   -0.186028    -0.163628    0.106972     0.231948   -0.458037     0.427253     0.0706634  -0.0220846   0.159027     0.222286   -0.491732    -0.0584851   -0.117983     0.137537     0.504097     0.169565    0.555051   -0.490946     0.739436     -0.262627    -0.576413  
 -0.387718    -0.774934      0.274754   -0.38185    -0.360031    0.313409     0.550467    0.252013     0.565665   -0.574447     0.00466724   0.239941   -0.752895    0.289317     0.696641    0.183991    -0.49268      0.110784     0.0802901   -0.314234    -0.599949    0.584297    0.182012    -0.253272     -0.0817677    0.418003  
 -0.257616    -1.09682      -0.082735   -0.203463    0.181459    0.325956     0.138935    0.200968     0.958337    0.0139972   -0.262238    -0.324789   -0.25758     0.22032     -0.21434    -0.30945     -0.046587    -0.53712      0.73706      0.491059    -0.0117728   0.124119    0.236296     0.0234699    -0.371472     0.169529  
  0.849256    -0.0106296    -0.294764    0.0055566  -0.114993   -0.175583     0.0893522  -0.617587    -0.121086   -0.132929    -0.233464    -0.771051    0.0833745   0.48566     -0.434534    0.292359    -0.294163    -0.117349    -0.6908      -0.717187    -0.742618    0.903689    0.115393    -0.240366     -0.763458     0.543477  
  0.526888     0.369428      0.669337   -0.456351   -0.444964   -0.624634     0.17927    -0.217646    -0.0995102  -0.804414    -0.40708     -0.31738    -0.271988    0.353686    -0.0819316  -0.308857    -0.0886093    0.396076     0.074371    -0.261995    -1.04621     0.273387   -0.296115    -0.617106      0.025486     0.0644524 
  0.161352     0.509784     -0.425561    0.385336    0.2822     -0.481668    -0.211485    0.329       -0.900126    0.0926609    0.175915     0.130153    0.319521   -0.262634    -0.260376   -0.0582106    0.0685288    0.122139    -0.534329     0.129973     0.406843   -0.0606312  -0.135717     0.0387261     0.441621    -0.0977393 
 -0.0842336    0.436372      0.189651    0.265982   -0.152364   -0.301921    -0.27911    -0.635279    -0.302516   -0.335879    -0.0355837    0.1061     -0.358748    0.0663329    0.706905    0.398737    -0.0859268    0.683807    -0.743251    -0.242922     0.0956823  -0.0966915   0.00740163  -0.175568      0.318573     0.470383  
 -0.236091     0.000545736  -0.436558   -0.382439    0.0527474  -0.12585     -0.476072    0.123653     0.0512623   0.710626    -0.0541216    0.157756   -0.374118   -0.150491    -0.523717   -0.0555642    0.0886969    0.624782     0.419134    -0.249361     0.257757   -0.0127887   1.08514      0.252527     -0.301944     0.136465  
  0.543194     0.239008      0.129406   -0.0440443  -0.100004    0.320894    -0.187679   -0.16297     -0.462354    1.04755     -0.214937    -0.0850728   0.130795   -0.208312    -0.362878    0.320176     0.341877     0.130476     0.1263      -0.109285     0.343227   -0.473315   -0.175462     0.118874     -0.407679    -0.462262  
  0.271988    -0.0193021     0.162684    0.301006   -0.632765    0.228432     0.297701    0.280686     0.170822   -0.127048    -0.0252148    0.172899    0.184889    0.0437402    0.182738    0.267496    -0.146623     0.0209965    0.0197738   -0.0691687   -0.143654    0.288608   -0.0783768   -0.127636      0.182029     0.22082   
  0.140365     0.289742     -0.368471   -0.0319454   1.01618    -0.149384     0.0619669  -0.00792198  -0.362314   -0.352246    -0.023602    -0.129565   -0.102954    0.172577    -0.219101   -0.307257    -0.0903591   -0.242224    -0.270345     0.0683569    0.0273345   0.0614472  -0.2613      -0.13028       0.17965     -0.264998  
 -0.468616    -0.156907     -0.0082219   0.0293744   0.272687   -0.0927655   -0.0335336  -0.148453     0.161252    0.0266054   -0.0380256   -0.260045    0.149193    0.00231885   0.0754014  -0.311022    -0.326142    -0.0325451    0.00478051   0.0432066   -0.0836958   0.322785   -0.0539217    0.233266     -0.363602    -0.102099  
 -0.133774    -0.120104     -0.104887    0.184764   -0.034168   -0.0403754    0.126986    0.101681     0.508964   -0.32728      0.483671     0.154298   -0.0614925  -0.100872     0.223114   -0.00959324  -0.277847    -0.112966     0.0380537    0.0115176    0.0421199   0.0237617  -0.0348906   -0.0161818     0.310597     0.258816  
  0.281177     0.154402      0.0871129  -0.0745284  -0.131847    0.0486938    0.0262263   0.0468627   -0.104067   -0.178611    -0.0956466    0.1483     -0.0875541   0.0657989    0.0314864   0.131986     0.0920355   -0.00847192  -0.0264554   -0.012591     0.0272951   0.119788   -0.113502    -0.00244534   -0.0197986   -0.0472563 
  0.0521032   -0.0352977    -0.110927   -0.0273804   0.0767476   0.0110377   -0.0658857  -0.083062    -0.0893912   0.498007    -0.0730556   -0.158012   -0.121687   -0.0518662   -0.22069     0.132661     0.0369969    0.226194    -0.0178039   -0.202635    -0.0539556  -0.18083     0.240982    -0.221557      0.107115     0.0989749 
  0.387924    -0.265106      0.450724    0.0588906  -0.0727798   0.25183     -0.178623   -0.172335     0.0255971   0.126903    -0.232704     0.215379    0.145398   -0.0628335    0.19105     0.439469    -0.17437     -0.363217     0.370053    -0.17607     -0.149549   -0.123584   -0.481559    -0.000268236  -0.168836    -0.428616  
  0.401995     0.100297      0.224459    0.2247     -0.134627    0.525755    -0.135188    0.0106848    0.0369325   0.218616    -0.170385     0.120993   -0.270219    0.0398802   -0.441758    0.263848     0.103483    -0.322919     0.130906    -0.0103244    0.0791539   0.305704   -0.464587    -0.444994      0.0225594    0.41039   
 -0.0393024    0.0146873    -0.293857    0.209735    0.132575    0.267501     0.0342934  -0.067493     0.200626   -0.00818701  -0.123       -0.373901    0.681534   -0.189738     0.0416729  -0.300667     0.225466    -1.11289      0.1983      -0.473748    -0.328954    0.0255967  -0.310382    -0.0695571     0.148254    -0.0857108 
 -0.131987     0.314221      0.34542     0.427134   -0.10816    -0.232976    -0.23515     0.406761    -0.451793    0.0891191   -0.0790194    0.0614092   0.565671    0.294848    -0.307398   -0.324659    -0.608159    -0.332838     0.420662    -0.38625     -0.557176    0.534256    0.221852    -0.406012     -0.00700818   0.184204  
  0.300869     0.129006     -0.0341063   0.603904   -0.22067    -0.565569     0.234194   -0.00521418  -0.12399    -0.290569     0.0384016   -0.148123   -0.948779   -0.216197    -0.0521996   0.114981    -0.464961     0.354587    -0.269677     0.236115     0.355306    0.503241    0.144336     0.0091698    -0.199263     0.252824  
  0.546408     0.241517     -0.553671   -0.305039    0.0820235  -0.624783     0.234427    0.791834     0.280875   -0.995893     0.28635      0.0625217  -0.62299     0.123158     0.171608    0.241956    -0.46444      0.497924     0.163416     0.180875    -0.160549    0.434609   -0.0979449    0.627207     -0.02666     -0.142835  
  0.686834     0.226358     -0.350275    0.0457901  -0.0704426   0.268614     0.240285    0.256873    -0.427937   -0.224281    -0.255003     0.0165871  -0.0692164   0.297237     0.114558    0.615487     0.96704     -0.14036     -0.129076     0.333649     0.0312471  -0.236839    0.226504     0.0944041    -0.0624695   -0.186304  
  0.00921615   0.442998     -0.252986    0.566475   -0.379177    0.730585    -0.0188598   0.388946     0.0609253  -0.221766    -0.192572     0.851953    0.080986   -0.329475     0.315446    0.123001     0.380243    -0.343057     0.0960861    0.00470532   0.499027    0.103291    0.220465     0.123042      0.774275     0.223857  
  0.11676     -0.376256     -0.28835    -0.291032    0.413209    0.00139958  -0.0376403   0.13919     -0.241806   -0.0668123   -0.47279     -0.0385241   0.0394862   0.317568     0.0188767   0.082564     0.00870633   0.113792     0.175181     0.353577    -0.112999   -0.0268981   0.526867     0.260673     -0.101002    -0.901676  
  0.689506     0.918551     -0.0760608  -0.248217    0.147585   -0.10594      0.0237965  -0.151091    -0.0243252   0.28488     -0.282757    -0.0359524  -0.180913    0.0434294   -0.320236   -0.314814     0.133412     0.442578     0.156729    -0.139963     0.504688   -0.0928998   0.0242174    0.443584      0.14923     -0.258947  
 -0.097028     0.0896926    -0.556894   -0.0489071   0.861788    0.259956     0.0017392  -0.255361     0.708512    0.611903     0.231665    -0.383821    0.181508    0.0312387   -0.174948   -0.229223     0.230472    -0.159291     0.137311     0.87906      0.526327   -0.364021   -0.00502977   0.530993      0.0285052   -0.428525  
  0.128398    -0.189756     -0.335732    0.357527    0.171316   -0.200561     0.0839604   0.189645     0.629172    0.56895      0.2099       0.137442   -0.308314   -0.685448     0.677876    0.361976     0.133532     0.116146     0.558962     0.284349     0.905235   -0.546739   -0.182423     0.512276      0.291052    -0.367605  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.411852
INFO: iteration 2, average log likelihood -1.411839
INFO: iteration 3, average log likelihood -1.411827
INFO: iteration 4, average log likelihood -1.411814
INFO: iteration 5, average log likelihood -1.411801
INFO: iteration 6, average log likelihood -1.411789
INFO: iteration 7, average log likelihood -1.411777
INFO: iteration 8, average log likelihood -1.411764
INFO: iteration 9, average log likelihood -1.411752
INFO: iteration 10, average log likelihood -1.411740
INFO: EM with 100000 data points 10 iterations avll -1.411740
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.320418e+05
      1       7.045208e+05      -2.275210e+05 |       32
      2       6.927418e+05      -1.177902e+04 |       32
      3       6.880570e+05      -4.684745e+03 |       32
      4       6.855969e+05      -2.460093e+03 |       32
      5       6.840247e+05      -1.572255e+03 |       32
      6       6.828163e+05      -1.208407e+03 |       32
      7       6.818805e+05      -9.357769e+02 |       32
      8       6.811140e+05      -7.664532e+02 |       32
      9       6.804445e+05      -6.695090e+02 |       32
     10       6.798772e+05      -5.673278e+02 |       32
     11       6.793502e+05      -5.269919e+02 |       32
     12       6.788579e+05      -4.923401e+02 |       32
     13       6.784231e+05      -4.347794e+02 |       32
     14       6.780664e+05      -3.566712e+02 |       32
     15       6.777369e+05      -3.295237e+02 |       32
     16       6.774435e+05      -2.934113e+02 |       32
     17       6.771895e+05      -2.539801e+02 |       32
     18       6.769456e+05      -2.439426e+02 |       32
     19       6.767182e+05      -2.274131e+02 |       32
     20       6.765007e+05      -2.174827e+02 |       32
     21       6.762967e+05      -2.039431e+02 |       32
     22       6.761091e+05      -1.876263e+02 |       32
     23       6.759211e+05      -1.879767e+02 |       32
     24       6.757330e+05      -1.880907e+02 |       32
     25       6.755528e+05      -1.801878e+02 |       32
     26       6.753985e+05      -1.543290e+02 |       32
     27       6.752540e+05      -1.445412e+02 |       32
     28       6.751336e+05      -1.204001e+02 |       32
     29       6.750300e+05      -1.035515e+02 |       32
     30       6.749301e+05      -9.991782e+01 |       32
     31       6.748401e+05      -9.002636e+01 |       32
     32       6.747528e+05      -8.728714e+01 |       32
     33       6.746722e+05      -8.055144e+01 |       32
     34       6.746026e+05      -6.964457e+01 |       32
     35       6.745406e+05      -6.198561e+01 |       32
     36       6.744828e+05      -5.782048e+01 |       32
     37       6.744285e+05      -5.431303e+01 |       32
     38       6.743795e+05      -4.892948e+01 |       32
     39       6.743325e+05      -4.708002e+01 |       32
     40       6.742922e+05      -4.022795e+01 |       32
     41       6.742564e+05      -3.582287e+01 |       32
     42       6.742228e+05      -3.360746e+01 |       32
     43       6.741932e+05      -2.964312e+01 |       32
     44       6.741651e+05      -2.811071e+01 |       32
     45       6.741338e+05      -3.125212e+01 |       32
     46       6.741057e+05      -2.814697e+01 |       32
     47       6.740751e+05      -3.057786e+01 |       32
     48       6.740497e+05      -2.539475e+01 |       32
     49       6.740261e+05      -2.354263e+01 |       32
     50       6.740042e+05      -2.191913e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 674004.2219165396)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.423497
INFO: iteration 2, average log likelihood -1.418448
INFO: iteration 3, average log likelihood -1.417135
INFO: iteration 4, average log likelihood -1.416237
INFO: iteration 5, average log likelihood -1.415335
INFO: iteration 6, average log likelihood -1.414458
INFO: iteration 7, average log likelihood -1.413771
INFO: iteration 8, average log likelihood -1.413333
INFO: iteration 9, average log likelihood -1.413071
INFO: iteration 10, average log likelihood -1.412902
INFO: iteration 11, average log likelihood -1.412779
INFO: iteration 12, average log likelihood -1.412680
INFO: iteration 13, average log likelihood -1.412597
INFO: iteration 14, average log likelihood -1.412524
INFO: iteration 15, average log likelihood -1.412459
INFO: iteration 16, average log likelihood -1.412399
INFO: iteration 17, average log likelihood -1.412344
INFO: iteration 18, average log likelihood -1.412292
INFO: iteration 19, average log likelihood -1.412244
INFO: iteration 20, average log likelihood -1.412199
INFO: iteration 21, average log likelihood -1.412156
INFO: iteration 22, average log likelihood -1.412117
INFO: iteration 23, average log likelihood -1.412079
INFO: iteration 24, average log likelihood -1.412044
INFO: iteration 25, average log likelihood -1.412011
INFO: iteration 26, average log likelihood -1.411981
INFO: iteration 27, average log likelihood -1.411952
INFO: iteration 28, average log likelihood -1.411926
INFO: iteration 29, average log likelihood -1.411901
INFO: iteration 30, average log likelihood -1.411878
INFO: iteration 31, average log likelihood -1.411856
INFO: iteration 32, average log likelihood -1.411835
INFO: iteration 33, average log likelihood -1.411816
INFO: iteration 34, average log likelihood -1.411798
INFO: iteration 35, average log likelihood -1.411781
INFO: iteration 36, average log likelihood -1.411765
INFO: iteration 37, average log likelihood -1.411749
INFO: iteration 38, average log likelihood -1.411735
INFO: iteration 39, average log likelihood -1.411721
INFO: iteration 40, average log likelihood -1.411707
INFO: iteration 41, average log likelihood -1.411695
INFO: iteration 42, average log likelihood -1.411683
INFO: iteration 43, average log likelihood -1.411671
INFO: iteration 44, average log likelihood -1.411660
INFO: iteration 45, average log likelihood -1.411649
INFO: iteration 46, average log likelihood -1.411638
INFO: iteration 47, average log likelihood -1.411628
INFO: iteration 48, average log likelihood -1.411618
INFO: iteration 49, average log likelihood -1.411608
INFO: iteration 50, average log likelihood -1.411599
INFO: EM with 100000 data points 50 iterations avll -1.411599
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.128502     0.267962     0.331915    -0.0667577   -0.555909   -0.192045     0.414293     0.11721    -0.235063   -0.186012    -0.0230207    0.217947    -0.103587    0.0743298   0.142751    -0.0892091   -0.181174     0.400601   -0.208567    0.0769574    0.173009     0.0494638   0.0750438   -0.223519    0.000331674   0.0949978 
  0.00596804  -1.24542e-5  -0.233897     0.00780603   0.139339   -0.00445987   0.0991756   -0.024379    0.171202    0.0153427    0.106371     0.00986186  -0.14328    -0.0409874   0.00695953   0.0421288    0.0442703    0.0901323  -0.0723964   0.0760514    0.114013    -0.0182804   0.0748347    0.0912785   0.0753794     0.0128119 
  0.0940286   -0.127596    -0.544849    -0.282655     0.910081    0.311211     0.145599    -0.320323    0.546188   -0.0271615   -0.235844    -0.798015    -0.179271    0.317234    0.0653478   -0.116219     0.269205    -0.0169059   0.0174851   0.939893     0.0744423    0.0547757   0.328084     0.58992    -0.425757     -0.397252  
 -0.0447789   -0.765039    -0.140718     0.0738688    0.248316    0.160949     0.0742821    0.268598    0.45749     0.111736    -0.314552    -0.159985     0.168755    0.264993   -0.131128    -0.159353     0.0643162   -0.967299    0.695792    0.0954048   -0.105703     0.0575225   0.072299    -0.141946   -0.34884       0.135905  
  0.435801     0.178814    -0.0591471   -0.172133    -0.0215064   0.159964    -0.313781     0.0401238   0.289181    0.123283    -0.256648    -0.150975    -0.0808356  -0.321517   -0.403204    -0.750723     0.181605    -0.427648    0.748724   -0.201496     0.197057     0.281105   -0.00277525   0.638827    0.259017     -0.563424  
  0.355669     0.998754    -0.225123     0.238263     0.313041   -0.402272    -0.237857    -0.171265   -0.772535    0.351594     0.042727    -0.00386007   0.403627   -0.293654   -0.199055     0.0710474    0.311695     0.196705   -0.396324    0.0454545    0.616576    -0.338723   -0.155289     0.454184    0.127065     -0.258437  
 -0.328698     0.550673     0.42031      0.274865    -0.0189432  -0.255101    -0.322011     0.378694   -0.53631     0.0764877   -0.0046824   -0.0189536    0.513446    0.523988   -0.578525    -0.476517    -0.543774    -0.158238    0.402482   -0.34076     -0.753279     0.569494    0.330991    -0.399107    0.0794549     0.336867  
  0.0968155   -0.204966     0.151014    -0.172597    -0.172964    0.844063     0.0109352   -0.335639   -0.129937    1.01012     -0.254499    -0.03381      0.306192   -0.136605   -0.422622     0.22704      0.418509    -0.281933    0.231327   -0.130622     0.0845926   -0.458228    0.00360331  -0.452971   -0.0584069    -0.188209  
 -0.227112     0.609388    -0.308051     0.681412    -0.402513    0.812134     0.0425625    0.370337    0.125659   -0.504501    -0.138386     0.719345     0.122167   -0.439674    0.361568     0.241216     0.521735    -0.459437    0.0418138  -0.240041     0.330158     0.173459    0.143723     0.211282    0.596883      0.290515  
  0.364907     0.459932    -0.547141    -0.130637    -0.159821    0.137983    -0.0738503    0.374702   -0.0332265   0.309311    -0.0510964    0.268549    -0.300326    0.333598   -0.152073    -0.191456     0.276553     0.431922    0.012328    0.259983     0.815488    -0.143708    0.304331    -0.336892    0.678522      0.437076  
  0.0495215    0.137287    -0.0285837   -0.122551     0.0975807  -0.00494151   0.00061942  -0.0461252  -0.0855358   0.0939994    0.0580997   -0.0229628   -0.115382   -0.0322122  -0.0787691    0.0173351   -0.0375363    0.150774    0.0373303  -0.0681672    0.0909411   -0.0572693  -0.0648332    0.0990358  -0.0908046    -0.169184  
  0.276779     0.136864    -0.382785     0.0781511    0.346088   -0.122033     0.188124     0.228076   -0.319803   -0.0455173   -0.113351    -0.304791     0.0638628  -0.344712    0.00409935   0.123347     0.00716299  -0.186639   -0.351585   -0.860373    -0.340018    -0.275437    0.0471081   -0.536795    0.0963533     0.34342   
 -0.121992    -0.287939     0.214807     0.60737     -0.199756   -0.256433    -0.191179    -0.139912    0.185809    0.270512     0.349842    -0.032224     0.188958   -0.300224    0.0131423    0.0510636   -0.697232    -0.133319    0.244447   -0.317727    -0.13917      0.133002   -0.0347811   -0.165759    0.183298      0.251267  
 -0.648027    -0.572509    -0.120117     0.222479     0.361426   -0.251305    -0.0597249    0.229576   -0.828158   -0.0134847   -0.0522001   -0.00764251  -0.0797099  -0.306438   -0.25883      0.165354     0.2856       0.259379   -0.442207    0.627117     0.184027     0.15976     0.307498     0.098192    0.12679      -0.621878  
  0.432407     0.0760042    0.229848     0.180921    -0.139226    0.0283865   -0.304054    -0.040318   -0.278205    0.0377813   -0.295168     0.053718    -0.173509    0.0427717  -0.126492     0.3194       0.162444    -0.179663    0.138727   -0.0264515   -0.0474083    0.253413   -0.209365    -0.0953653  -0.160995     -0.0507964 
  0.90904      0.0244242   -0.15956     -0.00361646  -0.306818   -0.144526     0.157877    -0.650418   -0.0396272  -0.200988    -0.275665    -0.673027     0.0818848   0.540252   -0.39292      0.234973    -0.310751    -0.0916945  -0.59332    -0.696522    -0.742889     1.0442      0.133434    -0.253359   -0.709366      0.514192  
 -0.104638    -0.149478     0.0600205   -0.47516      0.532925   -0.606927     0.0524572   -0.688614   -0.206329    0.136103    -0.0283537   -0.193421    -0.251649    0.457582   -0.291062    -0.498506    -0.452292     0.417184   -0.0483238  -0.32318     -0.0976889   -0.42023     0.299571    -0.163772   -0.148822     -0.185406  
  0.0682517   -0.606318    -0.0489645   -0.19942     -0.099786    0.104697     0.0975612   -0.239122    0.247704   -0.591754     0.337216    -0.0657421    0.0199133  -0.360226    0.400074     0.293045     0.0688531   -0.255603   -0.499393    0.220208    -0.00768505   0.0748109  -0.953972     0.40049    -0.405344     -0.56224   
 -0.426465    -0.928675     0.256866    -0.181394    -0.531439    0.157663     0.502112     0.351819    0.449987   -0.583803     0.00526552   0.219449    -0.666301    0.260911    0.779793     0.223616    -0.393966     0.0945713   0.121518   -0.226633    -0.584218     0.535126    0.315474    -0.284384   -0.0860659     0.410521  
  0.881427     0.304506    -0.355181    -0.327623     0.101612    0.172302     0.285136     0.406346   -0.314423   -0.141354    -0.207195     0.212677    -0.0385622   0.308372   -0.0110964    0.392481     0.492916     0.221194    0.0268153   0.0994535   -0.142906    -0.260329    0.226763     0.382466    0.0953006    -0.651528  
 -0.402199     0.136572    -0.0679321   -0.64463     -0.118641    0.305821     0.148813     0.270837    0.145524    0.26822      0.323026    -0.480508    -0.213139   -0.11847    -0.251986     0.109651     0.137399     0.587208   -0.226017    0.0880333   -0.275404     0.0159544   0.184679     0.115466   -0.0176494    -0.116128  
  0.370973     0.38027     -0.273879     0.0252995    0.0873554  -0.779654     0.162644     0.456849    0.158317   -1.01542      0.383841     0.0525378   -0.689887    0.106794    0.239522    -0.0360516   -0.467644     0.390385   -0.0318883   0.179047     0.154612     0.502575   -0.048533     0.466968   -0.0535735    -0.00230847
  0.0270545   -0.195707    -0.213144     0.282146     0.219212   -0.0441462    0.0167513    0.0841284   0.59149     0.586272     0.300426     0.157708    -0.20976    -0.505589    0.422158     0.215239     0.109699     0.0394209   0.476264    0.362256     0.833041    -0.495643   -0.12247      0.518133    0.22158      -0.330112  
 -0.0162849    0.415389     0.537542     0.320994    -0.337737   -0.0856603   -0.378972    -0.924297   -0.0692999  -0.262661    -0.172667     0.127799    -0.447275   -0.0171486   0.459915     0.446208    -0.407863     0.619775   -0.447901   -0.310631     0.010025     0.205805   -0.178996    -0.114722    0.360001      0.67858   
 -0.373358    -0.181188    -0.180221    -0.699973    -0.156063    0.210047     1.01511      0.177958    0.320192    0.0915973    0.522288     0.178274     0.576104   -0.0972279   0.291469    -0.477161    -0.066904     0.270811   -0.0977767  -0.344381    -0.185473    -0.491951    0.763577    -0.194778    0.342068      0.153757  
 -0.246701    -0.00777911   0.0460884   -0.048431     0.276846    0.212384    -0.0648237   -0.0081548   0.163924   -0.00357842  -0.133537    -0.135713     0.601118    0.317064    0.330394    -0.211143    -0.195298    -0.514483    0.296345   -0.08862     -0.28958      0.0608942  -0.0696343    0.133477   -0.00661039   -0.20528   
 -0.726824    -0.41661      0.256246     0.32478     -0.204478   -0.142892    -0.736894    -0.460348    0.13111     0.146367     0.54383      0.0345439    0.433856   -0.928511    0.191581     0.0603317   -0.403076    -0.283302    0.166487   -0.577324    -0.359296    -0.12316     0.54421      0.288004    0.188018      0.329336  
 -0.549591    -0.329585     0.28312      0.180325     0.22575    -0.230104     0.258402    -0.603168    0.882811   -0.487733     0.860973    -0.283778    -0.105205    0.520227   -0.29236      0.00505376   0.02362      0.407877   -0.494826    0.48343     -0.364466    -0.37747    -0.513109    -0.574966    0.013053      0.765563  
 -0.832381    -0.0647751   -0.00655935   0.369431     0.548321   -0.486544     0.0182093   -0.250051    0.205594   -0.291759    -0.0320689   -0.0818683    0.0929265  -0.41556    -0.0743783   -0.952715    -0.525461    -0.180018   -0.254229    0.201966     0.126951     0.557523   -0.261656     0.111583   -0.445062      0.0912023 
  0.0270672   -0.0622015    0.219101    -0.472168     0.0455381   0.565763     0.0148178    0.084328    0.767912   -0.188843     0.277497     0.504424    -0.245421    0.163838   -0.40362      0.0576238   -0.347525    -0.361305    0.27875    -0.00520883   0.0843631    0.18338    -0.538546    -0.152243    0.0738406     0.182831  
  0.940608     0.323884     0.187102     0.509532    -0.146658    0.40214      0.133119     0.354575   -0.372392   -0.111144    -0.344137     0.0145901    0.0266214   0.146123   -0.0520869    0.18389      0.0262507   -0.50692    -0.216773    0.27623     -0.0846073    0.38488    -0.70769     -0.355747    0.237476     -0.0140035 
  0.141068    -0.057125    -0.324028     0.0297809   -0.219228   -0.0337697   -0.384945     0.301758    0.240151    0.57934     -0.41486      0.240498    -0.227224   -0.0869049  -0.192104     0.470389    -0.292364     1.04983     0.210547   -0.0147804    0.290646     0.353957    1.00667      0.281443   -0.514211     -0.126739  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.411590
INFO: iteration 2, average log likelihood -1.411581
INFO: iteration 3, average log likelihood -1.411572
INFO: iteration 4, average log likelihood -1.411563
INFO: iteration 5, average log likelihood -1.411554
INFO: iteration 6, average log likelihood -1.411546
INFO: iteration 7, average log likelihood -1.411538
INFO: iteration 8, average log likelihood -1.411529
INFO: iteration 9, average log likelihood -1.411521
INFO: iteration 10, average log likelihood -1.411513
INFO: EM with 100000 data points 10 iterations avll -1.411513
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
