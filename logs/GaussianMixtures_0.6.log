>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.7.0
INFO: Installing JLD v0.6.6
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.1
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.3.0
INFO: Installing ScikitLearnBase v0.2.1
INFO: Installing StaticArrays v0.1.0
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
WARNING: takebuf_string(b) is deprecated, use String(take!(b)) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in takebuf_string(::Base.AbstractIOBuffer{Array{UInt8,1}}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/BinDeps/src/dependencies.jl:887 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::SubString{String}) at ./sysimg.jl:14
 in evalfile(::SubString{String}, ::Array{String,1}) at ./loading.jl:572 (repeats 2 times)
 in cd(::##2#4, ::String) at ./file.jl:69
 in (::##1#3)(::IOStream) at ./none:13
 in open(::##1#3, ::String, ::String) at ./iostream.jl:152
 in eval(::Module, ::Any) at ./boot.jl:236
 in process_options(::Base.JLOptions) at ./client.jl:248
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/Rmath/deps/build.jl, in expression starting on line 39
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1295
Commit 2a4b68a (2016-11-22 18:18 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-101-generic #148-Ubuntu SMP Thu Oct 20 22:08:32 UTC 2016 x86_64 x86_64
Memory: 2.939281463623047 GB (662.875 MB free)
Uptime: 28800.0 sec
Load Avg:  1.14404296875  1.04345703125  1.05078125
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3499 MHz    1751313 s       1419 s     208876 s     621792 s         52 s
#2  3499 MHz     744520 s       5209 s     109385 s    1940386 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.4
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.7.0
 - JLD                           0.6.6
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.1
 - Rmath                         0.1.4
 - SHA                           0.3.0
 - ScikitLearnBase               0.2.1
 - StaticArrays                  0.1.0
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: takebuf_string(b) is deprecated, use String(take!(b)) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in takebuf_string(::Base.AbstractIOBuffer{Array{UInt8,1}}) at ./deprecated.jl:50
 in #_write#17(::Array{Any,1}, ::Function, ::JLD.JldFile, ::String, ::Array{Any,1}, ::JLD.JldWriteSession) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:587
 in #write#14(::Array{Any,1}, ::Function, ::JLD.JldFile, ::String, ::Array{Any,1}, ::JLD.JldWriteSession) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:529
 in #jldopen#9(::Bool, ::Bool, ::Bool, ::Function, ::String, ::Bool, ::Bool, ::Bool, ::Bool, ::Bool) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:198
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::String, ::Bool, ::Bool, ::Bool, ::Bool, ::Bool) at ./<missing>:0
 in #jldopen#10(::Bool, ::Bool, ::Bool, ::Function, ::String, ::String) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:253
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::String, ::String) at ./<missing>:0
 in #jldopen#11(::Array{Any,1}, ::Function, ::JLD.##34#35{String,Array{Float64,2},Tuple{}}, ::String, ::Vararg{String,N}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:263
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::Function, ::String, ::String) at ./<missing>:0
 in #save#33(::Bool, ::Bool, ::Function, ::FileIO.File{FileIO.DataFormat{:JLD}}, ::String, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:1217
 in save(::FileIO.File{FileIO.DataFormat{:JLD}}, ::String, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:1214
 in #save#14(::Array{Any,1}, ::Function, ::String, ::String, ::Vararg{Any,N}) at /home/vagrant/.julia/v0.6/FileIO/src/loadsave.jl:54
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:8 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-1.589104902242729e6,[13204.3,86795.7],
[-1417.67 4111.81 -1146.35; 1076.35 -3769.39 1358.68],

Array{Float64,2}[
[11730.9 3061.21 2364.24; 3061.21 6454.78 -5810.19; 2364.24 -5810.19 10287.2],

[88260.4 -3661.7 -2320.31; -3661.7 94031.3 5914.77; -2320.31 5914.77 90257.8]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.711231e+03
      1       9.978228e+02      -7.134083e+02 |        7
      2       8.832888e+02      -1.145339e+02 |        2
      3       8.361048e+02      -4.718405e+01 |        2
      4       8.200737e+02      -1.603103e+01 |        2
      5       8.150919e+02      -4.981853e+00 |        0
      6       8.150919e+02       0.000000e+00 |        0
K-means converged with 6 iterations (objv = 815.091882939284)
INFO: K-means with 272 data points using 6 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.052558
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.802675
INFO: iteration 2, lowerbound -3.701170
INFO: iteration 3, lowerbound -3.594590
INFO: iteration 4, lowerbound -3.476748
INFO: iteration 5, lowerbound -3.362877
INFO: iteration 6, lowerbound -3.264492
INFO: dropping number of Gaussions to 7
INFO: iteration 7, lowerbound -3.174850
INFO: iteration 8, lowerbound -3.088097
INFO: dropping number of Gaussions to 5
INFO: iteration 9, lowerbound -2.990736
INFO: iteration 10, lowerbound -2.886758
INFO: iteration 11, lowerbound -2.801724
INFO: dropping number of Gaussions to 4
INFO: iteration 12, lowerbound -2.730996
INFO: iteration 13, lowerbound -2.666618
INFO: dropping number of Gaussions to 3
INFO: iteration 14, lowerbound -2.603180
INFO: iteration 15, lowerbound -2.531556
INFO: iteration 16, lowerbound -2.464862
INFO: iteration 17, lowerbound -2.408817
INFO: iteration 18, lowerbound -2.366050
INFO: iteration 19, lowerbound -2.334630
INFO: iteration 20, lowerbound -2.314265
INFO: iteration 21, lowerbound -2.307396
INFO: dropping number of Gaussions to 2
INFO: iteration 22, lowerbound -2.302940
INFO: iteration 23, lowerbound -2.299261
INFO: iteration 24, lowerbound -2.299257
INFO: iteration 25, lowerbound -2.299255
INFO: iteration 26, lowerbound -2.299254
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Wed 23 Nov 2016 01:30:14 PM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Wed 23 Nov 2016 01:30:16 PM UTC: K-means with 272 data points using 6 iterations
11.3 data points per parameter
,Wed 23 Nov 2016 01:30:17 PM UTC: EM with 272 data points 0 iterations avll -2.052558
5.8 data points per parameter
,Wed 23 Nov 2016 01:30:18 PM UTC: GMM converted to Variational GMM
,Wed 23 Nov 2016 01:30:20 PM UTC: iteration 1, lowerbound -3.802675
,Wed 23 Nov 2016 01:30:20 PM UTC: iteration 2, lowerbound -3.701170
,Wed 23 Nov 2016 01:30:20 PM UTC: iteration 3, lowerbound -3.594590
,Wed 23 Nov 2016 01:30:20 PM UTC: iteration 4, lowerbound -3.476748
,Wed 23 Nov 2016 01:30:20 PM UTC: iteration 5, lowerbound -3.362877
,Wed 23 Nov 2016 01:30:20 PM UTC: iteration 6, lowerbound -3.264492
,Wed 23 Nov 2016 01:30:20 PM UTC: dropping number of Gaussions to 7
,Wed 23 Nov 2016 01:30:20 PM UTC: iteration 7, lowerbound -3.174850
,Wed 23 Nov 2016 01:30:21 PM UTC: iteration 8, lowerbound -3.088097
,Wed 23 Nov 2016 01:30:21 PM UTC: dropping number of Gaussions to 5
,Wed 23 Nov 2016 01:30:21 PM UTC: iteration 9, lowerbound -2.990736
,Wed 23 Nov 2016 01:30:21 PM UTC: iteration 10, lowerbound -2.886758
,Wed 23 Nov 2016 01:30:21 PM UTC: iteration 11, lowerbound -2.801724
,Wed 23 Nov 2016 01:30:21 PM UTC: dropping number of Gaussions to 4
,Wed 23 Nov 2016 01:30:21 PM UTC: iteration 12, lowerbound -2.730996
,Wed 23 Nov 2016 01:30:21 PM UTC: iteration 13, lowerbound -2.666618
,Wed 23 Nov 2016 01:30:21 PM UTC: dropping number of Gaussions to 3
,Wed 23 Nov 2016 01:30:21 PM UTC: iteration 14, lowerbound -2.603180
,Wed 23 Nov 2016 01:30:21 PM UTC: iteration 15, lowerbound -2.531556
,Wed 23 Nov 2016 01:30:21 PM UTC: iteration 16, lowerbound -2.464862
,Wed 23 Nov 2016 01:30:21 PM UTC: iteration 17, lowerbound -2.408817
,Wed 23 Nov 2016 01:30:21 PM UTC: iteration 18, lowerbound -2.366050
,Wed 23 Nov 2016 01:30:21 PM UTC: iteration 19, lowerbound -2.334630
,Wed 23 Nov 2016 01:30:21 PM UTC: iteration 20, lowerbound -2.314265
,Wed 23 Nov 2016 01:30:21 PM UTC: iteration 21, lowerbound -2.307396
,Wed 23 Nov 2016 01:30:21 PM UTC: dropping number of Gaussions to 2
,Wed 23 Nov 2016 01:30:21 PM UTC: iteration 22, lowerbound -2.302940
,Wed 23 Nov 2016 01:30:22 PM UTC: iteration 23, lowerbound -2.299261
,Wed 23 Nov 2016 01:30:22 PM UTC: iteration 24, lowerbound -2.299257
,Wed 23 Nov 2016 01:30:22 PM UTC: iteration 25, lowerbound -2.299255
,Wed 23 Nov 2016 01:30:22 PM UTC: iteration 26, lowerbound -2.299254
,Wed 23 Nov 2016 01:30:22 PM UTC: iteration 27, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:22 PM UTC: iteration 28, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:22 PM UTC: iteration 29, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:22 PM UTC: iteration 30, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:22 PM UTC: iteration 31, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:22 PM UTC: iteration 32, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:22 PM UTC: iteration 33, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:22 PM UTC: iteration 34, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:22 PM UTC: iteration 35, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:22 PM UTC: iteration 36, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:22 PM UTC: iteration 37, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:22 PM UTC: iteration 38, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:22 PM UTC: iteration 39, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:22 PM UTC: iteration 40, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:23 PM UTC: iteration 41, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:23 PM UTC: iteration 42, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:23 PM UTC: iteration 43, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:23 PM UTC: iteration 44, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:23 PM UTC: iteration 45, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:23 PM UTC: iteration 46, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:23 PM UTC: iteration 47, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:23 PM UTC: iteration 48, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:23 PM UTC: iteration 49, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:23 PM UTC: iteration 50, lowerbound -2.299253
,Wed 23 Nov 2016 01:30:23 PM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.045,95.9549]
β = [178.045,95.9549]
m = [4.2503 79.2869; 2.00023 53.852]
ν = [180.045,97.9549]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.184042 -0.00764405; 0.0 0.00858171],

[0.375876 -0.00895312; 0.0 0.0127487]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 99999.99999999999
avll from stats: -0.9910785177936201
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9910785177936201
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9910785177936201
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -0.9869226277520339
avll from llpg:  -0.9869226277520335
avll direct:     -0.9869226277520338
sum posterior: 100000.0
32×26 Array{Float64,2}:
 -0.193749     -0.0531009   -0.0901474    0.1527       0.00271523  -0.0366021   0.088129     0.0663833    0.133395     0.140712      0.136622    -0.121118     0.0542189     0.18014     0.08064     -0.114395     0.134904      0.0217745   -0.0309501   -0.0148439   -0.115515   -0.142923     0.131682   -0.016083    0.186846    -0.155218   
  0.0370776     0.0680875   -0.0108022   -0.088899    -0.0146054    0.0679365   0.0480835    0.0171662    0.0665444    0.00510721    0.0072577    0.0499587    0.0428081    -0.058918    0.0547833   -0.0189778    0.0645062    -0.258933     0.0456705   -0.172815     0.126817    0.0299123   -0.0782485  -0.0653202  -0.164215     0.047688   
  0.0264662     0.0640473   -0.158105     0.152518    -0.284902     0.0380415  -0.0312661   -0.146193    -0.137831     0.118125      0.0443748   -0.109724     0.0490141     0.0575009  -0.134998     0.0357819   -0.0920174     0.139992    -0.0508885   -0.0924894   -0.0313412   0.0916015    0.0491398   0.0251256  -0.0942073    0.193323   
 -0.00859031    0.0603075    0.0330519   -0.00821953   0.0830448    0.0786118   0.102        0.124298    -0.163752    -0.14238      -0.0471674   -0.110415    -0.0188317     0.0882552  -0.161644    -0.00824766  -0.129641      0.0758702    0.142393     0.107596    -0.0333937  -0.185657     0.0625346   0.0597937   0.0211104    0.0472718  
  0.131175     -0.0311244   -0.01208     -0.0154641   -0.181589     0.118491   -0.0889782   -0.0321386    0.0060605    0.0959906    -0.0504047    0.0439792   -0.111175     -0.222073   -0.042769     0.0141791    0.0954342    -0.112089    -0.0882244   -0.054494    -0.0189712   0.020776     0.0224645  -0.0656493   0.0170307   -0.0753013  
 -0.0732022    -0.0875334    0.0797264    0.0200473   -0.104823     0.0741329   0.0152111    0.0839927   -0.0774871   -0.0113341    -0.093117    -0.0436363    0.066042     -0.131344    0.124031    -0.0542093   -0.0213095    -0.00296272  -0.170241     0.0924617   -0.0875319   0.0552654    0.0593842  -0.0569818   0.0725668    0.0203036  
 -0.0674997     0.00306573   0.0188356    0.0558135    0.0289461   -0.0871321   0.0547866    0.0380612    0.176536     0.0303389     0.242165    -0.00634952  -0.00458596   -0.152646   -0.0845317    0.133889    -0.0284757    -0.0151962   -0.149751     0.268702     0.0210703  -0.0102099   -0.0490593   0.0792374   0.0743999    0.238746   
 -0.129664     -0.161661     0.0526407   -0.0764522   -0.0014726   -0.0206754   0.0532323    0.021242    -0.107901     0.0645945     0.086849     0.0201925   -0.113571     -0.0432339  -0.134067    -0.117527     0.065164      0.0687985    0.0416241   -0.0332305   -0.0624487   0.00361429   0.0387569  -0.102672    0.033085    -0.0636033  
 -0.000919871  -0.0167118   -0.016464     0.0405134   -0.12345     -0.0863956   0.147538    -0.107489     0.0591533    0.152383     -0.0396171    0.25602     -0.0539778    -0.210607    0.138972    -0.0835297    0.0346927     0.103854     0.132842    -0.0571462    0.0418719  -0.0306835   -0.0292019   0.115511   -0.142037    -0.0342469  
  0.00085725   -0.0160237   -0.0402601    0.0524228    0.0764752    0.124319    0.016788    -0.106321    -0.0582156   -0.0151382    -0.0686251    0.141967     0.123197     -0.101679    0.133401     0.0530489    0.0421499     0.0569099    0.0250878    0.0100779   -0.0704271  -0.0443725    0.0459022  -0.152604   -0.00194184  -0.0539638  
  0.00689076   -0.0805057   -0.0751472   -0.165329    -0.141234    -0.172649   -0.1212      -0.133928    -0.0729269   -0.00327231   -0.0846243    0.0305743    0.00901061   -0.067018    0.0430207   -0.0245067   -0.0504978     0.00937      0.0801949   -0.0278924    0.0483802   0.0614986   -0.0454193   0.0384743  -0.0753479    0.196264   
  0.0569851    -0.0288494    0.0917839    0.154004     0.00347104   0.0599666   0.0076492   -0.0100869   -0.0743933   -0.0257351    -0.0818146   -0.137518    -0.101264     -0.0408807  -0.00641634  -0.026891    -0.106573      0.0284005    0.00293929  -0.23077      0.0343822  -0.0782769   -0.0264884   0.0218973   0.0741789    0.125661   
  0.0056288    -0.0769798   -0.049367    -0.0403684   -0.116398     0.0785998  -0.0390768    0.00670676  -0.0804426    0.14852      -0.0728652   -0.229703    -0.0924413     0.0943444   0.179377     0.0188401    0.0672169     0.13814      0.0208673   -0.08876      0.136641   -0.0539853   -0.223762   -0.0585384  -0.0358105    0.04305    
  0.0825708    -0.0586514    0.15422      0.0676944   -0.0230623   -0.100118   -0.122657     0.011068    -0.0119393   -0.0409122     0.0588695   -0.0649742   -0.0378578     0.0691678   0.0972408   -0.0779708    0.229317      0.00869585  -0.135298    -0.185891     0.0745748  -0.0761117   -0.200989    0.0318113  -0.00198519  -0.123898   
  0.0241868    -0.0352153   -0.0448538    0.0166749   -0.0803419   -0.0953582   0.182435     0.0700253    0.203509     0.0314968    -0.12375     -0.232096     0.000864875  -0.0756745  -0.0476531    0.0781416    0.00159499    0.0544869   -0.0158197   -0.00697011   0.175437   -0.0939357   -0.0679339   0.0919866  -0.0106418   -0.000385374
 -0.128921     -0.0236501   -0.105725     0.0905068   -0.108658    -0.0658894   0.0631732   -0.200675     0.0550485    0.00247393    0.060035     0.0357166    0.0492996    -0.0236043   0.195855    -0.0812059    0.000531747   0.0458298   -0.227768     0.0461993    0.135723   -0.0355396   -0.0173643  -0.0792804  -0.120202     0.0531476  
 -0.0372353     0.0319002    0.0915189    0.0681669   -0.0432002    0.1382      0.0299015   -0.0296951   -0.0500867   -0.000759334  -0.0585167    0.247599    -0.116146      0.0815197   0.0658697   -0.0337471   -0.00818867    0.125331     0.00374336  -0.0153597    0.0872805   0.0805673   -0.0653506   0.0785041   0.13656     -0.00689999 
 -0.115782     -0.122896    -0.155708     0.106953     0.0438065    0.0577567  -0.161317     0.0779311   -0.140763     0.0714937    -0.0509612    0.0118896   -0.107426     -0.149726   -0.102803    -0.148395     0.11726       0.0785499    0.147823    -0.0351807    0.0899045  -0.0774423    0.0716555   0.099226    0.057324    -0.0872803  
  0.159303      0.0336467   -0.117338     0.0363442   -0.0782938    0.151036   -0.0538404    0.0824119   -0.036707     0.0342254     0.059656    -0.160475    -0.0491064    -0.0692104  -0.0334922   -0.0317093   -0.0577678     0.063972     2.34471e-5   0.0700644   -0.0535097   0.105426     0.184195   -0.028272   -0.111326     0.0408351  
 -0.0150267    -0.0202531   -0.0199791    0.206347     0.0407138   -0.0426545  -0.152629     0.108269     0.0968987   -0.0206588    -0.0409524   -0.0402227    0.011285      0.115104   -0.137948     0.0624364    0.109397      0.0271265    0.152538     0.0404023   -0.10134    -0.0242306    0.0312571   0.201193   -0.0258754   -0.0287011  
 -0.130381     -0.0737978    0.101863    -0.178869    -0.161726    -0.0924338  -0.068011    -0.0533647   -0.0130595   -0.0797552    -0.00384587   0.0625       0.0152895    -0.0314047  -0.0190313    0.135879     0.0414854    -0.00596073   0.169401    -0.110975    -0.0895194   0.0877183   -0.114974    0.0490837   0.00854156   0.0421417  
 -0.150709      0.155697    -0.0205942   -0.087816    -0.12299     -0.109946   -0.0569829    0.0132831   -0.238726    -0.0971834     0.219781    -0.17975     -0.131201      0.0246888   0.10843      0.0205461    0.193331     -0.00368739  -0.0467137    0.089657    -0.0364373  -0.0865743    0.0774947  -0.0311114   0.0140019   -0.097577   
 -0.0156652    -0.0843821    0.122141    -0.0593056    0.164858     0.162716    0.00258227  -0.0465997   -0.0529495    0.0719322     0.0178127    0.0958321   -0.0505331    -0.0931845   0.0798026    0.0265364    0.000381333   0.0377337   -0.102124     0.124134    -0.0676192  -0.00889111  -0.0167501   0.0956536   0.0791094    0.109164   
  0.09558       0.0721108   -0.00551499  -0.0740559    0.0650092    0.113178    0.0399755    0.0425414   -0.079152    -0.0427629    -0.188064    -0.00160388   0.107851     -0.206643   -0.111562    -0.0543539   -0.00957939    0.00609827  -0.0680373    0.0926126   -0.0258149  -0.141267     0.0180795   0.0232249   0.0902044   -0.0963504  
  0.0930292     0.127255     0.0504297   -0.0373926    0.0841669   -0.102836   -0.0550858   -0.0788637   -0.0437084   -0.0948572     0.170512     0.0768517   -0.0991113     0.0879386  -0.0735507   -0.0618483   -0.157829      0.0675239   -0.101085    -0.0554592   -0.166285   -0.180987    -0.054601   -0.0633293  -0.0495823   -0.0261054  
 -0.0744761     0.0697978   -0.102464    -0.0610185   -0.0193001   -0.0304845   0.00115304  -0.0981862    0.0819772    0.0236764     0.127554    -0.0456393   -0.061669      0.024356   -0.112524    -0.0265897   -0.0403       -0.00385825   0.00185581  -0.0561694   -0.0403147   0.0859625    0.0273277   0.113747    0.0977048    0.0010745  
 -0.00899765   -0.042132    -0.141995    -0.056633    -0.0326068   -0.186351    0.0918939   -0.0667839    0.068723     0.107786      0.0775048    0.0418401   -0.251489      0.0585622  -0.125909    -0.00325949   0.0935136     0.113035     0.268467    -0.0206228   -0.103797    0.0110604   -0.0507841  -0.132643   -0.0491014    0.00342367 
 -0.146144     -0.0699093   -0.136171    -0.0144217    0.0308791   -0.0475634  -0.0312531    0.0313008   -0.0161318    0.0101394     0.0490343   -0.086405    -0.0712753     0.094475   -0.011258    -0.0556459    0.0364168    -0.0427143    0.145981    -0.0646074   -0.0982845  -0.0975425   -0.165459    0.11949    -0.00226024  -0.258901   
  0.113116      0.00822734  -0.0695453   -0.0133121   -0.153724     0.0722097   0.291546     0.0646727    0.204022     0.0654762     0.00146542   0.01975      0.113063     -0.0659202  -0.0717795    0.0885096   -0.196251      0.10606     -0.0374523    0.124131    -0.147389    0.0508534    0.107935    0.118829   -0.0087666   -0.149039   
 -0.136469      0.013172    -0.0651483    0.175188     0.0238149    0.084001    0.165178    -0.0585333    0.00126305   0.0443415     0.0673376    0.0913743   -0.0838065    -0.173865   -0.025303     0.0256362    0.0228728     0.0610752   -0.0575316   -0.0131188   -0.158064   -0.161433    -0.0128427   0.0660815   0.124951     0.0224945  
  0.0799944    -0.0428683    0.100854    -0.036564     0.0713352   -0.0232109  -0.057114     0.0908541   -0.0726744    0.16341      -0.0714334   -0.18797      0.218456      0.0550399  -0.0640401    0.138893    -0.057136     -0.0620618   -0.10211     -0.0728936    0.0661227  -0.00627602  -0.193566   -0.154294    0.107245     0.0883473  
 -0.0941283     0.0286611   -0.016666     0.174596     0.195728    -0.0682336  -0.0838124    0.0547283    0.0932952    0.0192516    -0.114381    -0.0192349   -0.0632558    -0.174028    0.107617    -0.227672    -0.072276     -0.101721    -0.108907     0.0475523   -0.0821497  -0.0121724    0.107062    0.0221555   0.0476716   -0.0935099  kind diag, method split
0: avll = -1.4007071290495599
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.400796
INFO: iteration 2, average log likelihood -1.400721
INFO: iteration 3, average log likelihood -1.400189
INFO: iteration 4, average log likelihood -1.393366
INFO: iteration 5, average log likelihood -1.374689
INFO: iteration 6, average log likelihood -1.368052
INFO: iteration 7, average log likelihood -1.366685
INFO: iteration 8, average log likelihood -1.365971
INFO: iteration 9, average log likelihood -1.365614
INFO: iteration 10, average log likelihood -1.365383
INFO: iteration 11, average log likelihood -1.365186
INFO: iteration 12, average log likelihood -1.365007
INFO: iteration 13, average log likelihood -1.364851
INFO: iteration 14, average log likelihood -1.364717
INFO: iteration 15, average log likelihood -1.364605
INFO: iteration 16, average log likelihood -1.364512
INFO: iteration 17, average log likelihood -1.364435
INFO: iteration 18, average log likelihood -1.364372
INFO: iteration 19, average log likelihood -1.364319
INFO: iteration 20, average log likelihood -1.364273
INFO: iteration 21, average log likelihood -1.364234
INFO: iteration 22, average log likelihood -1.364200
INFO: iteration 23, average log likelihood -1.364172
INFO: iteration 24, average log likelihood -1.364149
INFO: iteration 25, average log likelihood -1.364130
INFO: iteration 26, average log likelihood -1.364115
INFO: iteration 27, average log likelihood -1.364104
INFO: iteration 28, average log likelihood -1.364096
INFO: iteration 29, average log likelihood -1.364091
INFO: iteration 30, average log likelihood -1.364087
INFO: iteration 31, average log likelihood -1.364085
INFO: iteration 32, average log likelihood -1.364083
INFO: iteration 33, average log likelihood -1.364081
INFO: iteration 34, average log likelihood -1.364081
INFO: iteration 35, average log likelihood -1.364080
INFO: iteration 36, average log likelihood -1.364080
INFO: iteration 37, average log likelihood -1.364079
INFO: iteration 38, average log likelihood -1.364079
INFO: iteration 39, average log likelihood -1.364079
INFO: iteration 40, average log likelihood -1.364079
INFO: iteration 41, average log likelihood -1.364079
INFO: iteration 42, average log likelihood -1.364079
INFO: iteration 43, average log likelihood -1.364079
INFO: iteration 44, average log likelihood -1.364079
INFO: iteration 45, average log likelihood -1.364079
INFO: iteration 46, average log likelihood -1.364079
INFO: iteration 47, average log likelihood -1.364079
INFO: iteration 48, average log likelihood -1.364079
INFO: iteration 49, average log likelihood -1.364079
INFO: iteration 50, average log likelihood -1.364079
INFO: EM with 100000 data points 50 iterations avll -1.364079
952.4 data points per parameter
1: avll = [-1.4008,-1.40072,-1.40019,-1.39337,-1.37469,-1.36805,-1.36668,-1.36597,-1.36561,-1.36538,-1.36519,-1.36501,-1.36485,-1.36472,-1.3646,-1.36451,-1.36444,-1.36437,-1.36432,-1.36427,-1.36423,-1.3642,-1.36417,-1.36415,-1.36413,-1.36412,-1.3641,-1.3641,-1.36409,-1.36409,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.364232
INFO: iteration 2, average log likelihood -1.364112
INFO: iteration 3, average log likelihood -1.363816
INFO: iteration 4, average log likelihood -1.360849
INFO: iteration 5, average log likelihood -1.347971
INFO: iteration 6, average log likelihood -1.335209
INFO: iteration 7, average log likelihood -1.330743
INFO: iteration 8, average log likelihood -1.329143
INFO: iteration 9, average log likelihood -1.328407
INFO: iteration 10, average log likelihood -1.327981
INFO: iteration 11, average log likelihood -1.327679
INFO: iteration 12, average log likelihood -1.327438
INFO: iteration 13, average log likelihood -1.327228
INFO: iteration 14, average log likelihood -1.327038
INFO: iteration 15, average log likelihood -1.326865
INFO: iteration 16, average log likelihood -1.326710
INFO: iteration 17, average log likelihood -1.326571
INFO: iteration 18, average log likelihood -1.326443
INFO: iteration 19, average log likelihood -1.326324
INFO: iteration 20, average log likelihood -1.326212
INFO: iteration 21, average log likelihood -1.326105
INFO: iteration 22, average log likelihood -1.326001
INFO: iteration 23, average log likelihood -1.325900
INFO: iteration 24, average log likelihood -1.325800
INFO: iteration 25, average log likelihood -1.325699
INFO: iteration 26, average log likelihood -1.325590
INFO: iteration 27, average log likelihood -1.325463
INFO: iteration 28, average log likelihood -1.325306
INFO: iteration 29, average log likelihood -1.325091
INFO: iteration 30, average log likelihood -1.324770
INFO: iteration 31, average log likelihood -1.324307
INFO: iteration 32, average log likelihood -1.323720
INFO: iteration 33, average log likelihood -1.323047
INFO: iteration 34, average log likelihood -1.322346
INFO: iteration 35, average log likelihood -1.321671
INFO: iteration 36, average log likelihood -1.321081
INFO: iteration 37, average log likelihood -1.320574
INFO: iteration 38, average log likelihood -1.320126
INFO: iteration 39, average log likelihood -1.319666
INFO: iteration 40, average log likelihood -1.319127
INFO: iteration 41, average log likelihood -1.318509
INFO: iteration 42, average log likelihood -1.317880
INFO: iteration 43, average log likelihood -1.317352
INFO: iteration 44, average log likelihood -1.316982
INFO: iteration 45, average log likelihood -1.316735
INFO: iteration 46, average log likelihood -1.316569
INFO: iteration 47, average log likelihood -1.316456
INFO: iteration 48, average log likelihood -1.316375
INFO: iteration 49, average log likelihood -1.316319
INFO: iteration 50, average log likelihood -1.316280
INFO: EM with 100000 data points 50 iterations avll -1.316280
473.9 data points per parameter
2: avll = [-1.36423,-1.36411,-1.36382,-1.36085,-1.34797,-1.33521,-1.33074,-1.32914,-1.32841,-1.32798,-1.32768,-1.32744,-1.32723,-1.32704,-1.32687,-1.32671,-1.32657,-1.32644,-1.32632,-1.32621,-1.32611,-1.326,-1.3259,-1.3258,-1.3257,-1.32559,-1.32546,-1.32531,-1.32509,-1.32477,-1.32431,-1.32372,-1.32305,-1.32235,-1.32167,-1.32108,-1.32057,-1.32013,-1.31967,-1.31913,-1.31851,-1.31788,-1.31735,-1.31698,-1.31673,-1.31657,-1.31646,-1.31638,-1.31632,-1.31628]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.316415
INFO: iteration 2, average log likelihood -1.316210
INFO: iteration 3, average log likelihood -1.314972
INFO: iteration 4, average log likelihood -1.303511
INFO: iteration 5, average log likelihood -1.282298
INFO: iteration 6, average log likelihood -1.271356
INFO: iteration 7, average log likelihood -1.267685
INFO: iteration 8, average log likelihood -1.265151
WARNING: Variances had to be floored 4
INFO: iteration 9, average log likelihood -1.261971
INFO: iteration 10, average log likelihood -1.270523
INFO: iteration 11, average log likelihood -1.262617
INFO: iteration 12, average log likelihood -1.259241
INFO: iteration 13, average log likelihood -1.256131
WARNING: Variances had to be floored 4
INFO: iteration 14, average log likelihood -1.252281
INFO: iteration 15, average log likelihood -1.261815
INFO: iteration 16, average log likelihood -1.253535
INFO: iteration 17, average log likelihood -1.250554
WARNING: Variances had to be floored 4
INFO: iteration 18, average log likelihood -1.248333
INFO: iteration 19, average log likelihood -1.258416
INFO: iteration 20, average log likelihood -1.251296
INFO: iteration 21, average log likelihood -1.248814
WARNING: Variances had to be floored 4
INFO: iteration 22, average log likelihood -1.247006
INFO: iteration 23, average log likelihood -1.257447
INFO: iteration 24, average log likelihood -1.250923
INFO: iteration 25, average log likelihood -1.248627
WARNING: Variances had to be floored 4
INFO: iteration 26, average log likelihood -1.246825
INFO: iteration 27, average log likelihood -1.257304
INFO: iteration 28, average log likelihood -1.250784
INFO: iteration 29, average log likelihood -1.248483
WARNING: Variances had to be floored 4
INFO: iteration 30, average log likelihood -1.246676
INFO: iteration 31, average log likelihood -1.257246
INFO: iteration 32, average log likelihood -1.250720
INFO: iteration 33, average log likelihood -1.248418
WARNING: Variances had to be floored 4
INFO: iteration 34, average log likelihood -1.246615
INFO: iteration 35, average log likelihood -1.257230
INFO: iteration 36, average log likelihood -1.250698
INFO: iteration 37, average log likelihood -1.248394
WARNING: Variances had to be floored 4
INFO: iteration 38, average log likelihood -1.246591
INFO: iteration 39, average log likelihood -1.257225
INFO: iteration 40, average log likelihood -1.250690
INFO: iteration 41, average log likelihood -1.248385
WARNING: Variances had to be floored 4
INFO: iteration 42, average log likelihood -1.246583
INFO: iteration 43, average log likelihood -1.257223
INFO: iteration 44, average log likelihood -1.250686
INFO: iteration 45, average log likelihood -1.248382
WARNING: Variances had to be floored 4
INFO: iteration 46, average log likelihood -1.246579
INFO: iteration 47, average log likelihood -1.257222
INFO: iteration 48, average log likelihood -1.250685
INFO: iteration 49, average log likelihood -1.248380
WARNING: Variances had to be floored 4
INFO: iteration 50, average log likelihood -1.246577
INFO: EM with 100000 data points 50 iterations avll -1.246577
236.4 data points per parameter
3: avll = [-1.31642,-1.31621,-1.31497,-1.30351,-1.2823,-1.27136,-1.26768,-1.26515,-1.26197,-1.27052,-1.26262,-1.25924,-1.25613,-1.25228,-1.26181,-1.25354,-1.25055,-1.24833,-1.25842,-1.2513,-1.24881,-1.24701,-1.25745,-1.25092,-1.24863,-1.24682,-1.2573,-1.25078,-1.24848,-1.24668,-1.25725,-1.25072,-1.24842,-1.24661,-1.25723,-1.2507,-1.24839,-1.24659,-1.25723,-1.25069,-1.24839,-1.24658,-1.25722,-1.25069,-1.24838,-1.24658,-1.25722,-1.25068,-1.24838,-1.24658]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.257336
INFO: iteration 2, average log likelihood -1.250608
INFO: iteration 3, average log likelihood -1.246635
WARNING: Variances had to be floored 8
INFO: iteration 4, average log likelihood -1.231485
WARNING: Variances had to be floored 7
INFO: iteration 5, average log likelihood -1.204497
INFO: iteration 6, average log likelihood -1.199586
WARNING: Variances had to be floored 1
INFO: iteration 7, average log likelihood -1.181613
WARNING: Variances had to be floored 7
INFO: iteration 8, average log likelihood -1.182130
WARNING: Variances had to be floored 11
INFO: iteration 9, average log likelihood -1.179834
INFO: iteration 10, average log likelihood -1.176692
WARNING: Variances had to be floored 1 7
INFO: iteration 11, average log likelihood -1.163572
WARNING: Variances had to be floored 6
INFO: iteration 12, average log likelihood -1.178697
WARNING: Variances had to be floored 11
INFO: iteration 13, average log likelihood -1.167558
INFO: iteration 14, average log likelihood -1.169481
WARNING: Variances had to be floored 1 7
INFO: iteration 15, average log likelihood -1.158163
INFO: iteration 16, average log likelihood -1.177716
WARNING: Variances had to be floored 11
INFO: iteration 17, average log likelihood -1.161814
WARNING: Variances had to be floored 7
INFO: iteration 18, average log likelihood -1.168019
WARNING: Variances had to be floored 1
INFO: iteration 19, average log likelihood -1.169133
INFO: iteration 20, average log likelihood -1.168853
WARNING: Variances had to be floored 6 11
INFO: iteration 21, average log likelihood -1.158387
WARNING: Variances had to be floored 7
INFO: iteration 22, average log likelihood -1.171511
WARNING: Variances had to be floored 1
INFO: iteration 23, average log likelihood -1.169632
INFO: iteration 24, average log likelihood -1.168818
WARNING: Variances had to be floored 7 11
INFO: iteration 25, average log likelihood -1.158145
INFO: iteration 26, average log likelihood -1.178094
WARNING: Variances had to be floored 1
INFO: iteration 27, average log likelihood -1.161099
WARNING: Variances had to be floored 7
INFO: iteration 28, average log likelihood -1.166320
WARNING: Variances had to be floored 11
INFO: iteration 29, average log likelihood -1.167920
WARNING: Variances had to be floored 6
INFO: iteration 30, average log likelihood -1.167838
WARNING: Variances had to be floored 1
INFO: iteration 31, average log likelihood -1.164117
WARNING: Variances had to be floored 7
INFO: iteration 32, average log likelihood -1.166213
WARNING: Variances had to be floored 11
INFO: iteration 33, average log likelihood -1.169045
INFO: iteration 34, average log likelihood -1.169892
WARNING: Variances had to be floored 1 7
INFO: iteration 35, average log likelihood -1.158169
INFO: iteration 36, average log likelihood -1.176592
WARNING: Variances had to be floored 11
INFO: iteration 37, average log likelihood -1.160976
WARNING: Variances had to be floored 7
INFO: iteration 38, average log likelihood -1.167103
WARNING: Variances had to be floored 1 6
INFO: iteration 39, average log likelihood -1.166257
INFO: iteration 40, average log likelihood -1.173630
WARNING: Variances had to be floored 7 11
INFO: iteration 41, average log likelihood -1.159907
INFO: iteration 42, average log likelihood -1.177792
WARNING: Variances had to be floored 1
INFO: iteration 43, average log likelihood -1.160897
WARNING: Variances had to be floored 7
INFO: iteration 44, average log likelihood -1.166103
WARNING: Variances had to be floored 11
INFO: iteration 45, average log likelihood -1.168643
INFO: iteration 46, average log likelihood -1.169387
WARNING: Variances had to be floored 1 7
INFO: iteration 47, average log likelihood -1.157859
WARNING: Variances had to be floored 6
INFO: iteration 48, average log likelihood -1.173822
WARNING: Variances had to be floored 11
INFO: iteration 49, average log likelihood -1.165992
WARNING: Variances had to be floored 7
INFO: iteration 50, average log likelihood -1.168093
INFO: EM with 100000 data points 50 iterations avll -1.168093
118.1 data points per parameter
4: avll = [-1.25734,-1.25061,-1.24663,-1.23148,-1.2045,-1.19959,-1.18161,-1.18213,-1.17983,-1.17669,-1.16357,-1.1787,-1.16756,-1.16948,-1.15816,-1.17772,-1.16181,-1.16802,-1.16913,-1.16885,-1.15839,-1.17151,-1.16963,-1.16882,-1.15815,-1.17809,-1.1611,-1.16632,-1.16792,-1.16784,-1.16412,-1.16621,-1.16904,-1.16989,-1.15817,-1.17659,-1.16098,-1.1671,-1.16626,-1.17363,-1.15991,-1.17779,-1.1609,-1.1661,-1.16864,-1.16939,-1.15786,-1.17382,-1.16599,-1.16809]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 2
INFO: iteration 1, average log likelihood -1.168417
WARNING: Variances had to be floored 1 2
INFO: iteration 2, average log likelihood -1.157367
WARNING: Variances had to be floored 1 2 13 14 21 22
INFO: iteration 3, average log likelihood -1.150429
WARNING: Variances had to be floored 1 2 11 23
INFO: iteration 4, average log likelihood -1.141204
WARNING: Variances had to be floored 1 2 21 22 23 24 25 29
INFO: iteration 5, average log likelihood -1.090678
WARNING: Variances had to be floored 1 2 3 11 13 14 26
INFO: iteration 6, average log likelihood -1.076237
WARNING: Variances had to be floored 1 2 4 21 22 23 25 28
INFO: iteration 7, average log likelihood -1.076623
WARNING: Variances had to be floored 1 2 3 11 13 14 23 24 29
INFO: iteration 8, average log likelihood -1.077084
WARNING: Variances had to be floored 1 2 19 21 22 25 26
INFO: iteration 9, average log likelihood -1.076890
WARNING: Variances had to be floored 1 2 3 4 11 13 14 23
INFO: iteration 10, average log likelihood -1.072999
WARNING: Variances had to be floored 1 2 21 22 23 24 28
INFO: iteration 11, average log likelihood -1.081179
WARNING: Variances had to be floored 1 2 11 13 14 25 29
INFO: iteration 12, average log likelihood -1.073113
WARNING: Variances had to be floored 1 2 3 21 22 23 26 32
INFO: iteration 13, average log likelihood -1.077077
WARNING: Variances had to be floored 1 2 4 11 13 14 23 24 25
INFO: iteration 14, average log likelihood -1.067601
WARNING: Variances had to be floored 1 2 3 19 21 22 28 29
INFO: iteration 15, average log likelihood -1.078274
WARNING: Variances had to be floored 1 2 11 13 14 23 25 26
INFO: iteration 16, average log likelihood -1.076886
WARNING: Variances had to be floored 1 2 3 4 21 22 23 24
INFO: iteration 17, average log likelihood -1.080215
WARNING: Variances had to be floored 1 2 11 13 14 29
INFO: iteration 18, average log likelihood -1.075753
WARNING: Variances had to be floored 1 2 21 22 23 25 28
INFO: iteration 19, average log likelihood -1.072245
WARNING: Variances had to be floored 1 2 3 11 13 14 19 23 24 26
INFO: iteration 20, average log likelihood -1.064651
WARNING: Variances had to be floored 1 2 4 21 22 25
INFO: iteration 21, average log likelihood -1.084756
WARNING: Variances had to be floored 1 2 3 11 13 14 23 29
INFO: iteration 22, average log likelihood -1.067711
WARNING: Variances had to be floored 1 2 4 21 22 23 24 25 26 28
INFO: iteration 23, average log likelihood -1.067430
WARNING: Variances had to be floored 1 2 3 11 13 14
INFO: iteration 24, average log likelihood -1.087332
WARNING: Variances had to be floored 1 2 4 21 22 23
INFO: iteration 25, average log likelihood -1.077333
WARNING: Variances had to be floored 1 2 3 11 14 19 23 24 25 29
INFO: iteration 26, average log likelihood -1.053787
WARNING: Variances had to be floored 1 2 4 13 21 22 26 28
INFO: iteration 27, average log likelihood -1.073743
WARNING: Variances had to be floored 1 2 3 11 23 25
INFO: iteration 28, average log likelihood -1.089627
WARNING: Variances had to be floored 1 2 4 13 21 22 23 24
INFO: iteration 29, average log likelihood -1.068189
WARNING: Variances had to be floored 1 2 3 11 25 26 29
INFO: iteration 30, average log likelihood -1.067749
WARNING: Variances had to be floored 1 2 4 13 21 22 23 28 32
INFO: iteration 31, average log likelihood -1.067909
WARNING: Variances had to be floored 1 2 3 11 19 23 24
INFO: iteration 32, average log likelihood -1.078881
WARNING: Variances had to be floored 1 2 4 11 13 21 22 25 29
INFO: iteration 33, average log likelihood -1.059013
WARNING: Variances had to be floored 1 2 3 23 26 28
INFO: iteration 34, average log likelihood -1.074619
WARNING: Variances had to be floored 1 2 11 13 21 22 23 24 25
INFO: iteration 35, average log likelihood -1.056580
WARNING: Variances had to be floored 1 2 3 4 11 29
INFO: iteration 36, average log likelihood -1.062753
WARNING: Variances had to be floored 1 2 13 19 21 22 23 25 26 28 32
INFO: iteration 37, average log likelihood -1.049228
WARNING: Variances had to be floored 1 2 11 23 24 29
INFO: iteration 38, average log likelihood -1.093393
WARNING: Variances had to be floored 1 2 3 13 21 22
INFO: iteration 39, average log likelihood -1.069344
WARNING: Variances had to be floored 1 2 4 11 23 25 28 29
INFO: iteration 40, average log likelihood -1.050961
WARNING: Variances had to be floored 1 2 3 13 21 22 23 24 26
INFO: iteration 41, average log likelihood -1.064535
WARNING: Variances had to be floored 1 2 4 11 19 25
INFO: iteration 42, average log likelihood -1.065848
WARNING: Variances had to be floored 1 2 3 13 21 22 23 28 29 32
INFO: iteration 43, average log likelihood -1.057108
WARNING: Variances had to be floored 1 2 11 23 24 25 26
INFO: iteration 44, average log likelihood -1.075151
WARNING: Variances had to be floored 1 2 3 4 13 21 22
INFO: iteration 45, average log likelihood -1.063673
WARNING: Variances had to be floored 1 2 11 23 28 29
INFO: iteration 46, average log likelihood -1.064304
WARNING: Variances had to be floored 1 2 13 19 21 22 23 24 25
INFO: iteration 47, average log likelihood -1.058828
WARNING: Variances had to be floored 1 2 3 11 26
INFO: iteration 48, average log likelihood -1.068956
WARNING: Variances had to be floored 1 2 4 11 13 21 22 23 25 28 29 32
INFO: iteration 49, average log likelihood -1.047726
WARNING: Variances had to be floored 1 2 3 23 24
INFO: iteration 50, average log likelihood -1.090558
INFO: EM with 100000 data points 50 iterations avll -1.090558
59.0 data points per parameter
5: avll = [-1.16842,-1.15737,-1.15043,-1.1412,-1.09068,-1.07624,-1.07662,-1.07708,-1.07689,-1.073,-1.08118,-1.07311,-1.07708,-1.0676,-1.07827,-1.07689,-1.08022,-1.07575,-1.07224,-1.06465,-1.08476,-1.06771,-1.06743,-1.08733,-1.07733,-1.05379,-1.07374,-1.08963,-1.06819,-1.06775,-1.06791,-1.07888,-1.05901,-1.07462,-1.05658,-1.06275,-1.04923,-1.09339,-1.06934,-1.05096,-1.06453,-1.06585,-1.05711,-1.07515,-1.06367,-1.0643,-1.05883,-1.06896,-1.04773,-1.09056]
[-1.40071,-1.4008,-1.40072,-1.40019,-1.39337,-1.37469,-1.36805,-1.36668,-1.36597,-1.36561,-1.36538,-1.36519,-1.36501,-1.36485,-1.36472,-1.3646,-1.36451,-1.36444,-1.36437,-1.36432,-1.36427,-1.36423,-1.3642,-1.36417,-1.36415,-1.36413,-1.36412,-1.3641,-1.3641,-1.36409,-1.36409,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36408,-1.36423,-1.36411,-1.36382,-1.36085,-1.34797,-1.33521,-1.33074,-1.32914,-1.32841,-1.32798,-1.32768,-1.32744,-1.32723,-1.32704,-1.32687,-1.32671,-1.32657,-1.32644,-1.32632,-1.32621,-1.32611,-1.326,-1.3259,-1.3258,-1.3257,-1.32559,-1.32546,-1.32531,-1.32509,-1.32477,-1.32431,-1.32372,-1.32305,-1.32235,-1.32167,-1.32108,-1.32057,-1.32013,-1.31967,-1.31913,-1.31851,-1.31788,-1.31735,-1.31698,-1.31673,-1.31657,-1.31646,-1.31638,-1.31632,-1.31628,-1.31642,-1.31621,-1.31497,-1.30351,-1.2823,-1.27136,-1.26768,-1.26515,-1.26197,-1.27052,-1.26262,-1.25924,-1.25613,-1.25228,-1.26181,-1.25354,-1.25055,-1.24833,-1.25842,-1.2513,-1.24881,-1.24701,-1.25745,-1.25092,-1.24863,-1.24682,-1.2573,-1.25078,-1.24848,-1.24668,-1.25725,-1.25072,-1.24842,-1.24661,-1.25723,-1.2507,-1.24839,-1.24659,-1.25723,-1.25069,-1.24839,-1.24658,-1.25722,-1.25069,-1.24838,-1.24658,-1.25722,-1.25068,-1.24838,-1.24658,-1.25734,-1.25061,-1.24663,-1.23148,-1.2045,-1.19959,-1.18161,-1.18213,-1.17983,-1.17669,-1.16357,-1.1787,-1.16756,-1.16948,-1.15816,-1.17772,-1.16181,-1.16802,-1.16913,-1.16885,-1.15839,-1.17151,-1.16963,-1.16882,-1.15815,-1.17809,-1.1611,-1.16632,-1.16792,-1.16784,-1.16412,-1.16621,-1.16904,-1.16989,-1.15817,-1.17659,-1.16098,-1.1671,-1.16626,-1.17363,-1.15991,-1.17779,-1.1609,-1.1661,-1.16864,-1.16939,-1.15786,-1.17382,-1.16599,-1.16809,-1.16842,-1.15737,-1.15043,-1.1412,-1.09068,-1.07624,-1.07662,-1.07708,-1.07689,-1.073,-1.08118,-1.07311,-1.07708,-1.0676,-1.07827,-1.07689,-1.08022,-1.07575,-1.07224,-1.06465,-1.08476,-1.06771,-1.06743,-1.08733,-1.07733,-1.05379,-1.07374,-1.08963,-1.06819,-1.06775,-1.06791,-1.07888,-1.05901,-1.07462,-1.05658,-1.06275,-1.04923,-1.09339,-1.06934,-1.05096,-1.06453,-1.06585,-1.05711,-1.07515,-1.06367,-1.0643,-1.05883,-1.06896,-1.04773,-1.09056]
32×26 Array{Float64,2}:
 -0.162277    -0.510924    -0.0299588    0.265716    -0.0312082    -0.0510113  -0.0667543    0.120487     0.236313    -0.0183633  -0.016455    -0.019141     0.00926636   0.0801126  -0.727012     0.0626777    0.110044      0.0383201    0.129428     0.0405749    0.0249903     0.0559613   0.0256326    0.213198   -0.029784    -0.0423686 
  0.0896865    0.413259     0.0308785    0.168804     0.154473     -0.0217265  -0.218705     0.0591048   -0.0298666   -0.0189758  -0.0763717   -0.0418898    0.0235114    0.150836    0.367439     0.0625895    0.11006       0.0409369    0.190923     0.0363577   -0.237269     -0.137419    0.0539737    0.188397   -0.028886    -0.0419983 
  0.00324377  -0.0588705   -0.0792258   -0.173137    -0.154418     -0.184744   -0.122402    -0.110052    -0.102684    -0.0213959  -0.0763231   -0.00694207   0.008796    -0.0778674   0.0303327   -0.0188038   -0.0615248     0.0084144    0.120903    -0.0238073    0.045106      0.0737586  -0.100573     0.0251257  -0.0714396    0.209671  
 -0.142347    -0.00530285  -0.114601     0.106092    -0.102648     -0.0662175   0.0557674   -0.221721     0.0156797   -0.0215562   0.0495708    0.028389     0.0411973   -0.0201961   0.244959    -0.0782205   -0.00539138    0.0530381   -0.215247     0.0496496    0.13771      -0.0269298  -0.0140577   -0.0761632  -0.117166     0.0502166 
 -0.156045    -0.0862091   -0.18464     -0.00778053  -0.0058773    -0.0187569  -0.0240392    0.0250178   -0.0160686    0.0192128   0.0460667   -0.0609197   -0.0645098    0.102449   -0.0178796   -0.0319559    0.0485823    -0.0428925    0.13542     -0.0662805   -0.126223     -0.106343   -0.166996     0.145281   -0.00387294  -0.256108  
  0.0406222   -0.0695762    0.0433245    0.0647071    0.0220629     0.0127229  -0.0456614   -0.0504794   -0.0325175   -0.0277939  -0.00820452   0.0257667    0.0546179   -0.0180286   0.116404    -0.00551652   0.120318      0.0334452   -0.0457632   -0.111189    -0.000202398  -0.0612391  -0.0569371   -0.0697713   0.00221965  -0.063858  
 -0.0236809   -0.364054    -0.018154     0.0645081    0.0263229    -0.164397    0.0329053    0.0474458    0.0585594    0.0708916   0.226315    -0.012901    -0.0523269   -0.144313   -0.0701023    0.118767    -0.018239     -0.017826    -0.188712     0.264846    -0.00888951   -0.0365781  -0.0632832    0.0951255   0.077546     0.217064  
 -0.0581839    0.315012     0.0349691    0.049109     0.0137815    -0.0540481   0.0822853    0.0244609    0.258961     0.0520336   0.231591     0.00379257   0.0337875   -0.160503   -0.098878     0.135154    -0.109073     -0.013024    -0.160169     0.263639     0.0350355     0.0560773  -0.0599582    0.0748791   0.0530538    0.230911  
 -0.157651    -0.173356     0.0659514   -0.0907191   -0.0300241     0.0387193   0.0593317    0.00982823  -0.0689747    0.0837338   0.0884648    0.0210596   -0.0913574   -0.0424551  -1.05143     -0.12611      0.0529494     0.0717975    0.0274235   -0.0348684    0.0468059    -0.0321478   0.00450552   0.0215178   0.0151154   -0.0259355 
 -0.125307    -0.139522     0.0386499   -0.0597688    0.035403     -0.0982042   0.0403352    0.0222672   -0.141009     0.0451976   0.0827607    0.0194614   -0.0813052   -0.0350568   0.669245    -0.108681     0.0793205     0.0639986    0.0419962   -0.04709     -0.0901331    -0.0113926   0.0583755   -0.215086    0.0407191   -0.116284  
  0.0450962   -0.0418082    0.0897504    0.155075    -0.00770513    0.0528154   0.00748926  -0.00998421  -0.0708496   -0.0611774  -0.0857561   -0.143896    -0.0865841   -0.0430996  -0.00372066  -0.0327469   -0.0919181     0.027341     0.00134018  -0.22682      0.0547085    -0.0786946  -0.0305792    0.0495324   0.0828106    0.113007  
  0.00194847  -0.0204117   -0.016404     0.0548079   -0.134035     -0.0857031   0.143447    -0.110307     0.0752945    0.152634   -0.0380962    0.243821    -0.0512912   -0.218805    0.144671    -0.0813681    0.02324       0.105763     0.12862     -0.0593852    0.0413572    -0.0420601  -0.0275741    0.12241    -0.120466    -0.043888  
  0.0879284    0.0648904   -0.00943165  -0.0996463   -0.0109183     0.0457877   0.0653248    0.0131489    0.0535745    0.0176155   0.00652994   0.0433814    0.0494756   -0.0611452   0.048581     0.0216142    0.105102     -0.313778     0.0425924   -0.187525     0.0597975     0.0277427  -0.0602459   -0.0545045  -0.168316     0.0622202 
 -0.186951    -0.0638637   -0.0888735    0.132468    -0.0150111    -0.0503984   0.0895156    0.0671727    0.131084     0.140138    0.129465    -0.119401     0.0450947    0.167749    0.0725138   -0.146145     0.14393       0.0219352   -0.0306223   -0.0135711   -0.0977378    -0.144593    0.123508    -0.0238089   0.181016    -0.152868  
 -0.0056891   -0.0398947   -0.153441    -0.0770623   -0.11902      -0.22387     0.0886178   -0.112116     0.101764     0.210235    0.0618008    0.0264581   -0.24248      0.0826349  -0.112036    -0.0114457    0.0876727     0.0871258    0.232396     0.00463696  -0.0991376    -0.190739   -0.0767584   -0.0671116   0.0852518   -0.043895  
 -0.0166729   -0.0463472   -0.124822    -0.0254834    0.0306495    -0.125039    0.0945456   -0.0697306    0.0326566    0.0941351   0.117932     0.0229391   -0.247755     0.0277769  -0.113802    -0.0785018    0.0958402     0.148626     0.31774     -0.0337889   -0.111152      0.18792    -0.0586409   -0.19574    -0.218865     0.010354  
 -0.022366    -0.0923168    0.114885    -0.0492392    0.175718      0.171548    0.0050604   -0.043759    -0.0939178    0.0630748   0.0271922    0.0892289   -0.0502951   -0.113296    0.104247     0.0332823    0.0476701    -0.026171    -0.105858     0.12076     -0.0844212    -0.0100993  -0.00369301   0.0833411   0.0737102    0.112822  
 -0.0458913   -0.0133517    0.0553723   -0.0570516   -0.0578738     0.0484397  -0.00246846   0.0164858   -0.0910103   -0.0145649  -0.0414905    0.00330616  -0.0365076    0.0557331   0.0138445    0.0447684   -0.0209379     0.0906888    0.0912565   -0.0384941    0.0266289    -0.0124387  -0.0968814    0.0176519   0.0401905    0.0347668 
 -0.00132026  -0.0356132   -0.0263903    0.0256389   -0.0777978    -0.0758981   0.155285     0.0743174    0.162434     0.0248689  -0.0923554   -0.208948    -0.0909494   -0.120643   -0.0312893    0.0420878   -0.00117088    0.0644869   -0.0146981    0.0402588    0.162743     -0.104098   -0.0455918    0.0785131  -0.0235439   -0.00906299
 -0.0106668   -0.0643078    0.0970768   -0.0118371    0.000488826   0.041557   -0.0283136    0.0878241   -0.0782328    0.0795516  -0.0878368   -0.089204     0.138214    -0.0381322   0.0200515    0.0570034   -0.0437531    -0.0360178   -0.120176    -0.00430046  -0.00468455    0.014017   -0.0645497   -0.108496    0.113841     0.0486277 
 -0.0816543    0.0202216    0.0843188    0.199809     0.0989568    -0.0615094  -0.0760779   -0.1897      -0.769691     0.0171541  -0.300681    -0.139566    -0.429536    -0.126351    0.0917878   -0.214671    -0.0257097    -0.0731719   -0.0244307    0.0437047   -0.0902473    -0.0151594   0.0638674    0.0102978   0.110664    -0.00821793
 -0.0817632    0.0263807   -0.0631633    0.158297     0.227825     -0.0643412  -0.0673242    0.112104     0.350481     0.0172911  -0.0381707   -0.0112935    0.0745215   -0.168216    0.101906    -0.222744    -0.0855341    -0.115985    -0.102265     0.0468621   -0.0820087    -0.0142265   0.112175     0.0221101   0.0603468   -0.13653   
  0.145772     0.0362832   -0.124169     0.0535334   -0.0779412     0.151781   -0.100017     0.0765072   -0.0374835    0.0255662   0.0615924   -0.154049    -0.0504683   -0.0653483   0.022406    -0.0242481   -0.0625834     0.067909    -0.0171926    0.0631176   -0.0485576     0.116398    0.150509    -0.0218414  -0.0541183    0.0346433 
  0.114335     0.163476     0.0533447   -0.0251994    0.0688324    -0.103649   -0.0428233   -0.0782457   -0.0412961   -0.0942856   0.166014     0.073624    -0.10368      0.0705907  -0.0789491   -0.0675347   -0.18309       0.0715874   -0.112844    -0.0348971   -0.179173     -0.202011   -0.0706692   -0.0639527  -0.0362872   -0.0740464 
  0.0987614   -0.0625598   -0.0161115   -0.0132183   -0.168444      0.116259   -0.16668     -0.0231612    0.0116702    0.0785898  -0.0436403    0.0370988   -0.0870251   -0.208872   -0.0477066    0.0399791    0.102622     -0.0954573   -0.0801858   -0.0263743   -0.0225739     0.045862    0.0190604   -0.0551263   0.0219952   -0.0678126 
 -0.16913      0.152471    -0.0264018   -0.100194    -0.121133     -0.10783    -0.0232463   -0.0394136   -0.23432     -0.0722854   0.217073    -0.176991    -0.126896     0.0246975   0.131223     0.0146579    0.193894      0.00934123  -0.0788436    0.081465    -0.0493492    -0.0820364   0.0804091   -0.0334318   0.0281534   -0.107127  
  0.0286442    0.121711    -0.160909     0.154448    -0.299759      0.0471029  -0.0754155   -0.156965    -0.15971      0.133445    0.0540427   -0.130238     0.0530477    0.0506324  -0.158775     0.0482781   -0.0839516     0.150591    -0.0403734   -0.0862142   -0.0535764     0.0941407   0.0522703    0.0233073  -0.129073     0.221734  
  0.106937     0.00868825  -0.0726354   -0.0218697   -0.177754      0.0485419   0.275486     0.047342     0.184986     0.0419298   0.00411224   0.0228859    0.101008    -0.0576674  -0.0347072    0.0818988   -0.202743      0.111345    -0.0384168    0.0930118   -0.13508       0.0498025   0.102936     0.105159   -0.0262522   -0.154507  
 -0.096862    -0.125267    -0.161946     0.125887     0.02581       0.038048   -0.191593     0.0548897   -0.144485     0.0571482  -0.0881942   -0.00303578  -0.112118    -0.154442   -0.0934892   -0.168346     0.0993291     0.0798012    0.136525    -0.0484078    0.0874513    -0.0930857   0.0682117    0.0500981   0.0512323   -0.0420587 
  0.102632     0.0709381   -0.0471463   -0.0733329    0.0619076     0.11753     0.0399364    0.0455468   -0.059377    -0.0698473  -0.193278    -0.00180039   0.0813404   -0.20249    -0.104157    -0.0848347   -0.000953099   0.0106892   -0.0463814    0.0882481   -0.0595027    -0.141095    0.0219483    0.0305391   0.0731176   -0.0962229 
 -0.135396     0.0177562   -0.0611233    0.17626      0.0153385     0.0784268   0.157783    -0.0206499   -0.00970954   0.0486993   0.0687248    0.0899695   -0.106011    -0.176189   -0.0215569    0.0242027    0.0282423     0.0606846   -0.0610027   -0.0234619   -0.152189     -0.155658   -0.0114362    0.0874242   0.14943      0.0237643 
 -0.0428607    0.0479617   -0.109244    -0.0973364   -0.0346845    -0.0374758  -0.0155821   -0.121306     0.0894446    0.0265568   0.12742     -0.0553313   -0.0754617    0.0504036  -0.114458    -0.0449032   -0.0532097     0.00496239   0.0311267   -0.0555843   -0.0570782     0.0730629   0.016035     0.101073    0.0846631    0.00439413INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 2 4 11 13 21 22 25 26 29
INFO: iteration 1, average log likelihood -1.049035
WARNING: Variances had to be floored 1 2 3 11 13 19 21 22 23 25 26 28 29
INFO: iteration 2, average log likelihood -1.032681
WARNING: Variances had to be floored 1 2 4 11 13 21 22 23 24 25 26 29
INFO: iteration 3, average log likelihood -1.036641
WARNING: Variances had to be floored 1 2 3 11 13 19 21 22 25 26 28 29
INFO: iteration 4, average log likelihood -1.034532
WARNING: Variances had to be floored 1 2 4 11 13 21 22 23 25 26 29 32
INFO: iteration 5, average log likelihood -1.037340
WARNING: Variances had to be floored 1 2 3 11 13 19 21 22 23 24 25 26 28 29
INFO: iteration 6, average log likelihood -1.032579
WARNING: Variances had to be floored 1 2 4 11 13 21 22 25 26 29
INFO: iteration 7, average log likelihood -1.043613
WARNING: Variances had to be floored 1 2 3 11 13 19 21 22 23 25 26 28 29
INFO: iteration 8, average log likelihood -1.030934
WARNING: Variances had to be floored 1 2 4 11 13 21 22 23 24 25 26 29
INFO: iteration 9, average log likelihood -1.035813
WARNING: Variances had to be floored 1 2 3 11 13 19 21 22 25 26 28 29 32
INFO: iteration 10, average log likelihood -1.034442
INFO: EM with 100000 data points 10 iterations avll -1.034442
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.408066e+05
      1       6.743756e+05      -1.664310e+05 |       32
      2       6.433021e+05      -3.107348e+04 |       32
      3       6.283191e+05      -1.498296e+04 |       32
      4       6.176651e+05      -1.065401e+04 |       32
      5       6.104854e+05      -7.179718e+03 |       32
      6       6.053668e+05      -5.118562e+03 |       32
      7       6.002820e+05      -5.084799e+03 |       32
      8       5.964569e+05      -3.825170e+03 |       32
      9       5.941364e+05      -2.320502e+03 |       32
     10       5.928612e+05      -1.275120e+03 |       32
     11       5.921112e+05      -7.499914e+02 |       32
     12       5.916267e+05      -4.844994e+02 |       32
     13       5.911427e+05      -4.840816e+02 |       32
     14       5.904531e+05      -6.896039e+02 |       32
     15       5.897198e+05      -7.332592e+02 |       32
     16       5.893192e+05      -4.005832e+02 |       32
     17       5.891859e+05      -1.333289e+02 |       32
     18       5.891467e+05      -3.914581e+01 |       30
     19       5.891203e+05      -2.643369e+01 |       30
     20       5.890917e+05      -2.857919e+01 |       31
     21       5.890561e+05      -3.562215e+01 |       32
     22       5.889986e+05      -5.750195e+01 |       32
     23       5.889060e+05      -9.260449e+01 |       32
     24       5.888065e+05      -9.952011e+01 |       30
     25       5.887003e+05      -1.061893e+02 |       31
     26       5.885913e+05      -1.089937e+02 |       32
     27       5.884844e+05      -1.068622e+02 |       31
     28       5.883919e+05      -9.249159e+01 |       32
     29       5.883161e+05      -7.580659e+01 |       32
     30       5.882388e+05      -7.734359e+01 |       30
     31       5.881604e+05      -7.840772e+01 |       31
     32       5.880895e+05      -7.084063e+01 |       30
     33       5.880339e+05      -5.565843e+01 |       30
     34       5.879990e+05      -3.492971e+01 |       28
     35       5.879819e+05      -1.703384e+01 |       29
     36       5.879723e+05      -9.645940e+00 |       28
     37       5.879666e+05      -5.631016e+00 |       25
     38       5.879630e+05      -3.634877e+00 |       25
     39       5.879610e+05      -2.007204e+00 |       25
     40       5.879594e+05      -1.647926e+00 |       20
     41       5.879584e+05      -9.863098e-01 |       16
     42       5.879576e+05      -7.279460e-01 |       16
     43       5.879571e+05      -5.302011e-01 |       17
     44       5.879566e+05      -4.638718e-01 |        6
     45       5.879564e+05      -2.150619e-01 |        8
     46       5.879562e+05      -2.181445e-01 |        5
     47       5.879561e+05      -9.829973e-02 |        2
     48       5.879561e+05      -2.361523e-02 |        0
     49       5.879561e+05       0.000000e+00 |        0
K-means converged with 49 iterations (objv = 587956.091859178)
INFO: K-means with 32000 data points using 49 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.303835
INFO: iteration 2, average log likelihood -1.271794
INFO: iteration 3, average log likelihood -1.243007
INFO: iteration 4, average log likelihood -1.212851
INFO: iteration 5, average log likelihood -1.177653
WARNING: Variances had to be floored 1 16
INFO: iteration 6, average log likelihood -1.128770
WARNING: Variances had to be floored 7 8
INFO: iteration 7, average log likelihood -1.102562
WARNING: Variances had to be floored 5 18 22 24 26 27 30
INFO: iteration 8, average log likelihood -1.068970
WARNING: Variances had to be floored 12
INFO: iteration 9, average log likelihood -1.117250
WARNING: Variances had to be floored 1 8 16 19 20 29
INFO: iteration 10, average log likelihood -1.066642
WARNING: Variances had to be floored 2
INFO: iteration 11, average log likelihood -1.104129
WARNING: Variances had to be floored 3 18 22 24 27
INFO: iteration 12, average log likelihood -1.055398
WARNING: Variances had to be floored 30
INFO: iteration 13, average log likelihood -1.080001
WARNING: Variances had to be floored 1 8 12 16 19 23 26
INFO: iteration 14, average log likelihood -1.037790
WARNING: Variances had to be floored 18
INFO: iteration 15, average log likelihood -1.105627
WARNING: Variances had to be floored 22 24 27
INFO: iteration 16, average log likelihood -1.062862
WARNING: Variances had to be floored 3 29
INFO: iteration 17, average log likelihood -1.061574
WARNING: Variances had to be floored 1 8 16 19
INFO: iteration 18, average log likelihood -1.049244
WARNING: Variances had to be floored 7 18 30
INFO: iteration 19, average log likelihood -1.069822
WARNING: Variances had to be floored 12 22 24 27
INFO: iteration 20, average log likelihood -1.043789
WARNING: Variances had to be floored 1 3 16 19 23
INFO: iteration 21, average log likelihood -1.045986
WARNING: Variances had to be floored 8 26 29
INFO: iteration 22, average log likelihood -1.081317
WARNING: Variances had to be floored 18 25
INFO: iteration 23, average log likelihood -1.070160
WARNING: Variances had to be floored 1 8 16 22 23 27 30
INFO: iteration 24, average log likelihood -1.035207
WARNING: Variances had to be floored 12 19 24
INFO: iteration 25, average log likelihood -1.082824
WARNING: Variances had to be floored 3 7 18 29
INFO: iteration 26, average log likelihood -1.061193
WARNING: Variances had to be floored 16 26
INFO: iteration 27, average log likelihood -1.077461
WARNING: Variances had to be floored 1 8 22 27
INFO: iteration 28, average log likelihood -1.053341
WARNING: Variances had to be floored 12 18 19 24 25 30
INFO: iteration 29, average log likelihood -1.059844
WARNING: Variances had to be floored 16 23 29
INFO: iteration 30, average log likelihood -1.072100
WARNING: Variances had to be floored 3
INFO: iteration 31, average log likelihood -1.063265
WARNING: Variances had to be floored 1 7 8 22 26 27
INFO: iteration 32, average log likelihood -1.020624
WARNING: Variances had to be floored 16 18 19 24 30
INFO: iteration 33, average log likelihood -1.077588
WARNING: Variances had to be floored 12 29
INFO: iteration 34, average log likelihood -1.097240
WARNING: Variances had to be floored 3
INFO: iteration 35, average log likelihood -1.069915
WARNING: Variances had to be floored 1 8 16 18 22 27
INFO: iteration 36, average log likelihood -1.032290
WARNING: Variances had to be floored 19 24 25 26 30
INFO: iteration 37, average log likelihood -1.078060
WARNING: Variances had to be floored 12 29
INFO: iteration 38, average log likelihood -1.083695
WARNING: Variances had to be floored 1 7 16 18
INFO: iteration 39, average log likelihood -1.038912
WARNING: Variances had to be floored 3 8 19 22 23 26 27
INFO: iteration 40, average log likelihood -1.047541
WARNING: Variances had to be floored 24 30
INFO: iteration 41, average log likelihood -1.106179
WARNING: Variances had to be floored 16 29
INFO: iteration 42, average log likelihood -1.067420
WARNING: Variances had to be floored 1 8 18
INFO: iteration 43, average log likelihood -1.036125
WARNING: Variances had to be floored 3 7 12 19 22 23 25 26 27
INFO: iteration 44, average log likelihood -1.030907
WARNING: Variances had to be floored 16 24 30
INFO: iteration 45, average log likelihood -1.101219
WARNING: Variances had to be floored 8 29
INFO: iteration 46, average log likelihood -1.089928
WARNING: Variances had to be floored 18
INFO: iteration 47, average log likelihood -1.066650
WARNING: Variances had to be floored 1 8 16 22 23 26 27
INFO: iteration 48, average log likelihood -1.021084
WARNING: Variances had to be floored 12 19 24 30
INFO: iteration 49, average log likelihood -1.075673
WARNING: Variances had to be floored 3 7 18 29
INFO: iteration 50, average log likelihood -1.071567
INFO: EM with 100000 data points 50 iterations avll -1.071567
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0426117   -0.0113217   -0.0304987    0.10291     -0.0191465    0.0869238  -0.0408815    0.0147902   -0.0789609    0.0260833   -0.0443122     0.16257     -0.109462     0.0330731  -0.0279924   -0.0879477    0.0908766    0.0910979    0.101749    -0.029033    0.0922919   -0.0108226   -0.00672887    0.16652     0.0821051   -0.0184676 
 -0.0107417   -0.0428085   -0.140697    -0.0542586   -0.0493025   -0.179119    0.0913476   -0.0932552    0.0706429    0.157852     0.0874634     0.0247278   -0.245194     0.0573444  -0.114119    -0.0421096    0.0912476    0.115529     0.271695    -0.0131256  -0.106224    -0.015746    -0.0707432    -0.125576   -0.0542442   -0.0180423 
  0.0547008   -0.0454091    0.0944505   -0.0524872    0.0852287   -0.0213596  -0.0560808    0.0932215   -0.0663007    0.163356    -0.0727948    -0.184401     0.205711     0.0272363  -0.0641015    0.149406    -0.0639015   -0.0648052   -0.0890223   -0.0747193   0.0796309   -0.0120708   -0.196391     -0.141641    0.10808      0.0908458 
  0.00116738  -0.0210704   -0.0162229    0.0546871   -0.133451    -0.0859008   0.142369    -0.110385     0.075544     0.152467    -0.0383763     0.241329    -0.0517024   -0.21831     0.143635    -0.0817409    0.0237622    0.105654     0.128425    -0.0598644   0.0425589   -0.0449791   -0.0279493     0.120394   -0.120317    -0.0452238 
 -0.0399906    0.0742632    0.0408642   -0.0210704    0.0809598    0.0808989   0.0987713    0.109576    -0.191831    -0.137723    -0.0413565    -0.123786    -0.0159542    0.0570109  -0.163731    -0.00366593  -0.113075     0.0832199    0.137735     0.0925773  -0.00914426  -0.157848     0.0607026     0.0666414   0.0126736    0.0439305 
 -0.135146     0.0157612   -0.0616784    0.171938     0.0145115    0.0775714   0.153937    -0.0222729   -0.00933026   0.0489899    0.074039      0.0895392   -0.105152    -0.174344   -0.0238728    0.0257975    0.0285329    0.0589247   -0.0636624   -0.0230898  -0.150481    -0.150411    -0.0117589     0.0820548   0.146381     0.0260359 
 -0.0457032    0.0535313   -0.107394    -0.0856288   -0.020961    -0.03563    -0.0179847   -0.127094     0.0875871    0.0143946    0.130888     -0.0473123   -0.0659608    0.0339396  -0.118865    -0.057941    -0.0508916   -0.00298476   0.0409134   -0.0543135  -0.0800552    0.0788131    0.0243833     0.112883    0.0898883    3.36931e-5
  0.0543447    0.0659835   -0.0141347   -0.0873507   -0.0172108    0.0369128   0.0633435    0.00852029   0.0475542    0.00795286   0.00956838    0.0431291    0.0448681   -0.0534899   0.0255221    0.0290819    0.106301    -0.271195     0.0392826   -0.171907    0.052639     0.0296301   -0.0556735    -0.05508    -0.152948     0.0624156 
  0.00369916  -0.0911563   -0.0714093   -0.050089    -0.114318     0.0727632  -0.0388344    0.0161725   -0.0576547    0.147777    -0.0693075    -0.229177    -0.0932448    0.0915718   0.184405     0.0283722    0.0677589    0.141904     0.00655581  -0.0987052   0.168218    -0.0508158   -0.231308     -0.0531271  -0.0229701    0.035074  
 -0.0756482   -0.0843433    0.0803652    0.0275476   -0.104227     0.0964813   0.0184027    0.0838922   -0.0774769   -0.00782239  -0.0959815    -0.022774     0.0514161   -0.102812    0.120403    -0.0508982   -0.0263976    0.00214334  -0.143204     0.0768132  -0.0771628    0.0215126    0.0739744    -0.0585744   0.11591     -0.0110947 
 -0.039991    -0.00300986   0.00924955   0.0559293    0.0205697   -0.104313    0.0564952    0.0360993    0.168569     0.0607654    0.22722      -0.00794192  -0.00450503  -0.153575   -0.086779     0.13041     -0.0707097   -0.0142819   -0.172398     0.266156    0.0187756    0.0147179   -0.0557349     0.0896124   0.0642593    0.227167  
 -0.116361    -0.104459    -0.215775     0.135834    -0.0179565   -0.0229054  -0.153404     0.0663888   -0.0483598    0.0546177   -0.144551     -0.0817869   -0.111614    -0.205889   -0.0852174   -0.127268     0.0523738    0.0561528    0.188242    -0.0743786   0.12378     -0.128565     0.0528784     0.113473    0.00841544  -0.0370467 
  0.0993332    0.0718999   -0.0467075   -0.0727841    0.0628106    0.117665    0.0398202    0.0463905   -0.0600168   -0.0702888   -0.193832     -0.0016776    0.0797578   -0.202548   -0.106648    -0.0828739   -0.00162818   0.00987454  -0.0454432    0.0910382  -0.0563153   -0.141673     0.0216838     0.0286941   0.0740825   -0.0960737 
 -0.190154    -0.0642599   -0.0897084    0.13495     -0.0119839   -0.0501438   0.0897724    0.0676444    0.131589     0.140766     0.130241     -0.119675     0.0449677    0.166168    0.072964    -0.150779     0.141799     0.0201391   -0.0307296   -0.0133333  -0.0997965   -0.147278     0.126438     -0.0228048   0.182742    -0.155978  
 -0.0015567   -0.0593125   -0.0566865    0.0606954    0.0632672    0.106533    0.0180616   -0.112821    -0.0494613   -0.0147018   -0.0679868     0.138071     0.123977    -0.107228    0.138221     0.0722545    0.0141176    0.0559429    0.0294569   -0.0452996  -0.0662497   -0.0445187    0.0455626    -0.165198    0.00205462  -0.0218737 
 -0.0220608   -0.0556244   -0.00157984   0.205634     0.0511749   -0.0359386  -0.138154     0.0736072    0.0956269   -0.0134472   -0.0336953    -0.0254826    0.0145578    0.118535   -0.182023     0.0551658    0.1118       0.0429791    0.144453     0.0349781  -0.0962293   -0.0383779    0.0360195     0.183899   -0.0311719   -0.0453098 
 -0.153334    -0.0833464   -0.17681     -0.00965469  -0.00437308  -0.015249   -0.0275713    0.022861    -0.0156774    0.0208813    0.0466902    -0.0601299   -0.0639893    0.100276   -0.0211488   -0.0333523    0.0444125   -0.0410406    0.135647    -0.0692471  -0.128033    -0.10625     -0.160885      0.14339    -0.00495795  -0.251619  
 -0.0835978    0.0269775   -0.0302992    0.172838     0.202443    -0.071065   -0.0799169    0.0357418    0.0648451    0.0179765   -0.108151     -0.0339926   -0.0608135   -0.161311    0.107889    -0.226792    -0.065707    -0.104115    -0.0889877    0.0458376  -0.0885986   -0.0121357    0.091301      0.0181555   0.071062    -0.107264  
  0.0176853   -0.0534384   -0.0153667    0.00843366  -0.0865939   -0.0909272   0.277033     0.0425858    0.248273     0.0369874   -0.068147     -0.195125    -0.102921    -0.186327   -0.0549799    0.0824903    0.012282     0.0446346   -0.0436439    0.0738971   0.170942    -0.110248    -0.0590442     0.0583562  -0.0303728   -0.00832252
 -0.0718957   -0.0318485   -0.0976524   -0.0353911   -0.130058    -0.121743   -0.0276636   -0.165021    -0.0385422   -0.0176077   -0.0156323     0.0135512    0.020133    -0.054264    0.141829    -0.0499705   -0.0392628    0.0288171   -0.0521835    0.0135545   0.0902374    0.0235832   -0.0561594    -0.0274593  -0.0949075    0.136334  
 -0.110971    -0.0728855    0.102544    -0.201703    -0.155545    -0.0946452  -0.0688082   -0.0503306   -0.0314929   -0.0765852   -0.00183989    0.103809     0.00824756  -0.0163737  -0.00705394   0.145556    -0.00886832  -0.0129038    0.180621    -0.115304   -0.092335     0.100363    -0.109011      0.0613274   0.0279371    0.0389682 
 -0.16861      0.141585    -0.0169417   -0.089643    -0.123992    -0.0983095  -0.0331365   -0.0374095   -0.216299    -0.0695746    0.204898     -0.17241     -0.123849     0.0209512   0.129804     0.0199799    0.185021     0.00697967  -0.0774591    0.0856438  -0.0430473   -0.080806     0.0776349    -0.0333837   0.0248493   -0.105914  
 -0.0202846    0.0205037    0.18649      0.102375    -0.0462812    0.11967     0.0893991   -0.00959842  -0.0173352    0.0140795   -0.0803078     0.20935     -0.0852336    0.0768435   0.0122384   -0.0161054   -0.0175479    0.122719    -0.0332182   -0.0217671   0.063297     0.0439316   -0.0666113    -0.0816422   0.148593     0.0320677 
  0.0566144   -0.10662     -0.0599375    0.0249691   -0.0994917    0.0851393  -0.314655     0.0106903   -0.0410793    0.0895721   -0.0749484     0.0265613   -0.0975731   -0.311746   -0.0588081   -0.0203747    0.10822     -0.0401332   -0.037308    -0.0178122   0.027624     0.0170631    0.037574     -0.0583663   0.047991    -0.088937  
  0.0653844    0.043929     0.0870338   -0.0381352    0.123898    -0.113322   -0.1008      -0.0652735   -0.0294668   -0.0803149    0.0757101     0.221104    -0.121208     0.148542   -0.122298    -0.0950682   -0.0633752   -0.011648    -0.22517     -0.259187   -0.0825449   -0.0345189   -0.165517     -0.0288635   0.0963676   -0.606774  
  0.143968     0.0400879   -0.119668     0.0509191   -0.0738298    0.134671   -0.0822877    0.0650625   -0.0322484    0.0220553    0.0491258    -0.141235    -0.0505185   -0.0579446   0.0221011   -0.0288503   -0.0728454    0.0696101   -0.0185363    0.0631036  -0.0295487    0.0875043    0.14325      -0.0244887  -0.06796      0.0431905 
  0.0953683   -0.064065     0.152976     0.066714    -0.0272738   -0.097175   -0.117602     0.011197    -0.0191922   -0.0412608    0.0577952    -0.0965128   -0.0196804    0.0781732   0.0924886   -0.0962664    0.226616     0.00868301  -0.134736    -0.194093    0.0808579   -0.0837935   -0.18592       0.027026    0.00265768  -0.123288  
 -0.0235015   -0.0920394    0.115843    -0.0498373    0.175708     0.17256     0.00862335  -0.0438428   -0.0937597    0.0638087    0.0260812     0.0902122   -0.0503216   -0.123938    0.104548     0.0350197    0.0496637   -0.0186919   -0.106666     0.121745   -0.0853575   -0.00881217  -0.000779257   0.0840516   0.0745144    0.113915  
  0.115548     0.0112417   -0.0668599   -0.0305308   -0.165663     0.0590632   0.28419      0.0556026    0.204008     0.0356041    0.00345672    0.0266756    0.110228    -0.0661018  -0.0167026    0.0882571   -0.196392     0.108638    -0.0340306    0.0989882  -0.146401     0.0475264    0.10647       0.109068   -0.0159017   -0.150979  
  0.147798     0.158694     0.0412332   -0.020612     0.0586979   -0.0644085  -0.0281217   -0.0646334   -0.0363287   -0.0701579    0.146778      0.0743393   -0.10025      0.0894861  -0.0842316   -0.0586269   -0.20453      0.0722048   -0.128853    -0.0225534  -0.188827    -0.192086    -0.0615229    -0.0592325  -0.0360205   -0.0781606 
 -0.0435741   -0.0949696    0.0721148    0.0425193   -0.00220198   0.0109013   0.0272122    0.0019098   -0.0898251    8.31887e-5   0.000594446  -0.0608369   -0.0886932   -0.0418383  -0.109225    -0.0790103   -0.0117415    0.0466005    0.0169259   -0.136808    0.0175182   -0.0491583    0.00561259   -0.0222355   0.0518533    0.030609  
  0.0305248    0.100969    -0.154778     0.133085    -0.297641     0.0430413  -0.0297415   -0.130476    -0.134788     0.127441     0.0574504    -0.12086      0.0448217    0.0448361  -0.16594      0.0490382   -0.0965126    0.145305    -0.0394656   -0.0841061  -0.064074     0.089269     0.0534812     0.0239324  -0.123745     0.167502  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 16
INFO: iteration 1, average log likelihood -1.081556
WARNING: Variances had to be floored 8 16 22 23 25 26 27
INFO: iteration 2, average log likelihood -1.032928
WARNING: Variances had to be floored 8 12 16 18 19 26
INFO: iteration 3, average log likelihood -1.012982
WARNING: Variances had to be floored 1 7 16 22 23 24 25 27 29 30
INFO: iteration 4, average log likelihood -1.018691
WARNING: Variances had to be floored 3 8 16 19 25 26
INFO: iteration 5, average log likelihood -1.038537
WARNING: Variances had to be floored 12 16 18 22 23 26 27
INFO: iteration 6, average log likelihood -1.026250
WARNING: Variances had to be floored 8 16 24 25 30
INFO: iteration 7, average log likelihood -1.028310
WARNING: Variances had to be floored 7 12 16 19 22 23 25 26 27 29
INFO: iteration 8, average log likelihood -1.003798
WARNING: Variances had to be floored 3 8 16 18
INFO: iteration 9, average log likelihood -1.038616
WARNING: Variances had to be floored 1 8 16 22 23 24 25 26 27 30
INFO: iteration 10, average log likelihood -1.026118
INFO: EM with 100000 data points 10 iterations avll -1.026118
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0499698    -0.0802406    -0.00575349  -0.217656     0.107289     0.302119      0.149043    -0.10434      0.150737     0.0639916    -0.106089    -0.0342888    0.0433794    0.00895862  -0.206722    -0.0154028    -0.0143691    0.0691368    0.00939391   0.0797621   0.230744     0.0980428   -0.00210574   -0.237833     -0.163556      0.0794279
 -0.00192141   -0.015918     -0.0616589    0.024455     0.197042     0.163806      0.0127709    0.0734649   -0.175172     0.110315     -0.0166087    0.0693976   -0.0784188   -0.0564813   -0.0333893    0.102176     -0.100978     0.108204     0.188195    -0.0774317  -0.0336616    0.0651772   -0.0243976    -0.0843608     0.0292877    -0.0707081
 -0.0141906    -0.132177      0.0497106   -0.0168863   -0.185944     0.0335317     0.0581005    0.0586897    0.0385933   -0.0842979     0.07081      0.081717     0.0315101   -0.0397263    0.0693148    0.0117737    -0.0246616   -0.0832537   -0.00774889  -0.020889    0.0952856    0.00729967  -0.214652      0.0448967    -0.0277143    -0.0675229
  0.0367464     0.115587      0.108388     0.121883    -0.0941791   -0.0202775     0.0327907   -0.0519226   -0.0511757   -0.0675729     0.0194568   -0.0310588   -0.0195771   -0.127149    -0.036404     0.115516      0.094608    -0.253216    -0.0376843    0.141145    0.0343382    0.035472    -0.00499944    0.111936     -0.0515525    -0.118059 
  0.154336      0.089871     -0.187252    -0.127161     0.123934    -0.0304692    -0.157454     0.102508     0.126475    -0.152623     -0.193636     0.0507387    0.0396696   -0.148853    -0.0859654    0.0809848     0.0207174   -0.0277935   -0.0833374   -0.118634    0.0190031    0.0140757    0.0176096     0.268489     -0.213213     -0.0910525
  0.107737      0.019038     -0.0495269    0.0385553   -0.00854628   0.173165     -0.0248724   -0.0124499    0.124242    -0.0133418    -0.0806156   -0.108236    -0.0439933   -0.0259019    0.0434823   -0.252736     -0.0418558   -0.0655825    0.0802511   -0.0490092  -0.0286316   -0.0012903   -0.0292227     0.0579476    -0.126202      0.125767 
 -0.00350557   -0.0782881    -0.00661531   0.148844     0.0345038   -0.155558      0.00692676   0.00955829   0.026731     0.0833432     0.125686    -0.141929     0.0667132    0.043296     0.0128464    0.104033     -0.0364672   -0.0840958   -0.0140958   -0.216629   -0.0345239   -0.0487489   -0.0353366    -0.0813009    -0.130017     -0.161631 
 -0.0170049    -0.0204075     0.0575126   -0.0626943   -9.54448e-5  -0.0130848     0.150045     0.0508002    0.0993765    0.000809259  -0.111342     0.146193    -0.141516    -0.0112234    0.00478007  -0.0129582    -0.02584      0.13549      0.143002     0.0699512   0.108111     0.050089     0.128522      0.0237247     0.156973      0.136288 
  0.137177      0.137663     -0.0397574    0.0716481    0.0921008    0.079883     -0.103141    -0.0342635   -0.209584     0.0507881     0.0503739    0.0907203    0.0327515   -0.0285561    0.103336     0.110069      0.164233    -0.0374404    0.044825    -0.109233   -0.0880973   -0.129893     0.141403      0.0341865    -0.0829165     0.0242681
 -0.0252479    -0.000423455  -0.0491559   -0.0913135   -0.0245272   -0.0475822    -0.185271     0.102203    -0.133938    -0.0281881    -0.0323181   -0.0911743   -0.164416     0.0434556    0.339553    -0.000441605  -0.018553     0.0855458    0.00685117  -0.141823    0.00877713  -0.048833     0.0892563    -0.00772805    0.0435792     0.0703153
 -0.196032     -0.0298693    -0.13765     -0.165785    -0.139367     0.0137052     0.0401692   -0.128829     0.114816     0.101919     -0.0372031    0.0874627   -0.129891     0.0126041   -0.0975385   -0.112458     -0.174288     0.144963     0.119679    -0.137465   -0.270543     0.0541416   -0.0641925    -0.257613     -0.13524      -0.196105 
 -0.0308695     0.0666214     0.0241028   -0.0430005   -0.0471679    0.130913     -0.176026     0.101221     0.0665268   -0.106947      0.176038     0.0884493   -0.150972    -0.0670001   -0.0509057    0.128889      0.0251267   -0.0857608   -0.0157806   -0.163416   -0.0771197   -0.0251863    0.0447276    -0.0250616    -0.0968081    -0.090237 
  0.113125      0.0755065    -0.0571626    0.158171     0.0261127   -0.016158      0.0853179    0.105035    -0.119982     0.043395      0.0211636    0.0307664    0.035346     0.065769     0.0427982   -0.00826867    0.0105537   -0.142274    -0.04144      0.166244    0.105673     0.178532    -0.133631     -0.127091      0.128004     -0.0535948
 -0.0131093    -0.0884959    -0.225547     0.193071     0.115206     0.11817       0.19378      0.0220711    0.0306767   -0.0905998    -0.0510766    0.00927998   0.040763    -0.086249     0.0678129   -0.0238314    -0.0432016   -0.162688     0.0194672   -0.0473274   0.0270213    0.141954    -0.0949738    -0.0132635    -0.0268586    -0.111003 
  0.247615      0.0238645    -0.199643     0.0449595    0.0206121   -0.000335037   0.119284    -0.0517172    0.0300849    0.134035      0.00932509  -0.178575     0.0836229   -0.191428    -0.00443285   0.0727261     0.0556958   -0.108804    -0.0860658   -0.16253     0.0324888   -0.113547     0.0403722     0.000899194   0.164295     -0.113835 
 -0.0346103     0.00279165   -0.0676845    0.126593    -0.152318    -0.0749674     0.209026    -0.0842542   -0.0799473    0.089164     -0.118454    -0.104212     0.0182498    0.035117    -0.0366072   -0.0487513    -0.0599162   -0.132638    -0.0245658   -0.0491796  -0.247472     0.0236261   -0.0231003    -0.302262     -0.128888     -0.0296259
  0.0632758     0.121406      0.00336672   0.108358    -0.0345339    0.0045959    -0.091757    -0.107008    -0.0541958    0.121825     -0.0641887   -0.108316    -0.0889897   -0.0168228    0.0810555   -0.0142791    -0.106906    -0.181017    -0.0462847   -0.0742981   0.0440728    0.0331296   -0.0456015    -0.0746673    -0.0266792    -0.012675 
 -0.0643508     0.060417     -0.00930512  -0.0161888   -0.119327    -0.344504      0.035836    -0.0894818   -0.0239061   -0.0561364     0.113731     0.0610008    0.139421    -0.0268185   -0.109595    -0.103311     -0.00392309  -0.042592    -0.00513176  -0.0934748  -0.128141    -0.0973858    0.227532     -0.14694       0.000649274  -0.133425 
 -0.0539833    -0.150989      0.0745738   -0.137003     0.115396     0.130527     -0.0414879   -0.0530345    0.0427031   -0.138774     -0.0706962    0.070596    -0.12065     -0.132222     0.0692325   -0.165446      0.0473379    0.0182585   -0.0110254   -0.0588745  -0.0240053    0.10565      0.0162153     0.00877183    0.054367     -0.0708571
 -0.0320338     0.0636889    -0.185183    -0.0514987    0.0625094    0.0124162     0.025794     0.00993694  -0.00462837   0.0432531    -0.0548138    0.0369029   -0.0243386    0.059397     0.03386     -0.00744919   -0.171482     0.0780015    0.0472908   -0.0603384   0.0783388    0.104697     0.000279663   0.0665798     0.133903     -0.13539  
 -0.130445     -0.0394264    -0.0588244   -0.133908     0.107707     0.0672191     0.109415     0.0269431   -0.0496372   -0.0723071     0.044727     0.0149314    0.12707      0.064529     0.0980283    0.055445      0.149345    -0.0188246   -0.0701174   -0.102218   -0.0296483   -0.142001    -0.201644     -0.0551566     0.0483852     0.0212127
 -0.0701171     0.0175001     0.0135809    0.138402     0.0429915   -0.0134096     0.0672032    0.0395552   -0.0197046   -0.0697409     0.12358     -0.067579    -0.131984     0.0686552   -0.0183157    0.122164      0.100896     0.0217285    0.0262927    0.0278193   0.0697822   -0.177296     0.0264048     0.0457384     0.0582606     0.0788359
 -0.00172423   -0.155067      0.0841348    0.103587    -0.069962     0.119011     -0.0487359    0.183031    -0.0446509    0.0752654     0.177295    -0.0790504    0.102545     0.116033     0.133376    -0.00215167   -0.0334378   -0.107339     0.0997475   -0.133484   -0.0627585    0.267027    -0.0475671    -0.0802689     0.107083      0.149337 
  0.0106022    -0.155757     -0.26272      0.0730748    0.193459     0.0705959     0.111611    -0.107189    -0.0025841    0.0392563     0.00897383   0.132975     0.202994    -0.0974242    0.0361576   -0.100946     -0.00913853  -0.0252382   -0.0470584   -0.0634995  -0.0652036    0.0774984   -0.0920781     0.00259204   -0.0978485    -0.0650952
 -0.0960829    -0.0180045    -0.021202     0.139132    -0.12918      0.202367      0.00742827  -0.0340251    0.0133667   -0.0695059     0.0979667   -0.130392     0.00524517  -0.0132342    0.0565996   -0.189665      0.109508    -0.0221132   -0.168825     0.0373611  -0.131665     0.0202193    0.0180678     0.0363142    -0.0790197    -0.0461931
  0.0771811    -0.176355      0.0298933    0.0261947    0.0455038    0.0775938     0.110812    -0.141565    -0.243077     0.0146785    -0.167706     0.0923263    0.0971106    0.025864    -0.0982907    0.0150554    -0.0539681    0.0501884    0.207125     0.121518   -0.0180238    0.250193    -0.122426     -0.0834915    -0.177454      0.0435558
 -0.258901      0.110339      0.0198077    0.00530989  -0.10226     -0.165081      0.00693467  -0.029968     0.110288     0.0841014     0.0481338    0.12515     -0.0676757   -0.202319     0.153606     0.0766322    -0.075485     0.0129243    0.161984     0.126207    0.0090073    0.0836531    0.0616043     0.106574     -0.112973     -0.0788803
  0.0178401    -0.135628      0.0114083    0.0882159    0.275771     0.120716      0.0655205    0.109212     0.028283     0.140562      0.00638573  -0.0683909   -0.154161     0.00892705   0.096795     0.205485     -0.0450911   -0.113014     0.0619581    0.0202193   0.0312751   -0.0217917    0.0189675     0.184176     -0.015794     -0.106879 
 -0.0769299    -0.0363871    -0.128121    -0.144184    -0.170672     0.161853     -0.0819166    0.0649182   -0.221988     0.0978792     0.142721    -0.0352048    0.0691185   -0.0430201    0.00343063  -0.0268034     0.212635     0.00943489   0.0223024   -0.166681    0.096271    -0.0761275    0.101317     -0.0338994     0.0328642     0.0410644
  0.0591929    -0.0703253    -0.0395695   -0.0456967    0.11847      0.0349192     0.0594819    0.0979306    0.144327    -0.128872      0.192697     0.0140894    0.0390181    0.0986242    0.0842146    0.0540251    -0.128122    -0.253777    -0.0447334   -0.171106   -0.0611566    0.0966027    0.00454272    0.0402172     0.00194046   -0.0847701
 -0.0192042    -0.0580948     0.0422812   -0.249385     0.0746949    0.053274      0.00408171  -0.0795053    0.131646     0.134151      0.00961473  -0.0232247    0.00376236  -0.0668028    0.0230326   -0.0563082    -0.0218423    0.0969727    0.116799     0.0534454   0.0708267    0.0714921    0.064493     -0.081927      0.0973843     0.0783242
  0.000558517   0.00656983   -0.0489064   -0.0866056    0.185741     0.0152464     0.0454577    0.154501    -0.0700825    0.0763727    -0.109881    -0.0187205    0.0790369    0.0633743    0.153201     0.0290732     0.0063175    0.136055     0.0604962    0.105452   -0.0402089   -0.127879     0.0768788    -0.104389      0.0860296     0.0130517kind full, method split
0: avll = -1.418829814449335
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.418849
INFO: iteration 2, average log likelihood -1.418790
INFO: iteration 3, average log likelihood -1.418747
INFO: iteration 4, average log likelihood -1.418698
INFO: iteration 5, average log likelihood -1.418639
INFO: iteration 6, average log likelihood -1.418565
INFO: iteration 7, average log likelihood -1.418459
INFO: iteration 8, average log likelihood -1.418274
INFO: iteration 9, average log likelihood -1.417897
INFO: iteration 10, average log likelihood -1.417142
INFO: iteration 11, average log likelihood -1.415936
INFO: iteration 12, average log likelihood -1.414652
INFO: iteration 13, average log likelihood -1.413803
INFO: iteration 14, average log likelihood -1.413420
INFO: iteration 15, average log likelihood -1.413276
INFO: iteration 16, average log likelihood -1.413224
INFO: iteration 17, average log likelihood -1.413204
INFO: iteration 18, average log likelihood -1.413197
INFO: iteration 19, average log likelihood -1.413194
INFO: iteration 20, average log likelihood -1.413193
INFO: iteration 21, average log likelihood -1.413192
INFO: iteration 22, average log likelihood -1.413192
INFO: iteration 23, average log likelihood -1.413192
INFO: iteration 24, average log likelihood -1.413192
INFO: iteration 25, average log likelihood -1.413192
INFO: iteration 26, average log likelihood -1.413192
INFO: iteration 27, average log likelihood -1.413192
INFO: iteration 28, average log likelihood -1.413192
INFO: iteration 29, average log likelihood -1.413191
INFO: iteration 30, average log likelihood -1.413191
INFO: iteration 31, average log likelihood -1.413191
INFO: iteration 32, average log likelihood -1.413191
INFO: iteration 33, average log likelihood -1.413191
INFO: iteration 34, average log likelihood -1.413191
INFO: iteration 35, average log likelihood -1.413191
INFO: iteration 36, average log likelihood -1.413191
INFO: iteration 37, average log likelihood -1.413191
INFO: iteration 38, average log likelihood -1.413191
INFO: iteration 39, average log likelihood -1.413191
INFO: iteration 40, average log likelihood -1.413191
INFO: iteration 41, average log likelihood -1.413191
INFO: iteration 42, average log likelihood -1.413191
INFO: iteration 43, average log likelihood -1.413191
INFO: iteration 44, average log likelihood -1.413191
INFO: iteration 45, average log likelihood -1.413191
INFO: iteration 46, average log likelihood -1.413191
INFO: iteration 47, average log likelihood -1.413191
INFO: iteration 48, average log likelihood -1.413191
INFO: iteration 49, average log likelihood -1.413191
INFO: iteration 50, average log likelihood -1.413191
INFO: EM with 100000 data points 50 iterations avll -1.413191
952.4 data points per parameter
1: avll = [-1.41885,-1.41879,-1.41875,-1.4187,-1.41864,-1.41856,-1.41846,-1.41827,-1.4179,-1.41714,-1.41594,-1.41465,-1.4138,-1.41342,-1.41328,-1.41322,-1.4132,-1.4132,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.413206
INFO: iteration 2, average log likelihood -1.413150
INFO: iteration 3, average log likelihood -1.413106
INFO: iteration 4, average log likelihood -1.413054
INFO: iteration 5, average log likelihood -1.412993
INFO: iteration 6, average log likelihood -1.412922
INFO: iteration 7, average log likelihood -1.412845
INFO: iteration 8, average log likelihood -1.412765
INFO: iteration 9, average log likelihood -1.412688
INFO: iteration 10, average log likelihood -1.412619
INFO: iteration 11, average log likelihood -1.412558
INFO: iteration 12, average log likelihood -1.412508
INFO: iteration 13, average log likelihood -1.412467
INFO: iteration 14, average log likelihood -1.412435
INFO: iteration 15, average log likelihood -1.412411
INFO: iteration 16, average log likelihood -1.412392
INFO: iteration 17, average log likelihood -1.412377
INFO: iteration 18, average log likelihood -1.412365
INFO: iteration 19, average log likelihood -1.412355
INFO: iteration 20, average log likelihood -1.412347
INFO: iteration 21, average log likelihood -1.412339
INFO: iteration 22, average log likelihood -1.412331
INFO: iteration 23, average log likelihood -1.412324
INFO: iteration 24, average log likelihood -1.412317
INFO: iteration 25, average log likelihood -1.412310
INFO: iteration 26, average log likelihood -1.412304
INFO: iteration 27, average log likelihood -1.412298
INFO: iteration 28, average log likelihood -1.412292
INFO: iteration 29, average log likelihood -1.412286
INFO: iteration 30, average log likelihood -1.412280
INFO: iteration 31, average log likelihood -1.412275
INFO: iteration 32, average log likelihood -1.412270
INFO: iteration 33, average log likelihood -1.412265
INFO: iteration 34, average log likelihood -1.412260
INFO: iteration 35, average log likelihood -1.412255
INFO: iteration 36, average log likelihood -1.412251
INFO: iteration 37, average log likelihood -1.412246
INFO: iteration 38, average log likelihood -1.412242
INFO: iteration 39, average log likelihood -1.412239
INFO: iteration 40, average log likelihood -1.412235
INFO: iteration 41, average log likelihood -1.412232
INFO: iteration 42, average log likelihood -1.412228
INFO: iteration 43, average log likelihood -1.412225
INFO: iteration 44, average log likelihood -1.412222
INFO: iteration 45, average log likelihood -1.412219
INFO: iteration 46, average log likelihood -1.412217
INFO: iteration 47, average log likelihood -1.412214
INFO: iteration 48, average log likelihood -1.412212
INFO: iteration 49, average log likelihood -1.412209
INFO: iteration 50, average log likelihood -1.412207
INFO: EM with 100000 data points 50 iterations avll -1.412207
473.9 data points per parameter
2: avll = [-1.41321,-1.41315,-1.41311,-1.41305,-1.41299,-1.41292,-1.41284,-1.41276,-1.41269,-1.41262,-1.41256,-1.41251,-1.41247,-1.41244,-1.41241,-1.41239,-1.41238,-1.41237,-1.41236,-1.41235,-1.41234,-1.41233,-1.41232,-1.41232,-1.41231,-1.4123,-1.4123,-1.41229,-1.41229,-1.41228,-1.41227,-1.41227,-1.41226,-1.41226,-1.41226,-1.41225,-1.41225,-1.41224,-1.41224,-1.41223,-1.41223,-1.41223,-1.41223,-1.41222,-1.41222,-1.41222,-1.41221,-1.41221,-1.41221,-1.41221]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.412218
INFO: iteration 2, average log likelihood -1.412165
INFO: iteration 3, average log likelihood -1.412123
INFO: iteration 4, average log likelihood -1.412074
INFO: iteration 5, average log likelihood -1.412016
INFO: iteration 6, average log likelihood -1.411948
INFO: iteration 7, average log likelihood -1.411871
INFO: iteration 8, average log likelihood -1.411788
INFO: iteration 9, average log likelihood -1.411703
INFO: iteration 10, average log likelihood -1.411619
INFO: iteration 11, average log likelihood -1.411536
INFO: iteration 12, average log likelihood -1.411457
INFO: iteration 13, average log likelihood -1.411381
INFO: iteration 14, average log likelihood -1.411311
INFO: iteration 15, average log likelihood -1.411245
INFO: iteration 16, average log likelihood -1.411184
INFO: iteration 17, average log likelihood -1.411127
INFO: iteration 18, average log likelihood -1.411072
INFO: iteration 19, average log likelihood -1.411020
INFO: iteration 20, average log likelihood -1.410971
INFO: iteration 21, average log likelihood -1.410924
INFO: iteration 22, average log likelihood -1.410881
INFO: iteration 23, average log likelihood -1.410842
INFO: iteration 24, average log likelihood -1.410808
INFO: iteration 25, average log likelihood -1.410778
INFO: iteration 26, average log likelihood -1.410754
INFO: iteration 27, average log likelihood -1.410733
INFO: iteration 28, average log likelihood -1.410715
INFO: iteration 29, average log likelihood -1.410700
INFO: iteration 30, average log likelihood -1.410688
INFO: iteration 31, average log likelihood -1.410677
INFO: iteration 32, average log likelihood -1.410667
INFO: iteration 33, average log likelihood -1.410658
INFO: iteration 34, average log likelihood -1.410649
INFO: iteration 35, average log likelihood -1.410641
INFO: iteration 36, average log likelihood -1.410634
INFO: iteration 37, average log likelihood -1.410626
INFO: iteration 38, average log likelihood -1.410619
INFO: iteration 39, average log likelihood -1.410612
INFO: iteration 40, average log likelihood -1.410604
INFO: iteration 41, average log likelihood -1.410597
INFO: iteration 42, average log likelihood -1.410590
INFO: iteration 43, average log likelihood -1.410582
INFO: iteration 44, average log likelihood -1.410574
INFO: iteration 45, average log likelihood -1.410566
INFO: iteration 46, average log likelihood -1.410559
INFO: iteration 47, average log likelihood -1.410550
INFO: iteration 48, average log likelihood -1.410542
INFO: iteration 49, average log likelihood -1.410534
INFO: iteration 50, average log likelihood -1.410525
INFO: EM with 100000 data points 50 iterations avll -1.410525
236.4 data points per parameter
3: avll = [-1.41222,-1.41217,-1.41212,-1.41207,-1.41202,-1.41195,-1.41187,-1.41179,-1.4117,-1.41162,-1.41154,-1.41146,-1.41138,-1.41131,-1.41125,-1.41118,-1.41113,-1.41107,-1.41102,-1.41097,-1.41092,-1.41088,-1.41084,-1.41081,-1.41078,-1.41075,-1.41073,-1.41072,-1.4107,-1.41069,-1.41068,-1.41067,-1.41066,-1.41065,-1.41064,-1.41063,-1.41063,-1.41062,-1.41061,-1.4106,-1.4106,-1.41059,-1.41058,-1.41057,-1.41057,-1.41056,-1.41055,-1.41054,-1.41053,-1.41053]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.410527
INFO: iteration 2, average log likelihood -1.410472
INFO: iteration 3, average log likelihood -1.410422
INFO: iteration 4, average log likelihood -1.410365
INFO: iteration 5, average log likelihood -1.410296
INFO: iteration 6, average log likelihood -1.410212
INFO: iteration 7, average log likelihood -1.410114
INFO: iteration 8, average log likelihood -1.410002
INFO: iteration 9, average log likelihood -1.409880
INFO: iteration 10, average log likelihood -1.409753
INFO: iteration 11, average log likelihood -1.409626
INFO: iteration 12, average log likelihood -1.409503
INFO: iteration 13, average log likelihood -1.409387
INFO: iteration 14, average log likelihood -1.409280
INFO: iteration 15, average log likelihood -1.409185
INFO: iteration 16, average log likelihood -1.409101
INFO: iteration 17, average log likelihood -1.409028
INFO: iteration 18, average log likelihood -1.408965
INFO: iteration 19, average log likelihood -1.408910
INFO: iteration 20, average log likelihood -1.408861
INFO: iteration 21, average log likelihood -1.408818
INFO: iteration 22, average log likelihood -1.408778
INFO: iteration 23, average log likelihood -1.408742
INFO: iteration 24, average log likelihood -1.408708
INFO: iteration 25, average log likelihood -1.408677
INFO: iteration 26, average log likelihood -1.408647
INFO: iteration 27, average log likelihood -1.408618
INFO: iteration 28, average log likelihood -1.408591
INFO: iteration 29, average log likelihood -1.408565
INFO: iteration 30, average log likelihood -1.408540
INFO: iteration 31, average log likelihood -1.408516
INFO: iteration 32, average log likelihood -1.408494
INFO: iteration 33, average log likelihood -1.408472
INFO: iteration 34, average log likelihood -1.408451
INFO: iteration 35, average log likelihood -1.408431
INFO: iteration 36, average log likelihood -1.408412
INFO: iteration 37, average log likelihood -1.408394
INFO: iteration 38, average log likelihood -1.408376
INFO: iteration 39, average log likelihood -1.408359
INFO: iteration 40, average log likelihood -1.408343
INFO: iteration 41, average log likelihood -1.408328
INFO: iteration 42, average log likelihood -1.408313
INFO: iteration 43, average log likelihood -1.408299
INFO: iteration 44, average log likelihood -1.408286
INFO: iteration 45, average log likelihood -1.408273
INFO: iteration 46, average log likelihood -1.408261
INFO: iteration 47, average log likelihood -1.408250
INFO: iteration 48, average log likelihood -1.408239
INFO: iteration 49, average log likelihood -1.408228
INFO: iteration 50, average log likelihood -1.408218
INFO: EM with 100000 data points 50 iterations avll -1.408218
118.1 data points per parameter
4: avll = [-1.41053,-1.41047,-1.41042,-1.41036,-1.4103,-1.41021,-1.41011,-1.41,-1.40988,-1.40975,-1.40963,-1.4095,-1.40939,-1.40928,-1.40918,-1.4091,-1.40903,-1.40896,-1.40891,-1.40886,-1.40882,-1.40878,-1.40874,-1.40871,-1.40868,-1.40865,-1.40862,-1.40859,-1.40856,-1.40854,-1.40852,-1.40849,-1.40847,-1.40845,-1.40843,-1.40841,-1.40839,-1.40838,-1.40836,-1.40834,-1.40833,-1.40831,-1.4083,-1.40829,-1.40827,-1.40826,-1.40825,-1.40824,-1.40823,-1.40822]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.408219
INFO: iteration 2, average log likelihood -1.408153
INFO: iteration 3, average log likelihood -1.408090
INFO: iteration 4, average log likelihood -1.408013
INFO: iteration 5, average log likelihood -1.407915
INFO: iteration 6, average log likelihood -1.407790
INFO: iteration 7, average log likelihood -1.407639
INFO: iteration 8, average log likelihood -1.407472
INFO: iteration 9, average log likelihood -1.407300
INFO: iteration 10, average log likelihood -1.407133
INFO: iteration 11, average log likelihood -1.406978
INFO: iteration 12, average log likelihood -1.406838
INFO: iteration 13, average log likelihood -1.406714
INFO: iteration 14, average log likelihood -1.406605
INFO: iteration 15, average log likelihood -1.406510
INFO: iteration 16, average log likelihood -1.406427
INFO: iteration 17, average log likelihood -1.406354
INFO: iteration 18, average log likelihood -1.406289
INFO: iteration 19, average log likelihood -1.406232
INFO: iteration 20, average log likelihood -1.406180
INFO: iteration 21, average log likelihood -1.406132
INFO: iteration 22, average log likelihood -1.406089
INFO: iteration 23, average log likelihood -1.406048
INFO: iteration 24, average log likelihood -1.406010
INFO: iteration 25, average log likelihood -1.405975
INFO: iteration 26, average log likelihood -1.405942
INFO: iteration 27, average log likelihood -1.405910
INFO: iteration 28, average log likelihood -1.405881
INFO: iteration 29, average log likelihood -1.405853
INFO: iteration 30, average log likelihood -1.405826
INFO: iteration 31, average log likelihood -1.405801
INFO: iteration 32, average log likelihood -1.405776
INFO: iteration 33, average log likelihood -1.405753
INFO: iteration 34, average log likelihood -1.405730
INFO: iteration 35, average log likelihood -1.405709
INFO: iteration 36, average log likelihood -1.405688
INFO: iteration 37, average log likelihood -1.405669
INFO: iteration 38, average log likelihood -1.405649
INFO: iteration 39, average log likelihood -1.405631
INFO: iteration 40, average log likelihood -1.405613
INFO: iteration 41, average log likelihood -1.405596
INFO: iteration 42, average log likelihood -1.405580
INFO: iteration 43, average log likelihood -1.405564
INFO: iteration 44, average log likelihood -1.405548
INFO: iteration 45, average log likelihood -1.405533
INFO: iteration 46, average log likelihood -1.405519
INFO: iteration 47, average log likelihood -1.405505
INFO: iteration 48, average log likelihood -1.405492
INFO: iteration 49, average log likelihood -1.405479
INFO: iteration 50, average log likelihood -1.405466
INFO: EM with 100000 data points 50 iterations avll -1.405466
59.0 data points per parameter
5: avll = [-1.40822,-1.40815,-1.40809,-1.40801,-1.40791,-1.40779,-1.40764,-1.40747,-1.4073,-1.40713,-1.40698,-1.40684,-1.40671,-1.4066,-1.40651,-1.40643,-1.40635,-1.40629,-1.40623,-1.40618,-1.40613,-1.40609,-1.40605,-1.40601,-1.40597,-1.40594,-1.40591,-1.40588,-1.40585,-1.40583,-1.4058,-1.40578,-1.40575,-1.40573,-1.40571,-1.40569,-1.40567,-1.40565,-1.40563,-1.40561,-1.4056,-1.40558,-1.40556,-1.40555,-1.40553,-1.40552,-1.40551,-1.40549,-1.40548,-1.40547]
[-1.41883,-1.41885,-1.41879,-1.41875,-1.4187,-1.41864,-1.41856,-1.41846,-1.41827,-1.4179,-1.41714,-1.41594,-1.41465,-1.4138,-1.41342,-1.41328,-1.41322,-1.4132,-1.4132,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41319,-1.41321,-1.41315,-1.41311,-1.41305,-1.41299,-1.41292,-1.41284,-1.41276,-1.41269,-1.41262,-1.41256,-1.41251,-1.41247,-1.41244,-1.41241,-1.41239,-1.41238,-1.41237,-1.41236,-1.41235,-1.41234,-1.41233,-1.41232,-1.41232,-1.41231,-1.4123,-1.4123,-1.41229,-1.41229,-1.41228,-1.41227,-1.41227,-1.41226,-1.41226,-1.41226,-1.41225,-1.41225,-1.41224,-1.41224,-1.41223,-1.41223,-1.41223,-1.41223,-1.41222,-1.41222,-1.41222,-1.41221,-1.41221,-1.41221,-1.41221,-1.41222,-1.41217,-1.41212,-1.41207,-1.41202,-1.41195,-1.41187,-1.41179,-1.4117,-1.41162,-1.41154,-1.41146,-1.41138,-1.41131,-1.41125,-1.41118,-1.41113,-1.41107,-1.41102,-1.41097,-1.41092,-1.41088,-1.41084,-1.41081,-1.41078,-1.41075,-1.41073,-1.41072,-1.4107,-1.41069,-1.41068,-1.41067,-1.41066,-1.41065,-1.41064,-1.41063,-1.41063,-1.41062,-1.41061,-1.4106,-1.4106,-1.41059,-1.41058,-1.41057,-1.41057,-1.41056,-1.41055,-1.41054,-1.41053,-1.41053,-1.41053,-1.41047,-1.41042,-1.41036,-1.4103,-1.41021,-1.41011,-1.41,-1.40988,-1.40975,-1.40963,-1.4095,-1.40939,-1.40928,-1.40918,-1.4091,-1.40903,-1.40896,-1.40891,-1.40886,-1.40882,-1.40878,-1.40874,-1.40871,-1.40868,-1.40865,-1.40862,-1.40859,-1.40856,-1.40854,-1.40852,-1.40849,-1.40847,-1.40845,-1.40843,-1.40841,-1.40839,-1.40838,-1.40836,-1.40834,-1.40833,-1.40831,-1.4083,-1.40829,-1.40827,-1.40826,-1.40825,-1.40824,-1.40823,-1.40822,-1.40822,-1.40815,-1.40809,-1.40801,-1.40791,-1.40779,-1.40764,-1.40747,-1.4073,-1.40713,-1.40698,-1.40684,-1.40671,-1.4066,-1.40651,-1.40643,-1.40635,-1.40629,-1.40623,-1.40618,-1.40613,-1.40609,-1.40605,-1.40601,-1.40597,-1.40594,-1.40591,-1.40588,-1.40585,-1.40583,-1.4058,-1.40578,-1.40575,-1.40573,-1.40571,-1.40569,-1.40567,-1.40565,-1.40563,-1.40561,-1.4056,-1.40558,-1.40556,-1.40555,-1.40553,-1.40552,-1.40551,-1.40549,-1.40548,-1.40547]
32×26 Array{Float64,2}:
  0.00828816  -0.257037   -0.0844032   -0.356351   -0.328851   -0.407246   -0.726026    -0.0252396   -0.1998      -0.00251077   0.580922    -0.300926   -0.246244    0.277963    0.102232     0.0287228   0.276501     0.326932    0.0567537   0.193969     0.307194    0.497564     0.610421     0.0052124   -0.218861    -0.00386955
 -0.0303405    0.708958   -0.331235    -0.542608    0.0225709   0.135014   -0.305904     0.00234853  -0.22652     -0.306689     0.271784     0.173079   -0.209817    0.18707    -0.195202    -0.260215    0.527295    -0.0194804  -0.327361   -0.277486    -0.207375   -0.0541085    0.624829     0.10492     -0.424551    -0.307589  
 -0.285969     0.581114    0.104971     0.0258213  -0.368786   -0.224178    0.196914    -0.047061     0.372947     0.115449     0.416258    -0.2144     -0.0285379  -0.389115   -0.349651     0.228704   -0.0173955    0.247869    0.121764   -0.486946    -0.282947   -0.159508     0.495371    -0.180496     0.0441641    0.0970781 
 -0.396581     0.365237    0.224033    -0.190153    0.0860206  -0.357424    0.504412    -0.226615     0.290384    -0.27015     -0.0214019   -0.0706097  -0.260524    0.24874     0.654491    -0.0897198  -0.531745    -0.0768053  -0.192446   -0.295094    -0.159944    0.528171     0.719969    -0.100904    -0.0246412   -0.243822  
 -0.172273    -0.303053   -0.213158     0.233494   -0.970588   -0.176803   -0.161705    -0.737788     0.0198624   -0.253761    -0.165642     0.475661   -0.815082   -0.234669   -0.480603    -0.0401719  -0.139575     0.23568     0.305157   -0.77184     -0.44439     0.309361    -0.43834      0.239426     0.415421    -0.602963  
  0.260276     0.0781621  -0.46881      0.310703   -0.963295    0.48854     0.140606     0.245469     0.156707     0.190145    -0.462812    -0.126931   -0.796264    0.0504359  -0.355404    -0.683438    0.182961     0.144826    0.529811    0.0326957    0.0738594   0.0458272    0.222899     0.0184165    0.293928    -0.452035  
 -0.227782     0.275428   -0.241885    -0.192888    0.519494   -0.229465    0.352475    -0.345505     0.606407    -0.025905     0.160242     0.537661   -0.382663   -0.364567   -0.2295      -0.182229    0.163726     0.0997002   0.511112    0.204668     0.179854   -0.0545724    0.0440702    0.0935244    0.509304     0.512822  
 -0.418099     0.260568    0.105431     0.731303    0.264926   -1.05792    -0.311854    -0.181713     0.0266182   -0.184664    -0.108406     0.107102   -0.304944   -0.118456   -0.196058     0.659543    0.451706     0.290825    0.357977    0.860267     0.160591   -0.192207     0.0424538   -0.199394     0.540939     0.425349  
 -0.210893    -0.258277    0.0194873   -0.0259175   0.346106   -0.100806    0.560922    -0.0463112   -0.416966     0.329755    -0.147167    -0.437042    0.43306     0.372636   -0.334763    -0.19113    -0.137787     0.280501    0.0971784  -0.177358    -0.139394    0.0559472   -0.230918    -0.271281    -0.144452     0.505454  
  0.201928    -0.0961653  -0.218836    -0.0226727   0.256676    0.182211   -0.233777     0.62703     -0.0189376    0.46963      0.350654     0.0868037   0.203103    0.121173   -0.549179    -0.0183318   0.414816    -0.118492   -0.0797108   0.138396    -0.285104   -0.47954     -0.30439      0.011011    -0.0599832    0.542062  
 -0.0436409    0.0293272  -0.022883    -0.352338    0.0133454   0.148585    0.00879094   0.124741    -0.111679    -0.0692336    0.0160505    0.0998687   0.0232289   0.152549    0.234962     0.0169972   0.00637325  -0.202737   -0.0442306  -0.125089     0.0215354   0.0893123    0.0218258   -0.0479419   -0.138606    -0.170768  
  0.0360846   -0.0970759  -0.0113239    0.332283   -0.0560065  -0.0476435  -0.0391829   -0.0860668    0.102053    -0.0254135   -0.051995    -0.0154037  -0.0368373  -0.17505     0.00347951   0.0333199   0.00292257   0.0870242   0.104636    0.0770009    0.0097769  -0.00952263  -0.0597529    0.0392626    0.0776961    0.010785  
  0.593326    -0.387896   -0.00552277   0.634744   -0.168098   -0.0433663  -0.464186    -0.315059    -0.607251    -0.0485294   -0.441636    -0.16628    -0.222492    0.397746    0.389919    -0.080965    0.102469    -0.029229   -0.415817   -0.0664951   -0.636204   -0.213797    -0.244421     0.010858    -0.51458     -0.254639  
 -0.100727    -0.167184    0.159893     0.198045   -0.920044    0.339237    0.502511     0.944371    -0.978094    -0.561093    -0.407648    -0.282634   -0.075981   -0.027091    0.450048     0.385621   -0.260273    -0.0342492  -0.127217    0.00649114   0.152385    0.219038     0.3034       0.247138    -0.237214    -0.514306  
  0.31757     -0.293211   -0.0955025    0.276899   -0.0668931   0.327099   -0.315015    -0.26444      0.323254     0.0761671   -0.0414095   -0.311618    0.561774   -0.159258   -0.170234     0.14163    -0.559375     0.0391565  -0.124524   -0.178626     0.253329    0.16747      0.00228844  -0.224918    -0.454279    -0.689739  
  0.156962    -0.0463574   0.426873     0.288504   -0.20924    -0.335632    0.0364254    0.0366483    0.664069     0.0800626   -0.534983    -0.0654431  -0.0153424  -0.464614    0.75723      0.281059   -0.0940895   -0.0663757  -0.121813    0.73296      0.172108    0.0877426   -0.238586    -0.142038    -0.204496    -0.51671   
  0.571838     0.116796   -0.532476    -0.550461    0.555708   -0.425306   -0.858119    -0.579738     0.455085     0.249045     0.030927     0.709996    0.275075   -1.08081    -0.006049     0.520047    0.284793    -0.478657   -0.217305    0.164806    -0.223052    0.263398     0.0978794    0.952621    -0.36384     -0.408595  
  0.299522     0.184166   -0.664703    -0.516051    0.962342    0.517461   -0.484333    -0.14414     -0.526779    -0.363179     0.115996    -0.0618573  -0.233946   -0.447076    0.00228493   0.419145   -0.328086    -0.839815    0.0597371   0.335839     0.484508   -0.257447    -0.033456     0.549632     0.0812041   -0.401397  
 -0.0196742   -0.283383    0.0329963   -0.14196     0.24682    -0.109282    0.175556     0.203677    -0.155136     0.17059      0.00805588   0.667139   -0.459271    0.199746    0.482653     0.0705551   0.277411    -0.884655    0.0646943   0.475131     0.193866    0.412094    -0.0552759    0.299206     0.544292     0.136458  
  1.02309      0.206503   -0.103351    -0.429695   -0.30319     0.413575    0.289346    -0.242878     0.0685747   -0.0846058    0.104306     0.751919   -0.970924   -0.270126    0.23552      0.335443    0.713248     0.0278318  -0.277751   -0.447337     0.408465    0.893526     0.336011    -0.123796     0.587688    -0.104821  
 -0.152982    -0.0835911  -0.125876    -0.164107    0.109772    0.134009   -0.00407652   0.128155    -0.113544     0.0270419   -0.0346975    0.130752   -0.111903    0.0369495   0.131291    -0.147153    0.140322    -0.257198    0.0255648   0.0172358    0.0760854   0.0153218    0.0247297   -0.00971153  -0.0744571   -0.266252  
  0.250753     0.159391    0.125882     0.479613   -0.0448045  -0.144868    0.220809    -0.258886     0.16966      0.207203     0.232582    -0.501183    0.274485    0.0154394  -0.192281    -0.0834557  -0.425072     0.496673   -0.129475   -0.396217    -0.196437    0.0996099    0.088147    -0.125414    -0.177534     0.43713   
 -0.692371    -0.1114     -0.433165     0.0546034  -0.101474    0.0412797   0.0865622    0.539774    -0.15182     -0.404338    -0.733081     0.183054    0.242416    0.235295   -0.286887    -0.451554   -0.168903    -0.0631711  -0.111623    0.125175     0.285938    0.0944133    0.277965    -0.651736    -0.00916869  -0.466322  
 -0.183768     0.397786   -0.267513    -0.110404    0.0259363   0.249336    0.440594     0.546692    -0.403534     0.00641469  -0.599493     0.456689   -0.397199    0.343822   -0.109368     0.35832     0.318901    -0.031838   -0.105758   -0.107866    -0.472837   -0.384654    -0.267191    -0.540761     0.648153     0.356489  
  0.232712    -0.167139    0.122689    -0.0793809   0.887598   -0.290795   -0.402065    -0.557813    -0.613152    -0.252121     0.297252    -0.0961454   0.665687   -0.328109    0.467925    -0.0249272  -0.241786     0.150626   -0.454229    0.0799864   -0.115943    0.251131    -0.484041     0.182099    -0.200748     0.426136  
  0.266139     0.39661    -0.178416    -0.235466    1.1212     -0.286515   -0.824819    -1.3803       1.0459       0.720205     0.542964    -0.303247    0.181044    0.335449   -0.0128398   -0.230629   -0.0314929    0.328061   -0.0437202   0.0302254   -0.510786   -0.594714    -0.275098    -0.441722     0.321682     0.455713  
 -0.285543     0.520473    0.427617    -0.57146     1.17118     0.226325    0.169105     0.399863     0.194726    -0.0654631    0.215739    -0.0241075   0.778889    0.135971    0.754011     0.147889    0.00190719  -0.478789   -0.409542    0.219112     0.308551   -0.0738877    0.224325    -0.333098    -0.157665     0.484674  
 -0.184378    -0.300531    0.283681    -0.0425372   0.475833   -0.0617809  -0.083486     0.370224     0.180981    -0.0655974    0.0288152   -0.718811    0.698333   -0.0117478  -0.00606075  -0.126192   -0.031218    -0.0432188   0.094775    0.675209     0.268622   -0.224545     0.351461    -0.226822    -1.11782      0.143915  
  0.0896828   -0.288205   -0.0752814   -0.759414   -0.359988    0.760753    0.311782     0.323309     0.00455657   0.642548     0.152604    -0.301146   -0.140644    0.236056    0.37051      0.26377     0.0100525   -0.400224    0.265097   -0.437681    -0.141042   -0.272837    -0.356196     0.575654     0.028745    -0.34354   
  0.0614287    0.0740445   0.186456     0.188954    0.2052      0.595143    0.468992    -0.0149382    0.0292418    0.287833    -0.0384635    0.14477     0.124437    0.0407892   0.272416    -0.392637   -0.437401    -0.481864   -0.0601186  -0.68039     -0.268455   -0.133555    -0.66802      0.302193     0.279614     0.160543  
  0.144341    -0.569668   -0.319167     0.600728    0.0649437   0.43404    -0.201694    -0.115802    -0.122248    -0.444984     0.173099     0.421853    0.159669   -0.363568   -0.297672    -0.10131     0.408156     0.126823    0.671387    0.0114108    0.34487    -0.248628    -0.57182     -0.162805     0.236709    -0.309016  
  0.106647    -1.01694     0.379565     0.225135   -0.264631    0.242776    0.0873878    0.0851311   -0.00101647   0.0583788   -0.0424272   -0.130698    0.346422   -0.259725    0.104427     0.559436   -0.543661     0.25551     0.265402    0.127983     0.264941    0.0946394   -0.73991     -0.115709     0.288802     0.387614  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.405454
INFO: iteration 2, average log likelihood -1.405442
INFO: iteration 3, average log likelihood -1.405431
INFO: iteration 4, average log likelihood -1.405419
INFO: iteration 5, average log likelihood -1.405409
INFO: iteration 6, average log likelihood -1.405398
INFO: iteration 7, average log likelihood -1.405388
INFO: iteration 8, average log likelihood -1.405378
INFO: iteration 9, average log likelihood -1.405369
INFO: iteration 10, average log likelihood -1.405360
INFO: EM with 100000 data points 10 iterations avll -1.405360
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.850733e+05
      1       7.009696e+05      -1.841037e+05 |       32
      2       6.866186e+05      -1.435097e+04 |       32
      3       6.813514e+05      -5.267222e+03 |       32
      4       6.788641e+05      -2.487334e+03 |       32
      5       6.773189e+05      -1.545173e+03 |       32
      6       6.761866e+05      -1.132249e+03 |       32
      7       6.752843e+05      -9.022795e+02 |       32
      8       6.745469e+05      -7.374267e+02 |       32
      9       6.739520e+05      -5.949317e+02 |       32
     10       6.734831e+05      -4.689026e+02 |       32
     11       6.730750e+05      -4.080614e+02 |       32
     12       6.727350e+05      -3.400190e+02 |       32
     13       6.724481e+05      -2.868732e+02 |       32
     14       6.721861e+05      -2.619916e+02 |       32
     15       6.719557e+05      -2.304230e+02 |       32
     16       6.717519e+05      -2.037979e+02 |       32
     17       6.715713e+05      -1.806473e+02 |       32
     18       6.714049e+05      -1.663494e+02 |       32
     19       6.712412e+05      -1.637078e+02 |       32
     20       6.711016e+05      -1.395882e+02 |       32
     21       6.709509e+05      -1.507530e+02 |       32
     22       6.708199e+05      -1.309434e+02 |       32
     23       6.706995e+05      -1.204212e+02 |       32
     24       6.705826e+05      -1.168837e+02 |       32
     25       6.704765e+05      -1.060874e+02 |       32
     26       6.703807e+05      -9.580107e+01 |       32
     27       6.702807e+05      -1.000373e+02 |       32
     28       6.701800e+05      -1.006970e+02 |       32
     29       6.700881e+05      -9.190142e+01 |       32
     30       6.700053e+05      -8.276441e+01 |       32
     31       6.699306e+05      -7.473978e+01 |       32
     32       6.698550e+05      -7.559640e+01 |       32
     33       6.697741e+05      -8.085666e+01 |       32
     34       6.696922e+05      -8.190631e+01 |       32
     35       6.696066e+05      -8.564870e+01 |       32
     36       6.695113e+05      -9.529034e+01 |       32
     37       6.694268e+05      -8.447030e+01 |       32
     38       6.693518e+05      -7.500655e+01 |       32
     39       6.692701e+05      -8.170223e+01 |       32
     40       6.691887e+05      -8.144122e+01 |       32
     41       6.691100e+05      -7.870313e+01 |       32
     42       6.690224e+05      -8.760721e+01 |       32
     43       6.689249e+05      -9.742230e+01 |       32
     44       6.688366e+05      -8.837390e+01 |       32
     45       6.687461e+05      -9.048193e+01 |       32
     46       6.686494e+05      -9.673822e+01 |       32
     47       6.685589e+05      -9.048419e+01 |       32
     48       6.684861e+05      -7.272429e+01 |       32
     49       6.684131e+05      -7.308059e+01 |       32
     50       6.683418e+05      -7.128604e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 668341.7794580464)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.418164
INFO: iteration 2, average log likelihood -1.412971
INFO: iteration 3, average log likelihood -1.411414
INFO: iteration 4, average log likelihood -1.410163
INFO: iteration 5, average log likelihood -1.408930
INFO: iteration 6, average log likelihood -1.407958
INFO: iteration 7, average log likelihood -1.407374
INFO: iteration 8, average log likelihood -1.407053
INFO: iteration 9, average log likelihood -1.406857
INFO: iteration 10, average log likelihood -1.406717
INFO: iteration 11, average log likelihood -1.406605
INFO: iteration 12, average log likelihood -1.406511
INFO: iteration 13, average log likelihood -1.406428
INFO: iteration 14, average log likelihood -1.406354
INFO: iteration 15, average log likelihood -1.406288
INFO: iteration 16, average log likelihood -1.406229
INFO: iteration 17, average log likelihood -1.406175
INFO: iteration 18, average log likelihood -1.406125
INFO: iteration 19, average log likelihood -1.406081
INFO: iteration 20, average log likelihood -1.406040
INFO: iteration 21, average log likelihood -1.406002
INFO: iteration 22, average log likelihood -1.405968
INFO: iteration 23, average log likelihood -1.405936
INFO: iteration 24, average log likelihood -1.405906
INFO: iteration 25, average log likelihood -1.405879
INFO: iteration 26, average log likelihood -1.405854
INFO: iteration 27, average log likelihood -1.405830
INFO: iteration 28, average log likelihood -1.405807
INFO: iteration 29, average log likelihood -1.405787
INFO: iteration 30, average log likelihood -1.405767
INFO: iteration 31, average log likelihood -1.405749
INFO: iteration 32, average log likelihood -1.405731
INFO: iteration 33, average log likelihood -1.405715
INFO: iteration 34, average log likelihood -1.405699
INFO: iteration 35, average log likelihood -1.405685
INFO: iteration 36, average log likelihood -1.405671
INFO: iteration 37, average log likelihood -1.405658
INFO: iteration 38, average log likelihood -1.405645
INFO: iteration 39, average log likelihood -1.405633
INFO: iteration 40, average log likelihood -1.405622
INFO: iteration 41, average log likelihood -1.405611
INFO: iteration 42, average log likelihood -1.405600
INFO: iteration 43, average log likelihood -1.405590
INFO: iteration 44, average log likelihood -1.405581
INFO: iteration 45, average log likelihood -1.405571
INFO: iteration 46, average log likelihood -1.405562
INFO: iteration 47, average log likelihood -1.405554
INFO: iteration 48, average log likelihood -1.405545
INFO: iteration 49, average log likelihood -1.405537
INFO: iteration 50, average log likelihood -1.405529
INFO: EM with 100000 data points 50 iterations avll -1.405529
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.310127   -0.0874625   -0.431675     0.180294    -0.0365821   0.247993   -0.0959419   0.326199    0.0744546   -0.427424   -0.57164     0.15848      0.187179     0.219701    -0.022234   -0.397787   -0.174202    -0.0345371   -0.112654    0.042005     0.497254    -0.059619    0.261201    -0.677972    -0.158855   -0.663752 
  0.327188    0.0310474   -0.27494      0.218562    -0.918169    0.687172    0.201484   -0.142343   -0.262063    -0.0871515  -0.489758   -0.218782    -0.8286       0.112306    -0.0482837  -0.687094   -0.206768    -0.253227     0.3937     -0.216727     0.0679527    0.147682   -0.117067     0.402825     0.395064   -0.468686 
 -0.192259    0.179492    -0.27733     -0.060592    -0.309717   -0.127112   -0.0661389  -0.0458799   0.199625    -0.24376    -0.176986    0.278757    -0.33577     -0.246152    -0.0312534   0.0779835   0.15598      0.0876392    0.200429    0.00428054  -0.0608929    0.200445    0.243956    -0.098065     0.262419   -0.302648 
 -0.175869    0.216038    -0.211552    -0.815411    -0.542291    0.598774    0.394641    0.21722     0.0702361    0.173737    0.388805    0.00169719  -0.193591     0.168542    -0.0826103   0.0924046   0.202027    -0.0155848    0.16312    -0.894849    -0.298749    -0.243814    0.11253      0.0625252   -0.161432   -0.591419 
  0.393458    0.136345    -0.758151    -0.33103      0.506061    0.348854   -0.303686    0.269735   -0.51605     -0.0174499  -0.374752    0.627381    -0.134303    -0.43409      0.0287292   0.632256    0.238328    -0.925391    -0.365708    0.39793     -0.209293    -0.414417   -0.174237     0.273124    -0.0657812  -0.551539 
 -0.0318927  -0.0416999   -0.0152658    0.0887608    0.0185912   0.0190248  -0.0137825  -0.0244964   0.0202956   -0.0628081  -0.0327866   0.0299873   -0.0493263    0.0432256    0.0526924  -0.128694    0.00331424   0.0812723    0.0158325  -0.0414215    0.0336201    0.0476443  -0.0201198   -0.164235    -0.0909958  -0.119788 
 -0.114987    0.029944    -0.010723    -0.177807     0.123265   -0.0693659   0.233173   -0.0494564  -0.0474066    0.29673     0.237073   -0.134088     0.00412882   0.122836    -0.205479    0.0103427   0.0392525    0.120994     0.068895   -0.259534    -0.188152     0.0359538  -7.24151e-5  -0.0291725    0.0501068   0.475847 
  0.0176769  -0.514096     0.0187028    0.354002     0.372317   -0.0522944   0.301764    0.0950421  -0.265793     0.23947    -0.378698   -0.5451       0.634678     0.0941518   -0.216492   -0.0861873  -0.198363     0.148612     0.18228     0.389598     0.164705    -0.106166   -0.332591    -0.364871    -0.401815    0.206514 
  0.0132185  -0.17798     -0.205275    -0.409644     0.422054   -0.0188389   0.0848662   0.0732867   0.0730236   -0.146262    0.251408    0.705137    -0.0555277    0.190472     0.13274    -0.0696029   0.306437    -0.56732      0.488834    0.39495      0.719982     0.403935   -0.265733     0.0093024    0.387422    0.268133 
  0.212759    0.0193732    0.556607    -0.393445     0.0647829   0.063447   -0.222624    0.749229    0.255403    -0.288222   -0.298616   -0.0364265    0.237592    -0.153833    -0.203694    0.394946    0.021196    -0.136224    -0.848381    0.295483     0.0849621   -0.279651    0.191546     0.338212    -1.07563    -0.0100466
 -0.0333278  -0.478015     0.0807999    0.0765472   -0.854835    0.0652032   0.410311    1.03763    -0.730735    -0.469422   -0.455694    0.0169837   -0.197853    -0.129688     0.408804    0.563489    0.0211538    0.0719386    0.0656206   0.223061     0.32851      0.523387    0.155244     0.0441018    0.112735   -0.274029 
 -0.401143    0.145396     0.163951     0.584604     0.226912   -1.06389    -0.343873   -0.11253     0.0936056   -0.166209   -0.0498688  -0.0307323   -0.124846    -0.0845109   -0.105881    0.618225    0.387148     0.402321     0.275532    0.777777     0.120339    -0.183769    0.0241183   -0.343973     0.317667    0.49175  
 -0.0810059  -0.41135      0.034792    -0.470744    -0.0373372   0.27342    -0.304942    0.385603   -0.552893     0.14429     0.110831   -0.245806     0.156851     0.423091     0.658451    0.0143698  -0.038818    -0.329275    -0.314663   -0.0576812    0.039743     0.40141    -0.212464     0.0800528   -0.558078   -0.370676 
 -0.414768    0.533578     0.404662    -0.650539     0.210198   -0.15328     0.499572   -0.103878    0.391108    -0.299625   -0.283401   -0.253725    -0.0215634    0.129914     1.0338      0.193821   -0.546703    -0.32575     -0.440286   -0.0983996   -0.00835015   0.596381    0.392242    -0.0706657   -0.110403   -0.297713 
 -0.181407   -0.700342     0.609604     0.394924    -0.297224    0.370988    0.295111   -0.120812    0.459662    -0.170574   -0.0622935   0.221939     0.131843    -0.268777     0.142234   -0.0259234  -0.487592     0.247633     0.429142   -0.143039     0.301207     0.167605   -0.767747    -0.10917      0.444821    0.174486 
  0.232829   -0.583219    -0.501045     0.730863    -0.233483    0.275753   -0.411569   -0.0170567  -0.207059    -0.165475    0.272348    0.39401     -0.0878032   -0.312835    -0.58445    -0.187918    0.483185     0.196168     0.484607   -0.0341667   -0.0956272   -0.280688   -0.563491    -0.0080726    0.171705   -0.135489 
  0.25707     0.341233     0.00112543  -0.00346039   0.721935    0.0120811   0.16934     0.991949   -0.0354385    0.809482    0.439756   -0.521324     0.496454     0.325037     0.351105    0.098878   -0.0477749    0.0257237   -0.814014    0.040918    -0.215405     0.363345    0.954099    -0.201026    -0.485882    0.354744 
  0.552899   -0.476101     0.0927298    0.410125    -0.320541    0.214192   -0.445176   -0.496939    0.324224     0.0952335   0.0949635  -0.511363     0.390339    -0.360332    -0.0776958   0.500998   -0.571802     0.121163    -0.077099    0.101008     0.206412     0.0604201  -0.111017    -0.110568    -0.296068   -0.420538 
  0.142607   -0.0736012    0.0535747    0.117674     0.0130962   0.1479     -0.0753302   0.0401686   0.00289736  -0.0533008  -0.0224606   0.0235203    0.0874883   -0.0566699    0.162829    0.0452415  -0.088639    -0.132705    -0.0258115   0.0227256    0.0733448   -0.0766675  -0.118409     0.0551259   -0.0826786  -0.15172  
  0.508134    0.0599354   -0.322437    -0.232765     0.738298   -0.431208   -0.474731   -1.0328      0.427101     0.310017    0.320816    0.233587     0.359993    -0.920491     0.113357    0.191551   -0.0428634   -0.0547928    0.146997   -0.0470255   -0.0780998    0.598381    0.0662041    0.609426    -0.277231   -0.204788 
  0.0641433  -0.53205      0.122086    -0.160191     0.116083    0.758044    0.273589    0.422314   -0.122034     0.710304    0.0431936   0.0381515   -0.374752     0.065055     0.454041    0.514732    0.0975992   -0.584381     0.643554   -0.110331     0.137086    -0.529923   -0.283231     0.545398     0.562385   -0.252486 
  0.0872035  -0.172608     0.121058    -0.268116     0.879582   -0.103001   -0.269472   -0.461815   -0.8953      -0.778128    0.316635   -0.200574     0.683073    -0.301852     0.691434    0.0850247  -0.25161     -0.0439047   -0.592293   -0.0615444    0.0799849    0.134845   -0.56068      0.1362      -0.0176977   0.391541 
  0.31008     0.413674    -0.116767    -0.360585    -0.158371    0.0515573   0.293662   -0.20581    -0.204546    -0.0635004   0.0174687   0.853551    -0.883452     0.00461081  -0.0877077   0.319799    0.366512     0.00347068  -0.322005   -0.509728    -0.00436765   0.473738    0.209812     0.00834081   0.839657    0.295205 
 -0.0269148  -0.099681     0.196874     0.584749    -0.499606   -0.260801    0.0756211  -0.29555    -0.308472     0.0618226  -0.621612   -0.071924    -0.00451924   0.213588     0.101463   -0.153709   -0.246171     0.282652    -0.167445   -0.5103      -0.844236     0.269892    0.138526    -0.376826    -0.447568   -0.423624 
 -0.485339    0.413406     0.493289    -0.137078     0.519072    0.164965    0.293048   -0.103399   -0.00863709  -0.204386   -0.0320008   0.611544     0.305106     0.204527     0.592549   -0.395718    0.222738    -0.385067     0.106968   -0.052196    -0.552526    -0.303686   -0.141434     0.101804     0.044588    0.136095 
 -1.08271     0.00792341  -0.543804    -0.591977     0.362437   -0.0340052   0.439458    0.678625   -0.518744     0.156405   -0.254163   -0.00641636   0.268333     0.119672    -0.57294    -0.598962    0.196676    -0.154537     0.109097    0.287933    -0.188402     0.126791    0.200616    -0.255988    -0.141916    0.477515 
 -0.255752    0.0502465    0.394815    -0.132276    -0.20153    -0.306232   -0.0208689  -0.151515    0.09881     -0.188122    0.800642   -0.490321    -0.0305319    0.0401109    0.0246035  -0.227884   -0.271548     0.040885     0.256778   -0.0366892    0.398893     0.156359    0.82685      0.00116769  -0.374581    0.190273 
 -0.309372    0.797399    -0.0889899    0.294672    -0.287802   -0.213105    0.408232    0.0355003   0.699755     0.0186745   0.228871   -0.161733    -0.056529    -0.427955    -0.450004    0.047051   -0.122484     0.321624     0.251488   -0.376162    -0.199679    -0.228444    0.700519    -0.155678     0.40069    -0.0134036
  0.274567    0.331086    -0.480769    -0.336099    -0.030334   -0.0723745  -0.65329    -0.115457   -0.202395    -0.229296    0.315903   -0.0553677   -0.324597     0.206157    -0.12449    -0.251854    0.598815     0.116378    -0.210965    0.0638907   -0.038781    -0.0134296   0.56604      0.181384    -0.415396   -0.264898 
  0.0549646   0.370339    -0.134531    -0.342929     1.16434    -0.0441128  -0.350876   -0.499392    0.833673     0.336399    0.681589    0.0198707    0.109096     0.229974    -0.0248861  -0.274261   -0.0612392    0.0904514   -0.171238    0.182691    -0.0957929   -0.736744   -0.105191    -0.409127     0.216253    0.489996 
  0.165886    0.0175513    0.0369472    0.0487653    0.143805    0.393408    0.221941   -0.0507202  -0.0128141    0.505178    0.0114309  -0.410024     0.440449     0.20607     -0.0773001  -0.0106662  -0.303798    -0.0346893   -0.0956314  -0.58013     -0.456659    -0.422911   -0.835583     0.154545     0.133107    0.562917 
  0.396963   -0.0934051    0.231227     0.619876     0.279964   -0.384687    0.305514   -0.198486    0.458331     0.553854   -0.277137    0.0849297   -0.688633    -0.339254     0.539884    0.0150019  -0.0187576   -0.445608    -0.0781165   0.456944     0.059995     0.226021   -0.0789234    0.297027     0.304323   -0.0188854INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.405521
INFO: iteration 2, average log likelihood -1.405514
INFO: iteration 3, average log likelihood -1.405506
INFO: iteration 4, average log likelihood -1.405499
INFO: iteration 5, average log likelihood -1.405492
INFO: iteration 6, average log likelihood -1.405484
INFO: iteration 7, average log likelihood -1.405477
INFO: iteration 8, average log likelihood -1.405470
INFO: iteration 9, average log likelihood -1.405463
INFO: iteration 10, average log likelihood -1.405456
INFO: EM with 100000 data points 10 iterations avll -1.405456
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
