>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.7.0
INFO: Installing JLD v0.6.6
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.1
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.3.0
INFO: Installing ScikitLearnBase v0.2.1
INFO: Installing StaticArrays v0.1.0
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
WARNING: takebuf_string(b) is deprecated, use String(take!(b)) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in takebuf_string(::Base.AbstractIOBuffer{Array{UInt8,1}}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/BinDeps/src/dependencies.jl:887 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::SubString{String}) at ./sysimg.jl:14
 in evalfile(::SubString{String}, ::Array{String,1}) at ./loading.jl:572 (repeats 2 times)
 in cd(::##2#4, ::String) at ./file.jl:69
 in (::##1#3)(::IOStream) at ./none:13
 in open(::##1#3, ::String, ::String) at ./iostream.jl:152
 in eval(::Module, ::Any) at ./boot.jl:236
 in process_options(::Base.JLOptions) at ./client.jl:248
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/Rmath/deps/build.jl, in expression starting on line 39
INFO: Building HDF5
INFO: Package database updated
INFO: METADATA is out-of-date â€” you may not have the latest version of GaussianMixtures
INFO: Use `Pkg.update()` to get the latest versions of your packages

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1297
Commit 416f5f2 (2016-11-23 13:12 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-101-generic #148-Ubuntu SMP Thu Oct 20 22:08:32 UTC 2016 x86_64 x86_64
Memory: 2.939281463623047 GB (673.5234375 MB free)
Uptime: 27950.0 sec
Load Avg:  1.03564453125  1.0146484375  0.994140625
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3504 MHz    1666814 s       1891 s     179839 s     652075 s         47 s
#2  3504 MHz     797062 s       4655 s      97249 s    1790519 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.4
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.7.0
 - JLD                           0.6.6
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.1
 - Rmath                         0.1.4
 - SHA                           0.3.0
 - ScikitLearnBase               0.2.1
 - StaticArrays                  0.1.0
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: takebuf_string(b) is deprecated, use String(take!(b)) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in takebuf_string(::Base.AbstractIOBuffer{Array{UInt8,1}}) at ./deprecated.jl:50
 in #_write#17(::Array{Any,1}, ::Function, ::JLD.JldFile, ::String, ::Array{Any,1}, ::JLD.JldWriteSession) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:587
 in #write#14(::Array{Any,1}, ::Function, ::JLD.JldFile, ::String, ::Array{Any,1}, ::JLD.JldWriteSession) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:529
 in #jldopen#9(::Bool, ::Bool, ::Bool, ::Function, ::String, ::Bool, ::Bool, ::Bool, ::Bool, ::Bool) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:198
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::String, ::Bool, ::Bool, ::Bool, ::Bool, ::Bool) at ./<missing>:0
 in #jldopen#10(::Bool, ::Bool, ::Bool, ::Function, ::String, ::String) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:253
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::String, ::String) at ./<missing>:0
 in #jldopen#11(::Array{Any,1}, ::Function, ::JLD.##34#35{String,Array{Float64,2},Tuple{}}, ::String, ::Vararg{String,N}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:263
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::Function, ::String, ::String) at ./<missing>:0
 in #save#33(::Bool, ::Bool, ::Function, ::FileIO.File{FileIO.DataFormat{:JLD}}, ::String, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:1217
 in save(::FileIO.File{FileIO.DataFormat{:JLD}}, ::String, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:1214
 in #save#14(::Array{Any,1}, ::Function, ::String, ::String, ::Vararg{Any,N}) at /home/vagrant/.julia/v0.6/FileIO/src/loadsave.jl:54
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:8 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-831528.7322719716,[99256.1,743.925],
[2107.9 -2035.36 22.0722; -1413.92 1349.1 109.276],

Array{Float64,2}[
[97438.5 2190.99 105.809; 2190.99 96296.8 -435.194; 105.809 -435.194 99803.4],

[3012.64 -2336.72 -232.038; -2336.72 2818.96 83.1524; -232.038 83.1524 548.14]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.922570e+03
      1       1.351438e+03      -5.711322e+02 |        5
      2       1.297022e+03      -5.441570e+01 |        2
      3       1.282200e+03      -1.482235e+01 |        2
      4       1.242650e+03      -3.954994e+01 |        2
      5       1.189121e+03      -5.352859e+01 |        4
      6       1.099804e+03      -8.931713e+01 |        2
      7       1.098086e+03      -1.718061e+00 |        0
      8       1.098086e+03       0.000000e+00 |        0
K-means converged with 8 iterations (objv = 1098.0858772488573)
INFO: K-means with 272 data points using 8 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.063008
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.713018
INFO: iteration 2, lowerbound -3.600430
INFO: iteration 3, lowerbound -3.476260
INFO: iteration 4, lowerbound -3.335979
INFO: iteration 5, lowerbound -3.203518
INFO: iteration 6, lowerbound -3.103825
INFO: dropping number of Gaussions to 7
INFO: iteration 7, lowerbound -3.040587
INFO: dropping number of Gaussions to 5
INFO: iteration 8, lowerbound -2.982068
INFO: dropping number of Gaussions to 4
INFO: iteration 9, lowerbound -2.915631
INFO: iteration 10, lowerbound -2.837009
INFO: iteration 11, lowerbound -2.747559
INFO: iteration 12, lowerbound -2.652212
INFO: iteration 13, lowerbound -2.557760
INFO: iteration 14, lowerbound -2.469254
INFO: iteration 15, lowerbound -2.395485
INFO: iteration 16, lowerbound -2.345379
INFO: iteration 17, lowerbound -2.322665
INFO: dropping number of Gaussions to 3
INFO: iteration 18, lowerbound -2.311179
INFO: dropping number of Gaussions to 2
INFO: iteration 19, lowerbound -2.302931
INFO: iteration 20, lowerbound -2.299262
INFO: iteration 21, lowerbound -2.299257
INFO: iteration 22, lowerbound -2.299255
INFO: iteration 23, lowerbound -2.299254
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Thu 24 Nov 2016 01:15:49 PM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Thu 24 Nov 2016 01:15:51 PM UTC: K-means with 272 data points using 8 iterations
11.3 data points per parameter
,Thu 24 Nov 2016 01:15:52 PM UTC: EM with 272 data points 0 iterations avll -2.063008
5.8 data points per parameter
,Thu 24 Nov 2016 01:15:53 PM UTC: GMM converted to Variational GMM
,Thu 24 Nov 2016 01:15:54 PM UTC: iteration 1, lowerbound -3.713018
,Thu 24 Nov 2016 01:15:54 PM UTC: iteration 2, lowerbound -3.600430
,Thu 24 Nov 2016 01:15:54 PM UTC: iteration 3, lowerbound -3.476260
,Thu 24 Nov 2016 01:15:54 PM UTC: iteration 4, lowerbound -3.335979
,Thu 24 Nov 2016 01:15:55 PM UTC: iteration 5, lowerbound -3.203518
,Thu 24 Nov 2016 01:15:55 PM UTC: iteration 6, lowerbound -3.103825
,Thu 24 Nov 2016 01:15:55 PM UTC: dropping number of Gaussions to 7
,Thu 24 Nov 2016 01:15:55 PM UTC: iteration 7, lowerbound -3.040587
,Thu 24 Nov 2016 01:15:55 PM UTC: dropping number of Gaussions to 5
,Thu 24 Nov 2016 01:15:55 PM UTC: iteration 8, lowerbound -2.982068
,Thu 24 Nov 2016 01:15:55 PM UTC: dropping number of Gaussions to 4
,Thu 24 Nov 2016 01:15:55 PM UTC: iteration 9, lowerbound -2.915631
,Thu 24 Nov 2016 01:15:55 PM UTC: iteration 10, lowerbound -2.837009
,Thu 24 Nov 2016 01:15:55 PM UTC: iteration 11, lowerbound -2.747559
,Thu 24 Nov 2016 01:15:55 PM UTC: iteration 12, lowerbound -2.652212
,Thu 24 Nov 2016 01:15:55 PM UTC: iteration 13, lowerbound -2.557760
,Thu 24 Nov 2016 01:15:55 PM UTC: iteration 14, lowerbound -2.469254
,Thu 24 Nov 2016 01:15:55 PM UTC: iteration 15, lowerbound -2.395485
,Thu 24 Nov 2016 01:15:56 PM UTC: iteration 16, lowerbound -2.345379
,Thu 24 Nov 2016 01:15:56 PM UTC: iteration 17, lowerbound -2.322665
,Thu 24 Nov 2016 01:15:56 PM UTC: dropping number of Gaussions to 3
,Thu 24 Nov 2016 01:15:56 PM UTC: iteration 18, lowerbound -2.311179
,Thu 24 Nov 2016 01:15:56 PM UTC: dropping number of Gaussions to 2
,Thu 24 Nov 2016 01:15:56 PM UTC: iteration 19, lowerbound -2.302931
,Thu 24 Nov 2016 01:15:56 PM UTC: iteration 20, lowerbound -2.299262
,Thu 24 Nov 2016 01:15:56 PM UTC: iteration 21, lowerbound -2.299257
,Thu 24 Nov 2016 01:15:56 PM UTC: iteration 22, lowerbound -2.299255
,Thu 24 Nov 2016 01:15:56 PM UTC: iteration 23, lowerbound -2.299254
,Thu 24 Nov 2016 01:15:56 PM UTC: iteration 24, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:56 PM UTC: iteration 25, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:56 PM UTC: iteration 26, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:56 PM UTC: iteration 27, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:57 PM UTC: iteration 28, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:57 PM UTC: iteration 29, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:57 PM UTC: iteration 30, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:57 PM UTC: iteration 31, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:57 PM UTC: iteration 32, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:57 PM UTC: iteration 33, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:57 PM UTC: iteration 34, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:57 PM UTC: iteration 35, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:57 PM UTC: iteration 36, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:57 PM UTC: iteration 37, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:57 PM UTC: iteration 38, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:57 PM UTC: iteration 39, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:57 PM UTC: iteration 40, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:57 PM UTC: iteration 41, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:58 PM UTC: iteration 42, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:58 PM UTC: iteration 43, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:58 PM UTC: iteration 44, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:58 PM UTC: iteration 45, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:58 PM UTC: iteration 46, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:58 PM UTC: iteration 47, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:58 PM UTC: iteration 48, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:58 PM UTC: iteration 49, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:58 PM UTC: iteration 50, lowerbound -2.299253
,Thu 24 Nov 2016 01:15:58 PM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
Î± = [178.045,95.9549]
Î² = [178.045,95.9549]
m = [4.2503 79.2869; 2.00023 53.852]
Î½ = [180.045,97.9549]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.184042 -0.00764405; 0.0 0.00858171],

[0.375876 -0.00895312; 0.0 0.0127487]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -0.9790658244044415
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9790658244044416
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9790658244044417
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -0.9975645511580417
avll from llpg:  -0.9975645511580417
avll direct:     -0.9975645511580417
sum posterior: 100000.0
32Ã—26 Array{Float64,2}:
  0.14046     -0.0292417  -0.113724     0.0127499     0.135953     0.0166745     0.0505396     0.0906391    0.00272443   0.0515485    0.0310079  -0.216714     0.0201817    -0.103098     0.0228442   -0.00412087  -0.00449929   0.0242767    0.0166099  -0.00466459  -0.0492715   -0.0734097    0.0480356   -0.20516     -0.00249646   0.130651   
 -0.0652287    0.143124    0.0503383    0.0643687     0.102245    -0.0253811    -0.0674402    -0.0809867   -0.194006    -0.244334    -0.175994   -0.0179207   -0.00893592    0.216395    -0.00168326  -0.0469158   -0.077107    -0.0136212   -0.166554   -0.092088     0.0424163   -0.104345    -0.0525509    0.0781147    0.157046     0.105908   
  0.129285     0.123806   -0.00830365   0.179911      0.0194555    0.198166     -0.0339207     0.115622    -0.0601805   -0.0419517   -0.159002   -0.146354    -0.27348      -0.0542585    0.108146    -0.124149     0.0507092    0.069337    -0.0746919   0.00519905   0.123105     0.0629166   -0.0285309    0.0105968   -0.0734049   -0.113284   
  0.0646787    0.0214025   0.0504068    0.0437505     0.077148    -0.0292617     0.0125335     0.163421    -0.0187339   -0.144898    -0.0516111   0.111694     0.0679674    -0.00943099   0.0522856   -0.0282288   -0.0644474   -0.0362382   -0.045591    0.0992025   -0.0129315    0.0802662   -0.123411     0.0087743   -0.111179    -0.138206   
  0.145728     0.130568    0.0576328   -0.187167      0.141002    -0.11606      -0.116921      0.0682195    0.0754267   -0.100472    -0.0967849   0.0125874    0.00582726   -0.011261     0.0499671    0.0751227    0.0328088    0.098059    -0.090852   -0.390094     0.0600586   -0.0717009   -0.230394     0.0182633   -0.149226     0.0474327  
  0.189023    -0.199407    0.0167964   -0.0437475     0.037066    -0.0970314    -0.0410617     0.102274     0.129434     0.157772    -0.255223   -0.0590624   -0.0141469    -0.0317201   -0.0191134   -0.214287     0.0735978   -0.0748178    0.0367196  -0.00394654  -0.0296223    0.093648    -0.0581287   -0.116907     0.118958     0.000678079
  0.101064    -0.145959    0.0425507   -0.0725641     0.124365     0.0241283    -0.0828286     0.10839      0.0846622    0.189923    -0.0733306   0.0149865    0.0819946    -0.0199772   -0.167393    -0.0601695   -0.0251271   -0.0681      -0.0026104  -0.0256003    0.204342     0.146296    -0.0252328    0.0948963    0.14007     -0.107136   
 -0.0371602    0.127973   -0.0867416   -0.0809734     0.0432907   -0.0946573     0.0821384    -0.0523118    0.150659     0.0385855    0.198746   -0.0534878    0.00092803   -0.0549216    0.022339    -0.063787    -0.0427346    0.00525424  -0.0307605  -0.00781138   0.0144258    0.081885     0.192022    -0.0908043    0.0591612   -0.0255519  
 -0.0166805   -0.120386   -0.216996     0.0988277     0.188086     0.0751667     0.0623098     0.0904714   -0.0959652   -0.210386     0.100371   -0.0686079    0.0548506     0.0751696   -0.0260429   -0.222948    -0.0127712    0.0935264    0.140131   -0.200919    -0.0457409    0.0568128   -0.179222    -0.00613142   0.0445366   -0.118363   
  0.295451     0.180152    0.0650177    0.102654      0.0572405   -0.093159     -0.111915      0.0176167   -0.152623     0.172924     0.119272   -0.111942    -0.0869108    -0.10022     -0.0600782    0.0680832    0.267991     0.0139218    0.202887   -0.0184111   -0.0971385   -0.0708761    0.197792    -0.096897    -0.0677755   -0.01915    
 -0.0612054    0.0116851  -0.0739728    0.0158239     0.231575    -0.0343942     0.0655998    -0.00497175   0.0752766   -0.299638     0.0127717  -0.115593     0.000891065   0.0986075   -0.0849047    0.0211007    0.0998417    0.0759554    0.10633    -0.0904135    0.139439     0.0986324   -0.0777656    0.0169312   -0.225621     0.0727528  
 -0.0518946    0.110586    0.0237243    0.128229      0.0312046    0.0259062    -0.173036      0.0659405   -0.107494    -0.0795758   -0.0428001   0.0416781   -0.020688      0.215562    -0.0467177    0.0152627   -0.0799603    0.0883793    0.226792   -0.148958    -0.00494948  -0.0723531   -0.0367604   -0.0410724    0.12246      0.160629   
  0.157212    -0.0198222  -0.141        0.0196053    -0.0534594    0.0280403    -0.14399       0.0534774   -0.212234     0.0547976   -0.0689086   0.0503985   -0.0737996     0.0809721    0.133851    -0.00805643  -0.0474388   -0.090858    -0.0886071   0.25594      0.0476638    0.00207856  -0.0854092    0.0451765   -0.127092    -0.0985451  
 -0.128851     0.0647154  -0.16639     -0.0336277     0.0438226    0.231013      0.0439572     0.060449    -0.107318    -0.00258039  -0.0403644   0.0971152    0.109944     -0.0567333   -0.0328221   -0.118192     0.0559903    0.222507     0.173224    0.0267835   -0.0862584   -0.00674214  -0.143311    -0.00260568  -0.111629     0.114821   
  0.0495495   -0.0682589  -0.188812     0.0606468     0.0654589    0.127206     -0.0709749    -0.0397903    0.0470671    0.111546     0.152426    0.0586081   -0.0866677     0.0228228   -0.0505013    0.0351989    0.0181407   -0.0415507   -0.168745    0.143975    -0.0527937    0.0928463    0.0930102    0.0865378    0.137282    -0.0192816  
  0.00266093   0.0761968   0.00821802  -0.0653158    -0.107592     0.0289883     0.110672      0.011406     0.0225963    0.020672    -0.0555545   0.0466208   -0.00665608   -0.0886494    0.0560343   -0.100233     0.0929829   -0.0917928   -0.0535721   0.0452565   -0.0799525    0.123977    -0.0143526    0.00494888   0.0670323   -0.0423333  
  0.0180113   -0.135709    0.159647     0.208077     -0.0807158    0.00462351    0.0375421     0.0769744    0.22294      0.0364967   -0.171216    0.0306608    0.127516      0.0320484    0.028355     0.0602876    0.174052    -0.0729808    0.0544445   0.152229    -0.109956    -0.0767354    0.0679698    0.0107521    0.0806225    0.0435213  
 -0.120712    -0.0336023  -0.137318     0.0363223     0.0476527    0.0147143    -0.0416069    -0.0520455   -0.0499613    0.028876     0.0825275  -0.0846755    0.0620088    -0.158577    -0.0745729    0.195313    -0.0892306    0.0170304   -0.11925    -0.102009     0.00109495   0.0147737    0.0202471    0.1088       0.150101     0.0339914  
  0.0967585    0.203236    0.00362899   0.232411      0.00255619   0.0900181    -0.0543033     0.0783012    0.0258859   -0.126362    -0.0008337  -0.0775769    0.0381247    -0.0555826    0.0379932    0.0456181   -0.142883     0.173812    -0.034674    0.147489     0.0448053   -0.0774603    0.127375     0.110663     0.00975227  -0.0855234  
  0.0776369   -0.160736    0.205722     0.000219956   0.155529     0.100854     -0.238846      0.0337246    0.0540086    0.178753    -0.0533579  -0.0377969   -0.0335464    -0.0871023    0.0788235    0.16103      0.0348987   -0.0375241   -0.0360222  -0.0201576   -0.0810312   -0.073485     0.0873147    0.0310591    0.0998141    0.103304   
 -0.0755111   -0.0301992   0.0710939   -0.0869435     0.0820621    0.105193     -0.153527      0.00340658  -0.179813    -0.0122775   -0.0722524  -0.00785938  -0.0616189    -0.031293     0.0597399   -0.171885    -0.146455     0.0368965    0.0315835  -0.0734781    0.126074    -0.0788366   -0.0349968    0.0167064   -0.106261     0.155003   
 -0.0302259    0.0867946  -0.114118    -0.1817       -0.0385341   -0.10759       0.0775703     0.0300401   -0.062141    -0.0896122    0.124511    0.0356734   -0.00902284   -0.173931    -0.0400709    0.19109     -0.18357      0.0269837   -0.119105    0.0855946    0.0286401    0.00719055  -0.0896467   -0.196079    -0.0437075   -0.182818   
  0.0278548    0.14162    -0.0728703   -0.165744      0.00287883   0.174654      0.00874277    0.158279    -0.11432     -0.0807599    0.032945    0.0620852   -0.106448     -0.0363992   -0.0770157    0.132291     0.0418767    0.066486    -0.1817      0.231501    -0.0875329    0.00804437  -0.117066     0.150812    -0.159002    -0.0735599  
  0.145058     0.0104251   0.0176195   -0.0729982    -0.0116397    0.115982      0.0938348     0.137037    -0.0227291   -0.101061     0.160776    0.201303     0.0436098     0.0321638    0.102078    -0.0609991    0.276141     0.0195082    0.0263972   0.319487     0.196517     0.052557    -0.0422074   -0.0689603    0.0694308   -0.0919793  
 -0.00374685  -0.0616599   0.0729952    0.147712      0.0572859    0.0802046    -0.0608467     0.248297    -0.0615151   -0.190414     0.0729634   0.0985354    0.0077248    -0.0202527   -0.0890424   -0.207705    -0.114101     0.0316508   -0.0729333   0.0933007    0.117998    -0.011792     0.0148931    0.159397     0.0759311    0.0800535  
 -0.0492993    0.0439053  -0.0134661   -0.0797039     0.14718      0.0336072     0.0768393     0.109417    -0.0543487   -0.117719     0.018721    0.00347718   0.0655326     0.228908     0.106638     0.0687248   -0.123566     0.0605766   -0.0819734  -0.13373     -0.120924    -0.145243    -0.00865149  -0.028619    -0.0691386    0.0433421  
 -0.0345551    0.146856    0.0154743   -0.0422451    -0.022097     0.0307773     0.175889     -0.038967     0.0490195   -0.100024    -0.0270333   0.0305165    0.045242      0.0738621    0.0956271    0.0213383   -0.16596      0.0640254   -0.0123316  -0.0685431    0.108791    -0.015582     0.0608653    0.0182982    0.00406001  -0.130602   
  0.0391016    0.0339418   0.111849     0.0281886     0.140821    -0.000739802   0.0206125    -0.263783    -0.107435     0.112819    -0.181559   -0.13505      0.0350821     0.00338674   0.044116     0.00360924   0.168453     0.027689    -0.0112763   0.00932804  -0.0303611    0.186662    -0.140951    -0.0583773   -0.0149027   -0.0818791  
  0.0434595    0.054974   -0.0859621   -0.00324391    0.0410302    0.00453209    0.0352598     0.0277221    0.121712     0.03267     -0.0391814  -0.00185812  -0.00937151    0.0261163    0.0794421   -0.0457756   -0.0766152   -0.098951     0.173551    0.019891    -0.025497     0.0743872    0.107653     0.0913499   -0.0355239    0.034773   
 -0.0231993    0.111981   -0.0163367    0.00427181   -0.115154    -0.113162      0.000802458  -0.104839     0.0628298    0.19064     -0.102763   -0.0341204   -0.0394606     0.076017    -0.0640725    0.139773    -0.149613     0.025803     0.143929   -0.0992836    0.0147294    0.190421    -0.129555    -0.0375162    0.0654488   -0.233619   
 -0.106495     0.154806   -0.115474    -0.138226      0.107957     0.0372322    -0.150045      0.0839301    0.0856811    0.0792808   -0.122176    0.0958426   -0.068543     -0.0715489   -0.0036338   -0.107455    -0.0571717    0.272575     0.123579   -0.203691    -0.0848743   -0.0621791    0.158641     0.147683     0.106083    -0.212702   
 -0.192447     0.0440835  -0.0507063    0.167646      0.241691     0.0739926    -0.021914     -0.0159668   -0.082109    -0.0231539   -0.0549534  -0.0382354   -0.0193521     0.215738     0.160783     0.123784     0.135909    -0.103281    -0.0726409   0.0908543   -0.0785832    0.015685     0.116432     0.052261     0.150575     0.108624   kind diag, method split
0: avll = -1.399034668079422
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.399196
INFO: iteration 2, average log likelihood -1.399057
INFO: iteration 3, average log likelihood -1.398538
INFO: iteration 4, average log likelihood -1.391627
INFO: iteration 5, average log likelihood -1.370563
INFO: iteration 6, average log likelihood -1.363413
INFO: iteration 7, average log likelihood -1.362601
INFO: iteration 8, average log likelihood -1.362271
INFO: iteration 9, average log likelihood -1.362083
INFO: iteration 10, average log likelihood -1.361970
INFO: iteration 11, average log likelihood -1.361900
INFO: iteration 12, average log likelihood -1.361855
INFO: iteration 13, average log likelihood -1.361825
INFO: iteration 14, average log likelihood -1.361804
INFO: iteration 15, average log likelihood -1.361788
INFO: iteration 16, average log likelihood -1.361776
INFO: iteration 17, average log likelihood -1.361766
INFO: iteration 18, average log likelihood -1.361757
INFO: iteration 19, average log likelihood -1.361749
INFO: iteration 20, average log likelihood -1.361741
INFO: iteration 21, average log likelihood -1.361734
INFO: iteration 22, average log likelihood -1.361727
INFO: iteration 23, average log likelihood -1.361720
INFO: iteration 24, average log likelihood -1.361713
INFO: iteration 25, average log likelihood -1.361706
INFO: iteration 26, average log likelihood -1.361698
INFO: iteration 27, average log likelihood -1.361690
INFO: iteration 28, average log likelihood -1.361682
INFO: iteration 29, average log likelihood -1.361673
INFO: iteration 30, average log likelihood -1.361662
INFO: iteration 31, average log likelihood -1.361652
INFO: iteration 32, average log likelihood -1.361640
INFO: iteration 33, average log likelihood -1.361628
INFO: iteration 34, average log likelihood -1.361615
INFO: iteration 35, average log likelihood -1.361602
INFO: iteration 36, average log likelihood -1.361590
INFO: iteration 37, average log likelihood -1.361579
INFO: iteration 38, average log likelihood -1.361569
INFO: iteration 39, average log likelihood -1.361559
INFO: iteration 40, average log likelihood -1.361550
INFO: iteration 41, average log likelihood -1.361541
INFO: iteration 42, average log likelihood -1.361533
INFO: iteration 43, average log likelihood -1.361524
INFO: iteration 44, average log likelihood -1.361516
INFO: iteration 45, average log likelihood -1.361507
INFO: iteration 46, average log likelihood -1.361498
INFO: iteration 47, average log likelihood -1.361488
INFO: iteration 48, average log likelihood -1.361476
INFO: iteration 49, average log likelihood -1.361460
INFO: iteration 50, average log likelihood -1.361440
INFO: EM with 100000 data points 50 iterations avll -1.361440
952.4 data points per parameter
1: avll = [-1.3992,-1.39906,-1.39854,-1.39163,-1.37056,-1.36341,-1.3626,-1.36227,-1.36208,-1.36197,-1.3619,-1.36186,-1.36183,-1.3618,-1.36179,-1.36178,-1.36177,-1.36176,-1.36175,-1.36174,-1.36173,-1.36173,-1.36172,-1.36171,-1.36171,-1.3617,-1.36169,-1.36168,-1.36167,-1.36166,-1.36165,-1.36164,-1.36163,-1.36161,-1.3616,-1.36159,-1.36158,-1.36157,-1.36156,-1.36155,-1.36154,-1.36153,-1.36152,-1.36152,-1.36151,-1.3615,-1.36149,-1.36148,-1.36146,-1.36144]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.361592
INFO: iteration 2, average log likelihood -1.361403
INFO: iteration 3, average log likelihood -1.361062
INFO: iteration 4, average log likelihood -1.358277
INFO: iteration 5, average log likelihood -1.346664
INFO: iteration 6, average log likelihood -1.333559
INFO: iteration 7, average log likelihood -1.327131
INFO: iteration 8, average log likelihood -1.323481
INFO: iteration 9, average log likelihood -1.320776
INFO: iteration 10, average log likelihood -1.318755
INFO: iteration 11, average log likelihood -1.317547
INFO: iteration 12, average log likelihood -1.316856
INFO: iteration 13, average log likelihood -1.316384
INFO: iteration 14, average log likelihood -1.315999
INFO: iteration 15, average log likelihood -1.315646
INFO: iteration 16, average log likelihood -1.315292
INFO: iteration 17, average log likelihood -1.314899
INFO: iteration 18, average log likelihood -1.314407
INFO: iteration 19, average log likelihood -1.313756
INFO: iteration 20, average log likelihood -1.313022
INFO: iteration 21, average log likelihood -1.312310
INFO: iteration 22, average log likelihood -1.311602
INFO: iteration 23, average log likelihood -1.310843
INFO: iteration 24, average log likelihood -1.310066
INFO: iteration 25, average log likelihood -1.309342
INFO: iteration 26, average log likelihood -1.308763
INFO: iteration 27, average log likelihood -1.308372
INFO: iteration 28, average log likelihood -1.308134
INFO: iteration 29, average log likelihood -1.307990
INFO: iteration 30, average log likelihood -1.307900
INFO: iteration 31, average log likelihood -1.307840
INFO: iteration 32, average log likelihood -1.307797
INFO: iteration 33, average log likelihood -1.307765
INFO: iteration 34, average log likelihood -1.307741
INFO: iteration 35, average log likelihood -1.307722
INFO: iteration 36, average log likelihood -1.307707
INFO: iteration 37, average log likelihood -1.307695
INFO: iteration 38, average log likelihood -1.307686
INFO: iteration 39, average log likelihood -1.307677
INFO: iteration 40, average log likelihood -1.307670
INFO: iteration 41, average log likelihood -1.307664
INFO: iteration 42, average log likelihood -1.307658
INFO: iteration 43, average log likelihood -1.307654
INFO: iteration 44, average log likelihood -1.307649
INFO: iteration 45, average log likelihood -1.307645
INFO: iteration 46, average log likelihood -1.307642
INFO: iteration 47, average log likelihood -1.307639
INFO: iteration 48, average log likelihood -1.307636
INFO: iteration 49, average log likelihood -1.307634
INFO: iteration 50, average log likelihood -1.307632
INFO: EM with 100000 data points 50 iterations avll -1.307632
473.9 data points per parameter
2: avll = [-1.36159,-1.3614,-1.36106,-1.35828,-1.34666,-1.33356,-1.32713,-1.32348,-1.32078,-1.31876,-1.31755,-1.31686,-1.31638,-1.316,-1.31565,-1.31529,-1.3149,-1.31441,-1.31376,-1.31302,-1.31231,-1.3116,-1.31084,-1.31007,-1.30934,-1.30876,-1.30837,-1.30813,-1.30799,-1.3079,-1.30784,-1.3078,-1.30777,-1.30774,-1.30772,-1.30771,-1.3077,-1.30769,-1.30768,-1.30767,-1.30766,-1.30766,-1.30765,-1.30765,-1.30765,-1.30764,-1.30764,-1.30764,-1.30763,-1.30763]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.307868
INFO: iteration 2, average log likelihood -1.307616
INFO: iteration 3, average log likelihood -1.306682
INFO: iteration 4, average log likelihood -1.299638
INFO: iteration 5, average log likelihood -1.282461
INFO: iteration 6, average log likelihood -1.268214
INFO: iteration 7, average log likelihood -1.263106
INFO: iteration 8, average log likelihood -1.260892
INFO: iteration 9, average log likelihood -1.259342
INFO: iteration 10, average log likelihood -1.258101
INFO: iteration 11, average log likelihood -1.257220
INFO: iteration 12, average log likelihood -1.256599
INFO: iteration 13, average log likelihood -1.256074
INFO: iteration 14, average log likelihood -1.255517
INFO: iteration 15, average log likelihood -1.254902
INFO: iteration 16, average log likelihood -1.254433
INFO: iteration 17, average log likelihood -1.254025
INFO: iteration 18, average log likelihood -1.253504
INFO: iteration 19, average log likelihood -1.252766
INFO: iteration 20, average log likelihood -1.251677
INFO: iteration 21, average log likelihood -1.250060
INFO: iteration 22, average log likelihood -1.248112
INFO: iteration 23, average log likelihood -1.246679
INFO: iteration 24, average log likelihood -1.246080
INFO: iteration 25, average log likelihood -1.245813
INFO: iteration 26, average log likelihood -1.245669
INFO: iteration 27, average log likelihood -1.245573
INFO: iteration 28, average log likelihood -1.245501
INFO: iteration 29, average log likelihood -1.245444
INFO: iteration 30, average log likelihood -1.245398
INFO: iteration 31, average log likelihood -1.245362
INFO: iteration 32, average log likelihood -1.245331
INFO: iteration 33, average log likelihood -1.245302
INFO: iteration 34, average log likelihood -1.245271
INFO: iteration 35, average log likelihood -1.245235
INFO: iteration 36, average log likelihood -1.245190
INFO: iteration 37, average log likelihood -1.245132
INFO: iteration 38, average log likelihood -1.245053
INFO: iteration 39, average log likelihood -1.244952
INFO: iteration 40, average log likelihood -1.244848
INFO: iteration 41, average log likelihood -1.244771
INFO: iteration 42, average log likelihood -1.244724
INFO: iteration 43, average log likelihood -1.244698
INFO: iteration 44, average log likelihood -1.244682
INFO: iteration 45, average log likelihood -1.244671
INFO: iteration 46, average log likelihood -1.244663
INFO: iteration 47, average log likelihood -1.244656
INFO: iteration 48, average log likelihood -1.244651
INFO: iteration 49, average log likelihood -1.244647
INFO: iteration 50, average log likelihood -1.244644
INFO: EM with 100000 data points 50 iterations avll -1.244644
236.4 data points per parameter
3: avll = [-1.30787,-1.30762,-1.30668,-1.29964,-1.28246,-1.26821,-1.26311,-1.26089,-1.25934,-1.2581,-1.25722,-1.2566,-1.25607,-1.25552,-1.2549,-1.25443,-1.25402,-1.2535,-1.25277,-1.25168,-1.25006,-1.24811,-1.24668,-1.24608,-1.24581,-1.24567,-1.24557,-1.2455,-1.24544,-1.2454,-1.24536,-1.24533,-1.2453,-1.24527,-1.24523,-1.24519,-1.24513,-1.24505,-1.24495,-1.24485,-1.24477,-1.24472,-1.2447,-1.24468,-1.24467,-1.24466,-1.24466,-1.24465,-1.24465,-1.24464]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.244975
INFO: iteration 2, average log likelihood -1.244600
INFO: iteration 3, average log likelihood -1.242674
INFO: iteration 4, average log likelihood -1.226280
INFO: iteration 5, average log likelihood -1.190346
WARNING: Variances had to be floored 7 10
INFO: iteration 6, average log likelihood -1.163680
WARNING: Variances had to be floored 6 9
INFO: iteration 7, average log likelihood -1.165069
WARNING: Variances had to be floored 7 10
INFO: iteration 8, average log likelihood -1.167174
WARNING: Variances had to be floored 1
INFO: iteration 9, average log likelihood -1.161645
WARNING: Variances had to be floored 7 9
INFO: iteration 10, average log likelihood -1.156963
WARNING: Variances had to be floored 6 10
INFO: iteration 11, average log likelihood -1.159297
WARNING: Variances had to be floored 7
INFO: iteration 12, average log likelihood -1.161107
WARNING: Variances had to be floored 9 10
INFO: iteration 13, average log likelihood -1.153789
WARNING: Variances had to be floored 7
INFO: iteration 14, average log likelihood -1.153417
WARNING: Variances had to be floored 1 6
INFO: iteration 15, average log likelihood -1.151132
WARNING: Variances had to be floored 7 10
INFO: iteration 16, average log likelihood -1.161020
WARNING: Variances had to be floored 9
INFO: iteration 17, average log likelihood -1.159259
WARNING: Variances had to be floored 10
INFO: iteration 18, average log likelihood -1.154375
WARNING: Variances had to be floored 6 7
INFO: iteration 19, average log likelihood -1.149505
WARNING: Variances had to be floored 9 10
INFO: iteration 20, average log likelihood -1.158869
INFO: iteration 21, average log likelihood -1.155762
WARNING: Variances had to be floored 1 7
INFO: iteration 22, average log likelihood -1.145700
WARNING: Variances had to be floored 6 10
INFO: iteration 23, average log likelihood -1.156812
WARNING: Variances had to be floored 9
INFO: iteration 24, average log likelihood -1.159208
WARNING: Variances had to be floored 7 10
INFO: iteration 25, average log likelihood -1.154459
INFO: iteration 26, average log likelihood -1.157908
WARNING: Variances had to be floored 6 9 10
INFO: iteration 27, average log likelihood -1.143753
WARNING: Variances had to be floored 7
INFO: iteration 28, average log likelihood -1.158971
WARNING: Variances had to be floored 1
INFO: iteration 29, average log likelihood -1.153915
WARNING: Variances had to be floored 10
INFO: iteration 30, average log likelihood -1.152576
WARNING: Variances had to be floored 6 7 9
INFO: iteration 31, average log likelihood -1.148227
WARNING: Variances had to be floored 8 10
INFO: iteration 32, average log likelihood -1.166466
INFO: iteration 33, average log likelihood -1.159790
WARNING: Variances had to be floored 7 9 10
INFO: iteration 34, average log likelihood -1.143114
WARNING: Variances had to be floored 1 6
INFO: iteration 35, average log likelihood -1.155723
WARNING: Variances had to be floored 7
INFO: iteration 36, average log likelihood -1.165645
WARNING: Variances had to be floored 10
INFO: iteration 37, average log likelihood -1.155448
WARNING: Variances had to be floored 7 9
INFO: iteration 38, average log likelihood -1.149244
WARNING: Variances had to be floored 6 10
INFO: iteration 39, average log likelihood -1.156306
INFO: iteration 40, average log likelihood -1.158574
WARNING: Variances had to be floored 1 7 9 10
INFO: iteration 41, average log likelihood -1.142711
INFO: iteration 42, average log likelihood -1.166199
WARNING: Variances had to be floored 6
INFO: iteration 43, average log likelihood -1.152358
WARNING: Variances had to be floored 7 10
INFO: iteration 44, average log likelihood -1.153690
WARNING: Variances had to be floored 8 9
INFO: iteration 45, average log likelihood -1.156421
WARNING: Variances had to be floored 10
INFO: iteration 46, average log likelihood -1.156891
WARNING: Variances had to be floored 6 7
INFO: iteration 47, average log likelihood -1.145413
WARNING: Variances had to be floored 1 9 10
INFO: iteration 48, average log likelihood -1.152829
WARNING: Variances had to be floored 7
INFO: iteration 49, average log likelihood -1.162871
INFO: iteration 50, average log likelihood -1.158966
INFO: EM with 100000 data points 50 iterations avll -1.158966
118.1 data points per parameter
4: avll = [-1.24497,-1.2446,-1.24267,-1.22628,-1.19035,-1.16368,-1.16507,-1.16717,-1.16164,-1.15696,-1.1593,-1.16111,-1.15379,-1.15342,-1.15113,-1.16102,-1.15926,-1.15437,-1.14951,-1.15887,-1.15576,-1.1457,-1.15681,-1.15921,-1.15446,-1.15791,-1.14375,-1.15897,-1.15391,-1.15258,-1.14823,-1.16647,-1.15979,-1.14311,-1.15572,-1.16564,-1.15545,-1.14924,-1.15631,-1.15857,-1.14271,-1.1662,-1.15236,-1.15369,-1.15642,-1.15689,-1.14541,-1.15283,-1.16287,-1.15897]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 11 12 13 14 19 20
INFO: iteration 1, average log likelihood -1.145270
WARNING: Variances had to be floored 11 12 13 14 17 18 19 20
INFO: iteration 2, average log likelihood -1.142227
WARNING: Variances had to be floored 11 12 13 14 19 20
INFO: iteration 3, average log likelihood -1.139674
WARNING: Variances had to be floored 1 2 11 12 13 14 16 17 18 19 20
INFO: iteration 4, average log likelihood -1.113261
WARNING: Variances had to be floored 3 4 11 12 13 14 16 19 20 27 28
INFO: iteration 5, average log likelihood -1.080225
WARNING: Variances had to be floored 11 12 13 14 16 17 18 19 20 26
INFO: iteration 6, average log likelihood -1.083842
WARNING: Variances had to be floored 1 2 11 12 13 14 16 19 20 28
INFO: iteration 7, average log likelihood -1.076470
WARNING: Variances had to be floored 3 11 12 13 14 16 17 18 19 20 22 27
INFO: iteration 8, average log likelihood -1.065345
WARNING: Variances had to be floored 4 11 12 13 14 16 19 20 26 28
INFO: iteration 9, average log likelihood -1.078081
WARNING: Variances had to be floored 2 3 11 12 13 14 16 17 18 19 20 22 27
INFO: iteration 10, average log likelihood -1.073273
WARNING: Variances had to be floored 1 11 12 13 14 16 19 20 28
INFO: iteration 11, average log likelihood -1.080068
WARNING: Variances had to be floored 3 4 11 12 13 14 16 17 18 19 20 22 26 27
INFO: iteration 12, average log likelihood -1.062328
WARNING: Variances had to be floored 1 11 12 13 14 16 19 20 28
INFO: iteration 13, average log likelihood -1.082751
WARNING: Variances had to be floored 11 12 13 14 16 17 18 19 20 22 27
INFO: iteration 14, average log likelihood -1.066148
WARNING: Variances had to be floored 1 3 4 11 12 13 14 16 19 20 26 28
INFO: iteration 15, average log likelihood -1.059065
WARNING: Variances had to be floored 11 12 13 14 16 17 18 19 20 22 27 31
INFO: iteration 16, average log likelihood -1.071779
WARNING: Variances had to be floored 1 11 12 13 14 16 19 20 28
INFO: iteration 17, average log likelihood -1.079114
WARNING: Variances had to be floored 3 4 11 12 13 14 16 17 18 19 20 22 26 27
INFO: iteration 18, average log likelihood -1.056918
WARNING: Variances had to be floored 1 11 12 13 14 16 19 20 28
INFO: iteration 19, average log likelihood -1.078117
WARNING: Variances had to be floored 11 12 13 14 16 17 18 19 20 22 27 31
INFO: iteration 20, average log likelihood -1.063651
WARNING: Variances had to be floored 1 3 4 11 12 13 14 16 19 20 26 28
INFO: iteration 21, average log likelihood -1.067944
WARNING: Variances had to be floored 11 12 13 14 16 17 18 19 20 22 27
INFO: iteration 22, average log likelihood -1.075828
WARNING: Variances had to be floored 1 11 12 13 14 16 19 20 28
INFO: iteration 23, average log likelihood -1.070422
WARNING: Variances had to be floored 3 4 11 12 13 14 16 17 18 19 20 22 26 27 31
INFO: iteration 24, average log likelihood -1.052886
WARNING: Variances had to be floored 1 11 12 13 14 16 19 20 28
INFO: iteration 25, average log likelihood -1.086866
WARNING: Variances had to be floored 11 12 13 14 16 17 18 19 20 22 27
INFO: iteration 26, average log likelihood -1.067654
WARNING: Variances had to be floored 1 3 4 11 12 13 14 16 19 20 26 28
INFO: iteration 27, average log likelihood -1.059121
WARNING: Variances had to be floored 11 12 13 14 16 17 18 19 20 22 27 31
INFO: iteration 28, average log likelihood -1.071759
WARNING: Variances had to be floored 1 11 12 13 14 16 19 20 28
INFO: iteration 29, average log likelihood -1.079051
WARNING: Variances had to be floored 3 4 11 12 13 14 16 17 18 19 20 22 26 27
INFO: iteration 30, average log likelihood -1.056887
WARNING: Variances had to be floored 1 11 12 13 14 16 19 20 28
INFO: iteration 31, average log likelihood -1.078086
WARNING: Variances had to be floored 11 12 13 14 16 17 18 19 20 22 27 31
INFO: iteration 32, average log likelihood -1.063612
WARNING: Variances had to be floored 1 3 4 11 12 13 14 16 19 20 26 28
INFO: iteration 33, average log likelihood -1.067906
WARNING: Variances had to be floored 11 12 13 14 16 17 18 19 20 22 27
INFO: iteration 34, average log likelihood -1.075795
WARNING: Variances had to be floored 1 11 12 13 14 16 19 20 28
INFO: iteration 35, average log likelihood -1.070389
WARNING: Variances had to be floored 3 4 11 12 13 14 16 17 18 19 20 22 26 27 31
INFO: iteration 36, average log likelihood -1.052859
WARNING: Variances had to be floored 1 11 12 13 14 16 19 20 28
INFO: iteration 37, average log likelihood -1.086842
WARNING: Variances had to be floored 11 12 13 14 16 17 18 19 20 22 27
INFO: iteration 38, average log likelihood -1.067631
WARNING: Variances had to be floored 1 3 4 11 12 13 14 16 19 20 26 28
INFO: iteration 39, average log likelihood -1.059103
WARNING: Variances had to be floored 11 12 13 14 16 17 18 19 20 22 27 31
INFO: iteration 40, average log likelihood -1.071744
WARNING: Variances had to be floored 1 11 12 13 14 16 19 20 28
INFO: iteration 41, average log likelihood -1.079037
WARNING: Variances had to be floored 3 4 11 12 13 14 16 17 18 19 20 22 26 27
INFO: iteration 42, average log likelihood -1.056876
WARNING: Variances had to be floored 1 11 12 13 14 16 19 20 28
INFO: iteration 43, average log likelihood -1.078076
WARNING: Variances had to be floored 11 12 13 14 16 17 18 19 20 22 27 31
INFO: iteration 44, average log likelihood -1.063603
WARNING: Variances had to be floored 1 3 4 11 12 13 14 16 19 20 26 28
INFO: iteration 45, average log likelihood -1.067899
WARNING: Variances had to be floored 11 12 13 14 16 17 18 19 20 22 27
INFO: iteration 46, average log likelihood -1.075789
WARNING: Variances had to be floored 1 11 12 13 14 16 19 20 28
INFO: iteration 47, average log likelihood -1.070382
WARNING: Variances had to be floored 3 4 11 12 13 14 16 17 18 19 20 22 26 27 31
INFO: iteration 48, average log likelihood -1.052853
WARNING: Variances had to be floored 1 11 12 13 14 16 19 20 28
INFO: iteration 49, average log likelihood -1.086835
WARNING: Variances had to be floored 11 12 13 14 16 17 18 19 20 22 27
INFO: iteration 50, average log likelihood -1.067625
INFO: EM with 100000 data points 50 iterations avll -1.067625
59.0 data points per parameter
5: avll = [-1.14527,-1.14223,-1.13967,-1.11326,-1.08023,-1.08384,-1.07647,-1.06535,-1.07808,-1.07327,-1.08007,-1.06233,-1.08275,-1.06615,-1.05907,-1.07178,-1.07911,-1.05692,-1.07812,-1.06365,-1.06794,-1.07583,-1.07042,-1.05289,-1.08687,-1.06765,-1.05912,-1.07176,-1.07905,-1.05689,-1.07809,-1.06361,-1.06791,-1.0758,-1.07039,-1.05286,-1.08684,-1.06763,-1.0591,-1.07174,-1.07904,-1.05688,-1.07808,-1.0636,-1.0679,-1.07579,-1.07038,-1.05285,-1.08684,-1.06762]
[-1.39903,-1.3992,-1.39906,-1.39854,-1.39163,-1.37056,-1.36341,-1.3626,-1.36227,-1.36208,-1.36197,-1.3619,-1.36186,-1.36183,-1.3618,-1.36179,-1.36178,-1.36177,-1.36176,-1.36175,-1.36174,-1.36173,-1.36173,-1.36172,-1.36171,-1.36171,-1.3617,-1.36169,-1.36168,-1.36167,-1.36166,-1.36165,-1.36164,-1.36163,-1.36161,-1.3616,-1.36159,-1.36158,-1.36157,-1.36156,-1.36155,-1.36154,-1.36153,-1.36152,-1.36152,-1.36151,-1.3615,-1.36149,-1.36148,-1.36146,-1.36144,-1.36159,-1.3614,-1.36106,-1.35828,-1.34666,-1.33356,-1.32713,-1.32348,-1.32078,-1.31876,-1.31755,-1.31686,-1.31638,-1.316,-1.31565,-1.31529,-1.3149,-1.31441,-1.31376,-1.31302,-1.31231,-1.3116,-1.31084,-1.31007,-1.30934,-1.30876,-1.30837,-1.30813,-1.30799,-1.3079,-1.30784,-1.3078,-1.30777,-1.30774,-1.30772,-1.30771,-1.3077,-1.30769,-1.30768,-1.30767,-1.30766,-1.30766,-1.30765,-1.30765,-1.30765,-1.30764,-1.30764,-1.30764,-1.30763,-1.30763,-1.30787,-1.30762,-1.30668,-1.29964,-1.28246,-1.26821,-1.26311,-1.26089,-1.25934,-1.2581,-1.25722,-1.2566,-1.25607,-1.25552,-1.2549,-1.25443,-1.25402,-1.2535,-1.25277,-1.25168,-1.25006,-1.24811,-1.24668,-1.24608,-1.24581,-1.24567,-1.24557,-1.2455,-1.24544,-1.2454,-1.24536,-1.24533,-1.2453,-1.24527,-1.24523,-1.24519,-1.24513,-1.24505,-1.24495,-1.24485,-1.24477,-1.24472,-1.2447,-1.24468,-1.24467,-1.24466,-1.24466,-1.24465,-1.24465,-1.24464,-1.24497,-1.2446,-1.24267,-1.22628,-1.19035,-1.16368,-1.16507,-1.16717,-1.16164,-1.15696,-1.1593,-1.16111,-1.15379,-1.15342,-1.15113,-1.16102,-1.15926,-1.15437,-1.14951,-1.15887,-1.15576,-1.1457,-1.15681,-1.15921,-1.15446,-1.15791,-1.14375,-1.15897,-1.15391,-1.15258,-1.14823,-1.16647,-1.15979,-1.14311,-1.15572,-1.16564,-1.15545,-1.14924,-1.15631,-1.15857,-1.14271,-1.1662,-1.15236,-1.15369,-1.15642,-1.15689,-1.14541,-1.15283,-1.16287,-1.15897,-1.14527,-1.14223,-1.13967,-1.11326,-1.08023,-1.08384,-1.07647,-1.06535,-1.07808,-1.07327,-1.08007,-1.06233,-1.08275,-1.06615,-1.05907,-1.07178,-1.07911,-1.05692,-1.07812,-1.06365,-1.06794,-1.07583,-1.07042,-1.05289,-1.08687,-1.06765,-1.05912,-1.07176,-1.07905,-1.05689,-1.07809,-1.06361,-1.06791,-1.0758,-1.07039,-1.05286,-1.08684,-1.06763,-1.0591,-1.07174,-1.07904,-1.05688,-1.07808,-1.0636,-1.0679,-1.07579,-1.07038,-1.05285,-1.08684,-1.06762]
32Ã—26 Array{Float64,2}:
 -0.0342278     0.112382   -0.0679351   -0.0727245    0.0575179   -0.0845961    0.100325    -0.0506811     0.153636    0.0367648    0.193636    -0.05313     -0.0513797   -0.0408346    0.0220431    -0.0330225   -0.04459      0.0128642   -0.0160311    -0.0141072   -0.00986669   0.0867888    0.184579    -0.0717936    0.0652927    -0.0027159 
  0.0174044    -0.0477758  -0.131387     0.028905     0.10526      0.0221573   -0.0107139    0.0188806    -0.0366076   0.0473831    0.0595142   -0.1559       0.0419241   -0.13125     -0.0266896     0.0929386   -0.0256751    0.0376333   -0.0444369    -0.0760766   -0.0278577   -0.0233464    0.0302891   -0.0282577    0.0766939     0.0808995 
 -0.0319853     0.0233784  -0.0155709   -0.101459     0.175465     0.0773727    0.0784812    0.130024     -0.0590642  -0.110591     0.0244922   -0.00860714   0.0652257    0.236344     0.107213      0.0610632   -0.179797     0.0408825   -0.0855131    -0.182695    -0.119753    -0.140802    -0.00681626  -0.0358087   -0.048865      0.0442554 
  0.127849      0.0178573   0.0180227   -0.0628176   -0.00878698   0.114646     0.0738755    0.140401     -0.0744281  -0.0855338    0.164659     0.215886     0.0416867    0.0206318    0.103377     -0.0866813    0.298949     0.0540936    0.0561063     0.289607     0.207159     0.0570914   -0.0505173   -0.0944086    0.0691834    -0.101153  
  0.0657117     0.0550982  -0.0743872    0.0209546   -0.0771582   -0.0568802   -0.0593612   -0.0258538    -0.065604    0.120657    -0.0791867    0.00282336  -0.0583973    0.0648606    0.037518      0.0733125   -0.0996014   -0.0170411    0.031986      0.0815663    0.0383487    0.0972239   -0.135937    -0.0161936   -0.0396762    -0.151816  
 -0.0841429     0.0436095   0.0282372    0.0812021    0.177252     0.0337353   -0.00185216  -0.125197     -0.071417    0.0375118   -0.106195    -0.0889302   -0.0092157    0.112164     0.0867514     0.0640868    0.149974    -0.037329    -0.038927      0.0309602   -0.0561883    0.0946319   -0.0274205   -0.00835597   0.0599843     0.0199223 
  0.0410854    -0.093856   -0.199035    -1.13004      0.0911731    0.103354    -0.06956     -0.000826174   0.158657    0.126434     0.148212     0.0101773   -0.0804335    0.0218367   -0.0353155     0.0433054    0.028332    -0.0920207   -0.163366      0.141143    -0.0388649    0.100207     0.0200052    0.0818457    0.140059      0.0238414 
  0.0560685    -0.0499302  -0.180796     1.13736      0.0326526    0.147592    -0.072026    -0.0608524     0.0263495   0.108352     0.156513     0.124818    -0.131825     0.0245992   -0.0421867     0.0271658    0.0113863    0.0290468   -0.174212      0.120388    -0.0529632    0.0974235    0.12974      0.100275     0.181051     -0.0798518 
 -0.117915      0.187463   -0.120981    -0.284681     0.143431     0.0471888   -0.146872     0.0804379     0.0744352   0.0906864   -0.0888416    0.0530189   -0.0681214    0.0412962   -0.154539     -0.101348     0.174811     0.281797     0.134922     -0.192303    -0.78881     -0.184836     0.151923     0.148174     0.107229     -0.194426  
 -0.146846      0.186913   -0.110659     0.0675438    0.0721975    0.0191352   -0.150345     0.0966758     0.0651066   0.0566772   -0.321656     0.128015     0.0334317   -0.117359     0.0925646    -0.104788    -0.177489     0.23999      0.120191     -0.2086       0.652814    -0.00930952   0.164199     0.155395     0.103036     -0.222928  
 -0.0505341     0.0443326  -0.154336     0.00255739   0.0445988    0.00512246  -0.020125    -0.0272756     0.161078   -0.0131526   -0.285581     0.0793669   -0.044375     0.221839     0.101435     -0.0594649   -0.274172     0.0621641    0.141151      0.0233235   -0.0185992    0.120609     0.00850944   0.0938253   -0.00905168    0.0350371 
  0.123468      0.0807114  -0.025578    -0.026967     0.0368826    0.00360103   0.19183      0.0572813     0.0468098   0.089372     0.168431    -0.0865652    0.117687    -0.182028     0.0498281    -0.014706     0.0615538   -0.218373     0.220086      0.00816392  -0.0460356    0.0181416    0.202266     0.0874405   -0.0493918     0.0347562 
 -0.119882     -0.0303355   0.0673843   -0.18952      0.0695089    0.104532    -0.147541    -0.0936776    -0.159599   -0.0129985   -0.0787165   -0.0070757   -0.0603721    0.00883276   0.0555336    -0.155665     0.0148063    0.0440146    0.0591535    -0.186151     0.11116     -0.0757683   -0.0312324    0.0301314   -0.586492      0.153909  
 -0.0544003    -0.0825037   0.119128    -0.00388258   0.0823404    0.104795    -0.137447     0.0383121    -0.214974   -0.0187075   -0.119877    -0.00645786  -0.0508367   -0.0594663    0.0579519    -0.165559    -0.245931     0.0378601    0.0337416     0.0360061    0.106171    -0.126889    -0.0221941   -0.0252068    0.484231      0.156719  
 -0.0392422     0.115779    0.0224672    0.154906     0.0496778    0.0245878   -0.171932     0.0694141    -0.108326   -0.107364    -0.0368183    0.0498496   -0.0159551    0.218808    -0.0499181    -0.0319612   -0.0804587    0.0843745    0.235346     -0.156438    -0.0659163   -0.0498918   -0.0460779   -0.0369695    0.121824      0.166958  
 -0.0485604     0.148579    0.00367799  -0.0465068   -0.0211219    0.0305086    0.176029    -0.0534727     0.0467364  -0.0972206   -0.0395351    0.0487965    0.0254097    0.0704344    0.106552     -0.00559542  -0.14601      0.062069    -0.0177982    -0.0682786    0.114217    -0.0273124    0.0602621    0.0122577    0.00549569   -0.128084  
  0.086134      0.194048   -0.0778775    0.215343     0.00186656   0.0900923   -0.143177     0.0746285    -0.0510919  -0.213197    -0.0289293   -0.141166     0.074623    -0.0552192    0.0362731    -0.320706    -0.146109     0.179232    -0.0373404     0.139997     0.0163516   -0.114136     0.194421     0.0944851    0.0534247    -0.785584  
  0.117568      0.200688    0.106876     0.247304     0.00329357   0.0901379    0.0537158    0.0788018     0.0666757  -0.0854499    0.00737704   0.0144376    0.0225533   -0.0583343    0.0432739     0.451978    -0.149116     0.172638    -0.0331374     0.14636      0.0878528   -0.0690103    0.0148482    0.0678763   -0.0435895     0.574035  
 -0.0298382    -0.29602    -0.348617     0.0624917    0.209892    -0.0136021    0.0607633   -0.13593      -0.0934139  -0.494383     0.063603    -0.0579144    0.0282962    0.0576408   -0.026249     -0.630883    -0.0119042    0.145809     0.125247     -0.179805    -0.0481869    0.0912128   -0.141275    -0.0634268    0.0445731     0.0417802 
 -0.0173839     0.0367773  -0.142816     0.120233     0.167423     0.152574     0.0644097    0.338978     -0.0986184  -0.125714     0.138285    -0.114776     0.11133      0.0895884   -0.0263746     0.164918    -0.0090183    0.00733794   0.172087     -0.21945     -0.0279682   -0.0262609   -0.177924     0.058313     0.0509047    -0.306697  
 -0.00631962    0.098617   -0.101971    -0.0857359    0.116852     0.0851084    0.0506391    0.084929     -0.028238   -0.190449     0.0120173   -0.0213563   -0.0582557    0.0264045   -0.0550854     0.0658463    0.0606508    0.0702845   -0.0456166     0.0924457   -0.00495818   0.0502089   -0.0960422    0.090264    -0.193146     -0.00265888
 -0.0653517     0.144242    0.0531724    0.0652366    0.103462    -0.0232614   -0.0605734   -0.0823876    -0.193813   -0.246862    -0.185921    -0.0182387   -0.0258574    0.209826     6.71126e-5   -0.0401291   -0.0919518   -0.014222    -0.166772     -0.0950363    0.0798739   -0.100708    -0.0465581    0.0862343    0.144558      0.0899437 
  0.105082     -0.134085    0.201059    -0.00942557   0.134231     0.101099    -0.244318     0.0144182     0.0508045  -0.453406    -0.0790312   -0.0481619   -0.00373449  -0.0936451    0.010687      0.191819    -0.0683539   -0.0324705   -0.02755       0.0194571   -0.109228     0.0519702    0.0727582    0.00174572   0.0855715     0.230809  
  0.0649763    -0.190271    0.2353      -0.0247068    0.195363     0.110251    -0.231337     0.0592196     0.0944369   0.579067    -0.072156    -0.0277648   -0.0676841   -0.0796578    0.187926      0.110087     0.0758297   -0.0466334   -0.0175356    -0.0299371   -0.0853047   -0.174152     0.0965477    0.060821     0.16306      -0.0282484 
  0.000206256   0.0745021  -0.119785    -0.177702    -0.0438783   -0.101948     0.0829544    0.0176686    -0.0715036  -0.0829404    0.106547     0.0354024   -0.00541187  -0.170107    -0.0330011     0.177371    -0.271734     0.0252119   -0.140736      0.0834391    0.0623727    0.0145703   -0.104841    -0.193622    -0.0604053    -0.182914  
 -0.134473      0.0654574  -0.129228    -0.0322689    0.0456366    0.183358     0.0487567    0.061375     -0.127554    0.0308894   -0.0462024    0.0905967    0.0970898   -0.0580243   -0.0347679    -0.141683     0.0942698    0.239704     0.18959       0.0192376   -0.0759887   -0.00584437  -0.140201    -0.0338425   -0.138905      0.113111  
 -0.0105603     0.0752735  -0.0112264   -0.0669523   -0.105394     0.0213824    0.109491     0.014004      0.0208102   0.0450216   -0.0650225    0.0611809   -0.0217708   -0.0872513    0.117656     -0.100969     0.0975201   -0.089054    -0.0636253     0.0449115   -0.0809759    0.125022    -0.013974     0.00239495   0.0565991    -0.0428868 
  0.126436     -0.1441      0.0846876   -0.0709022    0.108373     0.0262703   -0.0776104    0.108992      0.110906    0.188357    -0.0749278    0.00710071   0.0874099   -0.0197466   -0.181811     -0.0750476   -0.0464401   -0.0740025   -0.000940999  -0.024391     0.177327     0.143005    -0.0275033    0.0937304    0.156239     -0.10347   
  0.00132715   -0.0994859   0.103101     0.158254     0.00340161   0.0490372   -0.0358754    0.161973      0.0768632  -0.0839205   -0.0536091    0.0807881    0.0701805    0.017839    -0.0243897    -0.126128     0.0340086   -0.0343912   -0.0109649     0.132902     0.0162067   -0.0296465    0.0371816    0.0833958    0.0767518     0.0547113 
  0.130619     -0.0784906   0.0349611   -0.0103178    0.0801231   -0.0633658   -0.0121853    0.116839      0.0450594  -0.00577921  -0.155502     0.0410943    0.0273923    0.00373576   0.000780275  -0.105857    -0.00944037  -0.0540253   -0.0127168     0.0349493    0.0270067    0.086189    -0.0979716   -0.0479857    0.000411356  -0.0884813 
  0.106047      0.0989124   0.00662558   0.175785     0.0554476    0.192756    -0.03275      0.105238     -0.057608   -0.0251506   -0.158815    -0.17431     -0.24501     -0.0517641    0.0804969    -0.145023     0.0630543    0.0885578   -0.0692659    -0.00258973   0.132761     0.0589747   -0.0108358    0.00700129  -0.0844853    -0.102201  
  0.213528      0.167035    0.0653489   -0.0532389    0.0950186   -0.115203    -0.11294      0.0365202    -0.0216007   0.032064    -0.0152777   -0.0454094   -0.0511392   -0.0506647    0.00480583    0.0897287    0.155965     0.0734032    0.0508024    -0.227365    -0.0140969   -0.0525666   -0.0323728   -0.0112555   -0.107574      0.00831598INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 3 4 11 12 13 14 16 19 20 26 28
INFO: iteration 1, average log likelihood -1.059096
WARNING: Variances had to be floored 1 3 4 11 12 13 14 16 17 18 19 20 22 26 27 28 31
INFO: iteration 2, average log likelihood -1.042586
WARNING: Variances had to be floored 1 3 4 11 12 13 14 16 19 20 26 28
INFO: iteration 3, average log likelihood -1.059060
WARNING: Variances had to be floored 1 3 4 11 12 13 14 16 17 18 19 20 22 26 27 28 31
INFO: iteration 4, average log likelihood -1.042533
WARNING: Variances had to be floored 1 3 4 11 12 13 14 16 19 20 26 28
INFO: iteration 5, average log likelihood -1.059060
WARNING: Variances had to be floored 1 3 4 11 12 13 14 16 17 18 19 20 22 26 27 28 31
INFO: iteration 6, average log likelihood -1.042519
WARNING: Variances had to be floored 1 3 4 11 12 13 14 16 19 20 26 28
INFO: iteration 7, average log likelihood -1.059060
WARNING: Variances had to be floored 1 3 4 11 12 13 14 16 17 18 19 20 22 26 27 28 31
INFO: iteration 8, average log likelihood -1.042514
WARNING: Variances had to be floored 1 3 4 11 12 13 14 16 19 20 26 28
INFO: iteration 9, average log likelihood -1.059059
WARNING: Variances had to be floored 1 3 4 11 12 13 14 16 17 18 19 20 22 26 27 28 31
INFO: iteration 10, average log likelihood -1.042513
INFO: EM with 100000 data points 10 iterations avll -1.042513
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.699691e+05
      1       6.621933e+05      -2.077758e+05 |       32
      2       6.308053e+05      -3.138800e+04 |       32
      3       6.160099e+05      -1.479536e+04 |       32
      4       6.075864e+05      -8.423495e+03 |       32
      5       6.024145e+05      -5.171969e+03 |       32
      6       5.996175e+05      -2.796957e+03 |       32
      7       5.979976e+05      -1.619894e+03 |       32
      8       5.971008e+05      -8.968454e+02 |       32
      9       5.964044e+05      -6.963635e+02 |       32
     10       5.956586e+05      -7.457744e+02 |       32
     11       5.948459e+05      -8.127375e+02 |       32
     12       5.941549e+05      -6.909450e+02 |       32
     13       5.936409e+05      -5.140753e+02 |       32
     14       5.932545e+05      -3.863302e+02 |       32
     15       5.929052e+05      -3.493312e+02 |       32
     16       5.925153e+05      -3.899138e+02 |       32
     17       5.920632e+05      -4.520995e+02 |       32
     18       5.915490e+05      -5.141488e+02 |       32
     19       5.911003e+05      -4.487436e+02 |       32
     20       5.907952e+05      -3.050708e+02 |       32
     21       5.906298e+05      -1.654775e+02 |       32
     22       5.905498e+05      -7.995842e+01 |       32
     23       5.904986e+05      -5.119828e+01 |       31
     24       5.904634e+05      -3.517278e+01 |       30
     25       5.904322e+05      -3.121686e+01 |       31
     26       5.904036e+05      -2.863979e+01 |       31
     27       5.903803e+05      -2.331516e+01 |       31
     28       5.903663e+05      -1.391664e+01 |       29
     29       5.903576e+05      -8.704344e+00 |       29
     30       5.903494e+05      -8.215662e+00 |       29
     31       5.903410e+05      -8.401597e+00 |       30
     32       5.903336e+05      -7.381996e+00 |       25
     33       5.903271e+05      -6.514938e+00 |       30
     34       5.903199e+05      -7.253253e+00 |       30
     35       5.903133e+05      -6.533499e+00 |       27
     36       5.903089e+05      -4.408780e+00 |       28
     37       5.903048e+05      -4.122346e+00 |       25
     38       5.903015e+05      -3.304098e+00 |       22
     39       5.902997e+05      -1.819971e+00 |       16
     40       5.902987e+05      -9.780661e-01 |       16
     41       5.902977e+05      -1.019902e+00 |       12
     42       5.902965e+05      -1.147184e+00 |       16
     43       5.902954e+05      -1.153160e+00 |       12
     44       5.902947e+05      -6.994976e-01 |       14
     45       5.902937e+05      -9.422818e-01 |       13
     46       5.902927e+05      -1.064127e+00 |       15
     47       5.902913e+05      -1.388410e+00 |       19
     48       5.902896e+05      -1.641771e+00 |       13
     49       5.902878e+05      -1.883408e+00 |       14
     50       5.902855e+05      -2.238640e+00 |       14
K-means terminated without convergence after 50 iterations (objv = 590285.522680429)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.301194
INFO: iteration 2, average log likelihood -1.270109
INFO: iteration 3, average log likelihood -1.239869
INFO: iteration 4, average log likelihood -1.206149
INFO: iteration 5, average log likelihood -1.162908
WARNING: Variances had to be floored 27
INFO: iteration 6, average log likelihood -1.108814
WARNING: Variances had to be floored 2 13 18 24
INFO: iteration 7, average log likelihood -1.062959
WARNING: Variances had to be floored 25 26 29
INFO: iteration 8, average log likelihood -1.068458
WARNING: Variances had to be floored 7 11 12 20 30
INFO: iteration 9, average log likelihood -1.068188
WARNING: Variances had to be floored 1 4 27 32
INFO: iteration 10, average log likelihood -1.093405
WARNING: Variances had to be floored 2 16 18 24
INFO: iteration 11, average log likelihood -1.089232
WARNING: Variances had to be floored 13
INFO: iteration 12, average log likelihood -1.092725
WARNING: Variances had to be floored 25 26
INFO: iteration 13, average log likelihood -1.043842
WARNING: Variances had to be floored 2 4 7 11 12 16 20 29 30
INFO: iteration 14, average log likelihood -1.026629
WARNING: Variances had to be floored 1 18 24
INFO: iteration 15, average log likelihood -1.121378
INFO: iteration 16, average log likelihood -1.115010
WARNING: Variances had to be floored 13 25 32
INFO: iteration 17, average log likelihood -1.058655
WARNING: Variances had to be floored 4 16 27
INFO: iteration 18, average log likelihood -1.042858
WARNING: Variances had to be floored 2 7 11 12 18 20 21 24
INFO: iteration 19, average log likelihood -1.045476
WARNING: Variances had to be floored 10 29 30
INFO: iteration 20, average log likelihood -1.105705
WARNING: Variances had to be floored 1 16 25
INFO: iteration 21, average log likelihood -1.080974
WARNING: Variances had to be floored 4 13 26
INFO: iteration 22, average log likelihood -1.068922
WARNING: Variances had to be floored 12 18 21 24 27 32
INFO: iteration 23, average log likelihood -1.060714
WARNING: Variances had to be floored 2 11 20
INFO: iteration 24, average log likelihood -1.091497
WARNING: Variances had to be floored 7 25
INFO: iteration 25, average log likelihood -1.071812
WARNING: Variances had to be floored 4 10 13 16 29 30
INFO: iteration 26, average log likelihood -1.035666
WARNING: Variances had to be floored 1 21 24 26
INFO: iteration 27, average log likelihood -1.078485
WARNING: Variances had to be floored 12 18 27
INFO: iteration 28, average log likelihood -1.079195
WARNING: Variances had to be floored 2 11 20 25 32
INFO: iteration 29, average log likelihood -1.068423
WARNING: Variances had to be floored 4 7
INFO: iteration 30, average log likelihood -1.087602
WARNING: Variances had to be floored 13 21 24 29
INFO: iteration 31, average log likelihood -1.061621
WARNING: Variances had to be floored 1 16 30
INFO: iteration 32, average log likelihood -1.074078
WARNING: Variances had to be floored 10 12 18 25 26
INFO: iteration 33, average log likelihood -1.053718
WARNING: Variances had to be floored 2 4 11 20
INFO: iteration 34, average log likelihood -1.081363
WARNING: Variances had to be floored 21 24 27
INFO: iteration 35, average log likelihood -1.089535
WARNING: Variances had to be floored 7 13 32
INFO: iteration 36, average log likelihood -1.075510
WARNING: Variances had to be floored 1 16 25
INFO: iteration 37, average log likelihood -1.060661
WARNING: Variances had to be floored 4 10 11 12 18 20 26 29 30
INFO: iteration 38, average log likelihood -1.040409
WARNING: Variances had to be floored 2 21 24
INFO: iteration 39, average log likelihood -1.123559
INFO: iteration 40, average log likelihood -1.109645
WARNING: Variances had to be floored 7 13 16 25 27
INFO: iteration 41, average log likelihood -1.043610
WARNING: Variances had to be floored 1 4 11 18 20 30
INFO: iteration 42, average log likelihood -1.064850
WARNING: Variances had to be floored 2 12 24 26 29
INFO: iteration 43, average log likelihood -1.089554
WARNING: Variances had to be floored 10 32
INFO: iteration 44, average log likelihood -1.111496
WARNING: Variances had to be floored 16 21 25
INFO: iteration 45, average log likelihood -1.073953
WARNING: Variances had to be floored 4 13
INFO: iteration 46, average log likelihood -1.064638
WARNING: Variances had to be floored 2 7 11 18 20 24 26
INFO: iteration 47, average log likelihood -1.041265
WARNING: Variances had to be floored 1 12 29 30
INFO: iteration 48, average log likelihood -1.087661
WARNING: Variances had to be floored 10 16 21 25 32
INFO: iteration 49, average log likelihood -1.078684
WARNING: Variances had to be floored 4
INFO: iteration 50, average log likelihood -1.100790
INFO: EM with 100000 data points 50 iterations avll -1.100790
59.0 data points per parameter
32Ã—26 Array{Float64,2}:
 -0.141977   -0.147447    0.121667     0.208914    -0.132221     0.024016     0.0666417    0.0775287    0.188532    -0.0150151   -0.185368     0.0702308    0.143436     0.061869     0.0486343   0.101673     0.161406    -0.127234     0.051388     0.252861    -0.151684     -0.116306     0.0829005   0.010878     0.0457361    0.038642  
 -0.0430515   0.0252127  -0.0123153   -0.102725     0.159041     0.0766497    0.0719573    0.111252    -0.0736378   -0.115064     0.0185917    0.00195478   0.0638335    0.206455     0.0996852   0.0460845   -0.169579     0.0504728   -0.0751907   -0.19168     -0.11576      -0.133425    -0.0130608  -0.0347363   -0.0535138    0.0493075 
  0.0895587  -0.159425    0.222178    -0.014387     0.162215     0.101387    -0.234011     0.0372143    0.0683838    0.0799483   -0.0733617   -0.0346654   -0.0370821   -0.0867139    0.105737    0.150542     0.00623683  -0.0405813   -0.0200271   -4.4836e-5   -0.0920202    -0.0745395    0.0856083   0.0358902    0.126505     0.0934522 
 -0.0477258   0.148937    0.00015717  -0.0468086   -0.0236674    0.0309484    0.179647    -0.0530585    0.0486298   -0.101336    -0.0418299    0.0501432    0.0203859    0.0655997    0.107584   -0.00670023  -0.148779     0.0593594   -0.0218374   -0.0688664    0.111423     -0.0233135    0.0607773   0.0123725    0.00394576  -0.131397  
  0.146679    0.162416    0.0613224   -0.169308     0.121022    -0.125084    -0.114282     0.0574626    0.0636227   -0.105609    -0.10898      0.00103354  -0.0338577   -0.0159143    0.0518537   0.0976353    0.0659827    0.109435    -0.0776744   -0.367352     0.0505857    -0.0445627   -0.208045    0.0196205   -0.138299     0.0309682 
  0.0203663   0.0346721   0.112061     0.00576795   0.130747     0.00459322   0.0168597   -0.249191    -0.10026      0.108012    -0.173957    -0.140551     0.0338495   -0.0137132    0.0128915   0.00501486   0.158783     0.0257907   -0.00941497   0.0207831   -0.0310149     0.189692    -0.0671257  -0.0616635   -0.0218784   -0.0714259 
 -0.0610486   0.13446     0.0446871    0.0608766    0.0869555   -0.0141273   -0.0560728   -0.074888    -0.185433    -0.220327    -0.177161    -0.018866    -0.0227983    0.2023       0.0118088  -0.0460612   -0.0963454   -0.0401625   -0.147762    -0.0821028    0.0712511    -0.0826749   -0.0368366   0.0713962    0.13641      0.0757435 
  0.0365475   0.160087   -0.142963    -0.163098     0.014763     0.183846     0.044979     0.15802     -0.112454    -0.0898026    0.0219944    0.0587287   -0.133265    -0.0358343   -0.0296423   0.114317     0.0408781    0.0559443   -0.199822     0.223776    -0.126038      0.00817333  -0.115238    0.124981    -0.15069     -0.0743981 
  0.0223894  -0.0464412   0.0736207    0.116507     0.0540234    0.0646652   -0.0897408    0.21884     -0.0490165   -0.160819     0.0736266    0.109721     0.00591494  -0.00493483  -0.0489593  -0.261857    -0.094411     0.017931    -0.0710599    0.0862073    0.104055      0.0268758    0.0224426   0.149542     0.0963122    0.0573557 
 -0.0644581   0.045935   -0.0309495   -0.0827497   -0.0262959    0.0480963    0.109246    -0.00722001  -0.043696     0.0306843   -0.123759     0.0620831   -0.0329761   -0.111059     0.105982   -0.117008     0.0599784   -0.0568834    0.0019441    0.061054    -0.0492985     0.0289336   -0.0128532   0.0236611    0.0218947    0.0109534 
  0.398629    0.202749    0.0886533    0.105353     0.0485435   -0.10911     -0.123201    -0.00496298  -0.136469     0.234526     0.116357    -0.112751    -0.0815385   -0.0917915   -0.0528542   0.0532678    0.268883    -0.00251789   0.210086    -0.0364732   -0.096372     -0.0595465    0.188641   -0.0490971   -0.0520115   -0.038115  
  0.100835    0.178888   -0.00124762   0.220874     0.00260189   0.0937737   -0.0659334    0.0690099    0.00611088  -0.128402    -0.00663249  -0.0579945    0.0373491   -0.0514176    0.0525487   0.0212168   -0.127026     0.169081    -0.0396507    0.159898     0.0470432    -0.0729324    0.0947109   0.0886267    0.00476871  -0.153787  
 -0.0168827  -0.106238   -0.232214     0.101347     0.173258     0.0717662    0.0606311    0.103832    -0.0964336   -0.295702     0.0945866   -0.0848642    0.0690887    0.0634071   -0.0247316  -0.205359    -0.0167177    0.0811656    0.136949    -0.186282    -0.034422      0.0384923   -0.152224    0.00297117   0.0439467   -0.131837  
 -0.0437174   0.115703    0.0230679    0.144057     0.0428607    0.0263089   -0.157993     0.0649771   -0.104276    -0.0974874   -0.0343104    0.0493668   -0.0140715    0.215797    -0.0413922  -0.0344094   -0.0800122    0.0812458    0.219032    -0.149206    -0.0528679    -0.0459996   -0.0410543  -0.0328541    0.118607     0.15706   
  0.0192619   0.0137654  -0.0466005   -0.143446    -0.00926627  -0.0636516    0.0476025    0.0389324   -0.0236869   -0.00275454   0.0443597    0.0298126    0.0140913   -0.166105    -0.0457677   0.0960182   -0.188866    -0.00490404  -0.0926858    0.0559138    0.0623475     0.0352657   -0.0813124  -0.100814    -0.0353599   -0.147515  
 -0.0537743   0.111158   -0.0856684   -0.12887      0.0525836   -0.131299     0.174714    -0.0480016    0.186601     0.0464872    0.204387    -0.0429502   -0.0640694   -0.0436359    0.0442628  -0.0270538   -0.0908458    0.00653798  -0.00110094   0.0102965   -0.0151937     0.0802009    0.190515   -0.0697022    0.0484225   -0.00787312
  0.111603   -0.0118718  -0.104298     0.025214     0.0622907   -0.00548249  -0.0283039    0.0650629   -0.0534838    0.095964    -0.0178674   -0.122974    -0.0270071   -0.0128187    0.0433578   0.0344067   -0.0400172   -0.00419664   0.00251155   0.0762874   -0.0120563     0.00920067  -0.0406561  -0.0931206   -0.0383108    0.0186705 
  0.121264    0.0183707   0.0157645   -0.0667468   -0.00568317   0.107843     0.0674018    0.138071    -0.0617826   -0.0844128    0.160513     0.207597     0.0428408    0.0309321    0.102514   -0.079798     0.272951     0.0514384    0.0496351    0.269886     0.187319      0.0539344   -0.0456631  -0.0897102    0.0679899   -0.0906869 
  0.0390712   0.0391974   0.0484213    0.0360913    0.0820279   -0.0291144   -0.00386956   0.145598    -0.0248029   -0.140286    -0.0703193    0.116818     0.0675559    0.0147776    0.0478442  -0.016324    -0.0676759   -0.0372836   -0.0528624    0.0698592   -0.0291871     0.0802838   -0.129356   -0.00959169  -0.0681971   -0.175863  
 -0.123952    0.058267   -0.119828    -0.0121786    0.0417614    0.167297     0.0303497    0.0628875   -0.114489     0.0597956   -0.0368024    0.0687407    0.0523326   -0.0668752   -0.0352503  -0.120029     0.116867     0.211021     0.180943     0.0200086   -0.0721316    -0.00221681  -0.0936364  -0.0381328   -0.117568     0.0919318 
 -0.172727   -0.0480115   0.08931     -0.12093      0.0471449    0.0658104   -0.134316    -0.058378    -0.107044     0.00746453  -0.137523     0.0058752   -0.0533228   -0.0341264    0.0969752  -0.152271    -0.071949     0.00617133   0.0223323   -0.0416193    0.0727742    -0.0533926   -0.0280995   0.00418902  -0.0730066    0.07939   
 -0.129744    0.187145   -0.115296    -0.106423     0.107679     0.0328447   -0.148991     0.0884317    0.0697496    0.071788    -0.205962     0.091062    -0.0158708   -0.0397847   -0.028527   -0.102953    -0.00451333   0.259901     0.127172    -0.200005    -0.0577619    -0.0952085    0.15776     0.151883     0.105094    -0.208017  
 -0.0650816   0.0187313  -0.0459745    0.0060906    0.230834    -0.0265134    0.0640234    0.00559497   0.0477409   -0.298807    -0.002564    -0.108208     0.0156857    0.0992398   -0.0870945   0.00539896   0.0859051    0.0780734    0.119725    -0.0750215    0.131052      0.0977897   -0.0756711   0.0515423   -0.236202     0.0825666 
 -0.189676    0.0506058  -0.055503     0.163304     0.213733     0.0691165   -0.0225444   -0.00394617  -0.0446283   -0.0232317   -0.0411143   -0.0422721   -0.0515496    0.230534     0.158794    0.116641     0.136595    -0.101732    -0.0642952    0.0517502   -0.0785201     0.00736113   0.0179351   0.0563021    0.135608     0.10766   
  0.0225634   0.0601062  -0.140943    -0.00818058   0.0309564    0.0178048    0.088275     0.0192785    0.0728761    0.0313367   -0.0859614    0.024549     0.0504235    0.0381154    0.105663   -0.0639155   -0.112632    -0.0765333    0.136257     0.0470167   -0.0279157     0.0568583    0.0808894   0.0739608   -0.0318363    0.036665  
 -0.0142348   0.111301   -0.0147335    0.0126214   -0.163717    -0.178812     0.00632657  -0.147785     0.0570241    0.166391    -0.0854941   -0.0356236   -0.0400187    0.0446887   -0.0594086   0.127195    -0.157751     0.0307002    0.168185    -0.116276     0.0186184     0.179852    -0.117292   -0.0866313    0.0390085   -0.284792  
 -0.0130504  -0.0570096   0.0586887   -0.097458     0.0355569    0.0113469   -0.107919     0.044199    -0.00559047   0.086458    -0.101621     0.00301219  -0.00428957   0.0199861    0.0382231  -0.147814    -0.00134509  -0.0108842    0.0384958   -0.185843     0.172026      0.088408    -0.0328588  -0.00198843   0.00781929   0.0811176 
 -0.0807596  -0.0249097  -0.13938      0.0287914    0.0562424    0.0290722   -0.0526733   -0.0553741   -0.0458316    0.0206943    0.0820835   -0.053874     0.0741274   -0.154493    -0.0718266   0.160469    -0.0503054    0.0182899   -0.106601    -0.115453    -0.000795347   0.0185497    0.0167543   0.097448     0.132065     0.0471756 
  0.151246   -0.130963    0.0921951   -0.0235428    0.0673021    0.0497193   -0.0651753    0.0939931    0.109592     0.148813    -0.124552     0.0224354    0.0831385    0.00580777  -0.13222    -0.115674     0.00569701  -0.0729015    0.02186      0.00888182   0.131112      0.120046    -0.0124323   0.0624371    0.168826    -0.0330169 
  0.133805    0.10569     0.0058909    0.170674     0.064654     0.189229    -0.0238981    0.0934935   -0.0339256   -0.0280837   -0.141093    -0.19564     -0.265391    -0.0448607    0.0860152  -0.167504     0.0793452    0.089828    -0.0816435    0.00491716   0.140485      0.0612362   -0.0196775   0.00930798  -0.093818    -0.0919797 
  0.0487869  -0.0710026  -0.189483     0.0536996    0.0603736    0.12634     -0.0708876   -0.032061     0.0899185    0.117004     0.152581     0.070508    -0.107063     0.0232666   -0.0387016   0.0346688    0.0192774   -0.029026    -0.168967     0.130073    -0.0459789     0.0989045    0.0768399   0.0914597    0.161103    -0.0306562 
  0.404729   -0.225802    0.00440343  -0.102123     0.0645455   -0.0761967   -0.0374427    0.0884918    0.0967678    0.167405    -0.271959    -0.0650442   -0.0299643   -0.00463688  -0.0332459  -0.186234     0.0641253   -0.0608392    0.0353315    0.0195656    0.0880764     0.0572201   -0.0515357  -0.122946     0.0769058    0.0161196 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 13 18 24
INFO: iteration 1, average log likelihood -1.064907
WARNING: Variances had to be floored 1 2 7 11 13 16 18 20 24 26
INFO: iteration 2, average log likelihood -1.012928
WARNING: Variances had to be floored 10 12 13 18 24 25 29 30 32
INFO: iteration 3, average log likelihood -1.004749
WARNING: Variances had to be floored 1 2 4 7 11 13 16 18 20 21 24 26 27
INFO: iteration 4, average log likelihood -1.000378
WARNING: Variances had to be floored 12 13 18 24
INFO: iteration 5, average log likelihood -1.036550
WARNING: Variances had to be floored 1 2 7 11 13 16 18 20 24 25 26 27 30
INFO: iteration 6, average log likelihood -0.993699
WARNING: Variances had to be floored 4 10 12 13 18 24 29 32
INFO: iteration 7, average log likelihood -1.017687
WARNING: Variances had to be floored 2 7 11 13 18 20 21 24 26
INFO: iteration 8, average log likelihood -1.008979
WARNING: Variances had to be floored 1 12 13 16 18 24 25 27
INFO: iteration 9, average log likelihood -1.001533
WARNING: Variances had to be floored 2 4 7 11 13 18 20 24 26 29 30 32
INFO: iteration 10, average log likelihood -1.008636
INFO: EM with 100000 data points 10 iterations avll -1.008636
59.0 data points per parameter
32Ã—26 Array{Float64,2}:
 -0.0218212    0.0498651     0.0268939    7.33243e-5   0.198637     -0.0394104  -0.151508     0.126166     0.0343167   -0.00383087  -0.057435      0.0900594   0.0893695    0.150961     0.129978     0.189265     0.0442069   -0.0163388    0.0900979    -0.139206    -0.0702588   0.0243196    0.0503928   -0.0937241    0.00383195  -0.18844   
  0.0131166    0.110643      0.157566    -0.0608183   -0.0372201     0.0377614  -0.0704285   -0.0968371    0.0435215    0.0614735   -0.0721942    -0.109193    0.187749    -0.0642244    0.150901    -0.00809104   0.00210761  -0.171685    -0.145303     -0.0765636   -0.101377    0.138055    -0.150946     0.0339643   -0.0525299    0.0227135 
 -0.0579987    0.0789075    -0.0197819   -0.0613351    0.020857     -0.086106    0.0384629   -0.0387441    0.198261     0.0228391   -0.107102      0.0803204  -0.0629651    0.0718394   -0.104123    -0.0770123   -0.0721515    0.0323106   -0.127872     -0.0685478    0.0325736   0.0433494    0.0517797   -0.0802448    0.0175396    0.0121415 
 -0.138782     0.0139495     0.0453671    0.128668    -0.0557147     0.106608   -0.0642275   -0.0764107    0.0348628   -0.12984     -0.0115731    -0.0142774   0.0577483    0.0429326    0.0369887    0.083265     0.0718224   -0.0875006   -0.0269346    -0.095193    -0.0811068   0.18957     -0.0461924    0.038413     0.0252422   -0.150195  
  0.106247    -0.159459     -0.0673056   -0.033295    -0.0215182     0.0144262   0.041997    -0.122497    -0.00994208   0.0490884    0.0178499    -0.0339983   0.108164     0.222005    -0.0220788    0.0377397   -0.176236     0.0862349   -0.155165     -0.00112176  -0.131538    0.136034     0.00760682   0.0450178   -0.0696202    0.115473  
  0.130171    -0.0129401     0.0176623    0.109145    -0.090726     -0.0297926   0.149175    -6.63615e-5   0.110117     0.182381     0.00618792   -0.0578566   0.07099      0.00164433   0.0528134   -0.181485    -0.0923806    0.117975    -0.0241285    -0.0992622    0.0229011   0.172948    -0.028353     0.122363     0.0401368   -0.0406894 
 -0.15132     -0.152377      0.0327674   -0.210981    -0.019521      0.0136532   0.00359462  -0.155583    -0.0953563   -0.0783825   -0.0302237     0.021901    0.0967601   -0.022445     0.0588407   -0.168816    -0.0123289   -0.00827402  -0.00655032   -0.0225841   -0.0162561   0.130631    -0.126158    -0.0942115    0.0285577   -0.0198241 
 -0.0458748    0.0531628     0.108419     0.0152988   -0.158047      0.03916     0.0140241   -0.020123    -0.195503    -0.0431535   -0.0365567     0.0278752   0.00621784   0.0327299   -0.0900608    0.0710148   -0.0434239   -0.1673       0.0756562     0.0552506   -0.0216127  -0.0825982    0.0897533   -0.152009     0.0481985    0.0289094 
  0.0243769   -0.07285      -0.0535726   -0.0816855    0.134144     -0.12306     0.0608019    0.16914      0.0240962   -0.24871     -0.119076     -0.153018   -0.321282     0.00827669  -0.126009     0.0051451    0.196022     0.0989769   -0.00910384   -0.148575     0.164208    0.0301515    0.0290347    0.0335662    0.120799    -0.0441053 
 -0.0731007   -0.000757361   0.00941593  -0.090589     0.0378368     0.20334    -0.0328112   -0.021379    -0.0587072    0.0242348    0.00734312    0.0770614  -0.16131     -0.0651313    0.0228767    0.0872604   -0.125153    -0.0258249    0.0774556     0.108187    -0.27686    -0.119129     0.0719196    0.00362761   0.0423405    0.0590252 
 -0.0177752   -0.13279       0.040299     0.0486255    0.0016348    -0.0682469   0.117334     0.0349263    0.130248     0.122382    -0.0953811     0.0952839   0.103843     0.20469     -0.0134864   -0.0049207    0.0802698   -0.12195     -0.0336865    -0.00237307   0.0727773  -0.0654621    0.0046285   -0.13951     -0.030538    -0.0518745 
 -0.0143208   -0.0362223     0.0948256   -0.0237157    0.0367116     0.0205264  -0.0350246   -0.0230547    0.0357209   -0.0369365    0.0867009     0.0318833   0.0742815   -0.128848     0.109934     0.070333    -0.0477371   -0.105896    -0.000945087   0.0368424    0.0284799   0.00402933  -0.0263075    0.0437093    0.172951     0.027262  
 -0.0988947   -0.0320707     0.0432274   -0.0383515   -0.0337563     0.184706    0.166637    -0.0532994    0.13278     -0.0941266    0.125672      0.215237   -0.018198    -0.0367454   -0.0706632    0.0428437    0.271872     0.192223    -0.0403671     0.24792      0.0156978   0.0293527    0.0392396    0.0370918   -0.0940278    0.012057  
 -0.186408    -0.029467     -0.0232962    0.196937     0.0726732    -0.0221294  -0.112797    -0.0418152    0.0860156   -0.0309145    0.063077     -0.159116   -0.0897273   -0.0274438   -0.168042     0.158241     0.0584409   -0.0767734   -0.144601     -0.0352004    0.0144313  -0.0500693    0.0394151   -0.141073    -0.0696343    0.107649  
  0.263379    -0.0858655    -0.104025     0.151162    -0.090925      0.124296   -0.0658406    0.0704885    0.0258787    0.0122225   -0.171437     -0.185134   -0.0648813   -0.0562637    0.051974    -0.0610258   -0.0716376    0.0369906   -0.109954     -0.146066    -0.0882475   0.0214851    0.0385099   -0.0420705   -0.120725     0.00330435
 -0.0295334   -0.00590752   -0.176613     0.00766018   0.151456     -0.0865101   0.0202933   -0.118548     0.112555     0.0173172   -0.0301938     0.106935    0.101784     0.0509463   -0.113887    -0.0456397    0.00210739   0.222651    -0.00724872   -0.158977     0.150768   -0.176944     0.0587576   -0.022614     0.0309742    0.0287514 
 -0.0703653    0.019369     -0.0330257   -0.164245    -0.0373926    -0.0145204   0.0833133    0.300839     0.101833     0.0425904    0.0539918     0.049736   -0.00655635  -0.0241002    0.0110381   -0.0821878   -0.00425498  -0.022102    -0.107748      0.00597176   0.112567   -0.0124049   -0.0876453    0.0822809    0.104639     0.103123  
  0.0137154    0.0834506    -0.114213     0.0148978    0.0624467    -0.134335   -0.130472    -0.125588     0.191678     0.108039     0.0373958    -0.0498705  -0.0342815    0.0275803    0.00156211   0.156332     0.106668    -0.203592    -0.000114108  -0.226519     0.0267307  -0.0293638    0.109174    -0.00495525  -0.0435775   -0.105605  
  0.124807    -0.427571      0.15179     -0.045861    -0.178254      0.064506    0.108641    -0.156169     0.118221    -0.0032334    0.000289176  -0.0571747  -0.0453126   -0.011229     0.186045     0.0386242    0.0816297   -0.0862835    0.0480032     0.0718597    0.174469    0.0221729   -0.0582279    0.0805452   -0.101425     0.0955515 
 -0.0675703    0.124558     -0.139024    -0.107322    -0.0353597    -0.023186   -0.0313019   -0.26092      0.029612     0.0410823   -0.0592103     0.0865339  -0.0268603   -0.00411463   0.107904    -0.0949199   -0.034996    -0.0756289    0.0427636    -0.0254886   -0.0471702   0.00713878  -0.0854141   -0.0601125   -0.0618917   -0.138417  
  0.046776     0.0545779    -0.20187     -0.0229574    0.0694573    -0.0312596   0.0694051    0.132687    -0.0686972   -0.0146853    0.0456862    -0.0457351   0.0934326   -0.112551     0.110008    -0.0118646   -0.0901633   -0.152264     0.157875     -0.126753     0.0760367   0.102301     0.00380436  -0.0927688   -0.0842045    0.0527875 
  0.0540805   -0.0110209     0.136271     0.13089      0.0808695    -0.0678273  -0.200844     0.0593472    0.0865341    0.0330108   -0.00331832   -0.09046     0.0874926   -0.0541191    0.085253    -0.129191    -0.0557808   -0.0102104    0.15316      -0.076446    -0.0760461   0.14411      0.0602752    0.113637    -0.136679     0.00704018
 -0.0191737   -0.0642267     0.024308    -0.0102739    0.100149     -0.0429756   0.0252462   -0.0975161    0.172869    -0.0824474    0.0445074    -0.0741179  -0.232084     0.0756514   -0.018691    -0.00301088  -0.0453479   -0.0526696    0.206712     -0.0618732   -0.0617689  -0.103437    -0.129839    -0.017634    -0.044845    -0.0517053 
  0.00434948   0.160112      0.0196547    0.127591    -0.166946     -0.0444203   0.00143887  -0.0152836   -0.0840111   -0.0265289    0.0116997    -0.109113   -0.080206     0.013089    -0.215355     0.0740138   -0.0712026   -0.209667    -0.0735785    -0.127757    -0.138484    0.110403     0.056799    -0.144342    -0.0242459    0.0583    
  0.075419    -0.104424      0.0212827   -0.0561569   -0.0222198     0.167664    0.11984      0.0391183   -0.0892841    0.0881606    0.0675209    -0.0754956  -0.124516     0.0170788   -0.0239083   -0.0982758   -0.033401     0.137052     0.208677      0.0614089    0.0194163   0.166723     0.0151996   -0.114191    -0.0558399   -0.0882027 
 -0.0137713   -0.0511108     0.0986389   -0.187763    -0.0395701    -0.194916   -0.0237136   -0.0109564   -0.0807006    0.196334    -0.148771     -0.0325765   0.0234002   -0.0685327   -0.149262     0.00198712   0.0436353   -0.170241    -0.195338     -0.0727499   -0.0467996   0.137371    -0.176797     0.0951616   -0.0220881    0.205204  
  0.0362956   -0.0445939    -0.0633016    0.084848     0.205884      0.0214949   0.0598211   -0.00967853  -0.0298895    0.133413     0.109773      0.0607963   0.0245993   -0.0341637   -0.0827848    0.0993358    0.130047     0.114001     0.0945761    -0.124028    -0.0742859   0.0904439   -0.0692065    0.0392091    0.110659    -0.201882  
 -0.0288678    0.0230978     0.252743    -0.0190251    0.210727     -0.121745    0.0841686    0.291598    -0.00655771  -0.0317011   -0.0261662    -0.0555299   0.138949     0.0395699   -0.0092783    0.028395    -0.0775559   -0.115777    -0.0689171     0.141172     0.0401279   0.227162     0.172823     0.0446464   -0.119352     0.03486   
 -0.0746314    0.027615      0.110093     0.0244472    0.000629636   0.0508305  -0.215251    -0.0789458   -0.0336332   -0.019658     0.176998     -0.0383566  -0.00426953   0.128334    -0.312172    -0.0512536   -0.0789084    0.106538    -0.0213929     0.0408732    0.0561023  -0.00815315   0.097897     0.0457355    0.0540985    0.0194386 
 -0.0650146   -0.13091       0.0392426    0.178735     0.132105      0.109306   -0.00209957  -0.11383     -0.0820222    0.0560983   -0.00265892    0.0770258   0.214525    -0.0382207   -0.0402036   -0.11958     -0.103277    -0.203862     0.0124759    -0.080364     0.161694   -0.109715    -0.0929233    0.103391     0.0469412    0.184849  
  0.0576799   -0.119412      0.106285    -0.0250855   -0.0196768     0.0275948  -0.0034929    0.0420614    0.07083     -0.0393783   -0.0979227    -0.0806141  -0.00104018  -0.101251    -0.0619857   -0.0789095    0.0447906   -0.0472245   -0.0955481     0.176942    -0.132636    0.174624    -0.0869384    0.0993831    0.0780191   -0.0417486 
  0.136197     0.108385      0.125163     0.113363    -0.11782       0.0448184   0.073086     0.135724    -0.0775992    0.033093     0.107131     -0.058854    0.0663456   -0.0669474    0.066746     0.107246     0.204176     0.0628558    0.0262958    -0.0196905    0.0770055  -0.0813608    0.0148251    0.0099831   -0.0746423   -0.149566  kind full, method split
0: avll = -1.4260879691306285
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.426107
INFO: iteration 2, average log likelihood -1.426026
INFO: iteration 3, average log likelihood -1.425962
INFO: iteration 4, average log likelihood -1.425886
INFO: iteration 5, average log likelihood -1.425797
INFO: iteration 6, average log likelihood -1.425696
INFO: iteration 7, average log likelihood -1.425594
INFO: iteration 8, average log likelihood -1.425503
INFO: iteration 9, average log likelihood -1.425430
INFO: iteration 10, average log likelihood -1.425374
INFO: iteration 11, average log likelihood -1.425328
INFO: iteration 12, average log likelihood -1.425275
INFO: iteration 13, average log likelihood -1.425191
INFO: iteration 14, average log likelihood -1.425036
INFO: iteration 15, average log likelihood -1.424748
INFO: iteration 16, average log likelihood -1.424248
INFO: iteration 17, average log likelihood -1.423498
INFO: iteration 18, average log likelihood -1.422605
INFO: iteration 19, average log likelihood -1.421821
INFO: iteration 20, average log likelihood -1.421315
INFO: iteration 21, average log likelihood -1.421055
INFO: iteration 22, average log likelihood -1.420937
INFO: iteration 23, average log likelihood -1.420885
INFO: iteration 24, average log likelihood -1.420862
INFO: iteration 25, average log likelihood -1.420852
INFO: iteration 26, average log likelihood -1.420847
INFO: iteration 27, average log likelihood -1.420845
INFO: iteration 28, average log likelihood -1.420843
INFO: iteration 29, average log likelihood -1.420842
INFO: iteration 30, average log likelihood -1.420841
INFO: iteration 31, average log likelihood -1.420841
INFO: iteration 32, average log likelihood -1.420840
INFO: iteration 33, average log likelihood -1.420840
INFO: iteration 34, average log likelihood -1.420839
INFO: iteration 35, average log likelihood -1.420839
INFO: iteration 36, average log likelihood -1.420839
INFO: iteration 37, average log likelihood -1.420839
INFO: iteration 38, average log likelihood -1.420838
INFO: iteration 39, average log likelihood -1.420838
INFO: iteration 40, average log likelihood -1.420838
INFO: iteration 41, average log likelihood -1.420838
INFO: iteration 42, average log likelihood -1.420838
INFO: iteration 43, average log likelihood -1.420837
INFO: iteration 44, average log likelihood -1.420837
INFO: iteration 45, average log likelihood -1.420837
INFO: iteration 46, average log likelihood -1.420837
INFO: iteration 47, average log likelihood -1.420837
INFO: iteration 48, average log likelihood -1.420837
INFO: iteration 49, average log likelihood -1.420837
INFO: iteration 50, average log likelihood -1.420837
INFO: EM with 100000 data points 50 iterations avll -1.420837
952.4 data points per parameter
1: avll = [-1.42611,-1.42603,-1.42596,-1.42589,-1.4258,-1.4257,-1.42559,-1.4255,-1.42543,-1.42537,-1.42533,-1.42527,-1.42519,-1.42504,-1.42475,-1.42425,-1.4235,-1.42261,-1.42182,-1.42132,-1.42106,-1.42094,-1.42089,-1.42086,-1.42085,-1.42085,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.420856
INFO: iteration 2, average log likelihood -1.420772
INFO: iteration 3, average log likelihood -1.420705
INFO: iteration 4, average log likelihood -1.420626
INFO: iteration 5, average log likelihood -1.420532
INFO: iteration 6, average log likelihood -1.420429
INFO: iteration 7, average log likelihood -1.420327
INFO: iteration 8, average log likelihood -1.420239
INFO: iteration 9, average log likelihood -1.420170
INFO: iteration 10, average log likelihood -1.420120
INFO: iteration 11, average log likelihood -1.420084
INFO: iteration 12, average log likelihood -1.420058
INFO: iteration 13, average log likelihood -1.420038
INFO: iteration 14, average log likelihood -1.420020
INFO: iteration 15, average log likelihood -1.420005
INFO: iteration 16, average log likelihood -1.419990
INFO: iteration 17, average log likelihood -1.419976
INFO: iteration 18, average log likelihood -1.419962
INFO: iteration 19, average log likelihood -1.419947
INFO: iteration 20, average log likelihood -1.419932
INFO: iteration 21, average log likelihood -1.419916
INFO: iteration 22, average log likelihood -1.419901
INFO: iteration 23, average log likelihood -1.419885
INFO: iteration 24, average log likelihood -1.419870
INFO: iteration 25, average log likelihood -1.419855
INFO: iteration 26, average log likelihood -1.419842
INFO: iteration 27, average log likelihood -1.419829
INFO: iteration 28, average log likelihood -1.419817
INFO: iteration 29, average log likelihood -1.419807
INFO: iteration 30, average log likelihood -1.419797
INFO: iteration 31, average log likelihood -1.419789
INFO: iteration 32, average log likelihood -1.419782
INFO: iteration 33, average log likelihood -1.419776
INFO: iteration 34, average log likelihood -1.419771
INFO: iteration 35, average log likelihood -1.419767
INFO: iteration 36, average log likelihood -1.419763
INFO: iteration 37, average log likelihood -1.419760
INFO: iteration 38, average log likelihood -1.419757
INFO: iteration 39, average log likelihood -1.419754
INFO: iteration 40, average log likelihood -1.419752
INFO: iteration 41, average log likelihood -1.419750
INFO: iteration 42, average log likelihood -1.419748
INFO: iteration 43, average log likelihood -1.419747
INFO: iteration 44, average log likelihood -1.419746
INFO: iteration 45, average log likelihood -1.419744
INFO: iteration 46, average log likelihood -1.419743
INFO: iteration 47, average log likelihood -1.419742
INFO: iteration 48, average log likelihood -1.419741
INFO: iteration 49, average log likelihood -1.419740
INFO: iteration 50, average log likelihood -1.419740
INFO: EM with 100000 data points 50 iterations avll -1.419740
473.9 data points per parameter
2: avll = [-1.42086,-1.42077,-1.4207,-1.42063,-1.42053,-1.42043,-1.42033,-1.42024,-1.42017,-1.42012,-1.42008,-1.42006,-1.42004,-1.42002,-1.42001,-1.41999,-1.41998,-1.41996,-1.41995,-1.41993,-1.41992,-1.4199,-1.41989,-1.41987,-1.41986,-1.41984,-1.41983,-1.41982,-1.41981,-1.4198,-1.41979,-1.41978,-1.41978,-1.41977,-1.41977,-1.41976,-1.41976,-1.41976,-1.41975,-1.41975,-1.41975,-1.41975,-1.41975,-1.41975,-1.41974,-1.41974,-1.41974,-1.41974,-1.41974,-1.41974]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.419751
INFO: iteration 2, average log likelihood -1.419675
INFO: iteration 3, average log likelihood -1.419603
INFO: iteration 4, average log likelihood -1.419512
INFO: iteration 5, average log likelihood -1.419397
INFO: iteration 6, average log likelihood -1.419254
INFO: iteration 7, average log likelihood -1.419091
INFO: iteration 8, average log likelihood -1.418922
INFO: iteration 9, average log likelihood -1.418763
INFO: iteration 10, average log likelihood -1.418627
INFO: iteration 11, average log likelihood -1.418517
INFO: iteration 12, average log likelihood -1.418432
INFO: iteration 13, average log likelihood -1.418368
INFO: iteration 14, average log likelihood -1.418320
INFO: iteration 15, average log likelihood -1.418284
INFO: iteration 16, average log likelihood -1.418256
INFO: iteration 17, average log likelihood -1.418234
INFO: iteration 18, average log likelihood -1.418217
INFO: iteration 19, average log likelihood -1.418203
INFO: iteration 20, average log likelihood -1.418191
INFO: iteration 21, average log likelihood -1.418180
INFO: iteration 22, average log likelihood -1.418171
INFO: iteration 23, average log likelihood -1.418162
INFO: iteration 24, average log likelihood -1.418154
INFO: iteration 25, average log likelihood -1.418147
INFO: iteration 26, average log likelihood -1.418140
INFO: iteration 27, average log likelihood -1.418134
INFO: iteration 28, average log likelihood -1.418128
INFO: iteration 29, average log likelihood -1.418122
INFO: iteration 30, average log likelihood -1.418116
INFO: iteration 31, average log likelihood -1.418111
INFO: iteration 32, average log likelihood -1.418106
INFO: iteration 33, average log likelihood -1.418101
INFO: iteration 34, average log likelihood -1.418097
INFO: iteration 35, average log likelihood -1.418092
INFO: iteration 36, average log likelihood -1.418088
INFO: iteration 37, average log likelihood -1.418085
INFO: iteration 38, average log likelihood -1.418081
INFO: iteration 39, average log likelihood -1.418078
INFO: iteration 40, average log likelihood -1.418074
INFO: iteration 41, average log likelihood -1.418071
INFO: iteration 42, average log likelihood -1.418068
INFO: iteration 43, average log likelihood -1.418065
INFO: iteration 44, average log likelihood -1.418063
INFO: iteration 45, average log likelihood -1.418060
INFO: iteration 46, average log likelihood -1.418058
INFO: iteration 47, average log likelihood -1.418055
INFO: iteration 48, average log likelihood -1.418053
INFO: iteration 49, average log likelihood -1.418051
INFO: iteration 50, average log likelihood -1.418048
INFO: EM with 100000 data points 50 iterations avll -1.418048
236.4 data points per parameter
3: avll = [-1.41975,-1.41968,-1.4196,-1.41951,-1.4194,-1.41925,-1.41909,-1.41892,-1.41876,-1.41863,-1.41852,-1.41843,-1.41837,-1.41832,-1.41828,-1.41826,-1.41823,-1.41822,-1.4182,-1.41819,-1.41818,-1.41817,-1.41816,-1.41815,-1.41815,-1.41814,-1.41813,-1.41813,-1.41812,-1.41812,-1.41811,-1.41811,-1.4181,-1.4181,-1.41809,-1.41809,-1.41808,-1.41808,-1.41808,-1.41807,-1.41807,-1.41807,-1.41807,-1.41806,-1.41806,-1.41806,-1.41806,-1.41805,-1.41805,-1.41805]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.418056
INFO: iteration 2, average log likelihood -1.418006
INFO: iteration 3, average log likelihood -1.417962
INFO: iteration 4, average log likelihood -1.417912
INFO: iteration 5, average log likelihood -1.417851
INFO: iteration 6, average log likelihood -1.417777
INFO: iteration 7, average log likelihood -1.417689
INFO: iteration 8, average log likelihood -1.417588
INFO: iteration 9, average log likelihood -1.417478
INFO: iteration 10, average log likelihood -1.417366
INFO: iteration 11, average log likelihood -1.417257
INFO: iteration 12, average log likelihood -1.417157
INFO: iteration 13, average log likelihood -1.417067
INFO: iteration 14, average log likelihood -1.416988
INFO: iteration 15, average log likelihood -1.416921
INFO: iteration 16, average log likelihood -1.416863
INFO: iteration 17, average log likelihood -1.416814
INFO: iteration 18, average log likelihood -1.416771
INFO: iteration 19, average log likelihood -1.416734
INFO: iteration 20, average log likelihood -1.416701
INFO: iteration 21, average log likelihood -1.416672
INFO: iteration 22, average log likelihood -1.416646
INFO: iteration 23, average log likelihood -1.416622
INFO: iteration 24, average log likelihood -1.416600
INFO: iteration 25, average log likelihood -1.416581
INFO: iteration 26, average log likelihood -1.416562
INFO: iteration 27, average log likelihood -1.416546
INFO: iteration 28, average log likelihood -1.416530
INFO: iteration 29, average log likelihood -1.416515
INFO: iteration 30, average log likelihood -1.416501
INFO: iteration 31, average log likelihood -1.416488
INFO: iteration 32, average log likelihood -1.416476
INFO: iteration 33, average log likelihood -1.416464
INFO: iteration 34, average log likelihood -1.416453
INFO: iteration 35, average log likelihood -1.416442
INFO: iteration 36, average log likelihood -1.416432
INFO: iteration 37, average log likelihood -1.416422
INFO: iteration 38, average log likelihood -1.416412
INFO: iteration 39, average log likelihood -1.416402
INFO: iteration 40, average log likelihood -1.416393
INFO: iteration 41, average log likelihood -1.416384
INFO: iteration 42, average log likelihood -1.416375
INFO: iteration 43, average log likelihood -1.416367
INFO: iteration 44, average log likelihood -1.416358
INFO: iteration 45, average log likelihood -1.416350
INFO: iteration 46, average log likelihood -1.416341
INFO: iteration 47, average log likelihood -1.416333
INFO: iteration 48, average log likelihood -1.416325
INFO: iteration 49, average log likelihood -1.416317
INFO: iteration 50, average log likelihood -1.416309
INFO: EM with 100000 data points 50 iterations avll -1.416309
118.1 data points per parameter
4: avll = [-1.41806,-1.41801,-1.41796,-1.41791,-1.41785,-1.41778,-1.41769,-1.41759,-1.41748,-1.41737,-1.41726,-1.41716,-1.41707,-1.41699,-1.41692,-1.41686,-1.41681,-1.41677,-1.41673,-1.4167,-1.41667,-1.41665,-1.41662,-1.4166,-1.41658,-1.41656,-1.41655,-1.41653,-1.41652,-1.4165,-1.41649,-1.41648,-1.41646,-1.41645,-1.41644,-1.41643,-1.41642,-1.41641,-1.4164,-1.41639,-1.41638,-1.41638,-1.41637,-1.41636,-1.41635,-1.41634,-1.41633,-1.41632,-1.41632,-1.41631]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.416308
INFO: iteration 2, average log likelihood -1.416246
INFO: iteration 3, average log likelihood -1.416189
INFO: iteration 4, average log likelihood -1.416125
INFO: iteration 5, average log likelihood -1.416047
INFO: iteration 6, average log likelihood -1.415952
INFO: iteration 7, average log likelihood -1.415836
INFO: iteration 8, average log likelihood -1.415702
INFO: iteration 9, average log likelihood -1.415554
INFO: iteration 10, average log likelihood -1.415398
INFO: iteration 11, average log likelihood -1.415242
INFO: iteration 12, average log likelihood -1.415094
INFO: iteration 13, average log likelihood -1.414956
INFO: iteration 14, average log likelihood -1.414832
INFO: iteration 15, average log likelihood -1.414722
INFO: iteration 16, average log likelihood -1.414627
INFO: iteration 17, average log likelihood -1.414544
INFO: iteration 18, average log likelihood -1.414472
INFO: iteration 19, average log likelihood -1.414409
INFO: iteration 20, average log likelihood -1.414354
INFO: iteration 21, average log likelihood -1.414306
INFO: iteration 22, average log likelihood -1.414263
INFO: iteration 23, average log likelihood -1.414224
INFO: iteration 24, average log likelihood -1.414188
INFO: iteration 25, average log likelihood -1.414155
INFO: iteration 26, average log likelihood -1.414123
INFO: iteration 27, average log likelihood -1.414094
INFO: iteration 28, average log likelihood -1.414066
INFO: iteration 29, average log likelihood -1.414040
INFO: iteration 30, average log likelihood -1.414014
INFO: iteration 31, average log likelihood -1.413989
INFO: iteration 32, average log likelihood -1.413965
INFO: iteration 33, average log likelihood -1.413942
INFO: iteration 34, average log likelihood -1.413919
INFO: iteration 35, average log likelihood -1.413897
INFO: iteration 36, average log likelihood -1.413875
INFO: iteration 37, average log likelihood -1.413854
INFO: iteration 38, average log likelihood -1.413833
INFO: iteration 39, average log likelihood -1.413814
INFO: iteration 40, average log likelihood -1.413794
INFO: iteration 41, average log likelihood -1.413776
INFO: iteration 42, average log likelihood -1.413758
INFO: iteration 43, average log likelihood -1.413741
INFO: iteration 44, average log likelihood -1.413724
INFO: iteration 45, average log likelihood -1.413708
INFO: iteration 46, average log likelihood -1.413693
INFO: iteration 47, average log likelihood -1.413678
INFO: iteration 48, average log likelihood -1.413664
INFO: iteration 49, average log likelihood -1.413651
INFO: iteration 50, average log likelihood -1.413638
INFO: EM with 100000 data points 50 iterations avll -1.413638
59.0 data points per parameter
5: avll = [-1.41631,-1.41625,-1.41619,-1.41613,-1.41605,-1.41595,-1.41584,-1.4157,-1.41555,-1.4154,-1.41524,-1.41509,-1.41496,-1.41483,-1.41472,-1.41463,-1.41454,-1.41447,-1.41441,-1.41435,-1.41431,-1.41426,-1.41422,-1.41419,-1.41415,-1.41412,-1.41409,-1.41407,-1.41404,-1.41401,-1.41399,-1.41397,-1.41394,-1.41392,-1.4139,-1.41387,-1.41385,-1.41383,-1.41381,-1.41379,-1.41378,-1.41376,-1.41374,-1.41372,-1.41371,-1.41369,-1.41368,-1.41366,-1.41365,-1.41364]
[-1.42609,-1.42611,-1.42603,-1.42596,-1.42589,-1.4258,-1.4257,-1.42559,-1.4255,-1.42543,-1.42537,-1.42533,-1.42527,-1.42519,-1.42504,-1.42475,-1.42425,-1.4235,-1.42261,-1.42182,-1.42132,-1.42106,-1.42094,-1.42089,-1.42086,-1.42085,-1.42085,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42084,-1.42086,-1.42077,-1.4207,-1.42063,-1.42053,-1.42043,-1.42033,-1.42024,-1.42017,-1.42012,-1.42008,-1.42006,-1.42004,-1.42002,-1.42001,-1.41999,-1.41998,-1.41996,-1.41995,-1.41993,-1.41992,-1.4199,-1.41989,-1.41987,-1.41986,-1.41984,-1.41983,-1.41982,-1.41981,-1.4198,-1.41979,-1.41978,-1.41978,-1.41977,-1.41977,-1.41976,-1.41976,-1.41976,-1.41975,-1.41975,-1.41975,-1.41975,-1.41975,-1.41975,-1.41974,-1.41974,-1.41974,-1.41974,-1.41974,-1.41974,-1.41975,-1.41968,-1.4196,-1.41951,-1.4194,-1.41925,-1.41909,-1.41892,-1.41876,-1.41863,-1.41852,-1.41843,-1.41837,-1.41832,-1.41828,-1.41826,-1.41823,-1.41822,-1.4182,-1.41819,-1.41818,-1.41817,-1.41816,-1.41815,-1.41815,-1.41814,-1.41813,-1.41813,-1.41812,-1.41812,-1.41811,-1.41811,-1.4181,-1.4181,-1.41809,-1.41809,-1.41808,-1.41808,-1.41808,-1.41807,-1.41807,-1.41807,-1.41807,-1.41806,-1.41806,-1.41806,-1.41806,-1.41805,-1.41805,-1.41805,-1.41806,-1.41801,-1.41796,-1.41791,-1.41785,-1.41778,-1.41769,-1.41759,-1.41748,-1.41737,-1.41726,-1.41716,-1.41707,-1.41699,-1.41692,-1.41686,-1.41681,-1.41677,-1.41673,-1.4167,-1.41667,-1.41665,-1.41662,-1.4166,-1.41658,-1.41656,-1.41655,-1.41653,-1.41652,-1.4165,-1.41649,-1.41648,-1.41646,-1.41645,-1.41644,-1.41643,-1.41642,-1.41641,-1.4164,-1.41639,-1.41638,-1.41638,-1.41637,-1.41636,-1.41635,-1.41634,-1.41633,-1.41632,-1.41632,-1.41631,-1.41631,-1.41625,-1.41619,-1.41613,-1.41605,-1.41595,-1.41584,-1.4157,-1.41555,-1.4154,-1.41524,-1.41509,-1.41496,-1.41483,-1.41472,-1.41463,-1.41454,-1.41447,-1.41441,-1.41435,-1.41431,-1.41426,-1.41422,-1.41419,-1.41415,-1.41412,-1.41409,-1.41407,-1.41404,-1.41401,-1.41399,-1.41397,-1.41394,-1.41392,-1.4139,-1.41387,-1.41385,-1.41383,-1.41381,-1.41379,-1.41378,-1.41376,-1.41374,-1.41372,-1.41371,-1.41369,-1.41368,-1.41366,-1.41365,-1.41364]
32Ã—26 Array{Float64,2}:
  0.398937     0.0240504   -0.162387   -0.371687    -0.418751    -0.551841    -0.133        0.106899     -0.742757    -0.0618643    0.0210021  -0.213753    0.14376     0.54148    -0.314212    -0.16837     0.0299244   0.046233     0.481944   -0.488344     0.663274     0.23389      -0.668097    -0.189738     0.164438     0.122267  
  0.0607153   -0.134377     0.0710083   0.0327767   -0.153598     0.120761     0.241814     0.0859192     0.204738    -0.0835016    0.33364    -0.463351   -0.18971    -0.138311   -0.0572066    0.300997    0.073592   -0.00990718   0.144033   -0.472887     0.549353     0.164021     -0.156209     0.112084    -0.310954    -0.0659084 
 -0.139149     0.240146    -0.223118    0.188616    -0.306288    -0.0191065    0.405592    -0.41858      -0.745399     0.706459     0.0553828  -0.84987     0.156309    0.332342    0.320581     0.0930577   0.0935643  -0.799627    -0.02554    -0.382871     0.670835     0.152708      0.0716836    0.205955     0.00144399  -0.329964  
 -0.216471     1.0682      -0.0146856  -0.348002    -0.0210037   -0.173096     0.301341    -0.139118     -0.504468     0.144487    -0.0198417   0.317817   -0.125506    0.280674    0.444611     0.155933   -0.30733    -0.31883      0.322274   -0.108435     0.323029     0.212053     -0.268388    -0.0949002   -0.354667     0.122999  
 -0.03837     -0.0269923    0.163913   -0.0503169    0.440791    -0.0630804   -0.0946336   -0.449331     -0.500291    -0.0777711   -0.544912    0.449169   -0.012829    0.178228   -0.545662    -0.282995   -0.19268     0.0603185    0.189695    0.249611    -0.322015    -0.364612     -0.0858689   -0.0793323   -0.0469553    0.125254  
  0.249366    -0.275295     0.471916   -0.278281     0.17263      0.0472821    0.0475719   -0.955419     -0.6433      -0.272465     0.490792   -0.285479   -0.28788    -0.391539   -0.336541    -0.403578   -0.339967   -0.140114     0.0591133  -0.162796    -0.116995    -0.452033     -0.0408317    0.36302     -0.224305    -0.171429  
 -0.492821     0.061744    -0.231916    0.144394    -0.0121779    0.160587    -0.00479215  -0.754768      0.342704     0.124159    -0.271233    0.251534   -0.240228    0.25684     0.0668358    0.0134463  -0.25785    -0.235813     0.0110756  -0.103191    -0.159077    -0.100854      0.703108     0.0854052   -0.138896    -0.0203892 
 -0.143687     0.217914    -0.243774   -0.0766979   -0.317086     0.116341    -0.500135    -0.000601921   0.227562     0.349264    -0.584081   -0.191069   -0.115987    0.341533    0.5845      -0.460389    0.0720885   0.134539    -0.0625324  -0.255344    -0.0275207   -0.360463      0.161443    -0.0695561   -0.464844    -0.133205  
 -0.671101     0.00690191   0.233732    0.183582     0.167096     0.547193    -0.0389147    0.352804     -0.0745808   -0.202444    -0.27107     0.247287   -0.438961   -0.329113   -0.136893     0.511862    0.507956    0.0735674   -0.105261    0.4115       0.287272    -0.185715     -0.0149375    0.40408     -0.218524    -0.422268  
 -0.644458    -0.17448      0.72085     0.156316    -0.0338987    0.266791     0.112803    -0.338771     -0.0602129    0.510377    -0.58832     0.203127   -0.0339777  -0.435318   -0.283981     0.124506    0.408338    0.260148    -0.398471   -0.0665644    0.0821984    0.0178254    -0.442897     0.304034    -0.190858     0.508483  
 -0.50187      0.175568    -0.198508    0.397831    -0.043947    -0.0623463    0.0870497    0.986977      0.355381     0.300263    -0.0848933  -0.102751    0.222457    0.369129    0.164924     0.533313    0.491449    0.105091     0.0826486   0.0986837    0.169115     0.728028     -0.0570321   -0.146822     0.328243     0.00278589
  0.0055046   -0.645761    -0.300428    0.918013     0.00485537  -0.533546     0.239149     0.140783      0.204873     0.213988    -0.456209   -0.336107   -0.0446284  -0.392064    0.0224877    0.154086    0.335922   -0.00111646  -0.510556   -0.0518537    0.0372899   -0.0342433     0.795445    -0.0542074    0.0743448    0.461824  
  0.334252    -0.155581    -0.159553   -0.249284    -0.0714493   -0.00173607   0.0784348    0.133904      0.30653     -0.708788     0.68551     0.0941957  -0.483574    0.27148    -0.271189    -0.260285   -0.102545   -0.0418514    0.271651    0.222394    -0.0210719   -0.225668      0.13652     -0.361145    -0.0756232   -0.491807  
  0.0841338   -0.524906     0.0652324  -0.181413     0.425832     0.0320629   -0.392709     0.0960159     0.0943434   -0.390219     0.295597   -0.279352    0.87324    -0.169761   -0.0428567   -0.382196    0.276378   -0.0704488    0.0540301  -0.148671    -0.101837     0.506839     -0.228957     0.00200085   0.332104     0.0315255 
 -0.15502     -0.0350888    0.0773942   0.0621454    0.0267955   -0.00585039  -0.071256    -0.00477982    0.0209317    0.0481643   -0.0834997   0.0391154   0.0280614  -0.0459183  -0.0432117   -0.0763917   0.0863925  -0.0286195   -0.0765963   0.0453694   -0.047515     0.0202398    -0.00582926  -0.0281437   -0.0315177   -0.00959638
  0.583221    -0.00856877  -0.110305   -0.0805016    0.152647    -0.0375451    0.0841265    0.194471     -0.0490544    0.141655     0.0205082   0.0815758   0.084559    0.0661658   0.197177     0.051406   -0.318463   -0.0839193    0.037972    0.100516    -0.448038    -0.000771159  -0.0878885    0.0786415    0.37562      0.017689  
  0.00638401  -0.178115    -0.0251746   0.254613     0.545683    -0.144885    -0.0333758    0.0599697    -0.528308    -0.446532     0.178925   -0.237663    0.189011   -0.0696489  -0.306646     0.0170885   0.231559    0.21887      0.836159   -0.264885     0.21667      0.0999846    -0.641782    -0.002255    -0.171182     0.140708  
  0.489839    -0.117462    -0.183905   -0.068241     0.361966     0.121206     0.183338     0.641759     -0.329334     0.491315    -0.135127   -0.540954   -0.118693    0.41242     0.044545     0.468826   -0.300299    0.536046     0.41535    -0.280692     0.0260016    0.38875      -0.389229     0.192476     0.233459    -0.0731997 
  0.170677    -0.293895     0.122239   -0.421037    -0.41089      0.46964     -0.0673119    0.0373806     0.251952     0.0619706    0.679911   -0.412471    0.221028    0.0870295   0.687423     0.155435    0.0499366  -0.0011213   -0.58043    -0.759955    -0.0936409    0.515784      0.369584     0.524256     0.0911436   -0.693016  
  0.327387    -0.111925     0.237494    0.310945     0.0788906   -0.244987     0.262765     0.130047      0.677978     0.120972     0.528273   -0.881545   -0.105538   -0.353174    0.333475     0.126879   -0.102613   -0.219825    -0.387415   -0.617221     0.207251     0.155221     -0.148902     0.0849732    0.167084     0.593591  
 -0.151272     0.273071     0.110427   -0.506622    -0.0849105   -0.476632     0.13738     -0.154991     -0.888596     0.296534    -0.111568    0.20672     0.231975    0.628868   -0.450375    -0.075909   -0.0218057  -0.276181     0.201361    0.637961     0.181916     0.260167     -0.0931055   -0.419311     0.0944564   -0.251616  
  0.0569707    0.668917    -0.105676    0.118998    -0.402297    -0.419102    -0.0311111    0.395881     -0.0214568    0.0276084    0.753932    0.107937    0.224369    0.147347    0.346542     0.108858   -0.0347793  -1.05775     -0.0782422   0.553649    -0.129266     0.0181321     0.180671    -0.349656     0.264038     0.221416  
  0.233022     0.569021     0.131301    0.00547877  -0.122152     0.0624932   -0.115205     0.294976     -0.760269    -0.00210204   0.0377877   0.818807    0.645024   -0.40949     0.266975     0.102961    0.0655594   0.435341     0.135175    0.353944    -0.415575     0.415581     -0.197257     0.0363859    0.979214     0.193895  
  0.0214208    0.216325     0.219096   -0.244824     0.463639     0.119504    -0.0571789    0.26782       0.702034     0.058367    -0.512684    0.668451    0.174732   -0.0538597  -0.0212118   -0.044773   -0.129234    0.445088    -0.328769    0.766006    -0.201494     0.2487        0.187755    -0.444968     0.0676155    0.489569  
  0.481591     0.105506     0.0375374  -0.310065    -0.757538    -0.502401    -0.325846    -0.0620787     0.0660045    0.0882757   -0.010231   -0.0332475   0.376323   -0.460848   -0.663862    -0.0531515  -0.106805    0.424072     0.0366944  -0.628337     0.17465      0.0205433     0.139596    -0.627914    -0.319672    -0.0405362 
  0.403509    -0.574964     0.306686   -0.546627    -0.241944    -0.663812     0.0392      -1.00901       0.362816     0.458853    -0.248862    0.194754    0.428227    0.265744    0.00861914  -1.0067     -0.739814    0.68836      0.0172678  -0.327281    -0.305721     0.141323     -0.153443    -0.183055     0.0824168    0.182057  
  0.257154    -1.0329      -0.157529    0.0559765    0.133992     0.0671229   -0.321409    -0.00709491    0.660935    -0.589704    -0.482725   -0.162455   -0.293288   -0.582616   -0.869958    -0.0349608   0.0394564   0.5416      -0.30818    -0.295379    -0.204124    -0.114202     -0.0519436    0.181291    -0.00866615   0.0510858 
 -0.119001    -0.871573     0.542733    0.244342    -0.110018    -0.021449    -0.226969     0.314207      0.288573    -0.0359496    0.216119   -0.0676163   0.290472    0.185718   -0.588662    -0.1087      0.196651    0.0852624   -0.606776    0.71687     -0.435145     0.0155829    -0.145725    -0.249855     0.549014    -0.194478  
  0.0421807    0.370453    -0.049387    0.0669804    0.102158     0.445509     0.0940901    0.192442      0.614971    -0.370156     0.18073     0.610655   -0.0446932  -0.737728    0.626225    -0.0972036   0.217095   -0.301732     0.0506363  -0.184083    -0.482545    -0.0783501    -0.405397     0.167789    -0.494534     0.417117  
  0.0348325    0.0277759   -0.150287    0.655388     0.376467     0.528119     0.395042     0.268781      0.00874506  -0.698923     0.494492   -0.117937   -0.303738   -0.230128   -0.00963502   0.350495    0.813245   -0.701633    -0.248136   -0.217452     0.382739    -0.404816      0.802919    -0.0124627   -0.291652    -0.440926  
 -0.0192988    0.517162    -0.0797753   0.0692125    0.104501     0.222464     0.147625    -0.429006     -0.211547     0.286668    -0.280601    0.397869   -0.319609    0.0533043   0.287833     0.256827   -0.32385    -0.182339    -0.132453    0.00957791  -0.00305473  -0.280151      0.376794     0.296776    -0.0216013    0.0933107 
 -0.640216    -0.423667    -0.125819    0.078542    -0.127999     0.189268    -0.13275      0.000457722   1.02116     -0.177593    -0.154706   -0.0978881  -0.173018    0.105724   -0.0967736   -0.0795438   0.0902566  -0.344469    -0.233359    0.132058    -0.0422425    0.176155      0.0965539   -0.0471372   -0.297997    -0.187215  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.413626
INFO: iteration 2, average log likelihood -1.413615
INFO: iteration 3, average log likelihood -1.413604
INFO: iteration 4, average log likelihood -1.413593
INFO: iteration 5, average log likelihood -1.413583
INFO: iteration 6, average log likelihood -1.413573
INFO: iteration 7, average log likelihood -1.413564
INFO: iteration 8, average log likelihood -1.413555
INFO: iteration 9, average log likelihood -1.413546
INFO: iteration 10, average log likelihood -1.413537
INFO: EM with 100000 data points 10 iterations avll -1.413537
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.260255e+05
      1       7.095138e+05      -2.165117e+05 |       32
      2       6.951157e+05      -1.439813e+04 |       32
      3       6.895330e+05      -5.582668e+03 |       32
      4       6.866700e+05      -2.863016e+03 |       32
      5       6.848674e+05      -1.802603e+03 |       32
      6       6.835950e+05      -1.272429e+03 |       32
      7       6.826254e+05      -9.695408e+02 |       32
      8       6.818274e+05      -7.980175e+02 |       32
      9       6.811557e+05      -6.717119e+02 |       32
     10       6.805809e+05      -5.747973e+02 |       32
     11       6.800945e+05      -4.864195e+02 |       32
     12       6.796660e+05      -4.284624e+02 |       32
     13       6.792897e+05      -3.763384e+02 |       32
     14       6.789699e+05      -3.197653e+02 |       32
     15       6.786933e+05      -2.766243e+02 |       32
     16       6.784611e+05      -2.321831e+02 |       32
     17       6.782547e+05      -2.064164e+02 |       32
     18       6.780622e+05      -1.925322e+02 |       32
     19       6.778682e+05      -1.939813e+02 |       32
     20       6.776803e+05      -1.878862e+02 |       32
     21       6.775089e+05      -1.714168e+02 |       32
     22       6.773568e+05      -1.521060e+02 |       32
     23       6.772209e+05      -1.358381e+02 |       32
     24       6.770885e+05      -1.324104e+02 |       32
     25       6.769614e+05      -1.271255e+02 |       32
     26       6.768547e+05      -1.066649e+02 |       32
     27       6.767524e+05      -1.023130e+02 |       32
     28       6.766375e+05      -1.148831e+02 |       32
     29       6.765117e+05      -1.258664e+02 |       32
     30       6.764027e+05      -1.089757e+02 |       32
     31       6.763079e+05      -9.476035e+01 |       32
     32       6.762242e+05      -8.368454e+01 |       32
     33       6.761479e+05      -7.637242e+01 |       32
     34       6.760799e+05      -6.799327e+01 |       32
     35       6.760139e+05      -6.597800e+01 |       32
     36       6.759536e+05      -6.034132e+01 |       32
     37       6.758985e+05      -5.504211e+01 |       32
     38       6.758505e+05      -4.798311e+01 |       32
     39       6.758067e+05      -4.388169e+01 |       32
     40       6.757677e+05      -3.899179e+01 |       32
     41       6.757311e+05      -3.653027e+01 |       32
     42       6.756977e+05      -3.340072e+01 |       32
     43       6.756698e+05      -2.794226e+01 |       32
     44       6.756481e+05      -2.164115e+01 |       32
     45       6.756269e+05      -2.129111e+01 |       32
     46       6.756082e+05      -1.869346e+01 |       32
     47       6.755918e+05      -1.632405e+01 |       32
     48       6.755792e+05      -1.260035e+01 |       32
     49       6.755663e+05      -1.292906e+01 |       32
     50       6.755539e+05      -1.245080e+01 |       31
K-means terminated without convergence after 50 iterations (objv = 675553.8574377118)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.425099
INFO: iteration 2, average log likelihood -1.420173
INFO: iteration 3, average log likelihood -1.418930
INFO: iteration 4, average log likelihood -1.418082
INFO: iteration 5, average log likelihood -1.417180
INFO: iteration 6, average log likelihood -1.416240
INFO: iteration 7, average log likelihood -1.415471
INFO: iteration 8, average log likelihood -1.414977
INFO: iteration 9, average log likelihood -1.414694
INFO: iteration 10, average log likelihood -1.414524
INFO: iteration 11, average log likelihood -1.414408
INFO: iteration 12, average log likelihood -1.414321
INFO: iteration 13, average log likelihood -1.414249
INFO: iteration 14, average log likelihood -1.414189
INFO: iteration 15, average log likelihood -1.414136
INFO: iteration 16, average log likelihood -1.414088
INFO: iteration 17, average log likelihood -1.414045
INFO: iteration 18, average log likelihood -1.414005
INFO: iteration 19, average log likelihood -1.413968
INFO: iteration 20, average log likelihood -1.413934
INFO: iteration 21, average log likelihood -1.413903
INFO: iteration 22, average log likelihood -1.413873
INFO: iteration 23, average log likelihood -1.413845
INFO: iteration 24, average log likelihood -1.413818
INFO: iteration 25, average log likelihood -1.413793
INFO: iteration 26, average log likelihood -1.413769
INFO: iteration 27, average log likelihood -1.413746
INFO: iteration 28, average log likelihood -1.413725
INFO: iteration 29, average log likelihood -1.413704
INFO: iteration 30, average log likelihood -1.413684
INFO: iteration 31, average log likelihood -1.413665
INFO: iteration 32, average log likelihood -1.413647
INFO: iteration 33, average log likelihood -1.413629
INFO: iteration 34, average log likelihood -1.413612
INFO: iteration 35, average log likelihood -1.413596
INFO: iteration 36, average log likelihood -1.413580
INFO: iteration 37, average log likelihood -1.413565
INFO: iteration 38, average log likelihood -1.413550
INFO: iteration 39, average log likelihood -1.413536
INFO: iteration 40, average log likelihood -1.413522
INFO: iteration 41, average log likelihood -1.413509
INFO: iteration 42, average log likelihood -1.413495
INFO: iteration 43, average log likelihood -1.413483
INFO: iteration 44, average log likelihood -1.413470
INFO: iteration 45, average log likelihood -1.413458
INFO: iteration 46, average log likelihood -1.413447
INFO: iteration 47, average log likelihood -1.413436
INFO: iteration 48, average log likelihood -1.413425
INFO: iteration 49, average log likelihood -1.413414
INFO: iteration 50, average log likelihood -1.413404
INFO: EM with 100000 data points 50 iterations avll -1.413404
59.0 data points per parameter
32Ã—26 Array{Float64,2}:
 -0.100571    0.205367   -0.00303237  -0.237447   -0.155925   -0.727342    -0.127556    -0.217672   -0.888532    0.240732   -0.351718    -0.182851    0.427636     0.357479    -0.341497   -0.181104      0.14268      -0.129846    0.0738966    0.178309     0.378206     0.135698    -0.196581   -0.34876       0.166084     0.148271 
 -0.148111   -0.869913   -0.0149387   -0.80762     0.274852    0.720569     0.474586    -0.274072    0.454078   -0.788127    0.741211    -0.367315    0.0482005   -0.0749263   -0.0171325   0.198152     -0.094579     -0.217051   -0.208267     0.426015    -0.221812     0.223853     0.0381388   0.373999     -0.387982     0.217836 
 -0.0429456  -0.630573   -0.132322    -0.300994   -0.561456   -0.056586    -0.226025     0.0606042   0.222015    0.248359    0.171207    -0.287516    0.195487     0.461481    -0.0763165  -0.0298157    -0.102343      0.0338929  -0.620609    -0.0404748   -0.435964     0.584955     0.207434    0.066139      0.492149    -0.630661 
 -0.0380863   0.230542   -0.0873391   -0.188799   -0.198553   -0.0169956    0.230425    -0.771917    0.223324    0.410683   -0.579702     0.705328   -0.315643     0.376781     0.0564863   0.0932445    -0.602257      0.0764839  -0.3564       0.309715    -0.157589    -0.346078     0.884457    0.064338     -0.00565957  -0.141791 
 -0.0862884   0.333667   -0.375884    -0.0105199  -0.248689    0.472975    -0.340482    -0.0222992   0.559803    0.0416511  -0.330968     0.261618    0.00864915  -0.127069     0.997563   -0.481256      0.0395373    -0.0868733  -0.00900238  -0.447771    -0.446418    -0.117668    -0.0508735   0.0188763    -0.547046     0.101597 
  0.229213   -0.632518    0.822636     0.315689   -0.103098   -0.10402     -0.123524     0.570122    0.208603   -0.111608    0.238508    -0.136613    0.399626    -0.0809139   -0.813581   -0.132308      0.116379      0.280881   -0.270497     0.452343    -0.174032     0.0573822   -0.276887   -0.637788      0.36142     -0.132313 
 -0.547021    0.170533   -0.225697     0.526396    0.434928    0.438449    -0.0187533    0.0705333   0.186682    0.101645    0.0900579   -0.470241   -0.455978     0.421452     0.396769    0.210284      0.199483     -0.59658    -0.310597    -0.15488      0.430385    -0.24428      0.513771    0.101447     -0.411821    -0.198162 
  0.0664095  -0.281516    0.243861     0.231465   -0.294788    0.110191     0.446144     0.207201    0.321407    0.146709    0.418473    -0.590329   -0.0527379   -0.303762     0.355914    0.471001      0.171075     -0.248367   -0.0394071   -0.709828     0.60804      0.379142    -0.269354    0.250177     -0.330912    -0.2259   
 -0.508706   -0.114453   -0.0748507    0.291317    0.109433    0.0391446   -0.129583    -0.0733054   0.251085    0.296989   -0.35168      0.0158359   0.0302438    0.166882     0.122748   -0.0215315     0.205721     -0.0651023  -0.392578     0.140689    -0.0638839   -0.0188207    0.353888    0.0407839     0.0850717    0.0734685
  0.85825     0.191867   -0.513849    -0.468518   -0.129527   -0.640849    -0.288428     0.467379    0.256004   -0.650518    0.534795    -0.260845    0.355865     0.0764793    0.042065   -0.134928     -0.0178357     0.0415801   0.223882    -0.00931112   0.271073    -0.403943     0.244658   -0.536313     -0.42687     -0.0511411
  0.182562    0.511717   -0.469011    -0.276202   -0.290027   -0.123813     0.338765    -0.489502   -0.892649    0.496421    0.0827195   -0.383744   -0.0192728    0.36335      0.358124   -0.0574695     0.101914     -0.508478    0.41357     -0.590299     0.250889     0.110487     0.199754    0.474855      0.113021    -0.302854 
  0.117327   -0.510145   -0.305804    -0.303641   -0.157321    0.350728    -0.231463     0.0721434   0.627906   -0.823839    0.0499901    0.349859   -0.676755     0.046802    -0.766302   -0.239336      0.100929      0.312695    0.209102     0.0631839    0.0235547   -0.220294     0.285346   -0.172468     -0.102366    -0.607576 
  0.367904    0.223359    0.1521       0.271329   -0.0483204  -0.383652     0.316238     0.0174865   0.46671     0.226544    0.392509    -0.510222    0.120734    -0.0904689    0.683184   -0.000784997  -0.274768     -0.132848   -0.538694    -0.197068    -0.00226336   0.336908     0.0529977  -0.129498      0.651641     0.951825 
  0.0120091   0.621817   -0.175094    -0.253631   -0.184405   -0.00479408   0.0648991    0.614679   -0.200844    0.528748   -0.233899    -0.0541419   0.0130791    0.569269     0.0949301   0.494479     -0.246126      0.245013    0.378128    -0.0183208    0.280978     0.468942    -0.232418   -0.354669     -0.111464    -0.151084 
  0.113892   -0.493285   -0.577386     0.462768    0.609887   -0.210689     0.342597     0.320631   -0.320589   -0.177876   -0.297577    -0.319155   -0.0291914    0.370325    -0.386584    0.502858      0.225682      0.591191    0.731987    -0.0398517    0.144803     0.437871    -0.742865    0.20786       0.131426     0.331003 
 -0.846732    0.182706   -0.177951     0.696148   -0.032527   -0.0377896   -0.392808    -0.706574    0.306777   -0.224469    0.0200144   -0.0931508   0.00943063  -0.122739    -0.348062   -0.195504     -0.259967     -0.274618    0.88197     -0.710914    -0.0159052    0.367113     0.388245   -0.157633     -0.514416    -0.455316 
 -0.137744   -0.439874   -0.0163932    0.629049    0.10415     0.352452     0.288929     0.642041    0.265681   -0.179939    0.0277555    0.0394315  -0.153056    -0.568305    -0.0693055   0.720002      0.873189      0.0354259  -0.349552     0.351302     0.00851202  -0.0903103    0.608092    0.285142      0.124425    -0.260425 
 -0.27835    -0.0793926  -0.131524     0.1615      0.361859   -0.449067    -0.542792     0.0765013  -0.0998574  -0.863999    0.467844    -0.421396    0.528575    -0.168074     0.130975   -0.471685     -0.000748235  -0.616098    0.0399536   -0.0782432   -0.419709     0.430279     0.0377633   0.0742894     1.16652     -0.30677  
  0.146782   -0.630472   -0.124388     0.256148   -0.0870442  -0.441306    -0.342523    -0.179725    0.731423   -0.148135   -0.563913    -0.691606   -0.239186    -0.578239    -0.590019   -0.103996     -0.0643966     0.270662   -0.764607    -0.535319    -0.216611    -0.20461      0.248388   -0.0627046    -0.303638     0.441088 
  0.131397   -0.271624    0.20717     -0.190982    0.0571304   0.00242641  -0.113206    -0.57085    -0.245837   -0.110103    0.0114516   -0.0562807  -0.114025    -0.00726043  -0.226368   -0.402095     -0.400642     -0.0614241   0.0285767   -0.171472    -0.178214    -0.336948     0.0407836  -0.000345803  -0.239305    -0.0982838
  0.716745   -0.168585    0.0214557   -0.171815    0.19729    -0.0587422    0.226549    -0.46182    -0.865957   -0.264333    0.901187    -0.523983   -0.519157    -0.28178     -0.510641    0.00050108   -0.484817      0.188134    0.790357    -0.413295    -0.0266297   -0.0384383   -0.130993    0.25395       0.0988229   -0.0513818
  0.45581    -0.0294556   0.08569      0.0994113   0.558452    0.1721       0.14486      0.225651   -0.181642    0.0195477   0.0134802    0.150123   -0.227495     0.106387     0.195966    0.0538777    -0.348005     -0.271828    0.118553     0.321507    -0.615238    -0.379198    -0.289821    0.146403      0.385134    -0.104463 
 -0.287112   -0.240443    0.661897     0.065497   -0.129335    0.255011     0.276263    -0.74027    -0.24692    -0.247083    0.142305     0.132902   -0.250514    -0.466114    -0.267176   -0.605251      0.496305     -0.482459   -0.338559     0.0894103    0.0112616   -0.764074    -0.0526649   0.201391     -0.339215    -0.332764 
 -0.875887   -0.0175115   0.607684     0.59169    -0.125781    0.472415    -0.174044     0.0727373   0.238523    0.188512   -0.516261     0.354571   -0.123022    -0.248877    -0.248375    0.169196      0.160032      0.361069   -0.741264     0.37018      0.15195      0.258692    -0.767672    0.104353      0.258473     0.422548 
 -0.122931    0.149192    0.155928    -0.323033    0.48542     0.06307     -0.281048     0.184253    0.163358   -0.100695   -0.473151     0.89516     0.289948    -0.126649    -0.104674   -0.151553      0.0959899     0.373027   -0.157532     0.758561    -0.445509     0.28885      0.104816   -0.217962      0.155514     0.218832 
 -0.0340045   0.306443    0.189108     0.189735    0.410834    0.207406     0.0170767   -0.760607   -0.242059    0.0414896  -0.310817     0.518902    0.134643    -0.516347    -0.161213    0.149219     -0.108845      0.194801    0.0593468   -0.434187    -0.106198    -0.240329     0.122873    0.240008     -0.230678     1.08248  
 -0.258267    0.252279    0.136582    -0.0693502  -0.0768087   0.101788     0.122109     0.0106374  -0.0932749  -0.0730062  -0.0226318    0.025292   -0.280379    -0.0105909   -0.0567021   0.182851      0.20199      -0.0562096   0.211048    -0.0773522    0.371681    -0.0715058   -0.118234    0.12072      -0.365186    -0.118255 
  0.152166   -0.624367    0.391942    -0.410037    0.67398     0.384222    -0.637421     0.437582    0.0636375  -0.0162785  -0.00494338  -0.370754    0.446297    -0.382211    -0.0592262  -0.182729      0.429016      0.585289    0.0980318   -0.636381     0.156078     0.214648    -0.428457    0.505079      0.289827    -0.0317874
  0.300071   -0.083395   -0.12295     -0.0432243  -0.066882   -0.0844075   -0.00206977   0.226836    0.145598    0.0134484   0.152866    -0.116718    0.254808     0.00180371   0.0109881   0.0426206    -0.045794      0.0477447   0.0487976   -0.143442    -0.0319588    0.375382    -0.110609   -0.0798048     0.21202      0.083256 
  0.495988   -0.322668    0.444344    -0.642083   -0.264173   -0.727782    -0.0468707   -0.724421   -0.121072    0.37921    -0.146257     0.168402    0.270496     0.408388    -0.24089    -1.05721      -0.615674      0.549505    0.395556    -0.533999     0.188548     0.246532    -0.691926   -0.154626     -0.119736     0.0909201
 -0.108957    0.707838   -0.0107463    0.0152195  -0.294752   -0.166532     0.223007     0.269641   -0.163395    0.0920258   0.57695      0.4893      0.156904     0.166763     0.258617    0.0888636     0.0333594    -0.919907    0.267974     0.605793    -0.0895777    0.180543    -0.111815   -0.316195      0.0736944    0.159179 
  0.0435349   0.229943    0.117033    -0.0256158   0.0867379   0.0191674    0.0378051    0.120128   -0.14108     0.0252722  -0.132031     0.348287    0.151645    -0.130881     0.0182546   0.130148      0.0299578     0.0509507   0.172271     0.138056    -0.00556175  -0.00857804  -0.154164   -0.0881995     0.0571745    0.0970113INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.413394
INFO: iteration 2, average log likelihood -1.413384
INFO: iteration 3, average log likelihood -1.413374
INFO: iteration 4, average log likelihood -1.413365
INFO: iteration 5, average log likelihood -1.413357
INFO: iteration 6, average log likelihood -1.413348
INFO: iteration 7, average log likelihood -1.413340
INFO: iteration 8, average log likelihood -1.413332
INFO: iteration 9, average log likelihood -1.413324
INFO: iteration 10, average log likelihood -1.413317
INFO: EM with 100000 data points 10 iterations avll -1.413317
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
