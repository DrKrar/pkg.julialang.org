>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.6.6
INFO: Installing JLD v0.6.5
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.0
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.2.0
INFO: Installing StaticArrays v0.0.11
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1156
Commit 9747250 (2016-10-29 17:44 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-100-generic #147-Ubuntu SMP Tue Oct 18 16:48:51 UTC 2016 x86_64 x86_64
Memory: 2.939289093017578 GB (607.53125 MB free)
Uptime: 23302.0 sec
Load Avg:  1.0771484375  1.04248046875  1.01318359375
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3499 MHz    1422793 s       3718 s     177092 s     479421 s         78 s
#2  3499 MHz     605730 s       4611 s      86537 s    1542038 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.3
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.6.6
 - JLD                           0.6.5
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.0
 - Rmath                         0.1.4
 - SHA                           0.2.1
 - ScikitLearnBase               0.2.0
 - StaticArrays                  0.0.11
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-1.7831955653829414e6,[3082.47,96917.5],
[1495.42 -140.359 -720.616; -1470.44 270.581 757.715],

Array{Float64,2}[
[2358.86 604.659 -1079.02; 604.659 2701.4 789.353; -1079.02 789.353 960.899],

[96691.9 -205.194 1671.25; -205.194 96417.2 -1158.69; 1671.25 -1158.69 99385.8]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.253886e+03
      1       9.319390e+02      -3.219471e+02 |        7
      2       8.407391e+02      -9.119990e+01 |        5
      3       8.154731e+02      -2.526600e+01 |        3
      4       8.082544e+02      -7.218757e+00 |        0
      5       8.082544e+02       0.000000e+00 |        0
K-means converged with 5 iterations (objv = 808.2543705402795)
INFO: K-means with 272 data points using 5 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.047069
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.771263
INFO: iteration 2, lowerbound -3.652985
INFO: iteration 3, lowerbound -3.532196
INFO: iteration 4, lowerbound -3.403774
INFO: iteration 5, lowerbound -3.275016
INFO: iteration 6, lowerbound -3.151969
INFO: dropping number of Gaussions to 7
INFO: iteration 7, lowerbound -3.039106
INFO: dropping number of Gaussions to 6
INFO: iteration 8, lowerbound -2.935350
INFO: dropping number of Gaussions to 5
INFO: iteration 9, lowerbound -2.843078
INFO: iteration 10, lowerbound -2.783347
INFO: dropping number of Gaussions to 4
INFO: iteration 11, lowerbound -2.747936
INFO: iteration 12, lowerbound -2.724567
INFO: dropping number of Gaussions to 3
INFO: iteration 13, lowerbound -2.693800
INFO: iteration 14, lowerbound -2.658852
INFO: iteration 15, lowerbound -2.621913
INFO: iteration 16, lowerbound -2.580660
INFO: iteration 17, lowerbound -2.537241
INFO: iteration 18, lowerbound -2.494220
INFO: iteration 19, lowerbound -2.453745
INFO: iteration 20, lowerbound -2.416768
INFO: iteration 21, lowerbound -2.383033
INFO: iteration 22, lowerbound -2.352295
INFO: iteration 23, lowerbound -2.326485
INFO: iteration 24, lowerbound -2.310481
INFO: iteration 25, lowerbound -2.308124
INFO: dropping number of Gaussions to 2
INFO: iteration 26, lowerbound -2.302915
INFO: iteration 27, lowerbound -2.299259
INFO: iteration 28, lowerbound -2.299256
INFO: iteration 29, lowerbound -2.299254
INFO: iteration 30, lowerbound -2.299254
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Sun 30 Oct 2016 10:57:48 AM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Sun 30 Oct 2016 10:57:49 AM UTC: K-means with 272 data points using 5 iterations
11.3 data points per parameter
,Sun 30 Oct 2016 10:57:50 AM UTC: EM with 272 data points 0 iterations avll -2.047069
5.8 data points per parameter
,Sun 30 Oct 2016 10:57:51 AM UTC: GMM converted to Variational GMM
,Sun 30 Oct 2016 10:57:53 AM UTC: iteration 1, lowerbound -3.771263
,Sun 30 Oct 2016 10:57:53 AM UTC: iteration 2, lowerbound -3.652985
,Sun 30 Oct 2016 10:57:53 AM UTC: iteration 3, lowerbound -3.532196
,Sun 30 Oct 2016 10:57:54 AM UTC: iteration 4, lowerbound -3.403774
,Sun 30 Oct 2016 10:57:54 AM UTC: iteration 5, lowerbound -3.275016
,Sun 30 Oct 2016 10:57:54 AM UTC: iteration 6, lowerbound -3.151969
,Sun 30 Oct 2016 10:57:54 AM UTC: dropping number of Gaussions to 7
,Sun 30 Oct 2016 10:57:54 AM UTC: iteration 7, lowerbound -3.039106
,Sun 30 Oct 2016 10:57:54 AM UTC: dropping number of Gaussions to 6
,Sun 30 Oct 2016 10:57:54 AM UTC: iteration 8, lowerbound -2.935350
,Sun 30 Oct 2016 10:57:54 AM UTC: dropping number of Gaussions to 5
,Sun 30 Oct 2016 10:57:54 AM UTC: iteration 9, lowerbound -2.843078
,Sun 30 Oct 2016 10:57:54 AM UTC: iteration 10, lowerbound -2.783347
,Sun 30 Oct 2016 10:57:54 AM UTC: dropping number of Gaussions to 4
,Sun 30 Oct 2016 10:57:54 AM UTC: iteration 11, lowerbound -2.747936
,Sun 30 Oct 2016 10:57:54 AM UTC: iteration 12, lowerbound -2.724567
,Sun 30 Oct 2016 10:57:54 AM UTC: dropping number of Gaussions to 3
,Sun 30 Oct 2016 10:57:54 AM UTC: iteration 13, lowerbound -2.693800
,Sun 30 Oct 2016 10:57:54 AM UTC: iteration 14, lowerbound -2.658852
,Sun 30 Oct 2016 10:57:55 AM UTC: iteration 15, lowerbound -2.621913
,Sun 30 Oct 2016 10:57:55 AM UTC: iteration 16, lowerbound -2.580660
,Sun 30 Oct 2016 10:57:55 AM UTC: iteration 17, lowerbound -2.537241
,Sun 30 Oct 2016 10:57:55 AM UTC: iteration 18, lowerbound -2.494220
,Sun 30 Oct 2016 10:57:55 AM UTC: iteration 19, lowerbound -2.453745
,Sun 30 Oct 2016 10:57:55 AM UTC: iteration 20, lowerbound -2.416768
,Sun 30 Oct 2016 10:57:55 AM UTC: iteration 21, lowerbound -2.383033
,Sun 30 Oct 2016 10:57:55 AM UTC: iteration 22, lowerbound -2.352295
,Sun 30 Oct 2016 10:57:55 AM UTC: iteration 23, lowerbound -2.326485
,Sun 30 Oct 2016 10:57:55 AM UTC: iteration 24, lowerbound -2.310481
,Sun 30 Oct 2016 10:57:55 AM UTC: iteration 25, lowerbound -2.308124
,Sun 30 Oct 2016 10:57:55 AM UTC: dropping number of Gaussions to 2
,Sun 30 Oct 2016 10:57:55 AM UTC: iteration 26, lowerbound -2.302915
,Sun 30 Oct 2016 10:57:55 AM UTC: iteration 27, lowerbound -2.299259
,Sun 30 Oct 2016 10:57:55 AM UTC: iteration 28, lowerbound -2.299256
,Sun 30 Oct 2016 10:57:56 AM UTC: iteration 29, lowerbound -2.299254
,Sun 30 Oct 2016 10:57:56 AM UTC: iteration 30, lowerbound -2.299254
,Sun 30 Oct 2016 10:57:56 AM UTC: iteration 31, lowerbound -2.299253
,Sun 30 Oct 2016 10:57:56 AM UTC: iteration 32, lowerbound -2.299253
,Sun 30 Oct 2016 10:57:56 AM UTC: iteration 33, lowerbound -2.299253
,Sun 30 Oct 2016 10:57:56 AM UTC: iteration 34, lowerbound -2.299253
,Sun 30 Oct 2016 10:57:56 AM UTC: iteration 35, lowerbound -2.299253
,Sun 30 Oct 2016 10:57:56 AM UTC: iteration 36, lowerbound -2.299253
,Sun 30 Oct 2016 10:57:56 AM UTC: iteration 37, lowerbound -2.299253
,Sun 30 Oct 2016 10:57:56 AM UTC: iteration 38, lowerbound -2.299253
,Sun 30 Oct 2016 10:57:56 AM UTC: iteration 39, lowerbound -2.299253
,Sun 30 Oct 2016 10:57:56 AM UTC: iteration 40, lowerbound -2.299253
,Sun 30 Oct 2016 10:57:56 AM UTC: iteration 41, lowerbound -2.299253
,Sun 30 Oct 2016 10:57:56 AM UTC: iteration 42, lowerbound -2.299253
,Sun 30 Oct 2016 10:57:56 AM UTC: iteration 43, lowerbound -2.299253
,Sun 30 Oct 2016 10:57:56 AM UTC: iteration 44, lowerbound -2.299253
,Sun 30 Oct 2016 10:57:56 AM UTC: iteration 45, lowerbound -2.299253
,Sun 30 Oct 2016 10:57:56 AM UTC: iteration 46, lowerbound -2.299253
,Sun 30 Oct 2016 10:57:56 AM UTC: iteration 47, lowerbound -2.299253
,Sun 30 Oct 2016 10:57:57 AM UTC: iteration 48, lowerbound -2.299253
,Sun 30 Oct 2016 10:57:57 AM UTC: iteration 49, lowerbound -2.299253
,Sun 30 Oct 2016 10:57:57 AM UTC: iteration 50, lowerbound -2.299253
,Sun 30 Oct 2016 10:57:57 AM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.045,95.9549]
β = [178.045,95.9549]
m = [4.2503 79.2869; 2.00023 53.852]
ν = [180.045,97.9549]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.184042 -0.00764405; 0.0 0.00858171],

[0.375876 -0.00895312; 0.0 0.0127487]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 99999.99999999996
avll from stats: -0.9917679377587645
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9917679377587674
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9917679377587673
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
WARNING: is is deprecated, use === instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in is(::Array{Float64,2}, ::Vararg{Array{Float64,2},N}) at ./deprecated.jl:30
 in _rcopy!(::Array{Float64,2}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/PDMats/src/utils.jl:10
 in unwhiten!(::Array{Float64,2}, ::PDMats.PDMat{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/PDMats/src/pdmat.jl:60
 in rand(::Distributions.MvNormal{Float64,PDMats.PDMat{Float64,Array{Float64,2}},Array{Float64,1}}, ::Int64) at /home/vagrant/.julia/v0.6/Distributions/src/multivariates.jl:18
 in rand(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:60
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -1.0028080787438538
avll from llpg:  -1.002808078743854
avll direct:     -1.0028080787438542
sum posterior: 100000.0
32×26 Array{Float64,2}:
 -0.000889121   0.017228     0.0576392    0.140499     0.0280904   -0.0252985   -0.0635792    0.142277     0.138905      0.0156527    0.0988747    0.203896   -0.0375843   -0.1518      -0.114904    -0.00447489    0.0150555   -0.198402    -0.210982     0.0303734   0.0197284    0.0947098    0.00177595  -0.244854    -0.065887      0.019072 
 -0.124828      0.0179412   -0.0342508   -0.0389628    0.0682464    0.0656005   -0.176586    -0.170439    -0.267831     -0.058139     0.0679752    0.0285526  -0.110998    -0.0011657   -0.00013588   0.238707      0.00440365  -0.159598    -0.0735416   -0.010071   -0.0258112    0.0396145    0.0599981   -0.0725168    0.0457992     0.0472118
  0.0305179     0.260631     0.0588679   -0.105168     0.019304    -0.0669248    0.213399     0.0158232   -0.137544     -0.0473958   -0.0378225    0.246286   -0.0749081   -0.0317131   -0.0200431   -0.0239854     0.053898     0.0114479    0.0491174    0.0446364   0.0386374   -0.0774315    0.0910126   -0.00372538   0.0222108     0.0411517
  0.0270109     0.0527687    0.0277964   -0.00963759  -0.0268258    0.0205103    0.0996093    0.00419671  -0.000795726  -0.067281     0.10147     -0.0610519   0.244225    -0.121602    -0.177377    -0.0183035     0.146938     0.0652786   -0.0633648    0.0761342  -0.033425     0.0934435    0.0749554    0.0138827   -0.0865149     0.0590872
  0.072874      0.0820572   -0.100046     0.123241    -0.0873853   -0.0279121   -0.0212166    0.0162488    0.0266073    -0.00734132   0.0516184   -0.190741    0.0834174   -0.0689919    0.0805145    0.124127      0.0307793    0.0246363    0.0594808    0.0439143   0.0543851   -0.0253197   -0.0372995   -0.0692614    0.124842     -0.116493 
 -0.0224941    -0.185902     0.0231744   -0.0179847    0.111496    -0.206428    -0.039406     0.126056    -0.0800092     0.164633     0.0704479   -0.0608519   0.0512098    0.110775     0.00455771   0.0115137    -0.14549      0.0896553    0.177005     0.104273    0.0966374    0.119275    -0.0723098   -0.00549533  -0.117122      0.0409213
 -0.0353107    -0.0303725   -0.0297137   -0.0429071    0.011628    -0.0733073    0.119773     0.0782781    0.00199156   -0.0828076    0.00526163   0.0469685  -0.0140635    0.206171    -0.0116787   -0.0258694     0.0504115    0.178253    -0.146435     0.0631222  -0.00526723  -0.0392217    0.0682695    0.0482181   -0.052437      0.0118494
 -0.00733513    0.229623     0.0914224   -0.147244     0.0723204    0.15727     -0.0737451    0.088297     0.0525141     0.236346    -0.0528362    0.0857892   0.152036     0.0467114   -0.110631    -0.0764122    -0.0674904   -0.116098    -0.078785     0.0768426  -0.039973     0.0857247    0.0752794   -0.09299     -0.158171     -0.143534 
  0.113714      0.0360202    0.113361     0.00755579   0.081115     0.0236339    0.145971     0.0210837    0.186295     -0.089233     0.211725     0.0488192   0.0622522   -0.0488746   -0.0661359    0.298845      0.0698346   -0.110885     0.0936463   -0.0133729  -0.0270297    0.0848593    0.0595061    0.127721    -0.258965      0.0729687
 -0.156777      0.05489     -0.0821675   -0.121728    -0.282273    -0.00866151  -0.0410743   -0.0185266    0.00461322   -0.128109    -0.0827733    0.18599    -0.0943827    0.00737281   0.150822     0.0621974    -0.134977    -0.0643947   -0.0771575    0.0847628  -0.0819213   -0.0453314   -0.0624045    0.125473    -0.0192996    -0.0468066
  0.0371074    -0.0473351    0.0714595    0.0639566    0.0134799   -0.00518295   0.0612643    0.0967442    0.0280322    -0.0451091   -0.117781    -0.013572   -0.142203    -0.0248679   -0.0266407   -0.11836       0.140978     0.1319       0.136073     0.0320992   0.109046    -0.00843216   0.0835231    0.208032     0.241102      0.0142273
  0.0291434    -0.0151627   -0.0601071   -0.080569     0.151759     0.029329     0.12449      0.173874     0.0579864    -0.0374886   -0.0176294    0.128928    0.0798623   -0.0654712    0.0273789    0.119602     -0.200506    -0.0682077   -0.059642    -0.0560012  -0.177482     0.159897     0.0207249   -0.0646606   -0.0783666     0.122529 
 -0.0665008     0.00409753  -0.103721    -0.119701    -0.123293    -0.128929     0.0120365    0.147243     0.0301983     0.00346976  -0.129016    -0.0562521   0.0170541    0.125012    -0.062003    -0.0354014    -0.117878    -0.0451099    0.0322419    0.0142339  -0.0855569    0.0336461    0.206651     0.188323     0.169086      0.178062 
  0.0117249     0.131487     0.145846    -0.0936049   -0.0620238    0.0618461    0.0075544    0.0956554    0.0737323    -0.0378609    0.00607958  -0.0782567   0.0386285    0.153825     0.061316     0.068308      0.0980407   -0.0418435   -0.0404759   -0.118875    0.153733    -0.0175054    0.0391867    0.0373284   -0.000822194   0.120845 
  0.00862724    0.142261    -0.151066     0.0961227    0.169222     0.0608462    0.0568298    0.06618     -0.0104384     0.0348484   -0.058273    -0.0364634  -0.0748513    0.0228354    0.170543     0.000896081  -0.0594836   -0.0769715   -0.0220245   -0.0936046  -0.0187882    0.118173    -0.108577    -0.0187612   -0.0793512    -0.172432 
 -0.0440433    -0.128683    -0.16677     -0.0398464    0.185852     0.158186     0.0358426   -0.0612714    0.0865972    -0.0645423    0.076361     0.0208483   0.138304    -0.0523804    0.186101     0.0100366     0.177099    -0.0452366    0.0475344   -0.121459   -0.0661544   -0.0347284    0.0796356    0.00373643  -0.0670527     0.0145402
  0.101967     -0.231086     0.167799     0.0804529   -0.0593167    0.0651358   -0.196086     0.0513021   -0.00373      -0.0065383   -0.0183196   -0.0534716   0.032514    -0.116769    -0.02888     -0.113812      0.0502056    0.184392    -0.0564395    0.0539051  -0.195468     0.00405632  -0.052454    -0.049733    -0.110492     -0.0354714
  0.0167888    -0.0500003    0.18727     -0.136297     0.172194     0.031903    -0.0487354    0.0176838   -0.0572022    -0.0254578    0.0114198   -0.014735   -0.128405     0.0855422   -0.0136477    0.0867073     0.111374    -0.0448482   -0.10469      0.058361    0.0533576    0.106779    -0.0249027    0.246802     0.162333      0.0979145
 -0.257771     -0.0628815    0.0696294    0.1216       0.213135     0.0957474   -0.136478    -0.0744737    0.0087701    -0.0657368   -0.0872665    0.0383265   0.00721882   0.0243281   -0.0781655    0.0273849     0.122848    -0.111654     0.141564     0.0256031  -0.0127346   -0.200988     0.104294     0.167081     0.0766469    -0.0948938
 -0.0561209    -0.111836    -0.00385086  -0.0223874   -0.120657    -0.0857996   -0.151202     0.018236    -0.0359289    -0.0312806   -0.0532682    0.0400025   0.0223994   -0.00605807   0.192453     0.0898994    -0.0627124   -0.0046664   -0.0688184   -0.114049   -0.082853     0.0132181   -0.0288692   -0.0179493   -0.0708396    -0.0849509
 -0.0125218     0.08491      0.108028    -0.152437    -0.0452594    0.0499364   -0.00250645   0.121784     0.000935491  -0.0136663    0.00585125   0.0566897   0.0135575    0.0498177    0.023106     0.162791      0.101141     0.0071613    0.00242875   0.0971247   0.017672     0.0498288    0.0969432   -0.00495825  -0.0316302     0.0995913
 -0.0562555    -0.166465    -0.124879    -0.0388127    0.199485     0.0651081    0.0158941   -0.0113115    0.181141      0.132506     0.100115    -0.114138    0.0865923    0.0706186   -0.0429655    0.120592     -0.100773    -0.0600866   -0.0833438   -0.11014    -0.0183292    0.142699    -0.0235248    0.178861    -0.0872606     0.0283383
 -0.00297915   -0.0171475   -0.0124681    0.00338252  -0.00426029  -0.138795     0.00511639   0.0490752   -0.112289     -0.0310918   -0.0724471    0.0183761   0.0825124   -0.0184591   -0.0113808    0.0776396    -0.155963    -0.0104773    0.00373727  -0.0333809   0.167294     0.0887644    0.033946     0.00720572  -0.0647211     0.0769603
  0.17213       0.0306021   -0.122213    -3.49647e-5  -0.00923427  -0.0122201    0.0590664    0.0274661    0.0785366    -0.231433     0.0220782    0.043767    0.0393036    0.0481614   -0.0262235    0.0234097     0.1711      -0.0943578   -0.13675     -0.0356286   0.108239     0.222577    -0.0520086   -0.07637     -0.11163      -0.0522141
 -0.0281146     0.00266661  -0.0125392    0.067109     0.0429345    0.0360674   -0.0288118    0.0880804   -0.0370841     0.130018     0.100996    -0.0930181   0.0731281   -0.119504     0.0643111   -0.168914      0.0757582    0.11206      0.12311     -0.0566113  -0.0218088    0.047155    -0.0940782    0.0698696    0.110772      0.137537 
  0.107599      0.156943     0.232703     0.0403246    0.114165     0.077598     0.0292023    0.178238    -0.0731564     0.202619    -0.157074     0.0788424  -0.0499631    0.178371     0.0214094    0.163436      0.12101      0.0577251    0.217497    -0.0852052   0.0454695    0.131415     0.0215369    0.193397     0.0315378    -0.0289835
 -0.0169074     0.0364058    0.0785015    0.0752743   -0.0976179   -0.23214      0.0329821    0.152061    -0.013119     -0.0780081   -0.114107     0.0489644   0.0466391   -0.0108094    0.035523     0.0253537     0.0090351   -0.00495044  -0.0155071   -0.111509   -0.169555    -0.0394071    0.14809     -0.165516    -0.170431      0.0158372
 -0.164683     -0.0734601    0.158471     0.0642774   -0.0375328   -0.0011417    0.0732009    0.101756     0.0278959    -0.0556138    0.130656     0.0639472  -0.00306029   0.19478      0.0977205    0.0447215    -0.0292063    0.0452424    0.0518343   -0.100867   -0.0274812    0.0207687    0.0319987   -0.0346105   -0.163511     -0.0407699
  0.0740855     0.00850113   0.0312298    0.146031    -0.0995243   -0.0888421    0.131737    -0.0536834   -0.244292      0.027642     0.139241    -0.215291    0.218781    -0.00720056   0.0247437   -0.13857      -0.0460997   -0.1828      -0.0255678    0.030919   -0.160102     0.0837713   -0.0525977   -0.091652     0.148326      0.0457107
 -0.182423     -0.11343      0.0902578   -0.184812     0.0895603    0.0612395   -0.173988    -0.014258     0.125844     -0.0199734   -0.00598148   0.0282642   0.158312    -0.105734     0.0027582    0.0891231     0.0757614   -0.0221214   -0.0200064   -0.157197    0.0625828   -0.110017     0.0101429    0.166668     0.0976757     0.0601165
  0.236367      0.196445    -0.042253    -0.0228857   -0.00652768   0.105809    -0.0189956    0.00154886  -0.0464229    -0.0240208   -0.0607904    0.0307197   0.161754     0.080694    -0.121708    -0.134628      0.208977     0.0516656    0.0852157   -0.0121426  -0.231461    -0.019029    -0.0548137    0.217351     0.0212744    -0.0127017
 -0.0288625     0.204006    -0.139367    -0.0252878    0.0583389   -0.0529074   -0.034009    -0.13885     -0.129855      0.00332841   0.0160196   -0.0994373   0.167636     0.0230283   -0.0682526   -0.0390493    -0.148898     0.0872215   -0.156356     0.0104837  -0.0457925   -0.0431157    0.0454742   -0.0579431   -0.0170392    -0.0284517kind diag, method split
0: avll = -1.413181436225129
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.413265
INFO: iteration 2, average log likelihood -1.413188
INFO: iteration 3, average log likelihood -1.412763
INFO: iteration 4, average log likelihood -1.408161
INFO: iteration 5, average log likelihood -1.393285
INFO: iteration 6, average log likelihood -1.384130
INFO: iteration 7, average log likelihood -1.381435
INFO: iteration 8, average log likelihood -1.379688
INFO: iteration 9, average log likelihood -1.378509
INFO: iteration 10, average log likelihood -1.377806
INFO: iteration 11, average log likelihood -1.377407
INFO: iteration 12, average log likelihood -1.377174
INFO: iteration 13, average log likelihood -1.377033
INFO: iteration 14, average log likelihood -1.376946
INFO: iteration 15, average log likelihood -1.376888
INFO: iteration 16, average log likelihood -1.376848
INFO: iteration 17, average log likelihood -1.376817
INFO: iteration 18, average log likelihood -1.376791
INFO: iteration 19, average log likelihood -1.376763
INFO: iteration 20, average log likelihood -1.376719
INFO: iteration 21, average log likelihood -1.376608
INFO: iteration 22, average log likelihood -1.376382
INFO: iteration 23, average log likelihood -1.376177
INFO: iteration 24, average log likelihood -1.376049
INFO: iteration 25, average log likelihood -1.375963
INFO: iteration 26, average log likelihood -1.375893
INFO: iteration 27, average log likelihood -1.375827
INFO: iteration 28, average log likelihood -1.375759
INFO: iteration 29, average log likelihood -1.375682
INFO: iteration 30, average log likelihood -1.375598
INFO: iteration 31, average log likelihood -1.375519
INFO: iteration 32, average log likelihood -1.375455
INFO: iteration 33, average log likelihood -1.375403
INFO: iteration 34, average log likelihood -1.375359
INFO: iteration 35, average log likelihood -1.375322
INFO: iteration 36, average log likelihood -1.375289
INFO: iteration 37, average log likelihood -1.375261
INFO: iteration 38, average log likelihood -1.375238
INFO: iteration 39, average log likelihood -1.375219
INFO: iteration 40, average log likelihood -1.375204
INFO: iteration 41, average log likelihood -1.375192
INFO: iteration 42, average log likelihood -1.375181
INFO: iteration 43, average log likelihood -1.375173
INFO: iteration 44, average log likelihood -1.375165
INFO: iteration 45, average log likelihood -1.375159
INFO: iteration 46, average log likelihood -1.375153
INFO: iteration 47, average log likelihood -1.375147
INFO: iteration 48, average log likelihood -1.375142
INFO: iteration 49, average log likelihood -1.375138
INFO: iteration 50, average log likelihood -1.375134
INFO: EM with 100000 data points 50 iterations avll -1.375134
952.4 data points per parameter
1: avll = [-1.41326,-1.41319,-1.41276,-1.40816,-1.39328,-1.38413,-1.38144,-1.37969,-1.37851,-1.37781,-1.37741,-1.37717,-1.37703,-1.37695,-1.37689,-1.37685,-1.37682,-1.37679,-1.37676,-1.37672,-1.37661,-1.37638,-1.37618,-1.37605,-1.37596,-1.37589,-1.37583,-1.37576,-1.37568,-1.3756,-1.37552,-1.37545,-1.3754,-1.37536,-1.37532,-1.37529,-1.37526,-1.37524,-1.37522,-1.3752,-1.37519,-1.37518,-1.37517,-1.37517,-1.37516,-1.37515,-1.37515,-1.37514,-1.37514,-1.37513]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.375286
INFO: iteration 2, average log likelihood -1.375149
INFO: iteration 3, average log likelihood -1.374577
INFO: iteration 4, average log likelihood -1.368610
INFO: iteration 5, average log likelihood -1.350699
INFO: iteration 6, average log likelihood -1.339905
INFO: iteration 7, average log likelihood -1.336450
INFO: iteration 8, average log likelihood -1.334017
INFO: iteration 9, average log likelihood -1.332295
INFO: iteration 10, average log likelihood -1.331145
INFO: iteration 11, average log likelihood -1.330351
INFO: iteration 12, average log likelihood -1.329745
INFO: iteration 13, average log likelihood -1.329255
INFO: iteration 14, average log likelihood -1.328845
INFO: iteration 15, average log likelihood -1.328483
INFO: iteration 16, average log likelihood -1.328152
INFO: iteration 17, average log likelihood -1.327842
INFO: iteration 18, average log likelihood -1.327538
INFO: iteration 19, average log likelihood -1.327222
INFO: iteration 20, average log likelihood -1.326879
INFO: iteration 21, average log likelihood -1.326496
INFO: iteration 22, average log likelihood -1.326059
INFO: iteration 23, average log likelihood -1.325580
INFO: iteration 24, average log likelihood -1.325142
INFO: iteration 25, average log likelihood -1.324794
INFO: iteration 26, average log likelihood -1.324527
INFO: iteration 27, average log likelihood -1.324323
INFO: iteration 28, average log likelihood -1.324168
INFO: iteration 29, average log likelihood -1.324053
INFO: iteration 30, average log likelihood -1.323967
INFO: iteration 31, average log likelihood -1.323899
INFO: iteration 32, average log likelihood -1.323843
INFO: iteration 33, average log likelihood -1.323795
INFO: iteration 34, average log likelihood -1.323753
INFO: iteration 35, average log likelihood -1.323714
INFO: iteration 36, average log likelihood -1.323680
INFO: iteration 37, average log likelihood -1.323648
INFO: iteration 38, average log likelihood -1.323617
INFO: iteration 39, average log likelihood -1.323588
INFO: iteration 40, average log likelihood -1.323558
INFO: iteration 41, average log likelihood -1.323529
INFO: iteration 42, average log likelihood -1.323500
INFO: iteration 43, average log likelihood -1.323472
INFO: iteration 44, average log likelihood -1.323447
INFO: iteration 45, average log likelihood -1.323424
INFO: iteration 46, average log likelihood -1.323405
INFO: iteration 47, average log likelihood -1.323390
INFO: iteration 48, average log likelihood -1.323378
INFO: iteration 49, average log likelihood -1.323370
INFO: iteration 50, average log likelihood -1.323363
INFO: EM with 100000 data points 50 iterations avll -1.323363
473.9 data points per parameter
2: avll = [-1.37529,-1.37515,-1.37458,-1.36861,-1.3507,-1.3399,-1.33645,-1.33402,-1.33229,-1.33115,-1.33035,-1.32974,-1.32926,-1.32885,-1.32848,-1.32815,-1.32784,-1.32754,-1.32722,-1.32688,-1.3265,-1.32606,-1.32558,-1.32514,-1.32479,-1.32453,-1.32432,-1.32417,-1.32405,-1.32397,-1.3239,-1.32384,-1.32379,-1.32375,-1.32371,-1.32368,-1.32365,-1.32362,-1.32359,-1.32356,-1.32353,-1.3235,-1.32347,-1.32345,-1.32342,-1.32341,-1.32339,-1.32338,-1.32337,-1.32336]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.323568
INFO: iteration 2, average log likelihood -1.323377
INFO: iteration 3, average log likelihood -1.322851
INFO: iteration 4, average log likelihood -1.317826
INFO: iteration 5, average log likelihood -1.301984
INFO: iteration 6, average log likelihood -1.286789
INFO: iteration 7, average log likelihood -1.278448
INFO: iteration 8, average log likelihood -1.273487
INFO: iteration 9, average log likelihood -1.270079
INFO: iteration 10, average log likelihood -1.267613
INFO: iteration 11, average log likelihood -1.265847
INFO: iteration 12, average log likelihood -1.264919
INFO: iteration 13, average log likelihood -1.264541
INFO: iteration 14, average log likelihood -1.264314
INFO: iteration 15, average log likelihood -1.264133
INFO: iteration 16, average log likelihood -1.263975
INFO: iteration 17, average log likelihood -1.263827
INFO: iteration 18, average log likelihood -1.263683
INFO: iteration 19, average log likelihood -1.263533
INFO: iteration 20, average log likelihood -1.263343
INFO: iteration 21, average log likelihood -1.263078
INFO: iteration 22, average log likelihood -1.262734
INFO: iteration 23, average log likelihood -1.262363
INFO: iteration 24, average log likelihood -1.262077
INFO: iteration 25, average log likelihood -1.261930
INFO: iteration 26, average log likelihood -1.261877
INFO: iteration 27, average log likelihood -1.261856
INFO: iteration 28, average log likelihood -1.261844
INFO: iteration 29, average log likelihood -1.261835
INFO: iteration 30, average log likelihood -1.261829
INFO: iteration 31, average log likelihood -1.261824
INFO: iteration 32, average log likelihood -1.261821
INFO: iteration 33, average log likelihood -1.261818
INFO: iteration 34, average log likelihood -1.261816
INFO: iteration 35, average log likelihood -1.261815
INFO: iteration 36, average log likelihood -1.261813
INFO: iteration 37, average log likelihood -1.261812
INFO: iteration 38, average log likelihood -1.261812
INFO: iteration 39, average log likelihood -1.261811
INFO: iteration 40, average log likelihood -1.261810
INFO: iteration 41, average log likelihood -1.261810
INFO: iteration 42, average log likelihood -1.261810
INFO: iteration 43, average log likelihood -1.261809
INFO: iteration 44, average log likelihood -1.261809
INFO: iteration 45, average log likelihood -1.261809
INFO: iteration 46, average log likelihood -1.261809
INFO: iteration 47, average log likelihood -1.261809
INFO: iteration 48, average log likelihood -1.261809
INFO: iteration 49, average log likelihood -1.261809
INFO: iteration 50, average log likelihood -1.261809
INFO: EM with 100000 data points 50 iterations avll -1.261809
236.4 data points per parameter
3: avll = [-1.32357,-1.32338,-1.32285,-1.31783,-1.30198,-1.28679,-1.27845,-1.27349,-1.27008,-1.26761,-1.26585,-1.26492,-1.26454,-1.26431,-1.26413,-1.26398,-1.26383,-1.26368,-1.26353,-1.26334,-1.26308,-1.26273,-1.26236,-1.26208,-1.26193,-1.26188,-1.26186,-1.26184,-1.26184,-1.26183,-1.26182,-1.26182,-1.26182,-1.26182,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.262054
INFO: iteration 2, average log likelihood -1.261780
INFO: iteration 3, average log likelihood -1.259999
WARNING: Variances had to be floored 10
INFO: iteration 4, average log likelihood -1.238661
WARNING: Variances had to be floored 3 4 10
INFO: iteration 5, average log likelihood -1.209063
WARNING: Variances had to be floored 10
INFO: iteration 6, average log likelihood -1.205449
WARNING: Variances had to be floored 4 10
INFO: iteration 7, average log likelihood -1.186142
WARNING: Variances had to be floored 3 10
INFO: iteration 8, average log likelihood -1.185974
WARNING: Variances had to be floored 4 10
INFO: iteration 9, average log likelihood -1.178471
WARNING: Variances had to be floored 3 10
INFO: iteration 10, average log likelihood -1.180662
WARNING: Variances had to be floored 4 10
INFO: iteration 11, average log likelihood -1.175393
WARNING: Variances had to be floored 3 10
INFO: iteration 12, average log likelihood -1.179753
WARNING: Variances had to be floored 4 10
INFO: iteration 13, average log likelihood -1.175040
WARNING: Variances had to be floored 3 10
INFO: iteration 14, average log likelihood -1.179556
WARNING: Variances had to be floored 4 10
INFO: iteration 15, average log likelihood -1.174913
WARNING: Variances had to be floored 3 10
INFO: iteration 16, average log likelihood -1.179462
WARNING: Variances had to be floored 4 10
INFO: iteration 17, average log likelihood -1.174850
WARNING: Variances had to be floored 3 10
INFO: iteration 18, average log likelihood -1.179407
WARNING: Variances had to be floored 4 10
INFO: iteration 19, average log likelihood -1.174806
WARNING: Variances had to be floored 3 10
INFO: iteration 20, average log likelihood -1.179361
WARNING: Variances had to be floored 4 10
INFO: iteration 21, average log likelihood -1.174758
WARNING: Variances had to be floored 3 10
INFO: iteration 22, average log likelihood -1.179301
WARNING: Variances had to be floored 4 10
INFO: iteration 23, average log likelihood -1.174675
WARNING: Variances had to be floored 3 10
INFO: iteration 24, average log likelihood -1.179177
WARNING: Variances had to be floored 4 10
INFO: iteration 25, average log likelihood -1.174482
WARNING: Variances had to be floored 3 10
INFO: iteration 26, average log likelihood -1.178861
WARNING: Variances had to be floored 4 10
INFO: iteration 27, average log likelihood -1.174015
WARNING: Variances had to be floored 3 10
INFO: iteration 28, average log likelihood -1.178088
WARNING: Variances had to be floored 4 10
INFO: iteration 29, average log likelihood -1.173061
WARNING: Variances had to be floored 3 10
INFO: iteration 30, average log likelihood -1.177164
WARNING: Variances had to be floored 4 10
INFO: iteration 31, average log likelihood -1.172198
WARNING: Variances had to be floored 3 10
INFO: iteration 32, average log likelihood -1.176354
WARNING: Variances had to be floored 4 10
INFO: iteration 33, average log likelihood -1.171283
WARNING: Variances had to be floored 3 10
INFO: iteration 34, average log likelihood -1.175543
WARNING: Variances had to be floored 4 10
INFO: iteration 35, average log likelihood -1.170625
WARNING: Variances had to be floored 3 10
INFO: iteration 36, average log likelihood -1.175115
WARNING: Variances had to be floored 4 10
INFO: iteration 37, average log likelihood -1.170353
WARNING: Variances had to be floored 3 10
INFO: iteration 38, average log likelihood -1.174941
WARNING: Variances had to be floored 4 10
INFO: iteration 39, average log likelihood -1.170259
WARNING: Variances had to be floored 3 10
INFO: iteration 40, average log likelihood -1.174880
WARNING: Variances had to be floored 4 10
INFO: iteration 41, average log likelihood -1.170230
WARNING: Variances had to be floored 3 10
INFO: iteration 42, average log likelihood -1.174860
WARNING: Variances had to be floored 4 10
INFO: iteration 43, average log likelihood -1.170216
WARNING: Variances had to be floored 3 10
INFO: iteration 44, average log likelihood -1.174851
WARNING: Variances had to be floored 4 10
INFO: iteration 45, average log likelihood -1.170205
WARNING: Variances had to be floored 3 10
INFO: iteration 46, average log likelihood -1.174845
WARNING: Variances had to be floored 4 10
INFO: iteration 47, average log likelihood -1.170196
WARNING: Variances had to be floored 3 10
INFO: iteration 48, average log likelihood -1.174839
WARNING: Variances had to be floored 4 10
INFO: iteration 49, average log likelihood -1.170188
WARNING: Variances had to be floored 3 10
INFO: iteration 50, average log likelihood -1.174834
INFO: EM with 100000 data points 50 iterations avll -1.174834
118.1 data points per parameter
4: avll = [-1.26205,-1.26178,-1.26,-1.23866,-1.20906,-1.20545,-1.18614,-1.18597,-1.17847,-1.18066,-1.17539,-1.17975,-1.17504,-1.17956,-1.17491,-1.17946,-1.17485,-1.17941,-1.17481,-1.17936,-1.17476,-1.1793,-1.17467,-1.17918,-1.17448,-1.17886,-1.17402,-1.17809,-1.17306,-1.17716,-1.1722,-1.17635,-1.17128,-1.17554,-1.17062,-1.17512,-1.17035,-1.17494,-1.17026,-1.17488,-1.17023,-1.17486,-1.17022,-1.17485,-1.17021,-1.17484,-1.1702,-1.17484,-1.17019,-1.17483]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 7 8 19 20
INFO: iteration 1, average log likelihood -1.170495
WARNING: Variances had to be floored 5 6 7 8 19 20
INFO: iteration 2, average log likelihood -1.164326
WARNING: Variances had to be floored 7 8 19 20
INFO: iteration 3, average log likelihood -1.168489
WARNING: Variances had to be floored 5 6 7 8 19 20
INFO: iteration 4, average log likelihood -1.140007
WARNING: Variances had to be floored 2 5 6 7 8 19 20 21 24 28 31 32
INFO: iteration 5, average log likelihood -1.081335
WARNING: Variances had to be floored 5 6 7 8 12 15 16 19 20
INFO: iteration 6, average log likelihood -1.090212
WARNING: Variances had to be floored 2 3 5 6 7 8 19 20 21 24 28 32
INFO: iteration 7, average log likelihood -1.082681
WARNING: Variances had to be floored 5 6 7 8 19 20 31 32
INFO: iteration 8, average log likelihood -1.092154
WARNING: Variances had to be floored 2 5 6 7 8 10 12 15 16 19 20 21 24 28
INFO: iteration 9, average log likelihood -1.045567
WARNING: Variances had to be floored 5 6 7 8 19 20 32
INFO: iteration 10, average log likelihood -1.103039
WARNING: Variances had to be floored 2 3 5 6 7 8 19 20 21 24 28 31 32
INFO: iteration 11, average log likelihood -1.056534
WARNING: Variances had to be floored 5 6 7 8 10 12 15 16 19 20
INFO: iteration 12, average log likelihood -1.074810
WARNING: Variances had to be floored 2 5 6 7 8 19 20 21 24 28 32
INFO: iteration 13, average log likelihood -1.079037
WARNING: Variances had to be floored 5 6 7 8 19 20 31 32
INFO: iteration 14, average log likelihood -1.079662
WARNING: Variances had to be floored 2 3 5 6 7 8 10 12 15 16 19 20 21 28
INFO: iteration 15, average log likelihood -1.040331
WARNING: Variances had to be floored 5 6 7 8 19 20 32
INFO: iteration 16, average log likelihood -1.104214
WARNING: Variances had to be floored 2 5 6 7 8 19 20 21 24 28 31 32
INFO: iteration 17, average log likelihood -1.058170
WARNING: Variances had to be floored 3 5 6 7 8 10 12 15 16 19 20
INFO: iteration 18, average log likelihood -1.062645
WARNING: Variances had to be floored 2 5 6 7 8 19 20 21 28 32
INFO: iteration 19, average log likelihood -1.083530
WARNING: Variances had to be floored 5 6 7 8 19 20 31 32
INFO: iteration 20, average log likelihood -1.078797
WARNING: Variances had to be floored 2 3 5 6 7 8 10 12 15 16 19 20 21 28
INFO: iteration 21, average log likelihood -1.039919
WARNING: Variances had to be floored 5 6 7 8 19 20 32
INFO: iteration 22, average log likelihood -1.104180
WARNING: Variances had to be floored 2 5 6 7 8 19 20 21 28 31 32
INFO: iteration 23, average log likelihood -1.058153
WARNING: Variances had to be floored 3 5 6 7 8 10 12 15 16 19 20
INFO: iteration 24, average log likelihood -1.059356
WARNING: Variances had to be floored 2 5 6 7 8 19 20 21 28 32
INFO: iteration 25, average log likelihood -1.083413
WARNING: Variances had to be floored 5 6 7 8 19 20 31 32
INFO: iteration 26, average log likelihood -1.077851
WARNING: Variances had to be floored 2 3 5 6 7 8 10 12 15 16 19 20 21 28
INFO: iteration 27, average log likelihood -1.038866
WARNING: Variances had to be floored 5 6 7 8 19 20 32
INFO: iteration 28, average log likelihood -1.103742
WARNING: Variances had to be floored 2 5 6 7 8 19 20 21 28 31 32
INFO: iteration 29, average log likelihood -1.057528
WARNING: Variances had to be floored 3 5 6 7 8 10 12 15 16 19 20
INFO: iteration 30, average log likelihood -1.059064
WARNING: Variances had to be floored 2 5 6 7 8 19 20 21 28 32
INFO: iteration 31, average log likelihood -1.083003
WARNING: Variances had to be floored 5 6 7 8 19 20 31 32
INFO: iteration 32, average log likelihood -1.077709
WARNING: Variances had to be floored 2 3 5 6 7 8 10 12 15 16 19 20 21 28
INFO: iteration 33, average log likelihood -1.038764
WARNING: Variances had to be floored 5 6 7 8 19 20 32
INFO: iteration 34, average log likelihood -1.103504
WARNING: Variances had to be floored 2 5 6 7 8 19 20 21 28 31 32
INFO: iteration 35, average log likelihood -1.057420
WARNING: Variances had to be floored 3 5 6 7 8 10 12 15 16 19 20
INFO: iteration 36, average log likelihood -1.059319
WARNING: Variances had to be floored 2 5 6 7 8 19 20 21 28 32
INFO: iteration 37, average log likelihood -1.082730
WARNING: Variances had to be floored 5 6 7 8 19 20 31 32
INFO: iteration 38, average log likelihood -1.077896
WARNING: Variances had to be floored 2 3 5 6 7 8 10 12 15 16 19 20 21 28
INFO: iteration 39, average log likelihood -1.038964
WARNING: Variances had to be floored 5 6 7 8 19 20 32
INFO: iteration 40, average log likelihood -1.103442
WARNING: Variances had to be floored 2 5 6 7 8 19 20 21 28 31 32
INFO: iteration 41, average log likelihood -1.057554
WARNING: Variances had to be floored 3 5 6 7 8 10 12 15 16 19 20
INFO: iteration 42, average log likelihood -1.059581
WARNING: Variances had to be floored 2 5 6 7 8 19 20 21 28 32
INFO: iteration 43, average log likelihood -1.082682
WARNING: Variances had to be floored 5 6 7 8 19 20 31 32
INFO: iteration 44, average log likelihood -1.078033
WARNING: Variances had to be floored 2 3 5 6 7 8 10 12 15 16 19 20 21 28
INFO: iteration 45, average log likelihood -1.039164
WARNING: Variances had to be floored 5 6 7 8 19 20 32
INFO: iteration 46, average log likelihood -1.103421
WARNING: Variances had to be floored 2 5 6 7 8 19 20 21 28 31 32
INFO: iteration 47, average log likelihood -1.057667
WARNING: Variances had to be floored 3 5 6 7 8 10 12 15 16 19 20
INFO: iteration 48, average log likelihood -1.059759
WARNING: Variances had to be floored 2 5 6 7 8 19 20 21 28 32
INFO: iteration 49, average log likelihood -1.082674
WARNING: Variances had to be floored 5 6 7 8 19 20 31 32
INFO: iteration 50, average log likelihood -1.078137
INFO: EM with 100000 data points 50 iterations avll -1.078137
59.0 data points per parameter
5: avll = [-1.1705,-1.16433,-1.16849,-1.14001,-1.08134,-1.09021,-1.08268,-1.09215,-1.04557,-1.10304,-1.05653,-1.07481,-1.07904,-1.07966,-1.04033,-1.10421,-1.05817,-1.06264,-1.08353,-1.0788,-1.03992,-1.10418,-1.05815,-1.05936,-1.08341,-1.07785,-1.03887,-1.10374,-1.05753,-1.05906,-1.083,-1.07771,-1.03876,-1.1035,-1.05742,-1.05932,-1.08273,-1.0779,-1.03896,-1.10344,-1.05755,-1.05958,-1.08268,-1.07803,-1.03916,-1.10342,-1.05767,-1.05976,-1.08267,-1.07814]
[-1.41318,-1.41326,-1.41319,-1.41276,-1.40816,-1.39328,-1.38413,-1.38144,-1.37969,-1.37851,-1.37781,-1.37741,-1.37717,-1.37703,-1.37695,-1.37689,-1.37685,-1.37682,-1.37679,-1.37676,-1.37672,-1.37661,-1.37638,-1.37618,-1.37605,-1.37596,-1.37589,-1.37583,-1.37576,-1.37568,-1.3756,-1.37552,-1.37545,-1.3754,-1.37536,-1.37532,-1.37529,-1.37526,-1.37524,-1.37522,-1.3752,-1.37519,-1.37518,-1.37517,-1.37517,-1.37516,-1.37515,-1.37515,-1.37514,-1.37514,-1.37513,-1.37529,-1.37515,-1.37458,-1.36861,-1.3507,-1.3399,-1.33645,-1.33402,-1.33229,-1.33115,-1.33035,-1.32974,-1.32926,-1.32885,-1.32848,-1.32815,-1.32784,-1.32754,-1.32722,-1.32688,-1.3265,-1.32606,-1.32558,-1.32514,-1.32479,-1.32453,-1.32432,-1.32417,-1.32405,-1.32397,-1.3239,-1.32384,-1.32379,-1.32375,-1.32371,-1.32368,-1.32365,-1.32362,-1.32359,-1.32356,-1.32353,-1.3235,-1.32347,-1.32345,-1.32342,-1.32341,-1.32339,-1.32338,-1.32337,-1.32336,-1.32357,-1.32338,-1.32285,-1.31783,-1.30198,-1.28679,-1.27845,-1.27349,-1.27008,-1.26761,-1.26585,-1.26492,-1.26454,-1.26431,-1.26413,-1.26398,-1.26383,-1.26368,-1.26353,-1.26334,-1.26308,-1.26273,-1.26236,-1.26208,-1.26193,-1.26188,-1.26186,-1.26184,-1.26184,-1.26183,-1.26182,-1.26182,-1.26182,-1.26182,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181,-1.26181,-1.26205,-1.26178,-1.26,-1.23866,-1.20906,-1.20545,-1.18614,-1.18597,-1.17847,-1.18066,-1.17539,-1.17975,-1.17504,-1.17956,-1.17491,-1.17946,-1.17485,-1.17941,-1.17481,-1.17936,-1.17476,-1.1793,-1.17467,-1.17918,-1.17448,-1.17886,-1.17402,-1.17809,-1.17306,-1.17716,-1.1722,-1.17635,-1.17128,-1.17554,-1.17062,-1.17512,-1.17035,-1.17494,-1.17026,-1.17488,-1.17023,-1.17486,-1.17022,-1.17485,-1.17021,-1.17484,-1.1702,-1.17484,-1.17019,-1.17483,-1.1705,-1.16433,-1.16849,-1.14001,-1.08134,-1.09021,-1.08268,-1.09215,-1.04557,-1.10304,-1.05653,-1.07481,-1.07904,-1.07966,-1.04033,-1.10421,-1.05817,-1.06264,-1.08353,-1.0788,-1.03992,-1.10418,-1.05815,-1.05936,-1.08341,-1.07785,-1.03887,-1.10374,-1.05753,-1.05906,-1.083,-1.07771,-1.03876,-1.1035,-1.05742,-1.05932,-1.08273,-1.0779,-1.03896,-1.10344,-1.05755,-1.05958,-1.08268,-1.07803,-1.03916,-1.10342,-1.05767,-1.05976,-1.08267,-1.07814]
32×26 Array{Float64,2}:
  0.149516      0.0236832   -0.117879    -0.00328927   0.0264176   -0.0154042    0.0616885     0.0320183     0.0909743   -0.190062     0.0729003    0.0471818   -0.0177765     0.0541752   -0.0399362    0.0296401    0.175442    -0.113451   -0.161721    -0.0356868    0.109778      0.223368    -0.0600135  -0.0703725    -0.103974    -0.124101  
 -0.0201546     0.183302    -0.118105    -0.0247662    0.0347597   -0.0571707   -0.0309569    -0.150052     -0.115438    -0.0104571    0.0181398   -0.100534     0.130247      0.0214968   -0.0544443   -0.0400801   -0.141367     0.0920296  -0.16242      0.00588336  -0.0436235    -0.0413677    0.0573285  -0.0680089    -0.0040965   -0.0371068 
 -0.224924     -0.100364     0.0732394   -0.177231     0.0949289    0.0473047   -0.159792     -0.00341015    0.123574    -0.0168666   -0.0122592    0.0185745    0.110071     -0.108709    -0.00311755   0.0908215    0.0740172   -0.0248563  -0.0253348   -0.152961     0.0585371    -0.116015     0.0220183   0.162673      0.112888     0.0384522 
  0.010568      0.143095     0.15522     -0.0752687   -0.0422921    0.127932     0.00498526    0.103873      0.0803642   -0.0404034    0.0056363   -0.0780473    0.0359258     0.155262     0.0579393    0.067293     0.114264    -0.0349134  -0.0134802   -0.105726     0.153018     -0.0113109    0.0573315   0.0490647    -0.00468843   0.12808   
  0.0928501     0.156564    -0.0776942   -0.257036     0.151268    -0.143253     0.0810974     0.176151     -0.0908989    0.347025    -0.158594     0.0863448   -0.0644804     0.175341     0.610864     0.158393     0.0933616    0.0998069   0.215541    -0.0861177    0.0423133    -0.326653     0.0251133   0.571856      0.528387    -0.058806  
  0.114394      0.157252     0.532811     0.289048     0.0320133    0.203738     0.0166221     0.181959     -0.0498943    0.115063    -0.151946     0.0739557   -0.0475212     0.178469    -0.566078     0.183252     0.107549     0.0370755   0.20378      0.00934086   0.0484722     0.575405     0.0197398  -0.0920354    -0.465805    -0.0351841 
 -0.174791     -0.0501582    0.183356    -0.136249     0.295004     0.0412396   -0.0535258    -0.0625667     0.0772352   -0.0286636   -0.0314631   -0.0179038   -0.19991       0.080493    -0.0134837    0.0862404   -0.160107    -0.243035   -0.210597     0.334469     0.112815      0.112186    -0.0249314   0.25071       0.152091    -0.0373335 
  0.229074     -0.0502488    0.214249    -0.136083     0.00360843   0.0484465   -0.0368736     0.0160987    -0.200569    -0.0407753    0.0437887   -0.0152402   -0.0663814     0.0891915   -0.0127225    0.0860611    0.377297     0.176474    0.0228636   -0.184784     0.0125803     0.109823    -0.024224    0.254319      0.165622     0.308284  
 -0.161307      0.0723956   -0.0787208   -0.122539    -0.289894    -0.0114644   -0.0301263    -0.0102068     0.0268705   -0.141334    -0.0815093    0.185201    -0.0955984     0.0106219    0.125074     0.0468447   -0.107871    -0.0609953  -0.0817268    0.112011    -0.063858     -0.0808653   -0.0718622   0.114805     -0.0113557   -0.0218751 
 -0.00572185    0.226277     0.0775141   -0.0701706    0.0703364    0.151987    -0.0406442     0.0606834     0.0675358    0.232391    -0.0671019    0.0948992    0.132224      0.0441139   -0.100418    -0.0870654   -0.0695274   -0.103947   -0.0805074    0.100668    -0.0441953     0.0892694    0.0848911  -0.101036     -0.166141    -0.155133  
  0.0096791     0.00391605  -0.00738335   0.0412008   -0.0155847   -0.145885     0.00882335    0.0641363    -0.112359    -0.0292812   -0.0784599    0.0187114    0.105304     -0.0252358   -0.00274592   0.0577106   -0.146071    -0.0106598   0.0041463   -0.028485     0.166957      0.0958523    0.0393972   0.00898545   -0.0587829    0.0759677 
  0.0249009    -0.0631955    0.0865659    0.0591058    0.0249049   -0.00559725   0.0488749     0.0569128     0.0283955   -0.0449878   -0.122256     0.155915    -0.153682     -0.00627144  -0.0221967   -0.117098     0.139862     0.130737    0.121386     0.0264048    0.102899      0.0358373    0.0801503   0.196205      0.238718     0.0141684 
 -0.262933     -0.0986954    0.0906929    0.12668      0.201726     0.041133    -0.0858413    -0.225979      0.0234099   -0.067299    -0.0178324    0.0632576    0.00377238    0.125453    -0.87347      0.0248698    0.184337    -0.157255    0.137307     0.0356508   -0.0146536    -0.209691     0.120687    0.170289      0.129331    -0.0938811 
 -0.23685      -0.0349958    0.0317091    0.153384     0.209304     0.144479    -0.051069     -0.00534513   -0.0349139   -0.0645515    0.0228228    0.0184034    0.0297715    -0.0983312    0.66081      0.0448029    0.0490545   -0.0536097   0.130232     0.00970413  -0.00297467   -0.140423     0.100101    0.238891      0.0475292   -0.142204  
 -0.167144     -0.100182     0.159043     0.0977589   -0.035948     0.0100048    0.080863      0.103941      0.0172899   -0.0746978    0.167234     0.048135    -0.0097027     0.200312     0.0874067    0.0325668    0.00136444   0.0603589   0.0484591   -0.107182    -0.032245      0.0157704    0.0344286  -0.0302799    -0.142847    -0.00957447
  0.123342      0.0256968    0.0864701   -0.00638724   0.0660057    0.0450407    0.14907       0.0164556     0.174846    -0.0878258    0.202865     0.0275255    0.0621       -0.0439305   -0.0750765    0.291103     0.0859144   -0.130656    0.0913943   -0.00819991  -0.0281268     0.0841749    0.0471034   0.130842     -0.282312     0.0748004 
 -0.352814     -0.257746     0.218763     0.0997202   -0.197641     0.0728978   -0.201855      0.0512809    -0.0100394    0.0290162   -0.00257824  -0.0681571   -0.0117037    -0.1352      -0.0175697   -0.142456     0.0673107    0.174948   -0.069936     0.0589051   -0.118744      0.10693     -0.176561   -0.0868358    -0.100472    -0.0465935 
  0.589007     -0.191294     0.107777     0.123249     0.0484764    0.0372799   -0.181317      0.0495449     0.00839808  -0.025037    -0.052804    -0.0424764    0.0843112    -0.0913413   -0.0757709   -0.0678761    0.0275625    0.196308   -0.0510363    0.0506601   -0.273793     -0.107572     0.099017   -0.00233359   -0.103362    -0.0242113 
  0.0652078     0.0791894   -0.107095     0.106365    -0.0912568   -0.00177868  -0.0223198     0.0155934     0.0274786   -0.105281     0.0495859   -0.189468     0.0833781    -0.0700955    0.0705292    0.136258     0.0299722    0.0295022   0.0358105   -1.08122     -0.0193905    -0.00733253  -0.0303611  -0.0740175     0.134583    -0.115293  
  0.0493161     0.0798161   -0.110602     0.126388    -0.092268    -0.0872198   -0.0223761     0.020316      0.0315157    0.0793557    0.0464161   -0.183322     0.0808245    -0.0722473    0.119274     0.0529038    0.0310044    0.0181901   0.0741184    1.22077      0.0930612    -0.0410492   -0.0346177  -0.0567765     0.129821    -0.115396  
 -0.0262124     0.0474743    0.0770552    0.0711341   -0.0482861   -0.236103     0.0358892     0.153961     -0.00764767  -0.0154294   -0.138755     0.052293     0.0209295    -0.00988346   0.0375227    0.0428725   -3.2772e-5   -0.0331495  -0.028615    -0.109227    -0.167085     -0.0493013    0.134453   -0.138493     -0.277837     0.00844443
 -0.127805     -0.0222493   -0.0355716   -0.0383131    0.059846     0.112123    -0.176638     -0.170469     -0.259757    -0.0511418    0.0488885    0.0317506   -0.120627     -0.0205216   -0.0119226    0.188315     0.00435077  -0.152488   -0.0762237   -0.011671    -0.00833125    0.0478202    0.028139   -0.0722832     0.0144794    0.0481378 
  0.0274855     0.011865     0.0552229    0.127857    -0.0446987   -0.0511657    0.0493672     0.00489229   -0.0878378   -0.020147     0.114106    -0.0453482    0.115088     -0.0443748   -0.0248418   -0.0621595   -0.0278453   -0.189075   -0.103134     0.0346636   -0.0970508     0.103973    -0.0332975  -0.150717      0.0706375    0.0444986 
 -0.0634983    -0.101671    -0.0944261   -0.0304835    0.0277346    0.0502108   -0.0514538    -0.0209084     0.0146372   -0.0661571    0.00678508   0.0309162    0.0821791    -0.0259416    0.205851     0.043296     0.0692502   -0.0385171  -0.00996193  -0.110281    -0.0735447    -0.00696764   0.0231084   0.0135133    -0.0728091   -0.0258079 
 -0.0912619     0.0261062   -0.107088    -0.14062     -0.119752    -0.112328     0.0205292     0.1338        0.0303999   -0.0134555   -0.150556    -0.060022    -0.000324883   0.113053    -0.0624393   -0.0640972   -0.119448    -0.0440623   0.0220367   -0.0200368   -0.10483       0.0219642    0.187162    0.194474      0.160572     0.179443  
 -0.0217612    -0.183917    -0.0350721   -0.0186382    0.105488    -0.190177    -0.0498969     0.161073     -0.0877291    0.165365     0.0699165   -0.0562278    0.0522336     0.10864      0.0288582    0.0153539   -0.159948     0.0661435   0.20504      0.131689     0.0581581     0.122463    -0.0539494  -0.0148675    -0.108141     0.0453675 
 -0.018789      0.0162248   -0.0129943    0.0743249    0.0348475    0.0493907   -0.0260665     0.0877655    -0.0563952    0.13468      0.106186    -0.0828915    0.0934146    -0.132843     0.0534244   -0.177981     0.0467172    0.131529    0.139358    -0.0785761   -0.0265119     0.0482938   -0.0837955   0.0781529     0.118412     0.146677  
 -0.00431187    0.118375    -0.0940535    0.0952962    0.17023      0.0487528    0.0593277     0.0744098    -0.00319635   0.0340138   -0.0495444   -0.019537    -0.110383      0.0122289    0.175554    -0.00317956  -0.074505    -0.0720627  -0.0162687   -0.073691     0.000108929   0.120013    -0.09904    -0.0190247    -0.0678685   -0.177871  
  0.000105592  -0.0491021   -0.0542941   -0.0364281    0.112123     0.0223639    0.0657715     0.0494399     0.0970474    0.00552165   0.061509    -0.00384541   0.136764     -0.0489564   -0.0772119    0.0795339   -0.0430255   -0.0325561  -0.0676166   -0.0249117   -0.0757626     0.136616     0.0209836   0.0461446    -0.104733     0.0530533 
 -0.0253421     0.0286406    0.0167597   -0.0721696   -0.00427711  -0.00432272   0.0561664     0.0976072     0.00478328  -0.044079     0.0115616    0.0614382   -0.00341477    0.0879895   -0.00472692   0.0379295    0.0624028    0.0995121  -0.0484416    0.0787488    0.0152506     0.0172365    0.0888607   0.000103407  -0.0600362    0.0648424 
  0.0144347     0.260323     0.0400048   -0.103342     0.020493    -0.094224     0.212731     -0.0123871    -0.107962    -0.045724    -0.101883     0.229275    -0.0906935    -0.0131381   -0.0201605   -0.0325908    0.0480745    0.0208974   0.0386229    0.037887     0.0365311    -0.0708698    0.0877731  -0.0444794     0.0222106    0.0400809 
  0.23487       0.180728    -0.0445226   -0.0413328   -0.00566127   0.0599923    0.000697178   0.000730362  -0.0352233   -0.017064    -0.0610897    0.0328217    0.156913      0.0850524   -0.123533    -0.14147      0.243904     0.0338405   0.0835496    0.0100043   -0.225018     -0.0158691   -0.0581453   0.223946      0.0184226   -0.013183  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 2 3 5 6 7 8 10 12 15 16 19 20 21 28
INFO: iteration 1, average log likelihood -1.039352
WARNING: Variances had to be floored 2 3 5 6 7 8 10 12 15 16 19 20 21 28 32
INFO: iteration 2, average log likelihood -1.035741
WARNING: Variances had to be floored 2 3 5 6 7 8 10 12 15 16 19 20 21 28 31 32
INFO: iteration 3, average log likelihood -1.033458
WARNING: Variances had to be floored 2 3 5 6 7 8 10 12 15 16 19 20 21 28
INFO: iteration 4, average log likelihood -1.039287
WARNING: Variances had to be floored 2 3 5 6 7 8 10 12 15 16 19 20 21 28 32
INFO: iteration 5, average log likelihood -1.035738
WARNING: Variances had to be floored 2 3 5 6 7 8 10 12 15 16 19 20 21 28 31 32
INFO: iteration 6, average log likelihood -1.033451
WARNING: Variances had to be floored 2 3 5 6 7 8 10 12 15 16 19 20 21 28
INFO: iteration 7, average log likelihood -1.039286
WARNING: Variances had to be floored 2 3 5 6 7 8 10 12 15 16 19 20 21 28 32
INFO: iteration 8, average log likelihood -1.035737
WARNING: Variances had to be floored 2 3 5 6 7 8 10 12 15 16 19 20 21 28 31 32
INFO: iteration 9, average log likelihood -1.033448
WARNING: Variances had to be floored 2 3 5 6 7 8 10 12 15 16 19 20 21 28
INFO: iteration 10, average log likelihood -1.039285
INFO: EM with 100000 data points 10 iterations avll -1.039285
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.074329e+05
      1       6.889264e+05      -2.185065e+05 |       32
      2       6.596915e+05      -2.923489e+04 |       32
      3       6.443351e+05      -1.535641e+04 |       32
      4       6.352506e+05      -9.084525e+03 |       32
      5       6.303187e+05      -4.931871e+03 |       32
      6       6.270885e+05      -3.230241e+03 |       32
      7       6.245241e+05      -2.564359e+03 |       32
      8       6.223403e+05      -2.183777e+03 |       32
      9       6.207572e+05      -1.583149e+03 |       32
     10       6.197816e+05      -9.755335e+02 |       32
     11       6.190020e+05      -7.796286e+02 |       32
     12       6.181073e+05      -8.947516e+02 |       32
     13       6.170132e+05      -1.094062e+03 |       32
     14       6.160341e+05      -9.791051e+02 |       32
     15       6.153120e+05      -7.220457e+02 |       32
     16       6.145762e+05      -7.358450e+02 |       32
     17       6.138080e+05      -7.682360e+02 |       31
     18       6.131734e+05      -6.345957e+02 |       32
     19       6.127724e+05      -4.009794e+02 |       31
     20       6.125104e+05      -2.619603e+02 |       31
     21       6.122603e+05      -2.501010e+02 |       31
     22       6.119381e+05      -3.221872e+02 |       31
     23       6.116449e+05      -2.932299e+02 |       31
     24       6.113754e+05      -2.694606e+02 |       31
     25       6.111892e+05      -1.862821e+02 |       31
     26       6.111048e+05      -8.433396e+01 |       30
     27       6.110613e+05      -4.355853e+01 |       31
     28       6.110354e+05      -2.588888e+01 |       30
     29       6.110210e+05      -1.442288e+01 |       28
     30       6.110073e+05      -1.367945e+01 |       30
     31       6.109969e+05      -1.040639e+01 |       25
     32       6.109867e+05      -1.015188e+01 |       27
     33       6.109789e+05      -7.800829e+00 |       26
     34       6.109715e+05      -7.408103e+00 |       26
     35       6.109614e+05      -1.010699e+01 |       27
     36       6.109498e+05      -1.158490e+01 |       28
     37       6.109356e+05      -1.425781e+01 |       29
     38       6.109157e+05      -1.986096e+01 |       25
     39       6.108958e+05      -1.987683e+01 |       27
     40       6.108701e+05      -2.574735e+01 |       31
     41       6.108310e+05      -3.903544e+01 |       31
     42       6.107899e+05      -4.110072e+01 |       30
     43       6.107237e+05      -6.629047e+01 |       30
     44       6.106188e+05      -1.048851e+02 |       30
     45       6.104616e+05      -1.572069e+02 |       31
     46       6.103746e+05      -8.697771e+01 |       31
     47       6.103328e+05      -4.175717e+01 |       31
     48       6.103042e+05      -2.866093e+01 |       30
     49       6.102800e+05      -2.417459e+01 |       30
     50       6.102611e+05      -1.893565e+01 |       29
K-means terminated without convergence after 50 iterations (objv = 610261.0559711917)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.327252
INFO: iteration 2, average log likelihood -1.294570
INFO: iteration 3, average log likelihood -1.261634
INFO: iteration 4, average log likelihood -1.220559
WARNING: Variances had to be floored 2
INFO: iteration 5, average log likelihood -1.165954
WARNING: Variances had to be floored 14 18
INFO: iteration 6, average log likelihood -1.108066
WARNING: Variances had to be floored 8 13 16 19 24 28 32
INFO: iteration 7, average log likelihood -1.078152
WARNING: Variances had to be floored 9 15 17 21 23
INFO: iteration 8, average log likelihood -1.113435
WARNING: Variances had to be floored 2 5
INFO: iteration 9, average log likelihood -1.134299
WARNING: Variances had to be floored 14 18 19
INFO: iteration 10, average log likelihood -1.101273
WARNING: Variances had to be floored 16 28 32
INFO: iteration 11, average log likelihood -1.107526
WARNING: Variances had to be floored 8 13 17 30
INFO: iteration 12, average log likelihood -1.078353
WARNING: Variances had to be floored 2 19 21 24
INFO: iteration 13, average log likelihood -1.083855
WARNING: Variances had to be floored 5 14 15 18 23
INFO: iteration 14, average log likelihood -1.080599
WARNING: Variances had to be floored 32
INFO: iteration 15, average log likelihood -1.111458
WARNING: Variances had to be floored 2 8 9 13 30
INFO: iteration 16, average log likelihood -1.052950
WARNING: Variances had to be floored 1 14 17 18 19 21 24
INFO: iteration 17, average log likelihood -1.067684
WARNING: Variances had to be floored 16
INFO: iteration 18, average log likelihood -1.131613
WARNING: Variances had to be floored 2 15 23 32
INFO: iteration 19, average log likelihood -1.071231
WARNING: Variances had to be floored 5 8 9 13 14 18 19 30
INFO: iteration 20, average log likelihood -1.050687
WARNING: Variances had to be floored 17 21 24
INFO: iteration 21, average log likelihood -1.131737
WARNING: Variances had to be floored 1
INFO: iteration 22, average log likelihood -1.116818
WARNING: Variances had to be floored 2 9 13 15 18 32
INFO: iteration 23, average log likelihood -1.050497
WARNING: Variances had to be floored 8 14 19 23 30
INFO: iteration 24, average log likelihood -1.077809
WARNING: Variances had to be floored 5 17 21 24
INFO: iteration 25, average log likelihood -1.110793
WARNING: Variances had to be floored 1 16 18
INFO: iteration 26, average log likelihood -1.103540
WARNING: Variances had to be floored 2 32
INFO: iteration 27, average log likelihood -1.083689
WARNING: Variances had to be floored 8 13 14 15 19 30
INFO: iteration 28, average log likelihood -1.043185
WARNING: Variances had to be floored 5 17 18 21 23 24
INFO: iteration 29, average log likelihood -1.086207
WARNING: Variances had to be floored 1 2 9
INFO: iteration 30, average log likelihood -1.129154
WARNING: Variances had to be floored 32
INFO: iteration 31, average log likelihood -1.102521
WARNING: Variances had to be floored 8 13 14 15 18 19
INFO: iteration 32, average log likelihood -1.034128
WARNING: Variances had to be floored 2 5 17 21 23 24
INFO: iteration 33, average log likelihood -1.084735
WARNING: Variances had to be floored 1 9 30
INFO: iteration 34, average log likelihood -1.123043
WARNING: Variances had to be floored 18 19 32
INFO: iteration 35, average log likelihood -1.091569
WARNING: Variances had to be floored 2 13 14 15 20
INFO: iteration 36, average log likelihood -1.066176
WARNING: Variances had to be floored 5 8 9 17 21 23 24
INFO: iteration 37, average log likelihood -1.081844
WARNING: Variances had to be floored 1 18 19 30
INFO: iteration 38, average log likelihood -1.116084
WARNING: Variances had to be floored 32
INFO: iteration 39, average log likelihood -1.118287
WARNING: Variances had to be floored 2 13 14 15 20
INFO: iteration 40, average log likelihood -1.047825
WARNING: Variances had to be floored 5 8 9 17 18 19 21 23 24
INFO: iteration 41, average log likelihood -1.051573
WARNING: Variances had to be floored 1
INFO: iteration 42, average log likelihood -1.148724
WARNING: Variances had to be floored 14 30 32
INFO: iteration 43, average log likelihood -1.089194
WARNING: Variances had to be floored 2 13 15 18 20
INFO: iteration 44, average log likelihood -1.062071
WARNING: Variances had to be floored 5 8 9 17 19 21 23 24
INFO: iteration 45, average log likelihood -1.064987
WARNING: Variances had to be floored 1
INFO: iteration 46, average log likelihood -1.134133
WARNING: Variances had to be floored 14 18 30 32
INFO: iteration 47, average log likelihood -1.070117
WARNING: Variances had to be floored 2 11 13 15 20
INFO: iteration 48, average log likelihood -1.072874
WARNING: Variances had to be floored 5 8 9 17 19 21 23
INFO: iteration 49, average log likelihood -1.070606
WARNING: Variances had to be floored 1 18 24
INFO: iteration 50, average log likelihood -1.103511
INFO: EM with 100000 data points 50 iterations avll -1.103511
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.218257    -0.0919295    0.0755174   -0.182751     0.0930882     0.0457627   -0.158119    -0.000875943   0.122709     -0.0141941   -0.0153416    0.0142693    0.105815     -0.104188    -0.00147883   0.0914085    0.0703416   -0.0252893  -0.0226981  -0.156764     0.0597766   -0.11878      0.016627     0.166567      0.112486     0.03482   
  0.213657     0.208367    -0.0426331   -0.0397548   -0.00462292    0.0565284   -0.02468      0.00361386   -0.0263485    -0.0217249   -0.0362656    0.0219138    0.163511      0.0774125   -0.12545     -0.137725     0.239496     0.0285261   0.0859978   0.0128004   -0.224788    -0.00530999  -0.0518577    0.210502      0.00928089  -0.0104343 
 -0.0390536    0.0190601   -0.0600632   -0.054158    -0.0682404    -0.132066     0.0142558    0.0954079    -0.0338401    -0.0199248   -0.113237    -0.0219429    0.0476028     0.051145    -0.0346535   -0.00347307  -0.129732    -0.0242444   0.0247819  -0.0256687    0.0318634    0.0536966    0.108716     0.114465      0.062683     0.123448  
 -0.0374406    0.00891308  -0.0627073   -0.0240669    0.0284428    -0.0729502    0.104841     0.0459564    -0.0189941    -0.0924448    0.0115247    0.0419278   -0.00918058    0.147974    -0.030447    -0.0453174    0.0125262    0.169997   -0.114315    0.060486    -0.020684    -0.0336064    0.0678999    0.0223238    -0.0478675    0.00565277
 -0.048834    -0.323554    -0.138084    -0.0474781    0.248007      0.0459947   -0.012123    -0.00873051    0.178813      0.206723     0.0959987   -0.102216     0.0786245     0.0445488   -0.0634571    0.183868    -0.115895    -0.0584931  -0.126112   -0.105343    -0.0129665    0.120384    -0.00424006   0.172942     -0.0478454    0.0415891 
  0.0094928    0.143942     0.155165    -0.0766408   -0.0411812     0.129743     0.00318747   0.106422      0.0805663    -0.0388781    0.00558818  -0.0786559    0.0356568     0.154437     0.0588257    0.0671792    0.110267    -0.0331004  -0.011853   -0.10348      0.151416    -0.0114201    0.0591392    0.0499829    -0.00554405   0.127279  
 -0.0468789    0.0141799    0.0606089    0.132966     0.0205213    -0.0115893   -0.0696606    0.0937556     0.136882     -0.0343088    0.0851041    0.194331    -0.0423601    -0.134113    -0.103914     0.00207784   0.0293466   -0.174589   -0.208664    0.0323765    0.0194754    0.103662     0.00602689  -0.241943     -0.068017     0.0335363 
  0.00712538   0.229644     0.0780775   -0.15794      0.0736891     0.138778    -0.0237182    0.053794      0.0966061     0.19808     -0.0299879    0.0980325    0.140195      0.0312291   -0.126693    -0.0395837   -0.0669746   -0.114774   -0.0641524   0.0899354   -0.0397866    0.0984848    0.0860876   -0.0681803    -0.208412    -0.167618  
  0.0410612    0.0278425    0.0879838    0.0163297    0.0748326     0.0496427    0.306653     0.0233453     0.169278     -0.0962974    0.210164    -0.00508605   0.0428929     0.0522598   -0.107698     0.196539     0.110404    -0.113883    0.0749215  -0.0429206   -0.0269995    0.0561586    0.0507221    0.0737263    -0.342438     0.0735224 
 -0.0196168   -0.181757    -0.0349407   -0.017646     0.106357     -0.190219    -0.0489763    0.162869     -0.0879175     0.167981     0.0687051   -0.0595634    0.0616244     0.112982     0.0312057    0.00279299  -0.159268     0.0692944   0.199205    0.127973     0.0505024    0.120854    -0.0469059   -0.020463     -0.107543     0.0448246 
  0.0186525   -0.152377     0.00951371   0.129378    -0.0199657    -0.0547953    0.141707    -0.0336472    -0.0452571    -0.074436     0.0940191   -0.250652     0.220772     -0.0994091   -0.139972    -0.0140902    0.073283    -0.346128   -0.0313911   0.0522709   -0.138785     0.0976632    0.0349174   -0.0164598     0.00204609   0.0420853 
 -0.23731     -0.0675989    0.0595217    0.139849     0.199576      0.0850827   -0.0551264   -0.085259      0.000944639  -0.0632744    0.00512186   0.0576256    0.0108775     0.0155443   -0.0754466    0.0439699    0.117985    -0.0838664   0.134515    0.0188328   -0.00749463  -0.146395     0.106578     0.203993      0.0737272   -0.0959362 
 -0.00356774   0.111224    -0.0979086    0.0984947    0.159238      0.0485095    0.0591611    0.0694487     0.0274136     0.0298908   -0.0486636   -0.0160116   -0.0996161     0.0207232    0.171147     0.0084955   -0.0674337   -0.0777033  -0.0252082  -0.077638     0.00379374   0.119743    -0.0855217   -0.00774551   -0.0815708   -0.172133  
  0.0267723   -0.0468012    0.194939    -0.134111     0.155668      0.0420986   -0.0446814   -0.0237939    -0.061094     -0.0359131    0.007152    -0.0166657   -0.12946       0.0835637   -0.0148852    0.0854851    0.0988493   -0.0332934  -0.0912202   0.0721989    0.0607512    0.10743     -0.0232607    0.247014      0.159643     0.129387  
  0.00830565   0.0157195    0.0134512   -0.0194068   -0.0205561     0.0555062    0.0870967    0.0294411     0.0553773    -0.0740749    0.0852965   -0.0615756    0.217766     -0.105009    -0.206988    -0.0217953    0.168406     0.261177   -0.0698962   0.0556951    0.0396441    0.0892314    0.0719293    0.0789746    -0.0893881    0.0632725 
 -0.0553272   -0.0936521   -0.0795833   -0.0287954    0.000405011   0.0325737   -0.0691204   -0.0171466    -0.00730593   -0.0641057   -0.0113487    0.0300563    0.073463     -0.0274432    0.217196     0.0471681    0.0491384   -0.0313129  -0.0193603  -0.11295     -0.0736487   -0.00674893   0.0171745   -0.00526539   -0.071318    -0.0340614 
 -0.0299872    0.0662964    0.0463195    0.0534646   -0.159354     -0.249888     0.0205671    0.212808     -0.0205957     0.00576006  -0.256472     0.0410977    0.033897     -0.0127883    0.0358109    0.0760854   -0.0100814   -0.0627074  -0.0111221  -0.11384     -0.187414    -0.0363998    0.131542    -0.109661     -0.503895     0.00213225
  0.0660327    0.0810071   -0.111128     0.117043    -0.0871763    -0.0405413   -0.0215674    0.0181443     0.0301648    -0.0182252    0.0501039   -0.187895     0.0811817    -0.0687924    0.091609     0.101194     0.0301058    0.0237485   0.057573   -0.00526735   0.0341152   -0.0200045   -0.0319947   -0.0678986     0.134655    -0.115734  
  0.0244236   -0.0172268    0.0768058    0.0511271    0.0228203    -0.00132237   0.03612      0.045774      0.0197968    -0.0312526   -0.0939059    0.213904    -0.172577     -0.00309047  -0.0191549   -0.0709769    0.0884807    0.105494    0.120274    0.030589     0.0728466    0.0339701    0.0713027    0.169764      0.223186     0.0110749 
 -0.0613534   -0.289089    -0.127525    -0.0262631    0.44579       0.140691     0.0569327   -0.113149      0.169508     -0.0916227    0.332485     0.020895     0.239811     -0.0551451    0.167175     0.01453      0.183427    -0.0479444   0.0572871  -0.127447    -0.0608927   -0.0108482    0.164487     0.145471     -0.134124     0.0120814 
 -0.0204159    0.158233    -0.0833964    0.00883138   0.11032      -0.0859926   -0.00907578  -0.221109     -0.0801021     0.013009     0.0498677   -0.129229     0.228409      0.0106132   -0.0648966   -0.0471344   -0.128141     0.0852658  -0.177052   -0.0182457   -0.0470919   -0.0231903    0.0761717   -0.0676276     0.141959    -0.00958591
 -0.15841      0.0742333   -0.0796877   -0.122854    -0.28142      -0.00572613  -0.0546544   -0.00984711    0.0162087    -0.134742    -0.0717744    0.18346     -0.076997      0.0155339    0.123036     0.0533295   -0.123937    -0.0544537  -0.0716456   0.103124    -0.0665378   -0.086363    -0.0595781    0.120104     -0.00724742  -0.022162  
 -0.121691    -0.115786     0.170019     0.120047    -0.066953      0.0142295    0.0411781    0.094901      0.012467     -0.0798814    0.198627     0.010247    -0.000118268   0.149992     0.137061     0.0625822   -0.00841009   0.0747389   0.0510078  -0.0936527   -0.0279017    0.0340167    0.036723    -0.0105805    -0.162762    -0.00708291
  0.0732583    0.0118029    0.0472207    0.120458    -0.107056     -0.0866241    0.142863    -0.0664813    -0.251544     -0.0305282    0.140608    -0.215431     0.217992     -0.00278714   0.0290549   -0.130974    -0.05638     -0.193563   -0.0244712   0.0366706   -0.160091     0.0884333   -0.051326    -0.0896761     0.196768     0.0541048 
 -0.00414296   0.0902219    0.0935775   -0.120137    -0.0386839     0.0526377    0.00367474   0.114138     -0.0182674     0.00680964  -0.0111158    0.0447054    0.0161163     0.0481753    0.0237731    0.112556     0.0925018    0.0127648   0.0213619   0.0973233    0.0266571    0.0526184    0.0996676   -0.000168543  -0.0527447    0.104093  
  0.0272614    0.0194046   -0.0534561   -0.0767199    0.171145      0.0123806    0.120016     0.136832      0.0772199    -0.0389348   -0.0122783    0.11988      0.0768474    -0.049916     0.0151141    0.132396    -0.196633    -0.0635934  -0.0528712  -0.0515315   -0.174062     0.163759     0.00289016  -0.0651842    -0.160622     0.105836  
  0.0958525   -0.220413     0.159899     0.112737    -0.0852768     0.0538571   -0.187388     0.0497671    -0.000702958   0.00296091  -0.0266299   -0.0591292    0.0348971    -0.11365     -0.0399256   -0.104236     0.0500014    0.183719   -0.0595209   0.0541504   -0.186879     0.00628272  -0.0423392   -0.0419686    -0.0967057   -0.0364034 
 -0.0117591    0.0409626   -0.0112061    0.0907094    0.0467952     0.0487343   -0.0153792    0.0844782    -0.0453051     0.131065     0.100614    -0.0853794    0.0880192    -0.123026     0.0600924   -0.157592     0.0311555    0.11091     0.12818    -0.0679848   -0.0236477    0.0608812   -0.0782231    0.0643462     0.103968     0.128181  
 -0.127244    -0.0195367   -0.035105    -0.0376786    0.0529616     0.109389    -0.176063    -0.170494     -0.258104     -0.051104     0.0497733    0.0312877   -0.119591     -0.0212982   -0.0122435    0.187752     0.00383016  -0.152907   -0.0765159  -0.0118393   -0.00768297   0.0473454    0.029335    -0.0731405     0.0112502    0.0480067 
  0.105272     0.157037     0.220154     0.0190499    0.0964955     0.0324562    0.0468155    0.177309     -0.06731       0.226613    -0.153188     0.0803789   -0.0495468     0.173482     0.0170867    0.169211     0.101424     0.0654075   0.212174   -0.0371143    0.043571     0.125801     0.0230295    0.236076      0.0243087   -0.044586  
  0.142386     0.0305827   -0.117584    -0.00140377   0.0225721    -0.0178384    0.0568003    0.0266586     0.0815971    -0.181035     0.0661048    0.0416067   -0.0129076     0.0525867   -0.0333134    0.0283949    0.170414    -0.105777   -0.163341   -0.0371295    0.101872     0.207637    -0.0549789   -0.0701488    -0.10094     -0.125268  
  0.0282093    0.248681     0.0392479   -0.0997264    0.0201328    -0.089509     0.211825    -0.01529      -0.104169     -0.0407344   -0.100912     0.222061    -0.0870604    -0.0184244   -0.0230409   -0.031077     0.0531395    0.0159031   0.0381218   0.0322662    0.0288993   -0.0620525    0.0826493   -0.0298606     0.0270189    0.0367808 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 14 30 32
INFO: iteration 1, average log likelihood -1.089438
WARNING: Variances had to be floored 2 13 14 15 17 20 30 32
INFO: iteration 2, average log likelihood -1.022553
WARNING: Variances had to be floored 1 2 5 8 9 11 13 14 18 19 21 23 24 30 32
INFO: iteration 3, average log likelihood -0.982282
WARNING: Variances had to be floored 14 15 17 20 30 32
INFO: iteration 4, average log likelihood -1.072447
WARNING: Variances had to be floored 2 13 14 30 32
INFO: iteration 5, average log likelihood -1.030787
WARNING: Variances had to be floored 1 2 5 8 9 11 13 14 15 17 18 19 20 21 23 24 30 32
INFO: iteration 6, average log likelihood -0.961972
WARNING: Variances had to be floored 14 30 32
INFO: iteration 7, average log likelihood -1.089087
WARNING: Variances had to be floored 2 13 14 15 17 20 30 32
INFO: iteration 8, average log likelihood -1.015801
WARNING: Variances had to be floored 1 2 5 8 9 11 13 14 18 19 21 23 24 30 32
INFO: iteration 9, average log likelihood -0.980421
WARNING: Variances had to be floored 14 15 17 20 30 32
INFO: iteration 10, average log likelihood -1.070151
INFO: EM with 100000 data points 10 iterations avll -1.070151
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0311462  -0.086447     0.111663    0.114897     0.0790266   -0.0370163   -0.0267712    0.0190532    0.0046623    0.0980076   -0.0235257     0.0278119  -0.014235     0.00326626  -0.111393      0.0341261   0.0394284   -0.0552378    -0.0639834    0.0953762     0.037224    0.0376302    0.0627071     0.00800502   0.0318932    0.11112   
 -0.106205    0.11073     -0.0278105  -0.193764    -0.115841    -0.189001    -0.0386398    0.204097    -0.0805767    0.0647607   -0.0264728    -0.0010891   0.0691656   -0.046651     0.0136416    -0.009688   -0.247626     0.0472144    -0.0609949    0.132118      0.165618    0.10771     -0.212261     -0.0957843   -0.111787     0.103908  
 -0.0341218   0.0389472    0.0242824  -0.0834323   -0.0307667   -0.100711    -0.0130303    0.17711      0.071782    -0.0175715    0.0275989     0.0173307  -0.167927    -0.00209509  -0.112659      0.123354   -0.11861     -0.0336999     0.147581     0.145003      0.101881   -0.306776    -0.172569     -0.0491129   -0.0435901    0.0579161 
  0.0583163   0.0103802   -0.103919    0.0183091    0.13274     -0.105117     0.0858859    0.00920019   0.0331262   -0.0354866   -0.315752      0.133842    0.0697713   -0.0944971    0.000748429  -0.11559    -0.0244053   -0.14691      -0.0987185    0.231275     -0.124492    0.0218872   -0.134672     -0.0806425    0.110041    -0.014046  
 -0.0640838  -0.0360141    0.052009    0.142488     0.0459529   -0.0824451   -0.0826776   -0.00210328   0.146393     0.041182     0.0879633    -0.0649671  -0.00618259   0.00401729  -0.0461688     0.0306809   0.126401     0.00925601    0.0112412   -0.0210883    -0.143079    0.138027    -0.053689      0.0422057   -0.0761374    0.128296  
  0.0506385  -0.051876    -0.0512997   0.0946154   -0.117098     0.112366     0.148007    -0.00733828   0.0522697    0.0893822   -0.111832      0.0516326  -0.0127013    0.106496     0.17564      -0.156378    0.0220105    0.0171075     0.0346149   -0.0407264     0.0814373   0.00816272  -0.181061     -0.0759388    0.120839    -0.00632924
 -0.103865   -0.0559169   -0.166061   -0.0595618   -0.0515245    0.014385    -0.104445     0.00545866   0.163304    -0.0260353    0.0287365     0.091683    0.0393461    0.0181861   -0.0914953    -0.135871    0.0594805    0.172698      0.1555      -0.152426      0.103114   -0.111187    -0.129211     -0.0846058   -0.013577     0.0537961 
 -0.0240688  -0.09409     -0.0115223  -0.034803    -0.293417     0.0624204   -0.113933    -0.0326984    0.164192    -0.0313784   -0.115571      0.153829    0.158683    -0.120116     0.0738714     0.109486    0.0442512    0.0263318    -0.0858063    0.0462639    -0.130294    0.0435087   -0.155385     -0.136384    -0.0015199    0.133811  
  0.111324    0.0572671   -0.0789075   0.0357668   -0.033266     0.169837     0.0207411    0.00360409  -0.018056     0.115145    -0.032492      0.0142316   0.0997061    0.135588     0.206058     -0.206946   -0.027416    -0.053537     -0.028065    -0.172945     -0.179452   -0.167534    -0.030344     -0.0109845    0.137673     0.0184181 
 -0.0928815   0.0980665   -0.104166   -0.0166701    0.139291    -0.175376     0.0564179   -0.141753    -0.0366539    0.123124    -0.273444      0.0407615   0.0820375   -0.022717    -0.0924181     0.105197    0.098925    -0.0256012    -0.0224781   -0.0390574    -0.0175312  -0.0209873   -0.178827      0.0346517   -0.0287711    0.0456188 
 -0.1114     -0.0746451    0.0377534   0.166522    -0.161214     0.0334572   -0.103492    -0.0369688    0.0521517    0.0393989    0.00891999    0.0612756   0.126608    -0.0475978    0.0466837     0.0948907  -0.09273     -0.140255      0.0585859    0.11977       0.120992   -0.0576898    0.215098      0.0123517   -0.00163746   0.0577566 
  0.04892     0.0848553   -0.0511037   0.00802144   0.210601    -0.0192839    0.0122059    0.165115    -0.0372713   -0.109191     0.138238      0.0891629   0.0088634    0.0373443   -0.128548      0.0177384   0.0540633   -0.12376       0.0884064   -0.0787357    -0.0709678  -0.0458275   -0.0603005     0.0162999   -0.10632     -0.141576  
  0.083258   -0.00139866  -0.0167119   0.0237268    0.0138563    0.204545    -0.0369805   -0.0598672    0.00563754  -0.028281     0.145665      0.150006   -0.00955224   0.0551622    0.0559289     0.10482     0.0319825   -0.0296635    -0.135179    -0.204993     -0.0291517   0.0338129   -0.101108     -0.10474     -0.116257     0.0213826 
 -0.0432199   0.0739652   -0.263549    0.0282849   -0.00916571  -0.0965845   -0.0876571   -0.0260933   -0.0351158    0.14091      0.0712698     0.07896    -0.0992873    0.0761735    0.146925     -0.12352     0.041332    -0.0179556    -0.134727    -0.0387851    -0.0853979   0.027959    -0.0217962     0.0317685    0.0346488    0.0808985 
  0.161664   -0.0627303   -0.116622   -0.0816308    0.131025    -0.0349524    0.117379     0.137584     0.0637659   -0.170752     0.0931722    -0.077905   -0.0978818    0.10137      0.028986     -0.0524451  -0.0886011   -0.0184296    -0.0387055   -0.100755     -0.0695589  -0.111192     0.0175203     0.439117    -0.0185378    0.00957852
 -0.0864756   0.0367884   -0.0802931   0.0500136    0.0267522   -0.172234     0.0118131   -0.0429277   -0.0621793    0.0353537   -0.0647374     0.0519015   0.0127355    0.0596783   -0.0856713    -0.036678   -0.205227    -0.0817092    -0.00952479   0.113124     -0.0987536   0.0589214    0.0902509    -0.0792819    0.0299485    0.0276079 
  0.132901    0.0467211    0.0289085  -0.00841134   0.124691     0.058896     0.0667117    0.0724733   -0.110815    -0.0012793   -0.069082     -0.087533    0.138483     0.0303323    0.077962      0.0181529  -0.0384      -0.0509258     0.149474    -0.0546519    -0.0711082  -0.1072      -0.0909109     0.0357823   -0.0279312    0.00290646
 -0.138381   -0.0793339   -0.221539   -0.00615466  -0.10091     -0.0994557    0.0984577   -0.0180942   -0.0589428   -0.136788     0.202064      0.0252138  -0.109611    -0.0443132    0.114326      0.0407075   0.0918541    0.047246      0.0200528   -0.000813511  -0.0642385   0.0160991    0.0260888     0.0372488   -0.0177251    0.00709432
 -0.0534133  -0.026163    -0.0256433  -0.0950494    0.0234444   -0.103928     0.042107     0.0613501    0.0761215   -0.0671502    0.119303     -0.140697    0.0413248    0.0435538   -0.17294      -0.0508725  -0.100489    -0.0937056     0.0917893   -0.0104066     0.133438    0.127112     0.0443761    -0.00111285   0.204501     0.0243721 
  0.0189612  -0.0326719   -0.120969    0.0701114   -0.0477393   -0.019087    -0.0246817   -0.1296      -0.155833     0.0193745    0.0540895     0.0690974   0.0892666   -0.0498408   -0.183502      0.0188961  -0.079494    -0.0614588     0.147797    -0.0106652     0.0102433   0.113345    -0.15903       0.0285462    0.08273      0.161917  
 -0.0445852   0.0794018   -0.0136666  -0.10303     -0.0359234    0.018388    -0.179682     0.0798416   -0.00212638  -0.043916    -0.0109137    -0.0823496  -0.170206    -0.00313432   0.0292882    -0.164795    0.0441836    0.16935      -0.0193372   -0.0252479     0.0991281   0.0386527    0.0240619     0.18404      0.10519     -0.0453562 
  0.0122934  -0.0251175   -0.0231648   0.0110179    0.1853      -0.211536     0.0960316    0.140778     0.00915812   0.0737517    0.0777038    -0.186231   -0.0875332   -0.077187     0.167221     -0.0481596  -0.0894225    0.000757931   0.13549     -0.00375888    0.101946    0.0112279    0.022982      0.131275    -0.0364312   -0.0147186 
  0.0250753  -0.0327932   -0.0299619   0.0203368    0.0227641    0.103944     0.0448369    0.112074    -0.0583794    0.0972831   -0.0402518    -0.01433     0.00389621   0.0109111   -0.0428195     0.175456   -0.0502906    0.029018      0.0979129   -0.0329205    -0.0524899   0.00976793   0.021266     -0.0381592   -0.0713237   -0.0253674 
 -0.0479179  -0.239752    -0.163291   -0.0400687   -0.0759102    0.00903785  -0.00714936  -0.0784308    0.00199581  -0.0688988   -0.129899      0.103989    0.0254118   -0.0832174   -0.0861105     0.0315079  -0.00989755   0.00234744    0.0740933    0.0819259     0.0613505  -0.0250288    0.0243915     0.0568425   -0.131712    -0.228506  
 -0.0827251  -0.0364783   -0.0152202  -0.00275848  -0.0329319   -0.0672008    0.013891    -0.00894149   0.015385    -0.030825     0.013267     -0.0294468   0.189033     0.0370195    0.392476     -0.0127231   0.0906126   -0.0312057    -0.108461    -0.0437379    -0.127849   -0.117034    -0.100078      0.0395676    0.168367    -0.00719549
  0.0745762  -0.0229018    0.106963    0.0658371    0.108934    -0.0373789    0.007942     0.184547     0.131999     0.101316     0.0457517     0.010342    0.0861292    0.0280451    0.116611     -0.0207545  -0.0401501    0.0561726    -0.0115604    0.0425774     0.0547158   0.00983296  -0.00791839    0.0738332   -0.101065    -0.0113614 
 -0.0305139   0.224165     0.0436153  -0.00660097   0.111995    -0.0171529   -0.173236    -0.097502    -0.0769678    0.115805     0.0481695    -0.150313   -0.150001     0.00319628  -0.0499001     0.0313073   0.128887    -0.0453534     0.0917167    0.0157139    -0.0457878   0.105594     0.000217447   0.0160269   -0.0757923   -0.029625  
  0.141782   -0.0945706   -0.0583451   0.0690053    0.00948819  -0.0740026    0.0459835   -0.0972284   -0.0512115    0.0929788    0.017362      0.0736931  -0.00312859   0.0833442    0.0412051     0.0470452  -0.0965599   -0.0698289    -0.0404454   -0.0568222     0.0765253   0.0147924   -0.105888     -0.058622     0.0885265   -0.0377008 
  0.125372   -0.156809    -0.0670984   0.0262547    0.0753568   -0.0493782    0.13275      0.0203816    0.0605029   -0.0574269    0.000152734   0.0486838   0.050641    -0.0758167    0.133369     -0.140798   -0.00378475  -0.00557645   -0.0258783   -0.162082     -0.0110216  -0.109393    -0.0454733     0.0500657    0.0631341   -0.0213322 
  0.194535    0.0591841    0.0764586  -0.13656      0.191369     0.0186828   -0.0347483    0.147318    -0.0211588   -0.00327378  -0.155924      0.0527845  -0.0487113   -0.0200913   -0.154649     -0.140854    0.0433995   -0.0120261    -0.0171932    0.203397      0.0302811   0.0703259   -0.150552     -0.17249     -0.14556      0.139943  
 -0.108138    0.0246857   -0.123402    0.0984014   -0.143424    -0.146426     0.111604    -0.105015    -0.0155382    0.0512658   -0.00786347   -0.118116   -0.200067    -0.0926851   -0.0198896     0.0646981  -0.0933676   -0.0278427     0.0576704   -0.103298     -0.0388509   0.23455      0.134377     -0.0101592    0.166536     0.0213547 
  0.172076    0.0184339   -0.0114621  -0.123199    -0.124447     0.0876228   -0.233148    -0.122503     0.0362816    0.0629526    0.165737      0.014624   -0.0658042   -0.0297667    0.227021     -0.0544215   0.172827     0.0813895    -0.0533462   -0.00261097   -0.145804    0.00814671  -0.0582867     0.115284    -0.0140442    0.0726532 kind full, method split
0: avll = -1.4217727488933012
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.421791
INFO: iteration 2, average log likelihood -1.421724
INFO: iteration 3, average log likelihood -1.421670
INFO: iteration 4, average log likelihood -1.421604
INFO: iteration 5, average log likelihood -1.421525
INFO: iteration 6, average log likelihood -1.421434
INFO: iteration 7, average log likelihood -1.421339
INFO: iteration 8, average log likelihood -1.421249
INFO: iteration 9, average log likelihood -1.421171
INFO: iteration 10, average log likelihood -1.421099
INFO: iteration 11, average log likelihood -1.421016
INFO: iteration 12, average log likelihood -1.420887
INFO: iteration 13, average log likelihood -1.420647
INFO: iteration 14, average log likelihood -1.420200
INFO: iteration 15, average log likelihood -1.419458
INFO: iteration 16, average log likelihood -1.418478
INFO: iteration 17, average log likelihood -1.417550
INFO: iteration 18, average log likelihood -1.416933
INFO: iteration 19, average log likelihood -1.416620
INFO: iteration 20, average log likelihood -1.416482
INFO: iteration 21, average log likelihood -1.416424
INFO: iteration 22, average log likelihood -1.416399
INFO: iteration 23, average log likelihood -1.416389
INFO: iteration 24, average log likelihood -1.416385
INFO: iteration 25, average log likelihood -1.416383
INFO: iteration 26, average log likelihood -1.416382
INFO: iteration 27, average log likelihood -1.416382
INFO: iteration 28, average log likelihood -1.416381
INFO: iteration 29, average log likelihood -1.416381
INFO: iteration 30, average log likelihood -1.416381
INFO: iteration 31, average log likelihood -1.416381
INFO: iteration 32, average log likelihood -1.416381
INFO: iteration 33, average log likelihood -1.416381
INFO: iteration 34, average log likelihood -1.416381
INFO: iteration 35, average log likelihood -1.416380
INFO: iteration 36, average log likelihood -1.416380
INFO: iteration 37, average log likelihood -1.416380
INFO: iteration 38, average log likelihood -1.416380
INFO: iteration 39, average log likelihood -1.416380
INFO: iteration 40, average log likelihood -1.416380
INFO: iteration 41, average log likelihood -1.416380
INFO: iteration 42, average log likelihood -1.416380
INFO: iteration 43, average log likelihood -1.416380
INFO: iteration 44, average log likelihood -1.416380
INFO: iteration 45, average log likelihood -1.416380
INFO: iteration 46, average log likelihood -1.416380
INFO: iteration 47, average log likelihood -1.416380
INFO: iteration 48, average log likelihood -1.416380
INFO: iteration 49, average log likelihood -1.416380
INFO: iteration 50, average log likelihood -1.416380
INFO: EM with 100000 data points 50 iterations avll -1.416380
952.4 data points per parameter
1: avll = [-1.42179,-1.42172,-1.42167,-1.4216,-1.42152,-1.42143,-1.42134,-1.42125,-1.42117,-1.4211,-1.42102,-1.42089,-1.42065,-1.4202,-1.41946,-1.41848,-1.41755,-1.41693,-1.41662,-1.41648,-1.41642,-1.4164,-1.41639,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.416394
INFO: iteration 2, average log likelihood -1.416330
INFO: iteration 3, average log likelihood -1.416271
INFO: iteration 4, average log likelihood -1.416197
INFO: iteration 5, average log likelihood -1.416103
INFO: iteration 6, average log likelihood -1.415990
INFO: iteration 7, average log likelihood -1.415866
INFO: iteration 8, average log likelihood -1.415741
INFO: iteration 9, average log likelihood -1.415627
INFO: iteration 10, average log likelihood -1.415531
INFO: iteration 11, average log likelihood -1.415455
INFO: iteration 12, average log likelihood -1.415398
INFO: iteration 13, average log likelihood -1.415356
INFO: iteration 14, average log likelihood -1.415326
INFO: iteration 15, average log likelihood -1.415305
INFO: iteration 16, average log likelihood -1.415289
INFO: iteration 17, average log likelihood -1.415277
INFO: iteration 18, average log likelihood -1.415266
INFO: iteration 19, average log likelihood -1.415257
INFO: iteration 20, average log likelihood -1.415249
INFO: iteration 21, average log likelihood -1.415242
INFO: iteration 22, average log likelihood -1.415235
INFO: iteration 23, average log likelihood -1.415229
INFO: iteration 24, average log likelihood -1.415223
INFO: iteration 25, average log likelihood -1.415218
INFO: iteration 26, average log likelihood -1.415213
INFO: iteration 27, average log likelihood -1.415208
INFO: iteration 28, average log likelihood -1.415204
INFO: iteration 29, average log likelihood -1.415200
INFO: iteration 30, average log likelihood -1.415197
INFO: iteration 31, average log likelihood -1.415194
INFO: iteration 32, average log likelihood -1.415191
INFO: iteration 33, average log likelihood -1.415189
INFO: iteration 34, average log likelihood -1.415187
INFO: iteration 35, average log likelihood -1.415185
INFO: iteration 36, average log likelihood -1.415183
INFO: iteration 37, average log likelihood -1.415181
INFO: iteration 38, average log likelihood -1.415180
INFO: iteration 39, average log likelihood -1.415178
INFO: iteration 40, average log likelihood -1.415177
INFO: iteration 41, average log likelihood -1.415176
INFO: iteration 42, average log likelihood -1.415175
INFO: iteration 43, average log likelihood -1.415174
INFO: iteration 44, average log likelihood -1.415174
INFO: iteration 45, average log likelihood -1.415173
INFO: iteration 46, average log likelihood -1.415172
INFO: iteration 47, average log likelihood -1.415172
INFO: iteration 48, average log likelihood -1.415171
INFO: iteration 49, average log likelihood -1.415171
INFO: iteration 50, average log likelihood -1.415170
INFO: EM with 100000 data points 50 iterations avll -1.415170
473.9 data points per parameter
2: avll = [-1.41639,-1.41633,-1.41627,-1.4162,-1.4161,-1.41599,-1.41587,-1.41574,-1.41563,-1.41553,-1.41546,-1.4154,-1.41536,-1.41533,-1.4153,-1.41529,-1.41528,-1.41527,-1.41526,-1.41525,-1.41524,-1.41524,-1.41523,-1.41522,-1.41522,-1.41521,-1.41521,-1.4152,-1.4152,-1.4152,-1.41519,-1.41519,-1.41519,-1.41519,-1.41518,-1.41518,-1.41518,-1.41518,-1.41518,-1.41518,-1.41518,-1.41518,-1.41517,-1.41517,-1.41517,-1.41517,-1.41517,-1.41517,-1.41517,-1.41517]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.415181
INFO: iteration 2, average log likelihood -1.415117
INFO: iteration 3, average log likelihood -1.415060
INFO: iteration 4, average log likelihood -1.414992
INFO: iteration 5, average log likelihood -1.414907
INFO: iteration 6, average log likelihood -1.414804
INFO: iteration 7, average log likelihood -1.414685
INFO: iteration 8, average log likelihood -1.414559
INFO: iteration 9, average log likelihood -1.414435
INFO: iteration 10, average log likelihood -1.414321
INFO: iteration 11, average log likelihood -1.414219
INFO: iteration 12, average log likelihood -1.414130
INFO: iteration 13, average log likelihood -1.414054
INFO: iteration 14, average log likelihood -1.413988
INFO: iteration 15, average log likelihood -1.413932
INFO: iteration 16, average log likelihood -1.413884
INFO: iteration 17, average log likelihood -1.413843
INFO: iteration 18, average log likelihood -1.413807
INFO: iteration 19, average log likelihood -1.413776
INFO: iteration 20, average log likelihood -1.413749
INFO: iteration 21, average log likelihood -1.413725
INFO: iteration 22, average log likelihood -1.413703
INFO: iteration 23, average log likelihood -1.413684
INFO: iteration 24, average log likelihood -1.413666
INFO: iteration 25, average log likelihood -1.413649
INFO: iteration 26, average log likelihood -1.413634
INFO: iteration 27, average log likelihood -1.413619
INFO: iteration 28, average log likelihood -1.413606
INFO: iteration 29, average log likelihood -1.413593
INFO: iteration 30, average log likelihood -1.413581
INFO: iteration 31, average log likelihood -1.413569
INFO: iteration 32, average log likelihood -1.413558
INFO: iteration 33, average log likelihood -1.413547
INFO: iteration 34, average log likelihood -1.413537
INFO: iteration 35, average log likelihood -1.413527
INFO: iteration 36, average log likelihood -1.413517
INFO: iteration 37, average log likelihood -1.413507
INFO: iteration 38, average log likelihood -1.413498
INFO: iteration 39, average log likelihood -1.413489
INFO: iteration 40, average log likelihood -1.413480
INFO: iteration 41, average log likelihood -1.413471
INFO: iteration 42, average log likelihood -1.413462
INFO: iteration 43, average log likelihood -1.413453
INFO: iteration 44, average log likelihood -1.413444
INFO: iteration 45, average log likelihood -1.413436
INFO: iteration 46, average log likelihood -1.413427
INFO: iteration 47, average log likelihood -1.413418
INFO: iteration 48, average log likelihood -1.413409
INFO: iteration 49, average log likelihood -1.413401
INFO: iteration 50, average log likelihood -1.413392
INFO: EM with 100000 data points 50 iterations avll -1.413392
236.4 data points per parameter
3: avll = [-1.41518,-1.41512,-1.41506,-1.41499,-1.41491,-1.4148,-1.41469,-1.41456,-1.41444,-1.41432,-1.41422,-1.41413,-1.41405,-1.41399,-1.41393,-1.41388,-1.41384,-1.41381,-1.41378,-1.41375,-1.41372,-1.4137,-1.41368,-1.41367,-1.41365,-1.41363,-1.41362,-1.41361,-1.41359,-1.41358,-1.41357,-1.41356,-1.41355,-1.41354,-1.41353,-1.41352,-1.41351,-1.4135,-1.41349,-1.41348,-1.41347,-1.41346,-1.41345,-1.41344,-1.41344,-1.41343,-1.41342,-1.41341,-1.4134,-1.41339]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.413391
INFO: iteration 2, average log likelihood -1.413330
INFO: iteration 3, average log likelihood -1.413273
INFO: iteration 4, average log likelihood -1.413208
INFO: iteration 5, average log likelihood -1.413130
INFO: iteration 6, average log likelihood -1.413037
INFO: iteration 7, average log likelihood -1.412928
INFO: iteration 8, average log likelihood -1.412807
INFO: iteration 9, average log likelihood -1.412680
INFO: iteration 10, average log likelihood -1.412553
INFO: iteration 11, average log likelihood -1.412434
INFO: iteration 12, average log likelihood -1.412326
INFO: iteration 13, average log likelihood -1.412232
INFO: iteration 14, average log likelihood -1.412151
INFO: iteration 15, average log likelihood -1.412083
INFO: iteration 16, average log likelihood -1.412026
INFO: iteration 17, average log likelihood -1.411977
INFO: iteration 18, average log likelihood -1.411936
INFO: iteration 19, average log likelihood -1.411901
INFO: iteration 20, average log likelihood -1.411870
INFO: iteration 21, average log likelihood -1.411843
INFO: iteration 22, average log likelihood -1.411818
INFO: iteration 23, average log likelihood -1.411795
INFO: iteration 24, average log likelihood -1.411775
INFO: iteration 25, average log likelihood -1.411755
INFO: iteration 26, average log likelihood -1.411737
INFO: iteration 27, average log likelihood -1.411720
INFO: iteration 28, average log likelihood -1.411703
INFO: iteration 29, average log likelihood -1.411687
INFO: iteration 30, average log likelihood -1.411671
INFO: iteration 31, average log likelihood -1.411656
INFO: iteration 32, average log likelihood -1.411641
INFO: iteration 33, average log likelihood -1.411627
INFO: iteration 34, average log likelihood -1.411613
INFO: iteration 35, average log likelihood -1.411599
INFO: iteration 36, average log likelihood -1.411585
INFO: iteration 37, average log likelihood -1.411572
INFO: iteration 38, average log likelihood -1.411558
INFO: iteration 39, average log likelihood -1.411545
INFO: iteration 40, average log likelihood -1.411532
INFO: iteration 41, average log likelihood -1.411520
INFO: iteration 42, average log likelihood -1.411507
INFO: iteration 43, average log likelihood -1.411495
INFO: iteration 44, average log likelihood -1.411483
INFO: iteration 45, average log likelihood -1.411471
INFO: iteration 46, average log likelihood -1.411459
INFO: iteration 47, average log likelihood -1.411448
INFO: iteration 48, average log likelihood -1.411436
INFO: iteration 49, average log likelihood -1.411425
INFO: iteration 50, average log likelihood -1.411414
INFO: EM with 100000 data points 50 iterations avll -1.411414
118.1 data points per parameter
4: avll = [-1.41339,-1.41333,-1.41327,-1.41321,-1.41313,-1.41304,-1.41293,-1.41281,-1.41268,-1.41255,-1.41243,-1.41233,-1.41223,-1.41215,-1.41208,-1.41203,-1.41198,-1.41194,-1.4119,-1.41187,-1.41184,-1.41182,-1.4118,-1.41177,-1.41176,-1.41174,-1.41172,-1.4117,-1.41169,-1.41167,-1.41166,-1.41164,-1.41163,-1.41161,-1.4116,-1.41159,-1.41157,-1.41156,-1.41155,-1.41153,-1.41152,-1.41151,-1.4115,-1.41148,-1.41147,-1.41146,-1.41145,-1.41144,-1.41143,-1.41141]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.411412
INFO: iteration 2, average log likelihood -1.411345
INFO: iteration 3, average log likelihood -1.411279
INFO: iteration 4, average log likelihood -1.411200
INFO: iteration 5, average log likelihood -1.411100
INFO: iteration 6, average log likelihood -1.410975
INFO: iteration 7, average log likelihood -1.410824
INFO: iteration 8, average log likelihood -1.410652
INFO: iteration 9, average log likelihood -1.410471
INFO: iteration 10, average log likelihood -1.410291
INFO: iteration 11, average log likelihood -1.410122
INFO: iteration 12, average log likelihood -1.409971
INFO: iteration 13, average log likelihood -1.409839
INFO: iteration 14, average log likelihood -1.409725
INFO: iteration 15, average log likelihood -1.409627
INFO: iteration 16, average log likelihood -1.409542
INFO: iteration 17, average log likelihood -1.409467
INFO: iteration 18, average log likelihood -1.409401
INFO: iteration 19, average log likelihood -1.409342
INFO: iteration 20, average log likelihood -1.409289
INFO: iteration 21, average log likelihood -1.409240
INFO: iteration 22, average log likelihood -1.409194
INFO: iteration 23, average log likelihood -1.409152
INFO: iteration 24, average log likelihood -1.409112
INFO: iteration 25, average log likelihood -1.409074
INFO: iteration 26, average log likelihood -1.409038
INFO: iteration 27, average log likelihood -1.409003
INFO: iteration 28, average log likelihood -1.408970
INFO: iteration 29, average log likelihood -1.408938
INFO: iteration 30, average log likelihood -1.408907
INFO: iteration 31, average log likelihood -1.408877
INFO: iteration 32, average log likelihood -1.408847
INFO: iteration 33, average log likelihood -1.408819
INFO: iteration 34, average log likelihood -1.408791
INFO: iteration 35, average log likelihood -1.408764
INFO: iteration 36, average log likelihood -1.408738
INFO: iteration 37, average log likelihood -1.408713
INFO: iteration 38, average log likelihood -1.408689
INFO: iteration 39, average log likelihood -1.408665
INFO: iteration 40, average log likelihood -1.408642
INFO: iteration 41, average log likelihood -1.408621
INFO: iteration 42, average log likelihood -1.408599
INFO: iteration 43, average log likelihood -1.408579
INFO: iteration 44, average log likelihood -1.408559
INFO: iteration 45, average log likelihood -1.408541
INFO: iteration 46, average log likelihood -1.408523
INFO: iteration 47, average log likelihood -1.408506
INFO: iteration 48, average log likelihood -1.408489
INFO: iteration 49, average log likelihood -1.408473
INFO: iteration 50, average log likelihood -1.408459
INFO: EM with 100000 data points 50 iterations avll -1.408459
59.0 data points per parameter
5: avll = [-1.41141,-1.41135,-1.41128,-1.4112,-1.4111,-1.41098,-1.41082,-1.41065,-1.41047,-1.41029,-1.41012,-1.40997,-1.40984,-1.40973,-1.40963,-1.40954,-1.40947,-1.4094,-1.40934,-1.40929,-1.40924,-1.40919,-1.40915,-1.40911,-1.40907,-1.40904,-1.409,-1.40897,-1.40894,-1.40891,-1.40888,-1.40885,-1.40882,-1.40879,-1.40876,-1.40874,-1.40871,-1.40869,-1.40867,-1.40864,-1.40862,-1.4086,-1.40858,-1.40856,-1.40854,-1.40852,-1.40851,-1.40849,-1.40847,-1.40846]
[-1.42177,-1.42179,-1.42172,-1.42167,-1.4216,-1.42152,-1.42143,-1.42134,-1.42125,-1.42117,-1.4211,-1.42102,-1.42089,-1.42065,-1.4202,-1.41946,-1.41848,-1.41755,-1.41693,-1.41662,-1.41648,-1.41642,-1.4164,-1.41639,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41638,-1.41639,-1.41633,-1.41627,-1.4162,-1.4161,-1.41599,-1.41587,-1.41574,-1.41563,-1.41553,-1.41546,-1.4154,-1.41536,-1.41533,-1.4153,-1.41529,-1.41528,-1.41527,-1.41526,-1.41525,-1.41524,-1.41524,-1.41523,-1.41522,-1.41522,-1.41521,-1.41521,-1.4152,-1.4152,-1.4152,-1.41519,-1.41519,-1.41519,-1.41519,-1.41518,-1.41518,-1.41518,-1.41518,-1.41518,-1.41518,-1.41518,-1.41518,-1.41517,-1.41517,-1.41517,-1.41517,-1.41517,-1.41517,-1.41517,-1.41517,-1.41518,-1.41512,-1.41506,-1.41499,-1.41491,-1.4148,-1.41469,-1.41456,-1.41444,-1.41432,-1.41422,-1.41413,-1.41405,-1.41399,-1.41393,-1.41388,-1.41384,-1.41381,-1.41378,-1.41375,-1.41372,-1.4137,-1.41368,-1.41367,-1.41365,-1.41363,-1.41362,-1.41361,-1.41359,-1.41358,-1.41357,-1.41356,-1.41355,-1.41354,-1.41353,-1.41352,-1.41351,-1.4135,-1.41349,-1.41348,-1.41347,-1.41346,-1.41345,-1.41344,-1.41344,-1.41343,-1.41342,-1.41341,-1.4134,-1.41339,-1.41339,-1.41333,-1.41327,-1.41321,-1.41313,-1.41304,-1.41293,-1.41281,-1.41268,-1.41255,-1.41243,-1.41233,-1.41223,-1.41215,-1.41208,-1.41203,-1.41198,-1.41194,-1.4119,-1.41187,-1.41184,-1.41182,-1.4118,-1.41177,-1.41176,-1.41174,-1.41172,-1.4117,-1.41169,-1.41167,-1.41166,-1.41164,-1.41163,-1.41161,-1.4116,-1.41159,-1.41157,-1.41156,-1.41155,-1.41153,-1.41152,-1.41151,-1.4115,-1.41148,-1.41147,-1.41146,-1.41145,-1.41144,-1.41143,-1.41141,-1.41141,-1.41135,-1.41128,-1.4112,-1.4111,-1.41098,-1.41082,-1.41065,-1.41047,-1.41029,-1.41012,-1.40997,-1.40984,-1.40973,-1.40963,-1.40954,-1.40947,-1.4094,-1.40934,-1.40929,-1.40924,-1.40919,-1.40915,-1.40911,-1.40907,-1.40904,-1.409,-1.40897,-1.40894,-1.40891,-1.40888,-1.40885,-1.40882,-1.40879,-1.40876,-1.40874,-1.40871,-1.40869,-1.40867,-1.40864,-1.40862,-1.4086,-1.40858,-1.40856,-1.40854,-1.40852,-1.40851,-1.40849,-1.40847,-1.40846]
32×26 Array{Float64,2}:
 -0.57738     0.182906   -0.233744   -0.442332    -0.298053   -0.283929     0.240413     0.0861557  -0.332208    -0.481846    -0.493035    -0.504364    0.133644   -0.362983    0.420392     0.799385    0.220311     -0.11739    -0.353365     0.0168947    0.697754    0.142234    -0.0982571    0.0791472  -0.342043     0.0885968 
 -0.165732    0.0285315   0.200415   -0.0171422   -0.665934   -0.0392766    0.25873      0.117153    0.238124     0.0415238    0.00180403   0.270371    0.165083    0.0146094   0.15094      0.828992   -0.34597      -0.0411233  -0.00187801  -0.176159     0.225301    0.311016    -0.474981    -0.321607    0.0447328    0.351817  
  0.33755    -0.0995426   0.192289    0.104336     0.172346   -0.371681     0.177015    -0.336038    0.429905     0.21615     -0.470101    -0.0387188  -0.724345    0.115707    0.0246913    0.13022     0.0853684     0.169537   -0.397465     0.845666     0.21601     0.038565     0.422534     0.0960594  -0.0249643   -0.0928178 
  0.691983   -0.425411   -0.353696    0.220219    -0.408247   -0.358064     0.382742    -0.0113006   0.019425     0.329953    -0.620418     0.464024   -0.122992    0.994036    0.00935541   0.263636    0.0180053    -0.0810654   0.678568     0.236315     0.385005    0.0316272    0.0724478   -0.498875    0.285626     0.163943  
  0.196038   -0.34978     0.408531   -0.592222     0.163078   -0.285852    -0.875404    -0.275025    0.123798    -0.368218     0.0804218   -0.0632308   0.120659   -0.179105    0.201474    -0.166697    0.896562      0.302443    0.0650276    0.0909068   -0.21285     0.0837379   -0.144146    -0.66889    -0.655378    -0.137052  
  0.168728   -0.483781    0.145721    0.0317547   -0.0512431   0.342922     0.368562    -0.419158    0.0609779    0.479888     0.163866     0.0668265  -0.625093   -0.450766    0.438015     0.389561    0.442073      0.323614    0.0537074    0.251852    -0.0734666   0.330262    -0.167848    -0.302947   -0.590363     0.0230978 
  0.163881   -0.482082    0.145566   -0.285492     0.0113247   0.323314    -0.552312    -0.841043   -0.346883     0.187748    -0.29659      0.535386   -0.253656   -0.595018   -0.599866     0.314752    0.612244      0.0857537  -0.216466     0.0148213    0.243439   -0.0213794    0.98045     -0.0159698   0.0980261    0.448072  
 -0.0602564  -0.796706    0.369663   -0.470034     0.0490532   0.648759    -0.223747     0.358926   -0.317731     0.242396     0.195415    -0.234152    0.215305   -0.618631   -0.360307     0.251335   -0.00915295   -0.0423729   0.248554    -0.35731      0.220706   -0.00930929   0.237338     0.0227335   0.317069     0.663179  
 -0.0607061   0.425359    0.0648932  -0.175198    -0.150601   -0.0198952   -0.379403    -0.384797   -0.0907937    0.00176381  -0.00254711  -0.0316138   0.496866   -0.515063   -0.392421    -0.0626637   0.185165      0.180118   -0.400257     0.0878773   -0.758357    0.326945    -0.515748     0.066859    0.00901248  -0.34426   
 -0.626354    0.338791   -0.19538     0.229477     0.372647    0.226629    -0.495124     0.811463    0.00326892  -0.21531      0.335174    -0.223875    0.220274    0.127455   -0.201259    -0.22951    -0.245593     -0.0663386  -0.0122681   -0.366765    -0.39639    -0.263241    -0.369422    -0.110185   -0.316932     0.228198  
 -0.157866   -0.135394   -0.544428   -0.0987687    0.47642     0.183019    -0.0251825    0.374451   -0.139994    -0.341301     0.130856    -0.0322263  -0.420084   -0.468634   -0.51799     -0.199304   -0.114423     -0.0431574  -0.356276     0.530173    -0.075121   -0.103025     0.571       -0.365254   -0.184884    -0.26286   
  0.320262    0.664707   -0.603881    0.205166    -0.346308   -0.191914    -0.206005    -0.0862014   0.00994207  -0.64444      0.0301345    0.309263    0.556818   -0.0127749  -0.424468     0.0144256  -0.45813      -0.0569826  -0.295586     0.343647     0.230175    0.154243     0.339914    -0.146382    0.302201     0.122582  
 -0.0204595  -0.204051    0.141579    0.0835947    0.2075      0.157362     0.249104    -0.250134    0.305704    -0.0803173   -0.00961923  -0.193094    0.291846   -0.0188072   0.229157    -0.15537     0.318592     -0.0438527   0.142439     0.260824    -0.2541     -0.323291    -0.158158     0.221434    0.349741    -0.332814  
 -0.0111099   0.182418   -0.124906    0.0836732    0.0130037  -0.228994     0.0856009   -0.154224    0.33962     -0.0859987    0.210906    -0.163972    0.0467471   0.235985    0.0124776   -0.214097   -0.144834      0.0397301   0.0136697    0.0601749   -0.0565498   0.441181    -0.0699511   -0.265946   -0.145873    -0.39529   
  0.0631274   0.110355   -0.220657   -0.0935864   -0.302439   -0.00291788   0.0897558    0.0402292  -0.116294     0.004144    -0.191994     0.134343   -0.208364   -0.0472443  -0.0268039    0.198149   -0.016953      0.0689507   0.0402676   -0.0551754    0.221238   -0.0489614    0.0953543    0.130267   -0.0500393    0.210063  
 -0.0134706  -0.0695158   0.11163     0.0893389    0.383786   -0.0970576   -0.298908     0.231415   -0.249268     0.0840804    0.0272931    0.0812783   0.093129   -0.0261956   0.020775    -0.135421   -0.0602373    -0.12345    -0.102084    -0.0994412   -0.180621   -0.0780606   -0.173107    -0.0709031  -0.0118324    0.146318  
  0.112096   -0.0270594  -0.245963   -0.261851    -0.319644   -0.141274    -0.137677    -0.0570752   0.470559     0.109368     0.607132    -0.0190876  -0.063867    0.0771809  -0.0773003    0.0339893  -0.249616      0.194807    0.0139081   -0.18885      0.216316    0.424304    -0.00440331  -0.448872   -0.706332    -0.0867678 
  0.159155    0.121733   -0.0209669   0.10194      0.370053   -0.0568374    0.00573016   0.150284   -0.188309    -0.0658024    0.00316152   0.0160836   0.411085   -0.106209    0.217093    -0.0143429  -0.0523025    -0.243416    0.154826     0.0201237   -0.0965691  -0.207428    -0.0700988    0.201684    0.637034     0.138432  
 -0.746258    0.403075   -0.531856   -0.643502    -0.355718    0.913256     0.597768     0.0548843   0.0541889   -0.403278     0.187389    -0.472752   -0.0241195  -0.926948   -0.488926    -0.418349   -0.136933      0.219894   -0.290298     0.558041    -0.127751    0.66979      0.0129875   -0.208798   -0.284035    -0.74349   
 -0.286103   -0.412991    0.0199629   0.0590178   -0.874383    0.0490858   -0.150877    -0.0925262  -0.298839     0.048129    -0.717575     0.0832753  -0.633445    0.0436697   0.0995535    0.559183    0.374594      0.341463    0.00723389  -0.437        0.104573   -0.177078    -0.177725     0.422187   -0.319318     0.556692  
  0.0769782   0.5523      0.18358     0.20403      0.279775   -0.145952    -0.114459     0.129854   -0.0442326   -0.404097    -0.622456    -0.0451727   0.421708    0.123312   -0.044849    -0.135441   -0.000658814  -0.0359583  -0.0943982    0.0849655   -0.0798428  -0.825092     0.0819451    0.844692    0.563551     0.381821  
 -0.368615   -0.39297     0.647343   -0.309402     0.358917    0.124813     0.575962     0.348306   -0.122122     0.622283    -0.494051    -0.0286722  -0.252328   -0.276061    0.294755    -0.415684   -0.149539     -0.31604    -0.128506     0.00623067  -0.174286   -0.352621    -0.291862     1.06195     0.0454301   -0.152879  
 -0.0925603   0.1907     -0.181133    0.677418     0.424997   -0.125447     0.0943613    0.666448   -0.323884    -0.680657    -0.959413     0.228454   -0.468538    0.376381   -0.264983    -0.748825   -0.00120423   -0.500599   -0.48814      0.0854224   -0.408203   -0.00963044  -0.270007    -0.0508298  -0.0558045    0.170649  
 -0.563968   -0.337696   -0.1188      0.663294     0.263971    0.291833     0.220955    -0.447065   -0.0302261   -0.0948732   -0.449173    -0.777814    0.135389    0.584739    0.0615256   -0.370397    0.247835      0.147957   -0.116257     0.213133    -0.506414   -0.128035     0.172001    -0.08624     0.384081    -0.415893  
 -0.23216     0.0733975  -0.43917    -0.00621131  -0.515282    0.161432     0.133073    -0.359664   -0.0751336    0.491399     0.0205091    0.207607   -0.385115    0.318804   -0.407906    -0.374705   -0.164265      0.0477182   0.060071    -0.768377    -0.61457     0.11904      0.162043     0.155937   -0.205193     0.302089  
  0.0825176   0.0515516  -0.881275    0.373855    -0.028516    0.242229    -0.0909227    0.316804   -0.365074     0.204431     0.300465     0.0979057  -0.591026    0.371652    0.37247     -0.326256   -0.0733413    -0.164011    0.145152    -0.578967     0.467479   -0.306201     0.741582     0.357368    0.171357     0.53343   
 -0.238161   -0.715777    0.421132    0.208015     0.595123   -0.037768    -0.258634    -0.0978347  -0.104131     0.798922     0.372159    -0.106852   -0.0786922   0.0773558   0.63603      0.681843    0.293823     -0.151652    0.695347    -0.183453     0.0864549  -0.278645     0.0635048   -0.133777    0.0577049    0.426797  
 -0.0166028   0.132064    0.22941     0.0764377   -0.277851   -0.314111     0.118037    -0.172244   -0.127951     0.169085     0.20708     -0.223225    0.513196    0.448403    0.453496     0.119106   -0.0798858     0.0129769   0.649101    -0.823242     0.30436     0.148813    -0.433401     0.646958    0.105326    -0.00552217
 -0.0333488   0.136311    0.290558   -0.858122     0.0971954  -0.218705     0.120474     0.244856    0.340296     0.423108     0.634127    -0.0908944   0.221369    0.0552822  -0.00615095  -0.0164714  -0.731128     -0.269315    0.00618192   0.303558     0.450585   -0.11088     -0.610152    -0.0594308  -0.107319    -0.242168  
  0.18366     0.584953    0.0092547   0.555345     0.164057   -0.580333     0.705428     0.257764    0.625728    -0.138189     0.203552     0.0295428   0.0283436   0.464145    0.880907    -0.210542   -0.235697     -0.0292336   0.124338     0.487625    -0.24055    -0.0342731   -0.93289      0.0905838  -0.00785311  -0.822296  
  0.828664   -0.0878198  -0.279821   -0.376731    -0.100806   -0.276097    -0.0233959    0.37747    -0.185058     0.0217089    0.643435     0.405704    0.247772   -0.36444     0.103708    -0.249533   -0.165582     -0.152269    0.487232    -0.458786     0.28094     0.297847    -0.233704    -0.29318     0.108509    -0.18762   
  0.70017    -0.19247    -0.159034    0.43767      0.510266    0.017282    -0.0629333   -0.24192     0.392194     0.324346     0.404516     0.490785    0.0522862   0.360528   -0.070395    -0.935057   -0.0812491    -0.0747837   0.099754     0.148904    -0.691463   -0.196186     0.0310772    0.0100329   0.499439    -0.286185  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.408444
INFO: iteration 2, average log likelihood -1.408431
INFO: iteration 3, average log likelihood -1.408418
INFO: iteration 4, average log likelihood -1.408406
INFO: iteration 5, average log likelihood -1.408395
INFO: iteration 6, average log likelihood -1.408384
INFO: iteration 7, average log likelihood -1.408373
INFO: iteration 8, average log likelihood -1.408363
INFO: iteration 9, average log likelihood -1.408353
INFO: iteration 10, average log likelihood -1.408344
INFO: EM with 100000 data points 10 iterations avll -1.408344
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.321920e+05
      1       7.096259e+05      -2.225661e+05 |       32
      2       6.904909e+05      -1.913501e+04 |       32
      3       6.840221e+05      -6.468851e+03 |       32
      4       6.807183e+05      -3.303736e+03 |       32
      5       6.787677e+05      -1.950595e+03 |       32
      6       6.774060e+05      -1.361766e+03 |       32
      7       6.764073e+05      -9.986599e+02 |       32
      8       6.756421e+05      -7.652149e+02 |       32
      9       6.749928e+05      -6.492596e+02 |       32
     10       6.744450e+05      -5.478266e+02 |       32
     11       6.739524e+05      -4.925959e+02 |       32
     12       6.735297e+05      -4.226876e+02 |       32
     13       6.731759e+05      -3.538005e+02 |       32
     14       6.728959e+05      -2.800441e+02 |       32
     15       6.726437e+05      -2.522256e+02 |       32
     16       6.724051e+05      -2.385159e+02 |       32
     17       6.722033e+05      -2.018287e+02 |       32
     18       6.720302e+05      -1.731095e+02 |       32
     19       6.718704e+05      -1.597976e+02 |       32
     20       6.717301e+05      -1.403175e+02 |       32
     21       6.716044e+05      -1.256684e+02 |       32
     22       6.714877e+05      -1.167645e+02 |       32
     23       6.713655e+05      -1.221666e+02 |       32
     24       6.712535e+05      -1.120205e+02 |       32
     25       6.711590e+05      -9.447686e+01 |       32
     26       6.710718e+05      -8.722244e+01 |       32
     27       6.709798e+05      -9.192956e+01 |       32
     28       6.708862e+05      -9.365472e+01 |       32
     29       6.708100e+05      -7.623471e+01 |       32
     30       6.707371e+05      -7.286533e+01 |       32
     31       6.706682e+05      -6.884863e+01 |       32
     32       6.706036e+05      -6.462365e+01 |       32
     33       6.705409e+05      -6.269187e+01 |       32
     34       6.704710e+05      -6.988000e+01 |       32
     35       6.704089e+05      -6.215078e+01 |       32
     36       6.703564e+05      -5.253242e+01 |       32
     37       6.703083e+05      -4.804045e+01 |       32
     38       6.702658e+05      -4.250303e+01 |       32
     39       6.702221e+05      -4.371168e+01 |       32
     40       6.701846e+05      -3.753941e+01 |       32
     41       6.701517e+05      -3.291264e+01 |       32
     42       6.701186e+05      -3.300658e+01 |       32
     43       6.700841e+05      -3.451261e+01 |       32
     44       6.700450e+05      -3.915231e+01 |       32
     45       6.700091e+05      -3.583687e+01 |       32
     46       6.699737e+05      -3.547421e+01 |       32
     47       6.699376e+05      -3.607696e+01 |       32
     48       6.699046e+05      -3.296075e+01 |       32
     49       6.698690e+05      -3.566555e+01 |       32
     50       6.698342e+05      -3.475220e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 669834.2185116204)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.420006
INFO: iteration 2, average log likelihood -1.415111
INFO: iteration 3, average log likelihood -1.413892
INFO: iteration 4, average log likelihood -1.413085
INFO: iteration 5, average log likelihood -1.412243
INFO: iteration 6, average log likelihood -1.411357
INFO: iteration 7, average log likelihood -1.410605
INFO: iteration 8, average log likelihood -1.410103
INFO: iteration 9, average log likelihood -1.409809
INFO: iteration 10, average log likelihood -1.409631
INFO: iteration 11, average log likelihood -1.409509
INFO: iteration 12, average log likelihood -1.409415
INFO: iteration 13, average log likelihood -1.409337
INFO: iteration 14, average log likelihood -1.409269
INFO: iteration 15, average log likelihood -1.409207
INFO: iteration 16, average log likelihood -1.409150
INFO: iteration 17, average log likelihood -1.409097
INFO: iteration 18, average log likelihood -1.409048
INFO: iteration 19, average log likelihood -1.409001
INFO: iteration 20, average log likelihood -1.408957
INFO: iteration 21, average log likelihood -1.408915
INFO: iteration 22, average log likelihood -1.408876
INFO: iteration 23, average log likelihood -1.408838
INFO: iteration 24, average log likelihood -1.408802
INFO: iteration 25, average log likelihood -1.408768
INFO: iteration 26, average log likelihood -1.408735
INFO: iteration 27, average log likelihood -1.408704
INFO: iteration 28, average log likelihood -1.408674
INFO: iteration 29, average log likelihood -1.408646
INFO: iteration 30, average log likelihood -1.408619
INFO: iteration 31, average log likelihood -1.408593
INFO: iteration 32, average log likelihood -1.408568
INFO: iteration 33, average log likelihood -1.408543
INFO: iteration 34, average log likelihood -1.408520
INFO: iteration 35, average log likelihood -1.408498
INFO: iteration 36, average log likelihood -1.408476
INFO: iteration 37, average log likelihood -1.408455
INFO: iteration 38, average log likelihood -1.408434
INFO: iteration 39, average log likelihood -1.408414
INFO: iteration 40, average log likelihood -1.408395
INFO: iteration 41, average log likelihood -1.408376
INFO: iteration 42, average log likelihood -1.408358
INFO: iteration 43, average log likelihood -1.408340
INFO: iteration 44, average log likelihood -1.408323
INFO: iteration 45, average log likelihood -1.408306
INFO: iteration 46, average log likelihood -1.408290
INFO: iteration 47, average log likelihood -1.408274
INFO: iteration 48, average log likelihood -1.408259
INFO: iteration 49, average log likelihood -1.408245
INFO: iteration 50, average log likelihood -1.408231
INFO: EM with 100000 data points 50 iterations avll -1.408231
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0322338   0.243284   -0.0955805    0.177777     0.167871   -0.202665      0.1522      0.343556     0.231757    -0.018773    0.158751   -0.0584343    0.0900134    0.303237     0.197635   -0.186414    -0.353378    -0.204018     -0.0375626  -0.000985699  -0.147265    0.0143049    -0.427805    -0.0745187   -0.031229    -0.139388  
 -0.30602     0.15099    -0.0144362   -0.51782     -1.01701    -0.0322164    -0.0983646   0.083573    -0.245663    -0.0628762  -0.322566    0.285072    -0.115202    -0.495138     0.241376    0.954961     0.206934     0.173215     -0.0822071  -0.208765      0.254696    0.133457     -0.557875     0.180658    -0.36899      0.515069  
 -0.400276    0.0119794  -0.0449565    0.564684     0.162205    0.34817      -0.317027    0.125253    -0.346446     0.137413    0.115829   -0.0633397    0.321198    -0.274055    -0.135519   -0.21617      0.0966211   -0.0822086     0.112114   -0.916454     -1.11582     0.273482     -0.438867     0.083128     0.0151301    0.144431  
 -0.0191145  -0.249835   -0.0434389    0.31446      0.48684     0.1886        0.670195   -0.284936     0.00162006  -0.333711   -0.102837   -0.279409     0.314609     0.354271     0.477948   -0.904548     0.514998    -0.473544      0.0517446   0.683234     -1.00325    -0.446307     -0.292547     0.0574853    0.677001    -0.630903  
 -0.582159    0.314418   -0.469901    -0.649444    -0.0250242   0.584849      0.391718    0.00892013  -0.0928143   -0.224467    0.428793   -0.46908      0.149791    -1.02059     -0.479797   -0.2488      -0.199927     0.137871     -0.316043    0.405951     -0.0633348   0.541621     -0.0685151   -0.341403    -0.280938    -0.707102  
 -0.16436    -0.597403    0.564718     0.0437055    0.255691    0.137332      0.250379   -0.448911     0.357665     0.586073    0.377417   -0.232705    -0.167974    -0.212581     0.685973    0.576839     0.467815     0.406554      0.381426    0.171648     -0.117133    0.040672     -0.209071    -0.198124    -0.287734    -0.110484  
  0.544972   -0.126637   -0.136171    -0.349746    -0.196762   -0.241931      0.282043    0.597295    -0.454785    -0.304979    0.608528    0.116179     0.218652    -0.183673     0.478171   -0.249434    -0.0807155   -0.245733      0.776688   -1.17382       0.438327    0.210726     -0.184735     0.0912653    0.0463518    0.0382564 
  0.0547169  -0.131733    0.172288    -0.15918      0.150904    0.111978      0.139447   -0.0574057    0.216604     0.114703    0.0619193  -0.0885169    0.0571819   -0.026782     0.0178112  -0.00235448   0.00672761  -0.000976644   0.104086    0.199905      0.103554   -0.203725     -0.00610226   0.182092     0.244263    -0.0124421 
 -0.30575     0.207584    0.341043     0.653882    -0.575359   -0.0757304     0.37622    -0.195439     0.605827    -0.119954   -0.174958   -0.301978     0.330348     0.41595      0.131425    0.357717    -0.00609507  -0.0861105     0.0673114  -0.385046     -0.359629    0.0279802    -0.44929      0.340803     0.401422     0.0315252 
  0.346753    0.621345   -0.120674    -0.344357    -0.733819   -0.1036        0.101889    0.0468759    0.471324    -0.468118    0.099565    0.550371     0.089093     0.0194524   -0.538586   -0.825459    -0.207888    -0.0665748    -0.401133    0.0191179    -0.121501    0.719038     -0.366756     0.28882      0.00524159  -0.788647  
  0.128917   -0.565427    0.459765     0.113678     0.0987541   0.0768018    -0.362773   -0.268995    -0.298378     0.227184   -0.51681     0.150651    -0.879466    -0.207168     0.149354    0.272366     0.698929     0.0426282    -0.255028   -0.0694974    -0.0173339   0.278957      0.262171     0.00209977  -0.52782      0.402775  
  0.406807    0.0454306  -0.045538    -0.240829    -0.488176   -0.312876      0.306165    0.0719757    0.116648     0.588804    0.100185    0.438489     0.446327     0.228135     0.135292    0.608776    -0.527351    -0.0136914     0.778132   -0.128691      0.555989    0.177723     -0.449596     0.0548565    0.231662     0.132903  
 -0.118565   -0.112085   -0.314867    -0.13514      0.786628    0.603622     -0.038826    0.486782     0.0144309   -0.0938539   0.150019    0.080321    -0.463955    -0.580967    -0.374547   -0.0670562    0.11659     -0.0268731     0.190544    0.28016      -0.293837   -0.508058      0.60097      0.298773     0.0290728    0.354348  
 -0.0508768   0.605213   -0.23248      0.149941     0.260282   -0.298449      0.236287    0.270736     0.409348    -0.0490027   0.557635   -0.269575     0.42345      0.38521      0.239344   -0.358826    -0.532933    -0.162215      0.219296    0.128461      0.0792091   0.000913203  -0.376546     0.0362681   -0.0643246   -0.408773  
  0.0782867  -0.518492   -0.0243365   -0.475758     0.141841    0.0528421    -0.223418    0.193625    -0.0106878    0.643551    0.782676    0.0944381   -0.126306    -0.221753     0.106357    0.0194507   -0.397459    -0.265152      0.136271   -0.0415062     0.278235    0.348098     -0.136547    -0.618803    -0.139811     0.0335411 
 -0.0239205   0.502281   -0.300371     0.302841     0.207359   -0.25749      -0.304468    0.371309    -0.198915    -0.801245   -0.409282    0.101733     0.0458557    0.236794    -0.354703   -0.652837     0.107674    -0.173116     -0.286864   -0.00271647   -0.151051   -0.374225     -0.176468     0.218726     0.0906046    0.180564  
 -0.0438804   0.477016   -1.08267     -0.225317    -0.553609   -0.179825      0.357548   -0.109489    -0.0524039    0.188322    0.0187355   0.275616    -0.38512      0.0502317   -0.150176   -0.323921    -0.137955    -0.394694     -0.53547    -0.676387     -0.41699     0.216402      0.435171     0.0967249   -0.2852       0.333487  
 -0.0473599   0.108707   -0.148366     0.00618454  -0.0771462  -0.0873571    -0.075023    0.0869454   -0.198323    -0.0542423  -0.0559718   0.0571638   -0.0322612   -0.0452911    0.045787    0.0922582   -0.0182708    0.00930934   -0.0291195  -0.147446      0.013952    0.107094     -0.0786561   -0.0623209   -0.155476     0.0994134 
  0.539693   -0.234984   -0.0553083    0.462315     0.324725   -0.0859125    -0.0787545  -0.263049     0.239233     0.448661    0.214692    0.508397    -0.0416291    0.352013    -0.0667958  -0.669307    -0.0427038    0.108938      0.268221   -0.0856115    -0.653964   -0.103968      0.044082     0.0253106    0.409541    -0.256593  
  0.22759     0.245729    0.675009     0.0930065    0.508781   -0.472559     -0.20488     0.0868709   -0.0848558   -0.312883   -0.297139   -0.276464     0.435888    -0.400215     1.05944     0.372876     0.434854    -0.0690283    -0.281001    0.446617      0.467146   -0.436683     -0.518034     0.313072     0.307307    -0.391104  
 -0.319444    0.309489   -0.297754     0.168833    -0.434366    0.000776512  -0.864733    0.0977507    0.817133    -0.211525    0.154517   -0.0882231   -0.393122     0.253254    -0.359122   -0.0187652   -0.534989     1.1003        0.299311   -0.726716      0.319433    0.0947112    -0.0696527   -0.222397    -0.9863       0.558111  
  0.265486    0.456994   -0.568238     0.300791    -0.20527    -0.00902175   -0.530802   -0.100188    -0.149879    -0.578923    0.216111    0.444461     0.712797    -0.176014    -0.26035     0.248676    -0.373007    -0.053778     -0.145882    0.248775      0.178652    0.211161      0.478446    -0.286861     0.50531      0.424022  
  0.135623   -0.124753    0.143251    -0.456462     0.172012   -0.227329     -0.785774   -0.218749     0.192969    -0.36929     0.321529   -0.0405093    0.366179    -0.228098    -0.0828071  -0.233988     0.669568     0.178316     -0.0617284   0.160385     -0.427572    0.0646794    -0.196327    -0.565519    -0.456041    -0.325157  
 -0.407869   -0.0849975   0.519885    -0.4701       0.154037    0.0715564     0.190551    0.239534    -0.144761     0.737969   -0.268877   -0.114202    -0.0163441   -0.00506621   0.245689   -0.360425    -0.314004    -0.205895     -0.123712   -0.0772351    -0.12252    -0.394216     -0.289531     0.875977    -0.0460131    0.00974046
 -0.0962833  -0.503945   -0.64383      0.554422    -0.401941    0.629102      0.163283   -0.240607    -0.356059     0.245207   -0.185369   -0.208815    -0.751076     0.783373     0.260933   -0.100337     0.382445     0.223579      0.45336    -0.637168      0.231649   -0.26738       0.348961     0.225868     0.0990919    0.0610922 
 -0.183152   -0.631261    0.289993    -0.538629    -0.319779    0.44828      -0.527688   -0.0795707   -0.629266     0.175868   -0.124888   -0.00578262   0.301294    -0.615342    -0.624147    0.362392     0.22955     -0.0741896    -0.0378914  -0.725127      0.269886   -0.114121      0.394565     0.0205756    0.226264     0.727975  
  0.705898   -0.0206586  -0.123653    -0.0703922   -0.200503   -0.467567      0.455114   -0.535922     0.260045     0.193472   -0.320008    0.201606    -0.484059     0.205323     0.0679299   0.107376     0.19939      0.12189      -0.0281141   0.645248      0.259396    0.247406      0.199547    -0.247929    -0.0571041   -0.17793   
  0.215332   -0.324339   -3.54095e-5  -0.226403     0.0122428   0.14586      -0.197295   -0.458396    -0.111433     0.109186   -0.212842    0.161105    -0.207571    -0.185733    -0.331718    0.0878877    0.423253     0.183029     -0.121416    0.313427      0.181693   -0.0804694     0.722612     0.0248189    0.0654548    0.249511  
  0.17756    -0.22101     0.210299     0.386558     0.680414   -0.502822     -0.518757   -0.134591    -0.594828     0.33537    -0.0009207  -0.128027     0.548906     0.710117     0.207641    0.334447     0.130181    -0.276623      0.429194   -0.707947      0.209068   -0.669807      0.0200943    0.209978     0.363793     0.676548  
 -0.61752    -0.0123712  -0.229839    -0.293176    -0.316862   -0.21934       0.507719    0.133567    -0.0388246   -0.66857    -0.541518   -0.548649     0.0205199    0.00479264   0.15394     0.755933    -0.0568745   -0.133631     -0.291225    0.245717      0.725934    0.0997855     0.158253    -0.0337439   -0.172844     0.0328674 
  0.263968   -0.131527   -0.384585     0.744931     0.33544    -0.0180145     0.558982    0.746051    -0.20888      0.0345599  -0.502197    0.245854    -0.616723     0.275715     0.280764   -0.188603    -0.696525    -0.152901     -0.105458    0.512568      0.18905    -0.338676      0.122972    -0.0649994    0.108397     0.305196  
 -0.245904   -0.0951041   0.0254104    0.346126     0.3131     -0.014909     -0.209452   -0.181802     0.130106    -0.107604   -0.500246   -0.271642    -0.00553942  -0.0216258   -0.655936   -0.0966159   -0.197741     0.217808     -0.724229    0.827358     -0.374952    0.0862045     0.204074    -0.169125     0.19798     -0.41639   INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.408217
INFO: iteration 2, average log likelihood -1.408204
INFO: iteration 3, average log likelihood -1.408192
INFO: iteration 4, average log likelihood -1.408180
INFO: iteration 5, average log likelihood -1.408168
INFO: iteration 6, average log likelihood -1.408157
INFO: iteration 7, average log likelihood -1.408147
INFO: iteration 8, average log likelihood -1.408137
INFO: iteration 9, average log likelihood -1.408128
INFO: iteration 10, average log likelihood -1.408119
INFO: EM with 100000 data points 10 iterations avll -1.408119
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
