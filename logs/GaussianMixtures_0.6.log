>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.7.0
INFO: Installing JLD v0.6.6
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.1
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.2.0
INFO: Installing StaticArrays v0.1.0
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1230
Commit ce6a0c3 (2016-11-10 21:09 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-101-generic #148-Ubuntu SMP Thu Oct 20 22:08:32 UTC 2016 x86_64 x86_64
Memory: 2.939281463623047 GB (655.44140625 MB free)
Uptime: 24353.0 sec
Load Avg:  1.0234375  0.97998046875  1.00048828125
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3499 MHz    1468924 s       2544 s     145794 s     579666 s         61 s
#2  3499 MHz     693105 s       4139 s      80596 s    1579316 s          2 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.3
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.7.0
 - JLD                           0.6.6
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.1
 - Rmath                         0.1.4
 - SHA                           0.2.1
 - ScikitLearnBase               0.2.0
 - StaticArrays                  0.1.0
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-985479.0008945011,[99999.1,0.875974],
[-922.01 -162.311 -17.3935; 1.14368 0.79178 -3.49699],

Array{Float64,2}[
[1.00172e5 679.427 665.373; 679.427 100037.0 118.841; 665.373 118.841 1.00478e5],

[1.55336 1.05757 -4.57039; 1.05757 0.983228 -3.19565; -4.57039 -3.19565 13.9665]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.445334e+03
      1       9.588237e+02      -4.865099e+02 |        8
      2       8.823987e+02      -7.642495e+01 |        2
      3       8.636791e+02      -1.871965e+01 |        0
      4       8.636791e+02       0.000000e+00 |        0
K-means converged with 4 iterations (objv = 863.6790814746882)
INFO: K-means with 272 data points using 4 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.076146
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.865984
INFO: iteration 2, lowerbound -3.775346
INFO: iteration 3, lowerbound -3.674121
INFO: iteration 4, lowerbound -3.537244
INFO: iteration 5, lowerbound -3.368440
INFO: iteration 6, lowerbound -3.181320
INFO: dropping number of Gaussions to 7
INFO: iteration 7, lowerbound -2.989422
INFO: iteration 8, lowerbound -2.821061
INFO: iteration 9, lowerbound -2.705743
INFO: dropping number of Gaussions to 5
INFO: iteration 10, lowerbound -2.627243
INFO: dropping number of Gaussions to 3
INFO: iteration 11, lowerbound -2.544681
INFO: iteration 12, lowerbound -2.470107
INFO: iteration 13, lowerbound -2.413160
INFO: iteration 14, lowerbound -2.369479
INFO: iteration 15, lowerbound -2.337180
INFO: iteration 16, lowerbound -2.315685
INFO: iteration 17, lowerbound -2.307444
INFO: dropping number of Gaussions to 2
INFO: iteration 18, lowerbound -2.302963
INFO: iteration 19, lowerbound -2.299262
INFO: iteration 20, lowerbound -2.299257
INFO: iteration 21, lowerbound -2.299255
INFO: iteration 22, lowerbound -2.299254
INFO: iteration 23, lowerbound -2.299253
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Fri 11 Nov 2016 12:15:28 PM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Fri 11 Nov 2016 12:15:29 PM UTC: K-means with 272 data points using 4 iterations
11.3 data points per parameter
,Fri 11 Nov 2016 12:15:30 PM UTC: EM with 272 data points 0 iterations avll -2.076146
5.8 data points per parameter
,Fri 11 Nov 2016 12:15:31 PM UTC: GMM converted to Variational GMM
,Fri 11 Nov 2016 12:15:32 PM UTC: iteration 1, lowerbound -3.865984
,Fri 11 Nov 2016 12:15:33 PM UTC: iteration 2, lowerbound -3.775346
,Fri 11 Nov 2016 12:15:33 PM UTC: iteration 3, lowerbound -3.674121
,Fri 11 Nov 2016 12:15:33 PM UTC: iteration 4, lowerbound -3.537244
,Fri 11 Nov 2016 12:15:33 PM UTC: iteration 5, lowerbound -3.368440
,Fri 11 Nov 2016 12:15:33 PM UTC: iteration 6, lowerbound -3.181320
,Fri 11 Nov 2016 12:15:33 PM UTC: dropping number of Gaussions to 7
,Fri 11 Nov 2016 12:15:33 PM UTC: iteration 7, lowerbound -2.989422
,Fri 11 Nov 2016 12:15:33 PM UTC: iteration 8, lowerbound -2.821061
,Fri 11 Nov 2016 12:15:33 PM UTC: iteration 9, lowerbound -2.705743
,Fri 11 Nov 2016 12:15:33 PM UTC: dropping number of Gaussions to 5
,Fri 11 Nov 2016 12:15:33 PM UTC: iteration 10, lowerbound -2.627243
,Fri 11 Nov 2016 12:15:33 PM UTC: dropping number of Gaussions to 3
,Fri 11 Nov 2016 12:15:33 PM UTC: iteration 11, lowerbound -2.544681
,Fri 11 Nov 2016 12:15:33 PM UTC: iteration 12, lowerbound -2.470107
,Fri 11 Nov 2016 12:15:34 PM UTC: iteration 13, lowerbound -2.413160
,Fri 11 Nov 2016 12:15:34 PM UTC: iteration 14, lowerbound -2.369479
,Fri 11 Nov 2016 12:15:34 PM UTC: iteration 15, lowerbound -2.337180
,Fri 11 Nov 2016 12:15:34 PM UTC: iteration 16, lowerbound -2.315685
,Fri 11 Nov 2016 12:15:34 PM UTC: iteration 17, lowerbound -2.307444
,Fri 11 Nov 2016 12:15:34 PM UTC: dropping number of Gaussions to 2
,Fri 11 Nov 2016 12:15:34 PM UTC: iteration 18, lowerbound -2.302963
,Fri 11 Nov 2016 12:15:34 PM UTC: iteration 19, lowerbound -2.299262
,Fri 11 Nov 2016 12:15:34 PM UTC: iteration 20, lowerbound -2.299257
,Fri 11 Nov 2016 12:15:34 PM UTC: iteration 21, lowerbound -2.299255
,Fri 11 Nov 2016 12:15:34 PM UTC: iteration 22, lowerbound -2.299254
,Fri 11 Nov 2016 12:15:34 PM UTC: iteration 23, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:34 PM UTC: iteration 24, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:34 PM UTC: iteration 25, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:34 PM UTC: iteration 26, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:34 PM UTC: iteration 27, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:35 PM UTC: iteration 28, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:35 PM UTC: iteration 29, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:35 PM UTC: iteration 30, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:35 PM UTC: iteration 31, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:35 PM UTC: iteration 32, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:35 PM UTC: iteration 33, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:35 PM UTC: iteration 34, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:35 PM UTC: iteration 35, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:35 PM UTC: iteration 36, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:35 PM UTC: iteration 37, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:35 PM UTC: iteration 38, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:35 PM UTC: iteration 39, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:35 PM UTC: iteration 40, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:35 PM UTC: iteration 41, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:35 PM UTC: iteration 42, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:35 PM UTC: iteration 43, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:36 PM UTC: iteration 44, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:36 PM UTC: iteration 45, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:36 PM UTC: iteration 46, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:36 PM UTC: iteration 47, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:36 PM UTC: iteration 48, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:36 PM UTC: iteration 49, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:36 PM UTC: iteration 50, lowerbound -2.299253
,Fri 11 Nov 2016 12:15:36 PM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.9549,178.045]
β = [95.9549,178.045]
m = [2.00023 53.852; 4.2503 79.2869]
ν = [97.9549,180.045]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.375876 -0.00895312; 0.0 0.0127487],

[0.184042 -0.00764405; 0.0 0.00858171]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.00000000001
avll from stats: -0.9877204509730684
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9877204509731046
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9877204509731045
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.00000000001
avll from stats: -0.9876477481683109
avll from llpg:  -0.9876477481683107
avll direct:     -0.9876477481683108
sum posterior: 100000.0
32×26 Array{Float64,2}:
  0.00040954  -0.0258942    -0.0263727    0.0592714     0.0382368    -0.235132    -0.0803694     0.0313384  -0.298602      0.00146788    0.0903145   0.148154    -0.163008    -0.12135    -0.0658432    0.0579877    0.0231572   -0.137453     0.159194     0.0807534   -0.0954218  -0.022946     0.00713571   0.0134282   -0.0565989    0.0381223 
  0.128568    -0.0968313    -0.00123374  -0.0569488     0.096994     -0.183064    -0.0718248    -0.0758623  -0.0177133     0.170403     -0.0025299  -0.00679524   0.0502186    0.0773321  -0.133989    -0.119771    -0.145943    -0.137772    -0.122811    -0.130856     0.0510734  -0.245085     0.0466696   -0.104584    -0.0167813    0.0895177 
 -0.0711671    0.0537994    -0.115694     0.154401     -0.0678636    -0.0411303    0.0781016    -0.214629    0.07846       0.0941226    -0.107011    0.129652    -0.0577185    0.0696389   0.0580648   -0.0702341    0.092341    -0.0114715    0.163599    -0.0341759   -0.140074   -0.0527242   -0.185019     0.212919    -0.159237     0.168809  
  0.225854     0.105578      0.0641894    0.00241075   -0.0866323     0.0907408   -0.000189891  -0.14732    -0.131382     -0.0538576    -0.141856   -0.0529638    0.00859106   0.0846742   0.226247    -0.0266192   -0.0517797    0.107806    -0.165432    -0.158811     0.0591608   0.235061     0.0353136    0.0493551   -0.106994     0.0496409 
  0.00983972   0.00411034   -0.0148283    0.119357     -0.037211     -0.0758579    0.132007      0.131666    0.00119216    0.0776867     0.117208    0.0357651    0.0708491   -0.172809    0.123098    -0.105444     0.00607045  -0.0563988   -0.158287    -0.013869     0.169047    0.148351    -0.148674    -0.0849723    0.0786805    0.121613  
  0.00998564   0.0479594    -0.0490909   -0.0155725     0.0205917    -0.13954     -0.0879621     0.0656905   0.0860687     0.109019      0.0114467   0.0903029   -0.125433     0.192715   -0.0769986   -0.0345821    0.187647     0.0810538    0.188241     0.130759    -0.0479983  -0.216055    -0.132908    -0.0535405   -0.0877116   -0.076156  
 -0.0486719    0.0289381     0.101548     0.0754471    -0.0947726     0.0118614   -0.0163239    -0.0446183   0.109484     -0.00274293    0.0943184  -0.023592    -0.0426264    0.0702978  -0.168432    -0.076268    -0.186252     0.139945     0.105697    -0.00833797  -0.119768   -0.125931     0.118026     0.0884889    0.0412991   -0.047515  
 -0.0101661    0.00577802    0.0717205    0.00797511   -0.0361821     0.0049347    0.0169662    -0.0969805   0.0215885    -0.166597     -0.0580942   0.196202     0.0606433    0.112455   -0.0336173    0.0188811    0.117676     0.0284694    0.195327    -0.0301693   -0.0679449   0.0916681   -0.0984605   -0.146791    -0.12785      0.024044  
 -0.047308    -0.0160189     0.0314134    0.000227357   0.0242961     0.0518272   -0.0428242     0.135905    0.00784704    0.0660741    -0.0534892   0.0245692   -0.0716477    0.0602463  -0.221595    -0.0811999   -0.0343938   -0.0491455    0.00451669   0.0916921   -0.0720143  -0.0711538    0.0262445   -0.110439     0.133781    -0.104454  
  0.159198     0.046481     -0.0853218   -0.0817171     0.045588      0.0943827    0.00591669   -0.0632505   0.131906     -0.034482      0.0950573  -0.0538056   -0.0868473   -0.0495929  -0.0729602    0.0303975    0.0343335    0.0285779   -0.00545443  -0.0266066   -0.104741    0.0775717    0.284115     0.100686    -0.110895    -0.0131575 
 -0.13933      0.0730739     0.222525     0.0908675     0.13417       0.127564    -0.112288      0.101758    0.132369      0.141455      0.115141   -0.143381     0.175671     0.0723928  -0.13371      0.0811179   -0.119175    -0.0311963    0.0911287    0.265682     0.13286     0.0172427   -0.00533186   0.00894749  -0.0181971   -0.0267208 
  0.141389    -0.0101348     0.0935756   -0.115045     -0.11797       0.0655237   -0.0481831    -0.226741   -0.0634207     0.0293468     0.0218252  -0.0279345   -0.0854703    0.0854862   0.0337316   -0.0115777    0.0698251    0.0563872    0.0950933    0.134984    -0.0693318  -0.0549648    0.0118289   -0.0439282   -0.126374     0.103914  
 -0.0718671    0.000997169  -0.0593069   -0.151451     -0.0646644     0.0688952   -0.0191276     0.0670197   0.128449      0.184851     -0.137964   -0.0420996   -0.0264024   -0.0933156  -0.0964943    0.147138     0.0221729    0.0101629    0.0329934   -0.0299869   -0.0105023   0.0517742    0.149229    -0.0396326   -0.0243494    0.120537  
 -0.00358808  -0.152533     -0.122551     0.0290776     0.219003     -0.0305728   -0.130951      0.012125   -0.0764962     0.185577     -0.200757    0.0102249   -0.0931643    0.177317   -0.0493358   -0.0163243   -0.0345122    0.190866     0.049779    -0.183135     0.0381672  -0.0968752    0.125993     0.19778     -0.185312     0.0195611 
  0.0225159   -0.0112964     0.0718412   -0.00800473   -0.127188      0.0682996    0.0257111    -0.0636543   0.143013      0.13179       0.0568002  -0.076288     0.129247    -0.0860217   0.150125    -0.0500436   -0.0556894    0.0856491    0.124298     0.0793545    0.118594    0.138398     0.154112     0.127439     0.0142819   -0.020154  
  0.10212      0.000222966  -0.0480234    0.0274208    -0.000445199   0.0174133    0.07894      -0.107149   -0.278488      0.0453737     0.0547138   0.0777724   -0.0157766   -0.114843    0.117833    -0.0375434    0.0764711    0.112028    -0.0184452    0.284277    -0.0817046   0.0182939   -0.116714     0.158807    -0.0520257   -0.0720146 
  0.0794352   -0.00121164   -0.102275    -0.0129613     0.0147574    -0.135798     0.0346599     0.0757987  -0.0990291     0.269703     -0.148251    0.0096287   -0.0383384    0.0629642   0.0718016   -0.0698961    0.0226134   -0.247051    -0.0225235   -0.154005     0.0162682   0.0619679    0.0190052   -0.0124614   -0.0624205    0.0182685 
  0.0147313    0.128475      0.0120961   -0.0959959    -0.201702     -0.0713739   -0.0923945    -0.121592   -0.13384       0.23645       0.193745   -0.159351    -0.0136799    0.0686561  -0.00668742   0.167033    -0.017931     0.145061    -0.0357708    0.106881    -0.105208   -0.0816867    3.48124e-5   0.0167711   -0.171656    -0.0427143 
  0.0685608    0.177047      0.0867431    0.00367323    0.0260452    -0.1103       0.008266     -0.117672   -0.0183085    -0.0622549    -0.0239972   0.085625    -0.121743     0.108453    0.0264782    0.13714      0.0604383   -0.0246902    0.0519934   -0.0598799    0.0223639   0.0501999    0.0949325    0.153953    -0.167024    -0.00615572
 -0.101867    -0.101111     -0.113115    -0.156819     -0.222295     -0.051266    -0.0205423     0.152438    0.0130509     0.000762096  -0.184322    0.0263053   -0.120982     0.0863224   0.0769593   -0.0748699   -0.117644     0.129084     0.0282414   -0.090949     0.0871418   0.146831     0.0308649   -0.146844    -0.131483    -0.10292   
  0.0370359    0.0701824    -0.07749      0.0573625    -0.0501225     0.0159527    0.0267051    -0.0939682   0.184822      0.0670256    -0.145713    0.00946236  -0.0242659    0.0394617  -0.0751812    0.0573333   -0.0561188   -0.0563022   -0.10491      0.152405     0.0335969  -0.0808617   -0.094007    -0.0694547   -0.0410246   -0.0258205 
 -0.0743988   -0.0722289     0.153407     0.112423     -0.00634629    0.101797     0.00540893    0.0486364  -0.0597653     0.0774826    -0.0349057  -0.0997659    0.0107626   -0.170403   -0.0130242    0.12601     -0.051971     0.0924455    0.0333341    0.0455845    0.186288   -0.0419471   -0.0925951   -0.159126    -0.00786122  -0.0490021 
  0.0989696    0.11287      -0.112154     0.130869     -0.0954268     0.0375427   -0.0607687     0.0499291   0.0901074     0.213125     -0.0598249  -0.0802226    0.0275908    0.13618     0.12343     -0.0835782    0.145573    -0.115871    -0.050892     0.133924    -0.221666    0.113954     0.0968348    0.0626862    0.176507     0.0417136 
  0.0758572    0.02261       0.197288    -0.0756855    -0.0614695    -0.168847     0.00784853   -0.0518486   0.0456462     0.159306     -0.0251858  -0.0546003   -0.0405481    0.0701977   0.0507143    0.0656184   -0.105465    -0.203813    -0.00473696  -0.0611883   -0.0536266   0.117617    -0.0241642    0.0360774   -0.0616447    0.0194991 
 -0.113595    -0.0595276     0.0110349    0.0228677     0.185095     -0.197935    -0.0739415     0.0519303   0.0108742     0.0755746    -0.145221   -0.179889     0.0112423    0.0495401  -0.0935739   -0.044046    -0.088439     0.226995    -0.17677      0.0377521    0.0345482  -0.00311921   0.2366      -0.0519548   -0.0296246    0.0343815 
 -0.0352485   -0.0473263    -0.0609514   -0.104234      0.0743084     0.0316647    0.088276      0.0217105   0.0309562    -0.074944     -0.0728935   0.14035      0.0148229    0.0495849   0.0740366    0.230085    -0.0379353    0.0525282   -0.0861849   -0.062565    -0.082848    0.0712527   -0.0197673   -0.121095     0.11965     -0.10984   
  0.28933     -0.250937      0.00545154   0.0043829    -0.0710277     0.00959664   0.103631      0.146709    0.104912     -0.0228319    -0.0589613   0.0812059    0.0216692   -0.0703857   0.0633164    0.0922283   -0.094823    -0.0114974    0.00304351   0.0678561   -0.069665   -0.00456655   0.0372316   -0.0792837    0.024754     0.00988126
 -0.0614947   -0.0566696     0.11162      0.177434      0.0475904     0.0753319    0.0698006    -0.0985224  -0.0337568     0.00171798    0.055909    0.0408134   -0.0424978   -0.0468779  -0.0265999   -0.081867     0.0706702    0.0377537    0.270425    -0.0761842   -0.0807692   0.0136441    0.0512798    0.0467237    0.048987    -0.110731  
  0.0201055    0.00975052    0.0936967   -0.00783021   -0.0782002     0.19305     -0.0999092    -0.12884    -0.154832     -0.0632114    -0.0348129   0.122392    -0.0392553   -0.113473    0.0200728    0.00702702   0.0167557   -0.114449     0.135408     0.0647682    0.171016    0.132563    -0.341033    -0.0636872    0.0314816    0.0428893 
 -0.0276214    0.0263792    -0.0131313   -0.0815172    -0.0652571    -0.0861161   -0.0406923    -0.120552   -0.15206      -0.105689     -0.0442806   0.0183325   -0.0468669    0.039362   -0.0308961    0.210428     0.00934822  -0.150608    -0.0118015   -0.165448    -0.0855692  -0.00449001   0.0174358   -0.0234014    0.0298109    0.199322  
  0.0610803    0.00739373   -0.0542897    0.021797     -0.0560265    -0.0871817   -0.0950151    -0.146738    0.000709928   0.0629422    -0.0639002  -0.0278993   -0.0841117    0.155523    0.0914617    0.094322    -0.116319     0.104454    -0.0428311    0.0358031    0.1081      0.141332    -0.14323     -0.128118    -0.040017    -0.0674739 
  0.00199085  -0.00715465    0.244027    -0.159589      0.0620595     0.00627318  -0.0365008     0.107541   -0.0788274     0.0163635    -0.14162     0.066195     0.0371881   -0.0698208   0.0152575   -0.0572195    0.0482791    0.00596113   0.100611     0.0570564   -0.051776    0.0592839   -0.0411351    0.15862     -0.23646      0.00566294kind diag, method split
0: avll = -1.4122086034283703
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.412279
INFO: iteration 2, average log likelihood -1.412211
INFO: iteration 3, average log likelihood -1.411603
INFO: iteration 4, average log likelihood -1.404424
INFO: iteration 5, average log likelihood -1.384821
INFO: iteration 6, average log likelihood -1.376684
INFO: iteration 7, average log likelihood -1.375179
INFO: iteration 8, average log likelihood -1.374370
INFO: iteration 9, average log likelihood -1.373833
INFO: iteration 10, average log likelihood -1.373591
INFO: iteration 11, average log likelihood -1.373478
INFO: iteration 12, average log likelihood -1.373411
INFO: iteration 13, average log likelihood -1.373365
INFO: iteration 14, average log likelihood -1.373331
INFO: iteration 15, average log likelihood -1.373306
INFO: iteration 16, average log likelihood -1.373286
INFO: iteration 17, average log likelihood -1.373270
INFO: iteration 18, average log likelihood -1.373256
INFO: iteration 19, average log likelihood -1.373245
INFO: iteration 20, average log likelihood -1.373236
INFO: iteration 21, average log likelihood -1.373228
INFO: iteration 22, average log likelihood -1.373221
INFO: iteration 23, average log likelihood -1.373216
INFO: iteration 24, average log likelihood -1.373213
INFO: iteration 25, average log likelihood -1.373210
INFO: iteration 26, average log likelihood -1.373208
INFO: iteration 27, average log likelihood -1.373206
INFO: iteration 28, average log likelihood -1.373205
INFO: iteration 29, average log likelihood -1.373204
INFO: iteration 30, average log likelihood -1.373204
INFO: iteration 31, average log likelihood -1.373203
INFO: iteration 32, average log likelihood -1.373203
INFO: iteration 33, average log likelihood -1.373203
INFO: iteration 34, average log likelihood -1.373203
INFO: iteration 35, average log likelihood -1.373203
INFO: iteration 36, average log likelihood -1.373203
INFO: iteration 37, average log likelihood -1.373203
INFO: iteration 38, average log likelihood -1.373202
INFO: iteration 39, average log likelihood -1.373202
INFO: iteration 40, average log likelihood -1.373202
INFO: iteration 41, average log likelihood -1.373202
INFO: iteration 42, average log likelihood -1.373202
INFO: iteration 43, average log likelihood -1.373202
INFO: iteration 44, average log likelihood -1.373202
INFO: iteration 45, average log likelihood -1.373202
INFO: iteration 46, average log likelihood -1.373202
INFO: iteration 47, average log likelihood -1.373202
INFO: iteration 48, average log likelihood -1.373202
INFO: iteration 49, average log likelihood -1.373202
INFO: iteration 50, average log likelihood -1.373202
INFO: EM with 100000 data points 50 iterations avll -1.373202
952.4 data points per parameter
1: avll = [-1.41228,-1.41221,-1.4116,-1.40442,-1.38482,-1.37668,-1.37518,-1.37437,-1.37383,-1.37359,-1.37348,-1.37341,-1.37336,-1.37333,-1.37331,-1.37329,-1.37327,-1.37326,-1.37325,-1.37324,-1.37323,-1.37322,-1.37322,-1.37321,-1.37321,-1.37321,-1.37321,-1.37321,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.373311
INFO: iteration 2, average log likelihood -1.373205
INFO: iteration 3, average log likelihood -1.372730
INFO: iteration 4, average log likelihood -1.368352
INFO: iteration 5, average log likelihood -1.355782
INFO: iteration 6, average log likelihood -1.346960
INFO: iteration 7, average log likelihood -1.343678
INFO: iteration 8, average log likelihood -1.341800
INFO: iteration 9, average log likelihood -1.340377
INFO: iteration 10, average log likelihood -1.339190
INFO: iteration 11, average log likelihood -1.338436
INFO: iteration 12, average log likelihood -1.337946
INFO: iteration 13, average log likelihood -1.337449
INFO: iteration 14, average log likelihood -1.336894
INFO: iteration 15, average log likelihood -1.336325
INFO: iteration 16, average log likelihood -1.335791
INFO: iteration 17, average log likelihood -1.335322
INFO: iteration 18, average log likelihood -1.334929
INFO: iteration 19, average log likelihood -1.334593
INFO: iteration 20, average log likelihood -1.334289
INFO: iteration 21, average log likelihood -1.333990
INFO: iteration 22, average log likelihood -1.333661
INFO: iteration 23, average log likelihood -1.333301
INFO: iteration 24, average log likelihood -1.332930
INFO: iteration 25, average log likelihood -1.332573
INFO: iteration 26, average log likelihood -1.332260
INFO: iteration 27, average log likelihood -1.331980
INFO: iteration 28, average log likelihood -1.331719
INFO: iteration 29, average log likelihood -1.331467
INFO: iteration 30, average log likelihood -1.331213
INFO: iteration 31, average log likelihood -1.330949
INFO: iteration 32, average log likelihood -1.330681
INFO: iteration 33, average log likelihood -1.330411
INFO: iteration 34, average log likelihood -1.330129
INFO: iteration 35, average log likelihood -1.329846
INFO: iteration 36, average log likelihood -1.329594
INFO: iteration 37, average log likelihood -1.329377
INFO: iteration 38, average log likelihood -1.329182
INFO: iteration 39, average log likelihood -1.328975
INFO: iteration 40, average log likelihood -1.328722
INFO: iteration 41, average log likelihood -1.328384
INFO: iteration 42, average log likelihood -1.327956
INFO: iteration 43, average log likelihood -1.327585
INFO: iteration 44, average log likelihood -1.327359
INFO: iteration 45, average log likelihood -1.327227
INFO: iteration 46, average log likelihood -1.327144
INFO: iteration 47, average log likelihood -1.327086
INFO: iteration 48, average log likelihood -1.327044
INFO: iteration 49, average log likelihood -1.327013
INFO: iteration 50, average log likelihood -1.326990
INFO: EM with 100000 data points 50 iterations avll -1.326990
473.9 data points per parameter
2: avll = [-1.37331,-1.37321,-1.37273,-1.36835,-1.35578,-1.34696,-1.34368,-1.3418,-1.34038,-1.33919,-1.33844,-1.33795,-1.33745,-1.33689,-1.33632,-1.33579,-1.33532,-1.33493,-1.33459,-1.33429,-1.33399,-1.33366,-1.3333,-1.33293,-1.33257,-1.33226,-1.33198,-1.33172,-1.33147,-1.33121,-1.33095,-1.33068,-1.33041,-1.33013,-1.32985,-1.32959,-1.32938,-1.32918,-1.32897,-1.32872,-1.32838,-1.32796,-1.32759,-1.32736,-1.32723,-1.32714,-1.32709,-1.32704,-1.32701,-1.32699]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.327119
INFO: iteration 2, average log likelihood -1.326954
INFO: iteration 3, average log likelihood -1.326359
INFO: iteration 4, average log likelihood -1.320791
INFO: iteration 5, average log likelihood -1.302988
INFO: iteration 6, average log likelihood -1.287227
INFO: iteration 7, average log likelihood -1.278473
INFO: iteration 8, average log likelihood -1.273588
INFO: iteration 9, average log likelihood -1.271865
INFO: iteration 10, average log likelihood -1.271377
INFO: iteration 11, average log likelihood -1.271189
INFO: iteration 12, average log likelihood -1.271096
INFO: iteration 13, average log likelihood -1.271044
INFO: iteration 14, average log likelihood -1.271011
INFO: iteration 15, average log likelihood -1.270987
INFO: iteration 16, average log likelihood -1.270968
INFO: iteration 17, average log likelihood -1.270953
INFO: iteration 18, average log likelihood -1.270939
INFO: iteration 19, average log likelihood -1.270927
INFO: iteration 20, average log likelihood -1.270916
INFO: iteration 21, average log likelihood -1.270906
INFO: iteration 22, average log likelihood -1.270897
INFO: iteration 23, average log likelihood -1.270886
INFO: iteration 24, average log likelihood -1.270874
INFO: iteration 25, average log likelihood -1.270857
INFO: iteration 26, average log likelihood -1.270829
INFO: iteration 27, average log likelihood -1.270776
INFO: iteration 28, average log likelihood -1.270677
INFO: iteration 29, average log likelihood -1.270506
INFO: iteration 30, average log likelihood -1.270273
INFO: iteration 31, average log likelihood -1.270016
INFO: iteration 32, average log likelihood -1.269776
INFO: iteration 33, average log likelihood -1.269563
INFO: iteration 34, average log likelihood -1.269366
INFO: iteration 35, average log likelihood -1.269185
INFO: iteration 36, average log likelihood -1.269020
INFO: iteration 37, average log likelihood -1.268862
INFO: iteration 38, average log likelihood -1.268712
INFO: iteration 39, average log likelihood -1.268567
INFO: iteration 40, average log likelihood -1.268418
INFO: iteration 41, average log likelihood -1.268256
INFO: iteration 42, average log likelihood -1.268077
INFO: iteration 43, average log likelihood -1.267887
INFO: iteration 44, average log likelihood -1.267701
INFO: iteration 45, average log likelihood -1.267527
INFO: iteration 46, average log likelihood -1.267370
INFO: iteration 47, average log likelihood -1.267229
INFO: iteration 48, average log likelihood -1.267114
INFO: iteration 49, average log likelihood -1.267026
INFO: iteration 50, average log likelihood -1.266959
INFO: EM with 100000 data points 50 iterations avll -1.266959
236.4 data points per parameter
3: avll = [-1.32712,-1.32695,-1.32636,-1.32079,-1.30299,-1.28723,-1.27847,-1.27359,-1.27186,-1.27138,-1.27119,-1.2711,-1.27104,-1.27101,-1.27099,-1.27097,-1.27095,-1.27094,-1.27093,-1.27092,-1.27091,-1.2709,-1.27089,-1.27087,-1.27086,-1.27083,-1.27078,-1.27068,-1.27051,-1.27027,-1.27002,-1.26978,-1.26956,-1.26937,-1.26919,-1.26902,-1.26886,-1.26871,-1.26857,-1.26842,-1.26826,-1.26808,-1.26789,-1.2677,-1.26753,-1.26737,-1.26723,-1.26711,-1.26703,-1.26696]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.267145
INFO: iteration 2, average log likelihood -1.266848
INFO: iteration 3, average log likelihood -1.265542
INFO: iteration 4, average log likelihood -1.251319
WARNING: Variances had to be floored 12
INFO: iteration 5, average log likelihood -1.218510
INFO: iteration 6, average log likelihood -1.201595
WARNING: Variances had to be floored 9 12
INFO: iteration 7, average log likelihood -1.182931
WARNING: Variances had to be floored 11 12
INFO: iteration 8, average log likelihood -1.191562
INFO: iteration 9, average log likelihood -1.192286
WARNING: Variances had to be floored 12
INFO: iteration 10, average log likelihood -1.179320
WARNING: Variances had to be floored 9 16
INFO: iteration 11, average log likelihood -1.173154
WARNING: Variances had to be floored 12
INFO: iteration 12, average log likelihood -1.179467
WARNING: Variances had to be floored 11
INFO: iteration 13, average log likelihood -1.173133
WARNING: Variances had to be floored 12
INFO: iteration 14, average log likelihood -1.171894
WARNING: Variances had to be floored 9
INFO: iteration 15, average log likelihood -1.168385
WARNING: Variances had to be floored 12
INFO: iteration 16, average log likelihood -1.165661
WARNING: Variances had to be floored 11
INFO: iteration 17, average log likelihood -1.165955
WARNING: Variances had to be floored 12 16
INFO: iteration 18, average log likelihood -1.166616
WARNING: Variances had to be floored 9
INFO: iteration 19, average log likelihood -1.173989
WARNING: Variances had to be floored 12
INFO: iteration 20, average log likelihood -1.169356
WARNING: Variances had to be floored 11
INFO: iteration 21, average log likelihood -1.169428
WARNING: Variances had to be floored 12
INFO: iteration 22, average log likelihood -1.169148
WARNING: Variances had to be floored 9
INFO: iteration 23, average log likelihood -1.165626
WARNING: Variances had to be floored 12
INFO: iteration 24, average log likelihood -1.163573
WARNING: Variances had to be floored 11 16
INFO: iteration 25, average log likelihood -1.165108
WARNING: Variances had to be floored 12
INFO: iteration 26, average log likelihood -1.176166
WARNING: Variances had to be floored 9
INFO: iteration 27, average log likelihood -1.170386
WARNING: Variances had to be floored 12
INFO: iteration 28, average log likelihood -1.167552
WARNING: Variances had to be floored 11
INFO: iteration 29, average log likelihood -1.167848
WARNING: Variances had to be floored 12
INFO: iteration 30, average log likelihood -1.168155
WARNING: Variances had to be floored 9
INFO: iteration 31, average log likelihood -1.164984
WARNING: Variances had to be floored 12 16
INFO: iteration 32, average log likelihood -1.163317
WARNING: Variances had to be floored 11
INFO: iteration 33, average log likelihood -1.174648
WARNING: Variances had to be floored 12
INFO: iteration 34, average log likelihood -1.172430
WARNING: Variances had to be floored 9
INFO: iteration 35, average log likelihood -1.168647
WARNING: Variances had to be floored 12
INFO: iteration 36, average log likelihood -1.166057
WARNING: Variances had to be floored 11
INFO: iteration 37, average log likelihood -1.166769
WARNING: Variances had to be floored 12
INFO: iteration 38, average log likelihood -1.167284
WARNING: Variances had to be floored 9 16
INFO: iteration 39, average log likelihood -1.164498
WARNING: Variances had to be floored 12
INFO: iteration 40, average log likelihood -1.172727
WARNING: Variances had to be floored 11
INFO: iteration 41, average log likelihood -1.170656
WARNING: Variances had to be floored 12
INFO: iteration 42, average log likelihood -1.170111
WARNING: Variances had to be floored 9
INFO: iteration 43, average log likelihood -1.166053
WARNING: Variances had to be floored 12
INFO: iteration 44, average log likelihood -1.163436
WARNING: Variances had to be floored 11 16
INFO: iteration 45, average log likelihood -1.164469
WARNING: Variances had to be floored 12
INFO: iteration 46, average log likelihood -1.175671
WARNING: Variances had to be floored 9
INFO: iteration 47, average log likelihood -1.169745
WARNING: Variances had to be floored 12
INFO: iteration 48, average log likelihood -1.166474
WARNING: Variances had to be floored 11
INFO: iteration 49, average log likelihood -1.165943
WARNING: Variances had to be floored 12
INFO: iteration 50, average log likelihood -1.165994
INFO: EM with 100000 data points 50 iterations avll -1.165994
118.1 data points per parameter
4: avll = [-1.26715,-1.26685,-1.26554,-1.25132,-1.21851,-1.20159,-1.18293,-1.19156,-1.19229,-1.17932,-1.17315,-1.17947,-1.17313,-1.17189,-1.16839,-1.16566,-1.16595,-1.16662,-1.17399,-1.16936,-1.16943,-1.16915,-1.16563,-1.16357,-1.16511,-1.17617,-1.17039,-1.16755,-1.16785,-1.16815,-1.16498,-1.16332,-1.17465,-1.17243,-1.16865,-1.16606,-1.16677,-1.16728,-1.1645,-1.17273,-1.17066,-1.17011,-1.16605,-1.16344,-1.16447,-1.17567,-1.16974,-1.16647,-1.16594,-1.16599]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 17 18 31 32
INFO: iteration 1, average log likelihood -1.163697
WARNING: Variances had to be floored 17 18 21 23 24 31 32
INFO: iteration 2, average log likelihood -1.154357
WARNING: Variances had to be floored 17 18 22 23 24 31 32
INFO: iteration 3, average log likelihood -1.159382
WARNING: Variances had to be floored 17 18 21 23 24 31 32
INFO: iteration 4, average log likelihood -1.144922
WARNING: Variances had to be floored 17 18 22 23 24 31 32
INFO: iteration 5, average log likelihood -1.108660
WARNING: Variances had to be floored 11 17 18 19 20 21 23 24 31 32
INFO: iteration 6, average log likelihood -1.086319
WARNING: Variances had to be floored 17 18 22 23 24 31 32
INFO: iteration 7, average log likelihood -1.101110
WARNING: Variances had to be floored 17 18 23 24 31 32
INFO: iteration 8, average log likelihood -1.090490
WARNING: Variances had to be floored 17 18 20 21 23 24 31 32
INFO: iteration 9, average log likelihood -1.072788
WARNING: Variances had to be floored 11 17 18 19 22 23 24 31 32
INFO: iteration 10, average log likelihood -1.073269
WARNING: Variances had to be floored 17 18 23 24 31 32
INFO: iteration 11, average log likelihood -1.088812
WARNING: Variances had to be floored 17 18 20 21 22 23 24 31 32
INFO: iteration 12, average log likelihood -1.072500
WARNING: Variances had to be floored 17 18 19 23 24 31 32
INFO: iteration 13, average log likelihood -1.078488
WARNING: Variances had to be floored 11 17 18 20 21 22 23 24 31 32
INFO: iteration 14, average log likelihood -1.073522
WARNING: Variances had to be floored 17 18 23 24 31 32
INFO: iteration 15, average log likelihood -1.088338
WARNING: Variances had to be floored 17 18 19 20 21 22 23 24 31 32
INFO: iteration 16, average log likelihood -1.072536
WARNING: Variances had to be floored 17 18 23 24 29 31 32
INFO: iteration 17, average log likelihood -1.085818
WARNING: Variances had to be floored 11 17 18 21 22 23 24 31 32
INFO: iteration 18, average log likelihood -1.075216
WARNING: Variances had to be floored 17 18 20 23 24 31 32
INFO: iteration 19, average log likelihood -1.081053
WARNING: Variances had to be floored 17 18 19 21 22 23 24 31 32
INFO: iteration 20, average log likelihood -1.078845
WARNING: Variances had to be floored 17 18 20 23 24 31 32
INFO: iteration 21, average log likelihood -1.080120
WARNING: Variances had to be floored 11 17 18 21 22 23 24 31 32
INFO: iteration 22, average log likelihood -1.073638
WARNING: Variances had to be floored 17 18 19 23 24 31 32
INFO: iteration 23, average log likelihood -1.079961
WARNING: Variances had to be floored 17 18 20 21 23 24 29 31 32
INFO: iteration 24, average log likelihood -1.078558
WARNING: Variances had to be floored 17 18 22 23 24 31 32
INFO: iteration 25, average log likelihood -1.077512
WARNING: Variances had to be floored 11 17 18 19 20 21 23 24 31 32
INFO: iteration 26, average log likelihood -1.068032
WARNING: Variances had to be floored 17 18 22 23 24 31 32
INFO: iteration 27, average log likelihood -1.091014
WARNING: Variances had to be floored 17 18 23 24 31 32
INFO: iteration 28, average log likelihood -1.085119
WARNING: Variances had to be floored 17 18 20 21 22 23 24 31 32
INFO: iteration 29, average log likelihood -1.070157
WARNING: Variances had to be floored 11 17 18 19 23 24 29 31 32
INFO: iteration 30, average log likelihood -1.074474
WARNING: Variances had to be floored 17 18 20 21 22 23 24 31 32
INFO: iteration 31, average log likelihood -1.085249
WARNING: Variances had to be floored 17 18 23 24 31 32
INFO: iteration 32, average log likelihood -1.083618
WARNING: Variances had to be floored 17 18 19 20 21 22 23 24 31 32
INFO: iteration 33, average log likelihood -1.069041
WARNING: Variances had to be floored 11 17 18 21 23 24 31 32
INFO: iteration 34, average log likelihood -1.081705
WARNING: Variances had to be floored 17 18 22 23 24 29 31 32
INFO: iteration 35, average log likelihood -1.086025
WARNING: Variances had to be floored 5 17 18 20 21 23 24 31 32
INFO: iteration 36, average log likelihood -1.069341
WARNING: Variances had to be floored 17 18 19 22 23 24 31 32
INFO: iteration 37, average log likelihood -1.073657
WARNING: Variances had to be floored 1 5 11 17 18 20 21 23 24 31 32
INFO: iteration 38, average log likelihood -1.058351
WARNING: Variances had to be floored 17 18 22 23 24 31 32
INFO: iteration 39, average log likelihood -1.092708
WARNING: Variances had to be floored 5 17 18 19 21 23 24 29 31 32
INFO: iteration 40, average log likelihood -1.072379
WARNING: Variances had to be floored 17 18 20 22 23 24 31 32
INFO: iteration 41, average log likelihood -1.076232
WARNING: Variances had to be floored 5 11 17 18 21 23 24 31 32
INFO: iteration 42, average log likelihood -1.064262
WARNING: Variances had to be floored 1 17 18 19 20 22 23 24 31 32
INFO: iteration 43, average log likelihood -1.073440
WARNING: Variances had to be floored 5 17 18 21 23 24 29 31 32
INFO: iteration 44, average log likelihood -1.086563
WARNING: Variances had to be floored 17 18 22 23 24 31 32
INFO: iteration 45, average log likelihood -1.079466
WARNING: Variances had to be floored 1 5 11 17 18 20 21 23 24 31 32
INFO: iteration 46, average log likelihood -1.051697
WARNING: Variances had to be floored 17 18 19 22 23 24 31 32
INFO: iteration 47, average log likelihood -1.090758
WARNING: Variances had to be floored 5 17 18 21 23 24 29 31 32
INFO: iteration 48, average log likelihood -1.080505
WARNING: Variances had to be floored 17 18 20 21 22 23 24 31 32
INFO: iteration 49, average log likelihood -1.069905
WARNING: Variances had to be floored 5 11 17 18 19 21 23 24 31 32
INFO: iteration 50, average log likelihood -1.062390
INFO: EM with 100000 data points 50 iterations avll -1.062390
59.0 data points per parameter
5: avll = [-1.1637,-1.15436,-1.15938,-1.14492,-1.10866,-1.08632,-1.10111,-1.09049,-1.07279,-1.07327,-1.08881,-1.0725,-1.07849,-1.07352,-1.08834,-1.07254,-1.08582,-1.07522,-1.08105,-1.07884,-1.08012,-1.07364,-1.07996,-1.07856,-1.07751,-1.06803,-1.09101,-1.08512,-1.07016,-1.07447,-1.08525,-1.08362,-1.06904,-1.0817,-1.08603,-1.06934,-1.07366,-1.05835,-1.09271,-1.07238,-1.07623,-1.06426,-1.07344,-1.08656,-1.07947,-1.0517,-1.09076,-1.0805,-1.0699,-1.06239]
[-1.41221,-1.41228,-1.41221,-1.4116,-1.40442,-1.38482,-1.37668,-1.37518,-1.37437,-1.37383,-1.37359,-1.37348,-1.37341,-1.37336,-1.37333,-1.37331,-1.37329,-1.37327,-1.37326,-1.37325,-1.37324,-1.37323,-1.37322,-1.37322,-1.37321,-1.37321,-1.37321,-1.37321,-1.37321,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.3732,-1.37331,-1.37321,-1.37273,-1.36835,-1.35578,-1.34696,-1.34368,-1.3418,-1.34038,-1.33919,-1.33844,-1.33795,-1.33745,-1.33689,-1.33632,-1.33579,-1.33532,-1.33493,-1.33459,-1.33429,-1.33399,-1.33366,-1.3333,-1.33293,-1.33257,-1.33226,-1.33198,-1.33172,-1.33147,-1.33121,-1.33095,-1.33068,-1.33041,-1.33013,-1.32985,-1.32959,-1.32938,-1.32918,-1.32897,-1.32872,-1.32838,-1.32796,-1.32759,-1.32736,-1.32723,-1.32714,-1.32709,-1.32704,-1.32701,-1.32699,-1.32712,-1.32695,-1.32636,-1.32079,-1.30299,-1.28723,-1.27847,-1.27359,-1.27186,-1.27138,-1.27119,-1.2711,-1.27104,-1.27101,-1.27099,-1.27097,-1.27095,-1.27094,-1.27093,-1.27092,-1.27091,-1.2709,-1.27089,-1.27087,-1.27086,-1.27083,-1.27078,-1.27068,-1.27051,-1.27027,-1.27002,-1.26978,-1.26956,-1.26937,-1.26919,-1.26902,-1.26886,-1.26871,-1.26857,-1.26842,-1.26826,-1.26808,-1.26789,-1.2677,-1.26753,-1.26737,-1.26723,-1.26711,-1.26703,-1.26696,-1.26715,-1.26685,-1.26554,-1.25132,-1.21851,-1.20159,-1.18293,-1.19156,-1.19229,-1.17932,-1.17315,-1.17947,-1.17313,-1.17189,-1.16839,-1.16566,-1.16595,-1.16662,-1.17399,-1.16936,-1.16943,-1.16915,-1.16563,-1.16357,-1.16511,-1.17617,-1.17039,-1.16755,-1.16785,-1.16815,-1.16498,-1.16332,-1.17465,-1.17243,-1.16865,-1.16606,-1.16677,-1.16728,-1.1645,-1.17273,-1.17066,-1.17011,-1.16605,-1.16344,-1.16447,-1.17567,-1.16974,-1.16647,-1.16594,-1.16599,-1.1637,-1.15436,-1.15938,-1.14492,-1.10866,-1.08632,-1.10111,-1.09049,-1.07279,-1.07327,-1.08881,-1.0725,-1.07849,-1.07352,-1.08834,-1.07254,-1.08582,-1.07522,-1.08105,-1.07884,-1.08012,-1.07364,-1.07996,-1.07856,-1.07751,-1.06803,-1.09101,-1.08512,-1.07016,-1.07447,-1.08525,-1.08362,-1.06904,-1.0817,-1.08603,-1.06934,-1.07366,-1.05835,-1.09271,-1.07238,-1.07623,-1.06426,-1.07344,-1.08656,-1.07947,-1.0517,-1.09076,-1.0805,-1.0699,-1.06239]
32×26 Array{Float64,2}:
 -0.00952347   0.00571272   0.0707311   -0.0104456    -0.0336021   0.0193234   0.03172     -0.0975476    0.0223406    -0.165811    -0.0621805     0.205404     0.0594363    0.112357    -0.0291618    0.0405881     0.128167      0.0266631   0.190141    -0.0711105  -0.0694232    0.06256      -0.10036     -0.118577   -0.10111      0.0296877 
 -0.115973    -0.0563265    0.0118418    0.0496069     0.140028   -0.194563   -0.0743468    0.0496014    0.00839143    0.0696392   -0.15467      -0.181639     0.00616205   0.0606375   -0.0930779   -0.0460855    -0.0876639     0.231615   -0.13761      0.0398073   0.0680744    0.00508212    0.242825    -0.0483437  -0.0563843    0.0537527 
  0.229701     0.0170713    0.00263232   0.0153366    -0.0871397   0.0468512   0.0121999   -0.165748    -0.123811     -0.0677573   -0.149322     -0.064941     0.00320535   0.120049     0.273819    -0.0439836    -0.115499      0.112145   -0.0955544   -0.150277    0.0904156    0.209301      0.0359756    0.0470938  -0.594346     0.0751078 
  0.185099     0.0722849    0.165008    -0.010748     -0.0852887   0.118613   -0.00426792  -0.130049    -0.142075      0.00860619  -0.110245     -0.0409985    0.00844209   0.0723       0.20277      0.0272196     0.00976557    0.105404   -0.202842    -0.147971    0.0475622    0.265224      0.031457     0.0503215   0.392287     0.115966  
  0.104169     0.00184336  -0.0542343    0.0165352    -0.0616202  -0.0981276  -0.0963032   -0.193731     0.000246082   0.0607733   -0.0628593    -0.0284003   -0.0902791    0.153512     0.0892612    0.0908908    -0.0874332     0.101721    0.0306164    0.0298165   0.100257     0.137606     -0.127297    -0.12484    -0.0342953   -0.0601168 
 -0.0597826    0.00150906   0.110936    -0.0199105    -0.0855154   0.193961   -0.0829245   -0.145062    -0.144289     -0.0672012   -0.035395      0.121671    -0.0266343   -0.116175     0.00864974   0.000678432   0.000348144  -0.165395    0.144356     0.0849298   0.16545      0.133264     -0.327661    -0.0500376   0.0383573    0.0284121 
  0.0578575    0.0224314   -0.0179379   -0.0242025    -0.053574   -0.0306067   0.0177236   -0.0822013   -0.205655     -0.039594    -0.00452968    0.0478539   -0.0316631   -0.037719     0.0504108    0.0781987     0.0411472    -0.0166901  -0.0113344    0.0572236  -0.0955716    0.0128037    -0.0890697    0.086094   -0.0216883    0.0749395 
  0.0955905    0.00675711   0.0328208   -0.0529132    -0.0221838  -0.153021    0.0330403    0.0165184   -0.0734801     0.202551    -0.0879059    -0.0216165   -0.0327585    0.0700672    0.0613009    0.00298231   -0.0295909    -0.226384   -0.0162637   -0.083137    0.0107273    0.0851498    -0.0186387    0.014067   -0.0625393    0.0244326 
 -0.0508715   -0.059891     0.0524814    0.15753      -0.184311    0.0504261   0.0751531   -0.0859997   -0.106037      0.00200125   0.0738223     0.0353945   -0.056232    -0.0206716   -0.0296099   -0.0627285     0.0614855     0.0455491   0.275201    -0.0731905  -0.0255566   -0.0118378    -1.00556      0.0473742  -0.00801328  -0.111814  
 -0.0649622   -0.0632254    0.153288     0.181604      0.173144    0.0647433   0.0608906   -0.118911    -0.00190983    0.00228711   0.0620842     0.0445369   -0.0489458   -0.0734556   -0.0353322   -0.0310744     0.110605      0.0205681   0.243148    -0.0800061  -0.104977     0.036288      1.07771      0.104462    0.122787    -0.106587  
  0.138569     0.0486068    0.0993032   -0.115172     -0.12257     0.0654173  -0.0425467   -0.21307     -0.0644164     0.0305396   -0.00173667   -0.0175053   -0.126766     0.114822     0.0244323   -0.013416      0.0697278     0.0634236   0.094956     0.136896   -0.0736758   -0.056093      0.01569     -0.0500396  -0.133482     0.104147  
 -0.0825737    0.0399031    0.121649     0.0436894     0.0792889   0.0879408  -0.0771271    0.106685     0.096112      0.103278     0.0196556    -0.0657758    0.0341821    0.0555546   -0.177357    -0.00332533   -0.0738387    -0.0145952   0.0460898    0.177483    0.0176965   -0.0276897    -0.00413608  -0.0571985   0.040597    -0.0530325 
  0.0306498   -0.00263117   0.0242734    0.056592     -0.0673256  -0.0062709   0.0776278    0.031124     0.0701625     0.0926781    0.0810334    -0.0294682    0.0801054   -0.114343     0.129493    -0.0675208    -0.0107514     0.0177649  -0.0366397    0.0380861   0.130744     0.133864     -0.00948034   0.0182823   0.0389332    0.0444826 
  0.285285    -0.246263    -0.0144666   -0.00580616   -0.0457749   0.0355636   0.102195     0.246934     0.0880472    -0.0191045   -0.0524986     0.0504365    0.0270814    0.0206926    0.0629865    0.0939062    -0.0923917    -0.0459803  -0.0300668    0.0577435  -0.0605327    0.000673664   0.0362718   -0.0831992   0.0209583    0.00384349
 -0.0265145    0.0140282   -0.0139125   -0.000854347  -0.0417527   0.0205372  -0.00542109   0.00648887   0.0860312     0.117728    -0.0798612    -0.00474766  -0.0464882    0.00699799  -0.0520398    0.0851332     0.0329197     0.0365035   0.0258392    0.0685887   0.04714     -0.0703819    -0.0393589   -0.0746154  -0.0291372   -0.00659672
  0.068154     0.173891     0.0909706   -0.00116114    0.0304824  -0.125014   -0.0185554   -0.130556    -0.00920765   -0.0654883   -0.0361194     0.092149    -0.134428     0.100507     0.0209414    0.149805      0.079494     -0.0163247   0.0662927   -0.0779629  -0.00840946   0.0516138     0.116173     0.153967   -0.164718    -0.0149146 
  0.227005     0.043476    -0.0580352   -0.0931157     0.0807599   0.108081   -0.010033    -0.05559      0.00398722    0.00156641  -0.00342313   -0.0440193   -0.0813496   -0.0856284   -0.0454695    0.124015      0.0312854    -0.455352   -0.0163279    0.0727695  -0.104069     0.0757011     0.281632     0.0992481   0.0891093    0.357718  
  0.0641881    0.0511389   -0.121905    -0.0816506     0.0497555   0.0466295   0.0393525    0.0555571    0.286179     -0.00698243   0.178446     -0.0385571   -0.0894319   -0.0162266   -0.0790232   -0.0734673     0.0359698     0.52385     0.029654    -0.125602   -0.104006     0.0669676     0.281422     0.101623   -0.297103    -0.454047  
  0.00357762  -0.142805    -0.135101     0.047918      0.219118   -0.029114   -0.141746     0.0216519   -0.0654257     0.153785    -0.20577       0.023753    -0.10438      0.177225    -0.0178221   -0.0480599    -0.0765427     0.201983    0.0433006   -0.197217    0.0297491   -0.0976971     0.133535     0.19894    -0.240992     0.0200465 
  0.0228335    0.133057    -0.00898175  -0.0938398    -0.147986   -0.0684983  -0.0747549   -0.120371    -0.132055      0.233423     0.191493     -0.146097    -0.0120771    0.0647478   -0.0256822    0.176566     -0.0417455     0.187521   -0.0366608    0.15276    -0.0945691   -0.0807351    -0.0101535    0.023383   -0.175948    -0.0663734 
 -0.0313776   -1.18743      0.101679    -0.0523444     1.09934    -0.606896   -0.845501    -0.552128     0.0113147     0.522556     0.000957205   1.52248      0.251656     0.831442     0.900443     0.134457     -0.0699976    -1.14324    -0.135957    -3.91221     0.564128    -0.169883     -0.49681     -0.602205   -0.0067705   -1.62781   
  0.128949    -0.133967    -0.0156066   -0.0567777     0.103436   -0.177992   -0.101458    -0.105687    -0.00546796    0.174194    -0.00480749   -0.0151014    0.0564064    0.0700923   -0.142251    -0.120157     -0.145198     -0.159716   -0.124356    -0.137958    0.0468081   -0.244097      0.0371865   -0.103409   -0.0143929    0.0920675 
 -0.13252     -0.095443    -0.0741284   -0.157273     -0.2194     -0.0203267  -0.00937      0.136611     0.0383822     0.0170363   -0.650148      0.0381174   -0.0990104    0.042659     0.0690615   -0.0752535    -0.120944      0.0846408  -0.129678    -0.0972833  -0.770313    -0.500909      0.0349716   -0.146808   -0.677461    -0.100793  
 -0.0998269   -0.116269    -0.0954039   -0.156663     -0.220362   -0.0901734  -0.0190719    0.127426    -0.0577727     0.00553314   0.169113      0.0644702   -0.149928     0.0722       0.0829348   -0.0752671    -0.121561      0.216909    0.211927    -0.0923079   1.07239      0.766708      0.0314872   -0.141542    0.363812    -0.0958875 
  0.0179695   -0.105992     0.226326    -0.161543      0.0647559  -0.0536259  -0.0359224    0.120664    -0.0730785     0.015       -0.0820133     0.0264864   -0.224059    -0.10113     -0.0145937   -0.0903085     0.16733      -0.0694559   0.0338101    0.0648914  -0.0945636    0.153631     -0.176552     0.106356   -0.242538     0.0251129 
 -0.0290975    0.138748     0.254453    -0.150759      0.0585703   0.0295709  -0.0427007    0.0908952   -0.0884357     0.0184339   -0.335542      0.129778     0.377334    -0.0437964    0.0353513   -0.0297326    -0.0377067     0.0624447   0.165597    -0.100561   -0.00625142  -0.0362249     0.0987442    0.210893   -0.21906     -0.0168557 
 -0.06718      0.0559626   -0.0167961    0.108694     -0.0747309  -0.0120711   0.0116282   -0.126739     0.100527      0.0181763    0.0112745     0.0356768   -0.0747749    0.0672384   -0.0631459   -0.0929127    -0.0555141     0.0787786   0.130341    -0.0172264  -0.126588    -0.0889384    -0.0288452    0.15042    -0.0612936    0.0614988 
  0.0988522    0.142601    -0.113068     0.103548     -0.0704218   0.053518   -0.0507689    0.0507394    0.0912857     0.218383    -0.0480575    -0.0701531    0.0228199    0.137105     0.107127    -0.0821654     0.190713     -0.107769   -0.0794428    0.153958   -0.21061      0.121503      0.0806208    0.0632756   0.155871     0.0391492 
  0.0164462    0.0091739   -0.0236302    0.0436919     0.0347135  -0.239635   -0.0784422    0.007024    -0.299388      0.146591     0.0926129     0.0852599   -0.171525    -0.133353    -0.0655383   -0.512907      0.0312927    -0.13674     0.241463     0.0592757  -0.0936718   -0.0758474    -0.084319    -0.041025   -0.0148362   -0.0130678 
 -0.0172044   -0.0818926   -0.0288713    0.0708646    -0.0175248  -0.224403   -0.0822024    0.0464206   -0.29965      -0.216659     0.088122      0.200686    -0.127434    -0.107914    -0.0682825    0.657822      0.0904287    -0.138585    0.114929     0.107198   -0.0995823    0.0577765     0.145462     0.0476287  -0.0864436    0.077196  
 -0.0356319    0.0164103   -0.0831461   -0.108759      0.0248099   0.132995    0.185391     0.0342064   -0.00227263    0.0772469   -0.207312      0.123522    -0.497469    -0.040594     0.112444     0.209932      0.0303836     0.0315016  -0.234783    -0.0647992  -0.138813     0.0781916     0.0504348   -0.0980366   0.111902    -0.107076  
 -0.0349839   -0.0717324   -0.0421867   -0.106861      0.0956278  -0.0557677  -0.0137381    0.00738677   0.13107      -0.320114    -0.0281243     0.140939     0.483561     0.142392     0.0491248    0.229147      0.0180027     0.0686316   0.00617373  -0.0552971  -0.0277825    0.0716104    -0.0977073   -0.181536    0.12284     -0.116346  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 17 18 20 21 22 23 24 31 32
INFO: iteration 1, average log likelihood -1.081616
WARNING: Variances had to be floored 1 5 17 18 20 21 22 23 24 29 31 32
INFO: iteration 2, average log likelihood -1.056395
WARNING: Variances had to be floored 1 17 18 19 20 21 22 23 24 29 31 32
INFO: iteration 3, average log likelihood -1.065301
WARNING: Variances had to be floored 1 5 11 17 18 20 21 22 23 24 29 31 32
INFO: iteration 4, average log likelihood -1.055568
WARNING: Variances had to be floored 1 17 18 20 21 22 23 24 29 31 32
INFO: iteration 5, average log likelihood -1.075224
WARNING: Variances had to be floored 1 5 17 18 19 20 21 22 23 24 29 31 32
INFO: iteration 6, average log likelihood -1.054496
WARNING: Variances had to be floored 1 17 18 20 21 22 23 24 29 31 32
INFO: iteration 7, average log likelihood -1.073567
WARNING: Variances had to be floored 1 5 11 17 18 20 21 22 23 24 29 31 32
INFO: iteration 8, average log likelihood -1.049220
WARNING: Variances had to be floored 1 17 18 19 20 21 22 23 24 29 31 32
INFO: iteration 9, average log likelihood -1.073324
WARNING: Variances had to be floored 1 5 17 18 20 21 22 23 24 29 31 32
INFO: iteration 10, average log likelihood -1.062770
INFO: EM with 100000 data points 10 iterations avll -1.062770
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.062455e+05
      1       6.786077e+05      -2.276378e+05 |       32
      2       6.456304e+05      -3.297721e+04 |       32
      3       6.306735e+05      -1.495692e+04 |       32
      4       6.222568e+05      -8.416688e+03 |       32
      5       6.165050e+05      -5.751812e+03 |       32
      6       6.122126e+05      -4.292451e+03 |       32
      7       6.094646e+05      -2.747935e+03 |       32
      8       6.081414e+05      -1.323193e+03 |       32
      9       6.075340e+05      -6.074820e+02 |       32
     10       6.071021e+05      -4.318941e+02 |       32
     11       6.065307e+05      -5.713685e+02 |       32
     12       6.057229e+05      -8.077887e+02 |       32
     13       6.047948e+05      -9.280632e+02 |       32
     14       6.041563e+05      -6.385416e+02 |       32
     15       6.037950e+05      -3.613451e+02 |       32
     16       6.034881e+05      -3.068156e+02 |       32
     17       6.032229e+05      -2.652956e+02 |       32
     18       6.028970e+05      -3.258215e+02 |       32
     19       6.023041e+05      -5.929006e+02 |       32
     20       6.010673e+05      -1.236846e+03 |       32
     21       5.995586e+05      -1.508728e+03 |       32
     22       5.987881e+05      -7.704166e+02 |       32
     23       5.985558e+05      -2.323844e+02 |       32
     24       5.984801e+05      -7.566396e+01 |       31
     25       5.984504e+05      -2.972273e+01 |       32
     26       5.984391e+05      -1.127551e+01 |       31
     27       5.984317e+05      -7.349035e+00 |       31
     28       5.984279e+05      -3.800476e+00 |       28
     29       5.984247e+05      -3.233683e+00 |       25
     30       5.984228e+05      -1.925432e+00 |       17
     31       5.984217e+05      -1.121830e+00 |       18
     32       5.984205e+05      -1.149837e+00 |       18
     33       5.984188e+05      -1.713829e+00 |       15
     34       5.984168e+05      -1.971384e+00 |       19
     35       5.984148e+05      -2.013613e+00 |       17
     36       5.984131e+05      -1.725528e+00 |       14
     37       5.984123e+05      -7.986764e-01 |       18
     38       5.984116e+05      -7.376318e-01 |        9
     39       5.984113e+05      -2.739112e-01 |        8
     40       5.984111e+05      -2.015185e-01 |       10
     41       5.984108e+05      -2.999415e-01 |        7
     42       5.984104e+05      -3.951602e-01 |       12
     43       5.984100e+05      -3.379492e-01 |        4
     44       5.984099e+05      -9.974440e-02 |        6
     45       5.984098e+05      -1.701915e-01 |        3
     46       5.984097e+05      -7.902554e-02 |        0
     47       5.984097e+05       0.000000e+00 |        0
K-means converged with 47 iterations (objv = 598409.6926716483)
INFO: K-means with 32000 data points using 47 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.321342
INFO: iteration 2, average log likelihood -1.293409
INFO: iteration 3, average log likelihood -1.266059
INFO: iteration 4, average log likelihood -1.227350
WARNING: Variances had to be floored 23
INFO: iteration 5, average log likelihood -1.170258
WARNING: Variances had to be floored 30
INFO: iteration 6, average log likelihood -1.110988
WARNING: Variances had to be floored 4 5 17 25
INFO: iteration 7, average log likelihood -1.057495
WARNING: Variances had to be floored 3 13 20 31 32
INFO: iteration 8, average log likelihood -1.067546
INFO: iteration 9, average log likelihood -1.100102
WARNING: Variances had to be floored 2 7 17 29
INFO: iteration 10, average log likelihood -1.042814
WARNING: Variances had to be floored 4 5 11 30
INFO: iteration 11, average log likelihood -1.048458
WARNING: Variances had to be floored 3 20 25
INFO: iteration 12, average log likelihood -1.062149
WARNING: Variances had to be floored 13 31
INFO: iteration 13, average log likelihood -1.068885
WARNING: Variances had to be floored 17 29
INFO: iteration 14, average log likelihood -1.063607
WARNING: Variances had to be floored 2 5 7
INFO: iteration 15, average log likelihood -1.048508
WARNING: Variances had to be floored 3 4 11 20 25 30
INFO: iteration 16, average log likelihood -1.032330
WARNING: Variances had to be floored 17
INFO: iteration 17, average log likelihood -1.082243
WARNING: Variances had to be floored 13 29 31 32
INFO: iteration 18, average log likelihood -1.046473
WARNING: Variances had to be floored 5 7 8
INFO: iteration 19, average log likelihood -1.047394
WARNING: Variances had to be floored 2 3 4 17 20 25 30
INFO: iteration 20, average log likelihood -1.033608
WARNING: Variances had to be floored 11
INFO: iteration 21, average log likelihood -1.096100
WARNING: Variances had to be floored 29
INFO: iteration 22, average log likelihood -1.058726
WARNING: Variances had to be floored 7 13 17 20 31 32
INFO: iteration 23, average log likelihood -1.023763
WARNING: Variances had to be floored 2 3 4 5 8 25
INFO: iteration 24, average log likelihood -1.056871
WARNING: Variances had to be floored 11 30
INFO: iteration 25, average log likelihood -1.077499
WARNING: Variances had to be floored 17 20 29
INFO: iteration 26, average log likelihood -1.055300
WARNING: Variances had to be floored 7
INFO: iteration 27, average log likelihood -1.067386
WARNING: Variances had to be floored 3 5 13 25 31 32
INFO: iteration 28, average log likelihood -1.020502
WARNING: Variances had to be floored 2 8 11 17 30
INFO: iteration 29, average log likelihood -1.051866
WARNING: Variances had to be floored 20 29
INFO: iteration 30, average log likelihood -1.084845
WARNING: Variances had to be floored 7
INFO: iteration 31, average log likelihood -1.069068
WARNING: Variances had to be floored 3 4 5 17 25 30 31
INFO: iteration 32, average log likelihood -1.015718
WARNING: Variances had to be floored 2 8 11 13 20 29
INFO: iteration 33, average log likelihood -1.057520
INFO: iteration 34, average log likelihood -1.115097
WARNING: Variances had to be floored 7 32
INFO: iteration 35, average log likelihood -1.062756
WARNING: Variances had to be floored 3 5 17 20 25 29 31
INFO: iteration 36, average log likelihood -1.011449
WARNING: Variances had to be floored 2 4 8 11 13 30
INFO: iteration 37, average log likelihood -1.071980
WARNING: Variances had to be floored 7
INFO: iteration 38, average log likelihood -1.106890
WARNING: Variances had to be floored 17
INFO: iteration 39, average log likelihood -1.058878
WARNING: Variances had to be floored 3 5 20 25 29 31
INFO: iteration 40, average log likelihood -1.009714
WARNING: Variances had to be floored 2 4 8 30 32
INFO: iteration 41, average log likelihood -1.067869
WARNING: Variances had to be floored 11 17
INFO: iteration 42, average log likelihood -1.082407
WARNING: Variances had to be floored 13
INFO: iteration 43, average log likelihood -1.056573
WARNING: Variances had to be floored 3 20 25 29 31
INFO: iteration 44, average log likelihood -1.012806
WARNING: Variances had to be floored 2 4 5 8 17
INFO: iteration 45, average log likelihood -1.055677
WARNING: Variances had to be floored 28 30 32
INFO: iteration 46, average log likelihood -1.065363
WARNING: Variances had to be floored 11 13 20
INFO: iteration 47, average log likelihood -1.048545
WARNING: Variances had to be floored 3 25 29 31
INFO: iteration 48, average log likelihood -1.046657
WARNING: Variances had to be floored 2 8 17
INFO: iteration 49, average log likelihood -1.067288
WARNING: Variances had to be floored 5 20 32
INFO: iteration 50, average log likelihood -1.047791
INFO: EM with 100000 data points 50 iterations avll -1.047791
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0556599     0.00174838    0.114653    -0.0206252    -0.0863267    0.192563   -0.0822716   -0.144734     -0.145103     -0.0675015   -0.0352939    0.121402    -0.0264176   -0.115922    0.0109776     0.00119664   0.000289855  -0.165215     0.143908     0.0866026   0.164492     0.133381    -0.3281      -0.0480378    0.0393923     0.0290347 
 -0.091221      0.00992624   -0.0566436   -0.120012     -0.0595174    0.0852658   0.0254157    0.0931297     0.147995      0.181958    -0.151701    -0.0255182   -0.0446344   -0.131729   -0.0688609     0.187579     0.0507269    -0.00123935   0.0372476   -0.0459272   0.0122923    0.0771205    0.138709    -0.0437628   -0.0263451     0.135774  
  0.111303      0.000762388  -0.0647678    0.0377815    -0.0427475    0.0143005   0.0857322   -0.0443163    -0.284083      0.0315227    0.0561217    0.0740078   -0.0274228   -0.114907    0.136051     -0.0320633    0.0853234     0.123285    -0.0233692    0.308391   -0.092168     0.0137867   -0.239672     0.215144    -0.0528475    -0.0823947 
  0.0373042     0.0418691    -0.0660922    0.015864     -0.0221039   -0.169213   -0.0543786    0.0759246     0.0482154     0.13482      0.0268128    0.138497    -0.099991     0.191616   -0.042031     -0.00350171   0.221976      0.137917     0.256063     0.126966   -0.00139327  -0.210588    -0.132244    -0.00390281  -0.074636     -0.016141  
  0.146968      0.0478221    -0.0886819   -0.0879151     0.0647551    0.0760639   0.0205483    0.000417059   0.145118     -0.00945398   0.0917319   -0.0427393   -0.0854141   -0.0466507  -0.0636245     0.0280892    0.0365848     0.033352     0.00680996  -0.0266419  -0.10449      0.0731389    0.283702     0.100898    -0.110988     -0.0605223 
  0.00184106    0.0353024    -0.00552634  -0.0753302    -0.0539649   -0.0789008  -0.0350396   -0.110309     -0.159723     -0.0983172   -0.0479703    0.0201886   -0.0399582    0.0341522  -0.0171946     0.176863     0.00885808   -0.166622     0.0193941   -0.151198   -0.109772     0.00477893   0.0159354    0.00604751   0.0363811     0.193123  
  0.0112717    -0.0237189     0.0697562    0.000539949  -0.106743     0.0602153   0.0246263    0.0137042     0.12621       0.0956362    0.0331603   -0.0632569    0.0470818   -0.070013    0.110798     -0.065999    -0.0334138     0.0558358    0.0857586    0.0770993   0.0731591    0.108476     0.116516     0.0795711    0.0405701    -0.0329569 
  0.0617215     0.0139584    -0.0165315    0.0973775     0.0252782   -0.0672713   0.111652     0.111298     -0.0115341     0.076814     0.103905     0.0248655    0.0861128   -0.225582    0.140057     -0.11667      0.00789666   -0.0380563   -0.332882    -0.0294952   0.13651      0.183445    -0.169752    -0.0790687    0.124483      0.142169  
 -0.0355141     0.00130934    0.102558     0.0728555    -0.0525394    0.0162633  -0.0370652   -0.0465931     0.107454     -0.0284197    0.127744    -0.0373691   -0.0709341    0.0785856  -0.160124     -0.0771749   -0.199009      0.165809     0.0859956   -0.011025   -0.11563     -0.123586     0.118398     0.0806489    0.0377423    -0.0494818 
  0.0696353     0.174679      0.0917734    0.00275084    0.0308248   -0.124541   -0.0169103   -0.130571     -0.00985951   -0.0744617   -0.0357921    0.0945409   -0.135701     0.103234    0.021777      0.15051      0.0803688    -0.0179979    0.0673758   -0.0832553  -0.00983287   0.0515214    0.119238     0.156289    -0.165948     -0.0167061 
  0.0046357    -0.140856     -0.128997     0.0449017     0.215769    -0.0284186  -0.139682     0.0232354    -0.065527      0.148631    -0.198808     0.0231806   -0.10455      0.174034   -0.0156119    -0.0475067   -0.0750484     0.201136     0.0406598   -0.187133    0.0294765   -0.0960249    0.13239      0.197443    -0.238212      0.0203777 
 -0.132375      0.0903685     0.225897     0.0900646     0.134416     0.124042   -0.109672     0.0811287     0.159463      0.139932     0.0820602   -0.145436     0.176082     0.0633269  -0.123304      0.0811496   -0.115483     -0.0311743    0.0839053    0.2575      0.125783     0.0141036   -0.0160144   -0.0142298   -0.0207597     0.00328644
 -0.0101315     0.00538479    0.0706945   -0.0105421    -0.0346803    0.0214974   0.0359495   -0.097365      0.0216919    -0.167008    -0.0659135    0.209722     0.0594599    0.111341   -0.0304305     0.0451118    0.128644      0.0250687    0.190935    -0.0700472  -0.0707935    0.0628778   -0.0998419   -0.120188    -0.102599      0.0330043 
  0.000848912  -0.0329903    -0.0260583    0.0562832     0.0105228   -0.232583   -0.080187     0.0252702    -0.299509     -0.0217288    0.0905313    0.138776    -0.151111    -0.121564   -0.0668114     0.0295411    0.0587057    -0.137595     0.18284      0.0814847  -0.0964082   -0.0139378    0.0221628    5.5016e-5   -0.0480005     0.0287652 
  0.208028      0.0451387     0.0820464    0.00196938   -0.086659     0.0830384   0.00399489  -0.14871      -0.133041     -0.0319271   -0.129972    -0.0538513    0.00590197   0.0966839   0.240733     -0.00773342  -0.0528804     0.108635    -0.147315    -0.150194    0.0691688    0.23717      0.0336949    0.0484268   -0.111422      0.0948706 
 -0.0789949    -0.0618179     0.133211     0.105253     -0.0361776    0.0869135   0.00863323   0.0316605    -0.0343911     0.0868948   -0.0226012   -0.100573     0.0121541   -0.100832   -0.000716616   0.121971    -0.0384788     0.0697198    0.00735433   0.0237946   0.19462     -0.0435576   -0.100324    -0.131343     0.0131895    -0.04208   
 -0.124666     -0.109793     -0.0833467   -0.135721     -0.223479    -0.0654829  -0.00339837   0.135703     -0.0244871     0.0184387   -0.220076     0.0599642   -0.119256     0.0654091   0.0715629    -0.0713842   -0.115713      0.148209     0.0473002   -0.0849679   0.214649     0.187241     0.025221    -0.145731    -0.128206     -0.0970323 
 -0.00425606    0.0102627     0.240294    -0.157589      0.0618704   -0.0148593  -0.042608     0.106925     -0.0812261     0.0166689   -0.204911     0.0759821    0.0652397   -0.0746149   0.00732362   -0.0615274    0.0660447    -0.00547475   0.102499    -0.0109418  -0.0528677    0.0636415   -0.0423261    0.155836    -0.236517      0.00452124
 -0.108349      0.105064     -0.143234     0.152545     -0.0890671   -0.0412904   0.0660203   -0.215823      0.0900478     0.0739158   -0.106208     0.107529    -0.0602865    0.0619458   0.0493985    -0.114882     0.0913863    -0.00958868   0.163334    -0.0299126  -0.13881     -0.0516977   -0.185619     0.225192    -0.164433      0.18126   
  0.104984      0.00263187   -0.0541359    0.0146187    -0.0587761   -0.0952946  -0.0972134   -0.193267     -0.000551654   0.0605564   -0.0629283   -0.0282199   -0.0909114    0.153416    0.0885992     0.0876614   -0.0849603     0.101811     0.031224     0.0271211   0.0998021    0.137285    -0.126271    -0.125252    -0.0348054    -0.0591509 
  0.289566     -0.248349     -0.0159708   -0.00543168   -0.0430774    0.0274337   0.0993684    0.254595      0.0854808    -0.0181911   -0.0495905    0.0466646    0.0280785    0.0266748   0.0645645     0.0931892   -0.0922153    -0.0455588   -0.0307095    0.0595792  -0.0612546    0.00279861   0.0373431   -0.0860708    0.0253218     0.00507596
 -0.116549     -0.0553955     0.0116536    0.0501826     0.141155    -0.195716   -0.0744345    0.0493635     0.00799014    0.0701134   -0.156384    -0.179986     0.00520673   0.0600193  -0.092992     -0.0471171   -0.0873939     0.230824    -0.137044     0.0383      0.0671602    0.00485382   0.244551    -0.046543    -0.053906      0.052886  
  0.0816253     0.0132273     0.122936    -0.0815863    -0.0437373   -0.161575    0.0347789   -0.0392441    -0.00694838    0.172611    -0.0554518   -0.0435551   -0.0296636    0.0706788   0.0523496     0.0461851   -0.0675242    -0.214156    -0.00960838  -0.0454347  -0.0114992    0.117501    -0.0365689    0.0334726   -0.0919242     0.0189377 
  0.03609       0.0800769    -0.0978235    0.0360315    -0.0511047    0.0128608   0.0318808   -0.133336      0.186779      0.098733    -0.124917     0.0199633   -0.0215638    0.0182526  -0.0669179     0.0421572   -0.0442246    -0.0717754   -0.135169     0.155447    0.0336197   -0.0811135   -0.0915975   -0.0566074   -0.0410978    -0.0274113 
  0.138623     -0.000660594  -0.140704     0.0094653     0.0133898   -0.124393    0.0381712    0.12571      -0.189543      0.251333    -0.13097      0.0181426   -0.0355271    0.0630823   0.0703844    -0.0709675    0.0349251    -0.245091    -0.0229079   -0.149212    0.0726511    0.0405764    0.0120352   -0.0313388    0.000347048   0.0331611 
 -0.0553852    -0.0619416     0.104077     0.166491     -0.00485474   0.0569504   0.0688181   -0.101767     -0.051673      0.00233868   0.0705074    0.0400252   -0.0535413   -0.0504006  -0.0322735    -0.0477573    0.085548      0.0331049    0.257703    -0.0769737  -0.0649368    0.013709     0.0297551    0.0765889    0.0573565    -0.1072    
  0.0987945     0.14226      -0.112747     0.103473     -0.0680783    0.0538263  -0.0505284    0.0508228     0.0926486     0.217973    -0.0480045   -0.0702276    0.023297     0.137145    0.107883     -0.0811036    0.192337     -0.108236    -0.080472     0.155132   -0.210731     0.123734     0.0803501    0.0632996    0.155336      0.0392546 
  0.00144947    0.0303872    -0.0307088   -0.00716638    0.00499393  -0.0314938  -0.0576048    0.09651       0.0500313     0.0699915   -0.0116137    0.0626361   -0.123826     0.147112   -0.185553     -0.0766273    0.0585649     0.0770856    0.0692486    0.113295   -0.0962103   -0.162689    -0.0893763   -0.110522     0.00208484   -0.101496  
  0.126128      0.0451199     0.0865532   -0.0949348    -0.101221     0.0541099  -0.0280144   -0.186603     -0.0918428     0.0294328    0.00094629  -0.00247005  -0.118439     0.0826057   0.00465452   -0.0245992    0.0590291     0.067306     0.0734701    0.144421   -0.0801308   -0.056972     0.0157483   -0.045117    -0.12252       0.0746515 
  0.0235019     0.132797     -0.0039724   -0.0958596    -0.149909    -0.0681488  -0.0709602   -0.121672     -0.132147      0.230256     0.187155    -0.144301    -0.0127617    0.0643971  -0.0274631     0.175094    -0.0424118     0.182304    -0.033548     0.144631   -0.0949924   -0.080564    -0.00654305   0.0222947   -0.17026      -0.0616131 
  0.129044     -0.140638     -0.0149792   -0.058228      0.106839    -0.176204   -0.107563    -0.105614     -0.00392036    0.169799    -0.00416176  -0.0116951    0.0548916    0.0710549  -0.14108      -0.119927    -0.146595     -0.156262    -0.122785    -0.15507     0.045432    -0.245213     0.0339936   -0.106991    -0.0155202     0.0891568 
 -0.0352871    -0.0298172    -0.0635267   -0.10845       0.0614424    0.0350774   0.0849457    0.020505      0.0638198    -0.127165    -0.118014     0.132981     0.00736624   0.0528978   0.0819311     0.220985     0.0224638     0.0515612   -0.115203    -0.0611579  -0.0798422    0.0754198   -0.0254325   -0.140211     0.117984     -0.112586  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 11 13 25 30
INFO: iteration 1, average log likelihood -1.041325
WARNING: Variances had to be floored 3 4 8 11 13 17 25 29 30 31
INFO: iteration 2, average log likelihood -0.988131
WARNING: Variances had to be floored 2 5 11 13 20 25 30 32
INFO: iteration 3, average log likelihood -0.981964
WARNING: Variances had to be floored 3 4 8 11 13 17 25 28 29 30 31
INFO: iteration 4, average log likelihood -1.005532
WARNING: Variances had to be floored 11 13 25 30
INFO: iteration 5, average log likelihood -1.008644
WARNING: Variances had to be floored 2 3 4 5 8 11 13 17 20 25 29 30 31 32
INFO: iteration 6, average log likelihood -0.959495
WARNING: Variances had to be floored 11 13 28 30
INFO: iteration 7, average log likelihood -1.033192
WARNING: Variances had to be floored 3 4 8 11 13 17 25 29 30 31
INFO: iteration 8, average log likelihood -0.981044
WARNING: Variances had to be floored 2 5 11 13 20 25 30 32
INFO: iteration 9, average log likelihood -0.981326
WARNING: Variances had to be floored 3 4 8 11 13 17 25 28 29 30 31
INFO: iteration 10, average log likelihood -1.005488
INFO: EM with 100000 data points 10 iterations avll -1.005488
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.120584     -0.103895    0.044998    0.0614379    0.01836      0.126995    -0.0524662    0.0227145   -0.128352     0.000841199   0.183943     0.0267829    0.210514     -0.0231973    -0.0542534     0.0925962     0.0386578    0.115687     0.134383     0.0220353    -0.0730364    0.0208774  -0.0537834   -0.0236362    0.0373043    0.170548  
 -0.233172      0.124252   -0.283878    0.0841593   -0.0391423   -0.00355767  -0.0559087   -0.0376409   -0.176798     0.0208608    -0.0920132   -0.1432      -0.0284083     0.093551     -0.0699952    -0.0501141     0.135135     0.0622467    0.0247359    0.256725      0.0431016   -0.15566    -0.0687365    0.00798699   0.02968      0.0681087 
 -0.208772      0.14599    -0.144607   -0.0930761    0.105711    -0.00665834   0.0359122    0.0224522    0.060179     0.128735      0.0292105   -0.0365206    0.00285826   -0.128294     -0.032523      0.0797421    -0.0779611   -0.0655539    0.0341407   -0.0239177     0.0237315   -0.113336    0.253042     0.0157245    0.0495115   -0.0771464 
  0.0670619     0.0880978  -0.0272704  -0.00561935  -0.0537678   -0.0848002   -0.0544386    0.0760514    0.134609    -0.0558118     0.127263     0.0558202    0.0440544     0.0374961    -0.0933694    -0.0064421     0.142994    -0.0905326   -0.123726     0.117903      0.0117939   -0.0338269  -0.136648     0.120835    -0.0268816    0.0476592 
 -0.0715871    -0.0373027   0.172021    0.00908341   0.0403808   -0.0258015    0.0506499    0.0103364    0.0327245   -0.0972844     0.0530141    0.0618787   -0.000491515   0.0698363    -0.111163     -0.0625285     0.10718     -0.0847787    0.0530566    0.00743821    0.0950017    0.249912    0.15614      0.105929    -0.112394     0.0321571 
  0.0797742    -0.0896945   0.186267   -0.0482268    0.0632977   -0.0240793   -0.0784915   -0.0811203    0.114494     0.022704      0.00450994  -0.190469     0.0131531    -0.0257314    -0.021566     -0.0950088    -0.0535703    0.0381789   -0.0837295    0.06344       0.11476      0.0585133  -0.134092     0.103242    -0.143592    -0.0476417 
  0.000842667   0.144733    0.0690416  -0.0260479   -0.105692     0.134154    -0.0655291   -0.0654652   -0.148568    -0.0600503    -0.166759    -0.0203233   -0.0612184    -0.0139876     0.106233     -0.065306     -0.102215     0.00800217   0.0331824   -0.0573582    -0.0016984    0.107483   -0.221136     0.0321861    0.0657722   -0.0183673 
 -0.174231      0.104386    0.0594312  -0.15216     -0.0818921   -0.00847188  -0.0554636    0.100645    -0.0164201   -0.133022      0.0507785    0.0149548    0.0992807     0.000158772  -0.0112712     0.0319649     0.01386      0.0357124   -0.0494542   -0.018327     -0.00189895  -0.022692    0.00886161   0.0419955   -0.00381102  -0.0123727 
 -0.0427808    -0.123274   -0.0314903   0.122433    -0.0536416    0.112088     0.0218658   -0.04957      0.0113737    0.198253      0.0707629    0.00925485   0.00918217   -0.0860931    -0.111655      0.0345205     0.0106197   -0.134287     0.0507354    0.0857946    -0.197878    -0.108936    0.0198786    0.11901      0.150816    -0.0847866 
  0.0790465     0.0498151  -0.212479    0.0860333    0.0191513   -0.0479287   -0.125795    -0.0654509    0.149557     0.29001      -0.0679617    0.0598817    0.0540171    -0.00257829    0.0291535     0.100514      0.0387923    0.055371     0.0284692    0.165876      0.155131    -0.0233424  -0.0689989    0.0290625   -0.037306     0.0116953 
  0.153061      0.113917    0.074294    0.0358379    0.00463421  -0.00577969   0.00209525  -0.082964     0.0623996   -0.0811492    -0.164106    -0.0867138    0.0601554    -0.13355       0.0188614     0.0414826     0.0292346    0.0564273    0.0869743   -0.0667245     0.0810135   -0.0662469  -0.0223513   -0.0248334    0.0408639    0.0263522 
 -0.00166804    0.200003   -0.0108715   0.139319    -0.0703055    0.0926391    0.0450865    0.00042988  -0.229395     0.0791569     0.0963758   -0.026809     0.0990959     0.0192453    -0.026903     -0.141975      0.0830036    0.00413362  -0.105397    -0.0337725     0.18175      0.0135715  -0.0174088    0.14877     -0.215024    -0.114495  
 -0.154648      0.134207   -0.13653    -0.0889102   -0.153192    -0.0474879   -0.10166      0.0528748    0.13558      0.0293721     0.0330427    0.0901028    0.231024     -0.0706424    -0.114704     -0.141469      0.0906866    0.10586     -0.0403928    0.160889      0.0148272   -0.237108    0.0313062   -0.0142647   -0.02765     -0.149127  
 -0.0532702    -0.0344751  -0.0302723   0.0215765    0.0879499    0.0102492   -0.0475798   -0.0493221    0.0404519   -0.135608      0.0550924    0.0110956   -0.0237976    -0.0114921     0.00152588   -0.00669029    0.0477158    0.115756    -0.064191    -0.00699775   -0.0765626    0.0044331  -0.0456818   -0.119622    -0.0018155   -0.02916   
 -0.0530115     0.0126145  -0.18365    -0.026957     0.05247      0.0482265   -0.0548184    0.0917511    0.0219936   -0.0588041     0.0583076    0.0061299    0.0696819     0.000598408  -0.213802      0.187264     -0.0351162   -0.0586615    0.0612839   -0.0641557    -0.124783     0.210371   -0.049572     0.0750989   -0.119284     0.0544126 
 -0.0900238    -0.0263134   0.158547   -0.0279939   -0.0445218    0.0493862    0.186698     0.00783681  -0.037564     0.196062      0.0267693    0.123178    -0.126324     -0.209702      0.109379      0.0879538    -0.108182     0.0270315   -0.204906    -0.0170657     0.0603569    0.142873    0.0662506   -0.0681601    0.0121431   -0.199657  
  0.213166      0.158984   -0.0943881  -0.0502311    0.193946    -0.0337312   -0.0424295   -0.069589     0.00205009  -0.09557      -0.0854437   -0.167618    -0.177685     -0.108941     -0.000153541   0.0343712     0.116018    -0.183717    -0.0748162   -0.0998291    -0.100712     0.0519308   0.0639692   -0.104467    -0.0335545    0.164537  
 -0.046775     -0.105368    0.0744053  -0.0884856   -0.0357512   -0.0581713   -0.0908821    0.0232456    0.0312935    0.0823206     0.00946057  -0.054835     0.171564     -0.025708     -0.0520316     0.0231599    -0.109876     0.0307781   -0.133008     0.0401584    -0.109638    -0.0389555   0.0145495    0.0835375   -0.116495     0.0194291 
  0.047836     -0.0423991   0.125025    0.0868228    0.120883    -0.163673    -0.123664    -0.147881     0.110918    -0.081527      0.0822973    0.0469815    0.125975      0.140076      0.0314666    -0.0447387     0.0147019   -0.10973     -0.041313    -0.000931239   0.180523     0.157885   -0.188895     0.00152473  -0.0233588    0.0384868 
 -0.0983375    -0.107513    0.0469163  -0.0381244    0.139574     0.0224884   -0.0554299   -0.0847848   -0.0695242    0.101539      0.0859094    0.126246     0.033209     -0.09138       0.0453714     0.0667303     0.154951    -0.270695    -0.029168     0.081566     -0.127042    -0.0838232  -0.090979    -0.035054    -0.122756     0.0739182 
 -0.0525074    -0.0819833  -0.0176394  -0.00274572   0.0588035   -0.0409102   -0.140964    -0.125071     0.120453    -0.060525      0.053348    -0.143303    -0.0544294    -0.0989003     0.0478673    -0.0797654    -0.132683     0.0861613   -0.0361879    0.0509069     0.0571401   -0.0723751  -0.132831    -0.00824088   0.00601808  -0.0423261 
 -0.0202734    -0.0473014   0.0291131   0.021935    -0.204192     0.15703      0.0499049    0.0626595    0.0635868    0.00507814    0.0492086   -0.0986368   -0.142198      0.111823      0.115543      0.0250147    -0.0159522   -0.00960152  -0.0876148   -0.12593       0.056509    -0.183054    0.0133747   -0.0381499    0.143462     0.0608884 
 -0.0052407    -0.175446    0.0323033  -0.0530243    0.0382304    0.0638354    0.0878347    0.086221    -0.0932292    0.0983235     0.0367322    0.0687375   -0.175383      0.14931       0.0282348    -0.122914     -0.104642    -0.147817     0.116916    -0.0845257     0.0398379    0.0992861   0.14589      0.006556     0.0656375   -0.198235  
  0.0134324    -0.133605    0.155521    0.0214137    0.0540656   -0.0710111    0.147355    -0.0774759   -0.129107     0.0932459    -0.049828    -0.0652907   -0.0694852    -0.0136939    -0.0972603    -0.0323135     0.0821818    0.128736     0.0617284   -0.119923      0.0769582   -0.177463    0.0651816   -0.00582419  -0.053615     0.0404155 
 -0.254025     -0.146896   -0.0721468   0.0753464   -0.0510407    0.0224742    0.0943865   -0.0784138    0.144339    -0.0988808     0.125844    -0.188688    -0.144605     -0.0907047     0.150413     -0.103828     -0.0959129   -0.281284     0.118175     0.0345349     0.0700371    0.0363236   0.0549169   -0.00546476  -0.0588898   -0.0977445 
 -0.136903     -0.0923321  -0.063217   -0.150135    -0.0348517   -0.296144     0.0154162    0.0753396    0.0945255    0.203235     -0.0886391    0.0902226   -0.132279      0.0383848    -0.00529971   -0.127962     -0.0317469    0.0707563    0.231044    -0.101037     -0.085132     0.029715   -0.0546234    0.0856868   -0.145008    -0.137255  
 -0.147126      0.184115    0.134096    0.00841288  -0.173847    -0.0875297    0.0608801   -0.0147403    0.0860838    0.0418724    -0.123062    -0.100661     0.0514542     0.092113     -0.0535741    -0.0385877    -0.00808541  -0.0638038    0.00605249  -0.0170204     0.13226     -0.0970974  -0.00664737  -0.239371     0.126634    -0.0716727 
  0.00854367    0.0621926   0.0522677  -0.0688734    0.0766041   -0.108251     0.0135667   -0.0661134   -0.190926    -0.0778124     0.0324038   -0.156295    -0.00385082    0.121012      0.111654     -0.135106     -0.089434    -0.0847709   -0.0280755   -0.032167      0.180164     0.0745465   0.123089    -0.0132593    0.0920823    0.077427  
  0.137295     -0.175713   -0.0374842  -0.018823    -0.0615069   -0.0951953   -0.0591041   -0.0852837   -0.014525    -0.110058      0.142848    -0.0466301   -0.00112839    0.0465577    -0.0646118    -0.123413      0.0286249    0.0540513    0.136853    -0.0443407    -0.049488    -0.191447    0.0265623   -0.0330533   -0.0649435   -0.199002  
  0.0816428    -0.0139867   0.0995249   0.149822     0.0734864    0.031194     0.102598     0.0582307    0.211854     0.00851254    0.0794079   -0.0485436   -0.043071      0.0584901    -0.000790124   0.216658      0.205901     0.00260882  -0.0657292    0.047792      0.0704321    0.0112041   0.0408445    0.102382     0.0451171    0.00109119
 -0.079767      0.0280266   0.0352595   0.215504    -0.12028     -0.184111    -0.0679712   -0.0360224    0.0393329   -0.0406598     0.00494062   0.123257     0.0525418    -0.119538      0.152587      0.0895208    -0.0166537    0.0182571    0.00745353  -0.0951252    -0.0247369    0.0124854  -0.00267799   0.102218     0.0889863    0.122354  
 -0.0603066    -0.18051     0.0237755  -0.0793597    0.202969     0.00604167  -0.0743018    0.0390276   -0.197957    -0.0318127     0.0862965   -0.0544205    0.0868384    -0.0844761     0.0718639    -0.000929294   0.0887336   -0.0321473    0.0284181    0.082568      0.031904     0.172832   -0.0676886   -0.0232661    0.0706654    0.129089  kind full, method split
0: avll = -1.419716382925338
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.419736
INFO: iteration 2, average log likelihood -1.419667
INFO: iteration 3, average log likelihood -1.419620
INFO: iteration 4, average log likelihood -1.419567
INFO: iteration 5, average log likelihood -1.419503
INFO: iteration 6, average log likelihood -1.419421
INFO: iteration 7, average log likelihood -1.419305
INFO: iteration 8, average log likelihood -1.419110
INFO: iteration 9, average log likelihood -1.418738
INFO: iteration 10, average log likelihood -1.418036
INFO: iteration 11, average log likelihood -1.416936
INFO: iteration 12, average log likelihood -1.415712
INFO: iteration 13, average log likelihood -1.414823
INFO: iteration 14, average log likelihood -1.414378
INFO: iteration 15, average log likelihood -1.414197
INFO: iteration 16, average log likelihood -1.414127
INFO: iteration 17, average log likelihood -1.414101
INFO: iteration 18, average log likelihood -1.414090
INFO: iteration 19, average log likelihood -1.414086
INFO: iteration 20, average log likelihood -1.414084
INFO: iteration 21, average log likelihood -1.414084
INFO: iteration 22, average log likelihood -1.414083
INFO: iteration 23, average log likelihood -1.414083
INFO: iteration 24, average log likelihood -1.414083
INFO: iteration 25, average log likelihood -1.414083
INFO: iteration 26, average log likelihood -1.414083
INFO: iteration 27, average log likelihood -1.414083
INFO: iteration 28, average log likelihood -1.414083
INFO: iteration 29, average log likelihood -1.414083
INFO: iteration 30, average log likelihood -1.414083
INFO: iteration 31, average log likelihood -1.414082
INFO: iteration 32, average log likelihood -1.414082
INFO: iteration 33, average log likelihood -1.414082
INFO: iteration 34, average log likelihood -1.414082
INFO: iteration 35, average log likelihood -1.414082
INFO: iteration 36, average log likelihood -1.414082
INFO: iteration 37, average log likelihood -1.414082
INFO: iteration 38, average log likelihood -1.414082
INFO: iteration 39, average log likelihood -1.414082
INFO: iteration 40, average log likelihood -1.414082
INFO: iteration 41, average log likelihood -1.414082
INFO: iteration 42, average log likelihood -1.414082
INFO: iteration 43, average log likelihood -1.414082
INFO: iteration 44, average log likelihood -1.414082
INFO: iteration 45, average log likelihood -1.414082
INFO: iteration 46, average log likelihood -1.414082
INFO: iteration 47, average log likelihood -1.414082
INFO: iteration 48, average log likelihood -1.414082
INFO: iteration 49, average log likelihood -1.414082
INFO: iteration 50, average log likelihood -1.414082
INFO: EM with 100000 data points 50 iterations avll -1.414082
952.4 data points per parameter
1: avll = [-1.41974,-1.41967,-1.41962,-1.41957,-1.4195,-1.41942,-1.4193,-1.41911,-1.41874,-1.41804,-1.41694,-1.41571,-1.41482,-1.41438,-1.4142,-1.41413,-1.4141,-1.41409,-1.41409,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.414097
INFO: iteration 2, average log likelihood -1.414034
INFO: iteration 3, average log likelihood -1.413981
INFO: iteration 4, average log likelihood -1.413919
INFO: iteration 5, average log likelihood -1.413844
INFO: iteration 6, average log likelihood -1.413757
INFO: iteration 7, average log likelihood -1.413663
INFO: iteration 8, average log likelihood -1.413568
INFO: iteration 9, average log likelihood -1.413482
INFO: iteration 10, average log likelihood -1.413410
INFO: iteration 11, average log likelihood -1.413352
INFO: iteration 12, average log likelihood -1.413307
INFO: iteration 13, average log likelihood -1.413273
INFO: iteration 14, average log likelihood -1.413247
INFO: iteration 15, average log likelihood -1.413228
INFO: iteration 16, average log likelihood -1.413212
INFO: iteration 17, average log likelihood -1.413199
INFO: iteration 18, average log likelihood -1.413188
INFO: iteration 19, average log likelihood -1.413178
INFO: iteration 20, average log likelihood -1.413169
INFO: iteration 21, average log likelihood -1.413159
INFO: iteration 22, average log likelihood -1.413150
INFO: iteration 23, average log likelihood -1.413141
INFO: iteration 24, average log likelihood -1.413132
INFO: iteration 25, average log likelihood -1.413123
INFO: iteration 26, average log likelihood -1.413114
INFO: iteration 27, average log likelihood -1.413105
INFO: iteration 28, average log likelihood -1.413096
INFO: iteration 29, average log likelihood -1.413086
INFO: iteration 30, average log likelihood -1.413077
INFO: iteration 31, average log likelihood -1.413068
INFO: iteration 32, average log likelihood -1.413059
INFO: iteration 33, average log likelihood -1.413050
INFO: iteration 34, average log likelihood -1.413041
INFO: iteration 35, average log likelihood -1.413032
INFO: iteration 36, average log likelihood -1.413023
INFO: iteration 37, average log likelihood -1.413015
INFO: iteration 38, average log likelihood -1.413007
INFO: iteration 39, average log likelihood -1.412999
INFO: iteration 40, average log likelihood -1.412991
INFO: iteration 41, average log likelihood -1.412984
INFO: iteration 42, average log likelihood -1.412977
INFO: iteration 43, average log likelihood -1.412970
INFO: iteration 44, average log likelihood -1.412964
INFO: iteration 45, average log likelihood -1.412958
INFO: iteration 46, average log likelihood -1.412952
INFO: iteration 47, average log likelihood -1.412946
INFO: iteration 48, average log likelihood -1.412941
INFO: iteration 49, average log likelihood -1.412937
INFO: iteration 50, average log likelihood -1.412932
INFO: EM with 100000 data points 50 iterations avll -1.412932
473.9 data points per parameter
2: avll = [-1.4141,-1.41403,-1.41398,-1.41392,-1.41384,-1.41376,-1.41366,-1.41357,-1.41348,-1.41341,-1.41335,-1.41331,-1.41327,-1.41325,-1.41323,-1.41321,-1.4132,-1.41319,-1.41318,-1.41317,-1.41316,-1.41315,-1.41314,-1.41313,-1.41312,-1.41311,-1.4131,-1.4131,-1.41309,-1.41308,-1.41307,-1.41306,-1.41305,-1.41304,-1.41303,-1.41302,-1.41301,-1.41301,-1.413,-1.41299,-1.41298,-1.41298,-1.41297,-1.41296,-1.41296,-1.41295,-1.41295,-1.41294,-1.41294,-1.41293]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.412939
INFO: iteration 2, average log likelihood -1.412881
INFO: iteration 3, average log likelihood -1.412832
INFO: iteration 4, average log likelihood -1.412779
INFO: iteration 5, average log likelihood -1.412717
INFO: iteration 6, average log likelihood -1.412644
INFO: iteration 7, average log likelihood -1.412560
INFO: iteration 8, average log likelihood -1.412470
INFO: iteration 9, average log likelihood -1.412377
INFO: iteration 10, average log likelihood -1.412288
INFO: iteration 11, average log likelihood -1.412207
INFO: iteration 12, average log likelihood -1.412136
INFO: iteration 13, average log likelihood -1.412075
INFO: iteration 14, average log likelihood -1.412025
INFO: iteration 15, average log likelihood -1.411983
INFO: iteration 16, average log likelihood -1.411949
INFO: iteration 17, average log likelihood -1.411920
INFO: iteration 18, average log likelihood -1.411895
INFO: iteration 19, average log likelihood -1.411875
INFO: iteration 20, average log likelihood -1.411857
INFO: iteration 21, average log likelihood -1.411841
INFO: iteration 22, average log likelihood -1.411826
INFO: iteration 23, average log likelihood -1.411813
INFO: iteration 24, average log likelihood -1.411802
INFO: iteration 25, average log likelihood -1.411791
INFO: iteration 26, average log likelihood -1.411781
INFO: iteration 27, average log likelihood -1.411771
INFO: iteration 28, average log likelihood -1.411763
INFO: iteration 29, average log likelihood -1.411755
INFO: iteration 30, average log likelihood -1.411748
INFO: iteration 31, average log likelihood -1.411741
INFO: iteration 32, average log likelihood -1.411735
INFO: iteration 33, average log likelihood -1.411729
INFO: iteration 34, average log likelihood -1.411724
INFO: iteration 35, average log likelihood -1.411719
INFO: iteration 36, average log likelihood -1.411714
INFO: iteration 37, average log likelihood -1.411710
INFO: iteration 38, average log likelihood -1.411706
INFO: iteration 39, average log likelihood -1.411702
INFO: iteration 40, average log likelihood -1.411699
INFO: iteration 41, average log likelihood -1.411695
INFO: iteration 42, average log likelihood -1.411692
INFO: iteration 43, average log likelihood -1.411689
INFO: iteration 44, average log likelihood -1.411685
INFO: iteration 45, average log likelihood -1.411682
INFO: iteration 46, average log likelihood -1.411679
INFO: iteration 47, average log likelihood -1.411676
INFO: iteration 48, average log likelihood -1.411673
INFO: iteration 49, average log likelihood -1.411671
INFO: iteration 50, average log likelihood -1.411668
INFO: EM with 100000 data points 50 iterations avll -1.411668
236.4 data points per parameter
3: avll = [-1.41294,-1.41288,-1.41283,-1.41278,-1.41272,-1.41264,-1.41256,-1.41247,-1.41238,-1.41229,-1.41221,-1.41214,-1.41208,-1.41202,-1.41198,-1.41195,-1.41192,-1.4119,-1.41187,-1.41186,-1.41184,-1.41183,-1.41181,-1.4118,-1.41179,-1.41178,-1.41177,-1.41176,-1.41175,-1.41175,-1.41174,-1.41173,-1.41173,-1.41172,-1.41172,-1.41171,-1.41171,-1.41171,-1.4117,-1.4117,-1.4117,-1.41169,-1.41169,-1.41169,-1.41168,-1.41168,-1.41168,-1.41167,-1.41167,-1.41167]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.411673
INFO: iteration 2, average log likelihood -1.411620
INFO: iteration 3, average log likelihood -1.411571
INFO: iteration 4, average log likelihood -1.411513
INFO: iteration 5, average log likelihood -1.411443
INFO: iteration 6, average log likelihood -1.411359
INFO: iteration 7, average log likelihood -1.411259
INFO: iteration 8, average log likelihood -1.411149
INFO: iteration 9, average log likelihood -1.411034
INFO: iteration 10, average log likelihood -1.410921
INFO: iteration 11, average log likelihood -1.410814
INFO: iteration 12, average log likelihood -1.410716
INFO: iteration 13, average log likelihood -1.410626
INFO: iteration 14, average log likelihood -1.410547
INFO: iteration 15, average log likelihood -1.410476
INFO: iteration 16, average log likelihood -1.410415
INFO: iteration 17, average log likelihood -1.410361
INFO: iteration 18, average log likelihood -1.410315
INFO: iteration 19, average log likelihood -1.410273
INFO: iteration 20, average log likelihood -1.410236
INFO: iteration 21, average log likelihood -1.410203
INFO: iteration 22, average log likelihood -1.410172
INFO: iteration 23, average log likelihood -1.410143
INFO: iteration 24, average log likelihood -1.410115
INFO: iteration 25, average log likelihood -1.410089
INFO: iteration 26, average log likelihood -1.410063
INFO: iteration 27, average log likelihood -1.410037
INFO: iteration 28, average log likelihood -1.410012
INFO: iteration 29, average log likelihood -1.409988
INFO: iteration 30, average log likelihood -1.409963
INFO: iteration 31, average log likelihood -1.409939
INFO: iteration 32, average log likelihood -1.409915
INFO: iteration 33, average log likelihood -1.409892
INFO: iteration 34, average log likelihood -1.409869
INFO: iteration 35, average log likelihood -1.409846
INFO: iteration 36, average log likelihood -1.409823
INFO: iteration 37, average log likelihood -1.409801
INFO: iteration 38, average log likelihood -1.409780
INFO: iteration 39, average log likelihood -1.409759
INFO: iteration 40, average log likelihood -1.409739
INFO: iteration 41, average log likelihood -1.409719
INFO: iteration 42, average log likelihood -1.409700
INFO: iteration 43, average log likelihood -1.409681
INFO: iteration 44, average log likelihood -1.409664
INFO: iteration 45, average log likelihood -1.409646
INFO: iteration 46, average log likelihood -1.409630
INFO: iteration 47, average log likelihood -1.409614
INFO: iteration 48, average log likelihood -1.409599
INFO: iteration 49, average log likelihood -1.409585
INFO: iteration 50, average log likelihood -1.409571
INFO: EM with 100000 data points 50 iterations avll -1.409571
118.1 data points per parameter
4: avll = [-1.41167,-1.41162,-1.41157,-1.41151,-1.41144,-1.41136,-1.41126,-1.41115,-1.41103,-1.41092,-1.41081,-1.41072,-1.41063,-1.41055,-1.41048,-1.41042,-1.41036,-1.41031,-1.41027,-1.41024,-1.4102,-1.41017,-1.41014,-1.41012,-1.41009,-1.41006,-1.41004,-1.41001,-1.40999,-1.40996,-1.40994,-1.40992,-1.40989,-1.40987,-1.40985,-1.40982,-1.4098,-1.40978,-1.40976,-1.40974,-1.40972,-1.4097,-1.40968,-1.40966,-1.40965,-1.40963,-1.40961,-1.4096,-1.40958,-1.40957]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.409567
INFO: iteration 2, average log likelihood -1.409501
INFO: iteration 3, average log likelihood -1.409439
INFO: iteration 4, average log likelihood -1.409369
INFO: iteration 5, average log likelihood -1.409286
INFO: iteration 6, average log likelihood -1.409186
INFO: iteration 7, average log likelihood -1.409068
INFO: iteration 8, average log likelihood -1.408934
INFO: iteration 9, average log likelihood -1.408790
INFO: iteration 10, average log likelihood -1.408641
INFO: iteration 11, average log likelihood -1.408494
INFO: iteration 12, average log likelihood -1.408354
INFO: iteration 13, average log likelihood -1.408222
INFO: iteration 14, average log likelihood -1.408103
INFO: iteration 15, average log likelihood -1.407995
INFO: iteration 16, average log likelihood -1.407898
INFO: iteration 17, average log likelihood -1.407813
INFO: iteration 18, average log likelihood -1.407737
INFO: iteration 19, average log likelihood -1.407670
INFO: iteration 20, average log likelihood -1.407610
INFO: iteration 21, average log likelihood -1.407555
INFO: iteration 22, average log likelihood -1.407506
INFO: iteration 23, average log likelihood -1.407460
INFO: iteration 24, average log likelihood -1.407418
INFO: iteration 25, average log likelihood -1.407378
INFO: iteration 26, average log likelihood -1.407341
INFO: iteration 27, average log likelihood -1.407305
INFO: iteration 28, average log likelihood -1.407271
INFO: iteration 29, average log likelihood -1.407239
INFO: iteration 30, average log likelihood -1.407208
INFO: iteration 31, average log likelihood -1.407177
INFO: iteration 32, average log likelihood -1.407148
INFO: iteration 33, average log likelihood -1.407121
INFO: iteration 34, average log likelihood -1.407094
INFO: iteration 35, average log likelihood -1.407068
INFO: iteration 36, average log likelihood -1.407042
INFO: iteration 37, average log likelihood -1.407018
INFO: iteration 38, average log likelihood -1.406995
INFO: iteration 39, average log likelihood -1.406972
INFO: iteration 40, average log likelihood -1.406950
INFO: iteration 41, average log likelihood -1.406928
INFO: iteration 42, average log likelihood -1.406908
INFO: iteration 43, average log likelihood -1.406888
INFO: iteration 44, average log likelihood -1.406868
INFO: iteration 45, average log likelihood -1.406850
INFO: iteration 46, average log likelihood -1.406831
INFO: iteration 47, average log likelihood -1.406814
INFO: iteration 48, average log likelihood -1.406797
INFO: iteration 49, average log likelihood -1.406780
INFO: iteration 50, average log likelihood -1.406764
INFO: EM with 100000 data points 50 iterations avll -1.406764
59.0 data points per parameter
5: avll = [-1.40957,-1.4095,-1.40944,-1.40937,-1.40929,-1.40919,-1.40907,-1.40893,-1.40879,-1.40864,-1.40849,-1.40835,-1.40822,-1.4081,-1.40799,-1.4079,-1.40781,-1.40774,-1.40767,-1.40761,-1.40756,-1.40751,-1.40746,-1.40742,-1.40738,-1.40734,-1.40731,-1.40727,-1.40724,-1.40721,-1.40718,-1.40715,-1.40712,-1.40709,-1.40707,-1.40704,-1.40702,-1.40699,-1.40697,-1.40695,-1.40693,-1.40691,-1.40689,-1.40687,-1.40685,-1.40683,-1.40681,-1.4068,-1.40678,-1.40676]
[-1.41972,-1.41974,-1.41967,-1.41962,-1.41957,-1.4195,-1.41942,-1.4193,-1.41911,-1.41874,-1.41804,-1.41694,-1.41571,-1.41482,-1.41438,-1.4142,-1.41413,-1.4141,-1.41409,-1.41409,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.41408,-1.4141,-1.41403,-1.41398,-1.41392,-1.41384,-1.41376,-1.41366,-1.41357,-1.41348,-1.41341,-1.41335,-1.41331,-1.41327,-1.41325,-1.41323,-1.41321,-1.4132,-1.41319,-1.41318,-1.41317,-1.41316,-1.41315,-1.41314,-1.41313,-1.41312,-1.41311,-1.4131,-1.4131,-1.41309,-1.41308,-1.41307,-1.41306,-1.41305,-1.41304,-1.41303,-1.41302,-1.41301,-1.41301,-1.413,-1.41299,-1.41298,-1.41298,-1.41297,-1.41296,-1.41296,-1.41295,-1.41295,-1.41294,-1.41294,-1.41293,-1.41294,-1.41288,-1.41283,-1.41278,-1.41272,-1.41264,-1.41256,-1.41247,-1.41238,-1.41229,-1.41221,-1.41214,-1.41208,-1.41202,-1.41198,-1.41195,-1.41192,-1.4119,-1.41187,-1.41186,-1.41184,-1.41183,-1.41181,-1.4118,-1.41179,-1.41178,-1.41177,-1.41176,-1.41175,-1.41175,-1.41174,-1.41173,-1.41173,-1.41172,-1.41172,-1.41171,-1.41171,-1.41171,-1.4117,-1.4117,-1.4117,-1.41169,-1.41169,-1.41169,-1.41168,-1.41168,-1.41168,-1.41167,-1.41167,-1.41167,-1.41167,-1.41162,-1.41157,-1.41151,-1.41144,-1.41136,-1.41126,-1.41115,-1.41103,-1.41092,-1.41081,-1.41072,-1.41063,-1.41055,-1.41048,-1.41042,-1.41036,-1.41031,-1.41027,-1.41024,-1.4102,-1.41017,-1.41014,-1.41012,-1.41009,-1.41006,-1.41004,-1.41001,-1.40999,-1.40996,-1.40994,-1.40992,-1.40989,-1.40987,-1.40985,-1.40982,-1.4098,-1.40978,-1.40976,-1.40974,-1.40972,-1.4097,-1.40968,-1.40966,-1.40965,-1.40963,-1.40961,-1.4096,-1.40958,-1.40957,-1.40957,-1.4095,-1.40944,-1.40937,-1.40929,-1.40919,-1.40907,-1.40893,-1.40879,-1.40864,-1.40849,-1.40835,-1.40822,-1.4081,-1.40799,-1.4079,-1.40781,-1.40774,-1.40767,-1.40761,-1.40756,-1.40751,-1.40746,-1.40742,-1.40738,-1.40734,-1.40731,-1.40727,-1.40724,-1.40721,-1.40718,-1.40715,-1.40712,-1.40709,-1.40707,-1.40704,-1.40702,-1.40699,-1.40697,-1.40695,-1.40693,-1.40691,-1.40689,-1.40687,-1.40685,-1.40683,-1.40681,-1.4068,-1.40678,-1.40676]
32×26 Array{Float64,2}:
  0.109826    -0.232177   -0.193516    0.0701302  -0.0617024   0.267307   -0.184349   -0.225191    -0.10621      0.201088   -0.0795824     0.0845376   0.602731   -0.0624891   0.143019    0.0257551  -0.0995345  -0.838568    0.264038    -0.535978   -0.0247948    -0.0935381   0.419325    -0.546196     0.203383    -0.145465  
 -0.0696448   -0.585216    0.193548    0.228591    0.227868    0.644334   -0.459278    0.0730239    0.155274    -0.0214202  -0.0271729    -0.168531    0.148978   -0.164385    0.400373   -0.0682792  -0.0393102   0.126376    0.534315     0.363846   -0.163048     -0.368914   -0.021437    -0.858237     0.418162    -0.578703  
  0.144742    -0.293167   -0.179059    0.0454929   0.055301    0.18144     0.183116    0.189543     0.330397    -0.0143824  -0.080933     -0.214445   -0.0920662   0.146919    0.112246   -0.101803   -0.0245025  -0.203718   -0.056299    -0.0877872  -0.0097405    -0.0838298   0.20377     -0.154888    -0.0828682    0.161598  
 -0.112779    -0.118347    0.163733   -0.0587342  -0.0331198   0.0234141  -0.157124   -0.0988311   -0.122663    -0.114125    0.0696723     0.146363    0.0753509  -0.131591   -0.0403654   0.0108017   0.0181734  -0.0431088   0.00825991   0.0265739  -0.0304271    -0.103624    0.00590662  -0.0765249   -0.0125366   -0.0393453 
 -0.0672489    0.174612   -0.29616     0.0355599  -0.0194781   0.616059    0.197802   -0.561216     0.228572    -0.526089    0.445613      0.35233    -0.36124     0.408299    0.290143    0.420888   -0.495058   -0.0852601  -0.131036     0.107562   -0.04711       0.0317536   0.471283     0.124055     0.426012    -0.14797   
  0.35911      0.606354   -0.137199    0.301254    0.364122   -0.252674    0.270296    0.331801     0.292476    -0.73933     0.196631     -0.0271262  -0.556798    0.185662   -0.0119734   0.0817737  -0.0898652  -0.422279   -0.613047    -0.312671   -0.128509      0.312201    0.466053    -0.337444     0.432955    -0.297489  
  0.00724345   0.357969   -0.278511    0.12586     0.231091   -0.031807   -0.42754     0.0726926    0.0632676    0.671276    0.178807      0.0992944   0.203145    0.276548   -0.138342   -0.495601   -0.479921    0.122664   -0.196709     0.15419    -0.042704      0.335173   -0.48169     -0.0699309    0.106678    -0.265276  
  0.0395617    0.632416   -0.170378   -0.27301     0.0251692  -0.140684    0.393992   -0.11879     -0.0692825    0.11276     0.0694098    -0.0343481  -0.170028    0.141807   -0.0485342   0.232674   -0.178363    0.112612   -0.26047     -0.0893488   0.0642527     0.0787798  -0.260598     0.447909     0.00557384  -0.123513  
 -0.0189859   -0.13857     0.184159   -0.422085   -0.663562   -0.17393     0.11628    -0.614086    -0.105179    -0.210002   -0.241309      0.0278506  -0.0437414  -0.230296   -0.0269644   0.352995    0.548293    0.0200393   0.216502    -0.207438   -0.357645     -0.0871027   0.452728     0.622102    -0.161338     0.0245824 
 -0.0691208   -0.0912518  -0.178028    0.158637   -0.207936    0.0187228  -0.161137   -0.307142     0.4942      -0.334581   -0.45343      -0.414003   -0.0520833  -0.0293552  -0.642814    0.164779    0.103965    0.539029    0.140783     0.397734    0.182449      0.0829342  -0.262898     0.420842    -0.286708     0.319028  
 -0.333628     0.358717    0.703579    0.496541    0.0232024  -0.155921   -0.359518    0.0295001    0.0668029   -0.0277463   0.692605     -0.0641997   0.0262808   0.121608   -0.103878    0.128008    0.68992     0.305079    0.165565    -0.219909   -0.437365      0.446213    0.00605588   0.204579     0.159939    -0.118002  
 -0.141671    -0.0688142   0.322019    0.479334    0.346339   -0.252952    0.051774   -0.213995    -0.293401    -0.0045244  -0.000311451  -0.137959    0.0446633  -0.250076   -0.346087    0.144183    0.155408    0.15148     0.532574     0.0140747   0.588009     -0.231868    0.109295     0.154584     0.10176     -0.00945662
 -0.170545     0.113173   -0.127288   -0.0779945   0.214616    0.0969925   0.245202    0.764284    -0.210871    -0.240231    0.620449     -0.427068   -0.149362    0.333401    0.379471   -0.184546    0.358796    0.181239    0.122146     0.0879483   0.000214084  -0.0864164  -0.407179    -0.23021     -0.620404     0.331795  
 -0.350414    -0.565665   -0.132433   -0.394922   -0.254262    0.118785    0.133836   -0.300399    -0.315365     0.871795   -0.402495     -0.172465    0.101724   -0.0235917   0.187813   -0.211963    0.0298082   0.22358     0.316292     0.628969    0.43652      -0.488657   -0.673944     0.0459155   -0.30637      0.359553  
 -0.0247801   -0.110551    0.445928   -0.325751   -0.284258   -0.462109   -0.606373    0.525654    -0.141789     0.265919    0.016764      0.244626    0.302562   -0.37686     0.040419   -0.494901    0.174983    0.0599823  -0.473046     0.139851    0.032337      0.203113   -0.474772    -0.166162    -0.434716     0.0373259 
  0.555655     0.230679   -0.163213   -0.126612    0.167053   -0.620262   -0.182033    0.774813     0.00215478   0.509994    0.0200785     0.309608   -0.0791002  -0.712114   -0.311441    0.251722    0.266765   -0.423383   -0.2847       0.238555   -0.3787        1.09939    -0.0315291    0.426004     0.163536    -0.196161  
 -0.113159    -0.478511   -0.222008   -0.271541   -0.0567455   0.237627    0.12063    -0.443274    -0.08262     -0.326213   -0.833784      0.341965   -0.0504718  -0.248855   -0.45756    -0.104272   -0.235789   -0.494799   -0.195928    -0.156477    0.300068     -0.566936    0.256973    -0.368758    -0.051536    -0.142598  
  0.0355798    0.124312   -0.846777    0.0176195  -0.49249     0.662887    0.617169    0.123694    -0.813753     0.231555   -0.5426        0.0684302  -0.395953   -0.406648   -0.373061    0.586772   -0.165346    0.220679   -0.392097    -0.88329     0.163627     -0.220693   -0.403735     0.00807441   0.130817    -0.0698927 
  0.14489     -0.647657   -0.525236    0.186592   -0.102442   -0.132925   -0.0502902   0.657254     0.52312     -0.513072    0.198079     -0.659241    0.0506443  -0.225011    0.590757   -0.0680415   0.17118    -0.100052    0.241212     0.447642    0.0768251     0.0731848   0.730159     0.228868     0.243901     0.491345  
 -0.26007     -0.353106   -0.0573494   0.206581   -0.668849   -0.331649    0.111612    0.281429    -0.379073    -0.624924   -0.381698     -0.324863    0.142395   -0.303709    0.405465    0.542307    0.0527706  -0.312124    0.536752     0.309267    0.0451783     0.149294   -0.356674    -0.526906     0.352819     0.585599  
 -0.745016    -0.655281    0.744114    0.274565   -0.0567877  -0.0421026  -0.262807    0.274952     0.164467    -0.398436    0.0533973     0.0721791   0.0949855  -0.812333   -0.262905    0.244897   -0.0409914   0.0458557  -0.269817    -0.0429695  -0.238253      0.263093   -0.0235223    0.057789    -0.285484     0.663734  
 -0.304543    -0.313687    0.251311    0.454921    0.150375    0.420777    0.328749   -0.456715     0.191749    -0.56572    -0.00952876   -0.0777417  -0.0511145   0.369727   -0.263956    0.0673115   0.155204    0.067889    0.189721    -0.403954    0.198593     -0.741601    0.0926123   -0.0729995   -0.338031     0.321822  
  0.0830745    0.157994   -0.0807788   0.0486032  -0.28375    -0.118903    0.559939    0.08784      0.536777     0.66273    -0.900994     -0.907689    0.3572      0.128894   -0.16826    -0.648466    0.447232    0.214737    0.374148     0.0497429   0.437899     -0.121461    0.213725    -0.410666     0.212134    -0.472423  
  0.45399      0.591842    0.956646    0.32984    -0.309458    0.0767044   0.568766   -0.18698      0.150807     0.032943    0.069526     -0.74176     0.245283   -0.0337241   0.396167    0.127173    0.0688555  -0.337103    0.413966    -0.137663    0.0815006    -0.383031   -0.0796754   -0.307824    -0.616367    -0.166306  
 -0.673642    -0.611883   -0.545052   -0.217321    0.138345    0.0514516  -0.541526    0.056684    -0.114063    -0.0530114   0.0779606     0.6335     -0.177098    0.0175477  -0.29492    -0.213911   -0.205955    0.397291   -0.548486     0.147416   -0.067907      0.167875   -0.112731     0.184241     0.396285     0.228973  
  0.334958     0.194215    0.294833   -0.153058    0.81085     0.292296   -0.467811   -0.440296     0.238653     0.579943    0.189041      0.367733   -0.0390326   0.24632    -0.66833    -0.0219495   0.189089    0.288706   -0.464451    -0.275224   -0.104723     -0.42044     0.393542     0.569784    -0.082134    -0.291925  
  0.0802679    0.636583    0.145017   -0.298385    0.0803044  -0.528828    0.107383   -0.033699    -0.153659    -0.0526344   0.174175     -0.107309   -0.408153    0.286476   -0.257294    0.305644    0.023862    0.564087   -0.33687      0.315409    0.104573      0.414045   -0.561647     1.06226     -0.157577     0.179644  
  0.0418001    1.4306      0.0249792   0.357851    0.0551592  -0.652394    0.0989076   0.0314021   -0.577503     0.317774   -0.0152872    -0.147042    0.133695    0.282196   -0.501162   -0.39482    -0.0146004   0.0974611  -0.100263    -0.618646    0.250743      0.379929   -0.206043    -0.0919302   -0.346351    -0.362888  
  0.240399     0.0168115   0.0620186  -0.275245    0.125917   -0.732539    0.0298652  -0.125952    -0.241802     0.190958    0.428925     -0.148186    0.143662   -0.05897     0.936694   -0.223687   -0.229235   -0.341195    0.427354     0.279011    0.0714662     0.336366    0.533636     0.0246571    0.223418    -0.308293  
 -0.339067    -0.325506    0.142013   -0.519653    0.30627    -0.107754    0.557042   -0.450514    -0.236591     0.483086    0.490299      1.11843    -0.402691   -0.470759    0.55827    -0.24345     0.032609   -0.858803    0.232473    -0.152187   -0.253016     -0.212986   -0.438912     0.137208    -0.270992     0.0983573 
 -0.0286904   -0.117146    0.134811   -0.199781    0.0825907  -0.092327   -0.370659    0.00375325   0.110558     0.348701    0.0954367     0.0277285   0.212134   -0.241513    0.164971   -0.0662882   0.0195836  -0.0673371   0.286542     0.399217    0.0105262     0.0225569   0.0271344   -0.067514     0.264089    -0.268604  
  0.462567     0.255317   -0.406884    0.0903427  -0.34178     0.0449164   0.0695443   0.38316      0.3118       0.46211     0.0404542    -0.0354673   0.246563    0.638336    0.414464   -0.332204   -0.556455   -0.352495   -0.509211    -0.0611599  -0.110626      0.350165   -0.283471    -0.296265     0.149804    -0.16776   INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.406749
INFO: iteration 2, average log likelihood -1.406734
INFO: iteration 3, average log likelihood -1.406719
INFO: iteration 4, average log likelihood -1.406705
INFO: iteration 5, average log likelihood -1.406692
INFO: iteration 6, average log likelihood -1.406679
INFO: iteration 7, average log likelihood -1.406666
INFO: iteration 8, average log likelihood -1.406654
INFO: iteration 9, average log likelihood -1.406642
INFO: iteration 10, average log likelihood -1.406630
INFO: EM with 100000 data points 10 iterations avll -1.406630
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.014640e+05
      1       7.120306e+05      -1.894333e+05 |       32
      2       6.891003e+05      -2.293031e+04 |       32
      3       6.829879e+05      -6.112416e+03 |       32
      4       6.802298e+05      -2.758131e+03 |       32
      5       6.785487e+05      -1.681108e+03 |       32
      6       6.773389e+05      -1.209741e+03 |       32
      7       6.764646e+05      -8.743602e+02 |       32
      8       6.757741e+05      -6.904878e+02 |       32
      9       6.751731e+05      -6.009916e+02 |       32
     10       6.746761e+05      -4.969931e+02 |       32
     11       6.742384e+05      -4.376685e+02 |       32
     12       6.738243e+05      -4.141715e+02 |       32
     13       6.734625e+05      -3.617819e+02 |       32
     14       6.731682e+05      -2.942370e+02 |       32
     15       6.729159e+05      -2.523003e+02 |       32
     16       6.727042e+05      -2.117820e+02 |       32
     17       6.725090e+05      -1.951518e+02 |       32
     18       6.723304e+05      -1.786025e+02 |       32
     19       6.721836e+05      -1.468500e+02 |       32
     20       6.720551e+05      -1.284356e+02 |       32
     21       6.719459e+05      -1.091995e+02 |       32
     22       6.718500e+05      -9.589971e+01 |       32
     23       6.717618e+05      -8.824434e+01 |       32
     24       6.716777e+05      -8.406413e+01 |       32
     25       6.715878e+05      -8.991239e+01 |       32
     26       6.715046e+05      -8.316120e+01 |       32
     27       6.714290e+05      -7.559406e+01 |       32
     28       6.713701e+05      -5.898310e+01 |       32
     29       6.713045e+05      -6.555153e+01 |       32
     30       6.712342e+05      -7.032996e+01 |       32
     31       6.711667e+05      -6.752139e+01 |       32
     32       6.711004e+05      -6.631026e+01 |       32
     33       6.710408e+05      -5.950666e+01 |       32
     34       6.709805e+05      -6.034996e+01 |       32
     35       6.709109e+05      -6.961263e+01 |       32
     36       6.708335e+05      -7.742908e+01 |       32
     37       6.707553e+05      -7.815506e+01 |       32
     38       6.706762e+05      -7.905980e+01 |       32
     39       6.705949e+05      -8.135338e+01 |       32
     40       6.705147e+05      -8.017675e+01 |       32
     41       6.704409e+05      -7.384589e+01 |       32
     42       6.703827e+05      -5.811494e+01 |       32
     43       6.703291e+05      -5.359789e+01 |       32
     44       6.702786e+05      -5.059037e+01 |       32
     45       6.702332e+05      -4.540098e+01 |       32
     46       6.701886e+05      -4.457577e+01 |       32
     47       6.701416e+05      -4.701139e+01 |       32
     48       6.700980e+05      -4.361781e+01 |       32
     49       6.700549e+05      -4.300304e+01 |       32
     50       6.700134e+05      -4.152881e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 670013.4200835901)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.419170
INFO: iteration 2, average log likelihood -1.413975
INFO: iteration 3, average log likelihood -1.412524
INFO: iteration 4, average log likelihood -1.411414
INFO: iteration 5, average log likelihood -1.410253
INFO: iteration 6, average log likelihood -1.409216
INFO: iteration 7, average log likelihood -1.408522
INFO: iteration 8, average log likelihood -1.408136
INFO: iteration 9, average log likelihood -1.407916
INFO: iteration 10, average log likelihood -1.407771
INFO: iteration 11, average log likelihood -1.407662
INFO: iteration 12, average log likelihood -1.407573
INFO: iteration 13, average log likelihood -1.407497
INFO: iteration 14, average log likelihood -1.407430
INFO: iteration 15, average log likelihood -1.407370
INFO: iteration 16, average log likelihood -1.407316
INFO: iteration 17, average log likelihood -1.407266
INFO: iteration 18, average log likelihood -1.407220
INFO: iteration 19, average log likelihood -1.407178
INFO: iteration 20, average log likelihood -1.407138
INFO: iteration 21, average log likelihood -1.407101
INFO: iteration 22, average log likelihood -1.407067
INFO: iteration 23, average log likelihood -1.407034
INFO: iteration 24, average log likelihood -1.407003
INFO: iteration 25, average log likelihood -1.406974
INFO: iteration 26, average log likelihood -1.406947
INFO: iteration 27, average log likelihood -1.406921
INFO: iteration 28, average log likelihood -1.406896
INFO: iteration 29, average log likelihood -1.406873
INFO: iteration 30, average log likelihood -1.406851
INFO: iteration 31, average log likelihood -1.406830
INFO: iteration 32, average log likelihood -1.406810
INFO: iteration 33, average log likelihood -1.406791
INFO: iteration 34, average log likelihood -1.406773
INFO: iteration 35, average log likelihood -1.406755
INFO: iteration 36, average log likelihood -1.406739
INFO: iteration 37, average log likelihood -1.406723
INFO: iteration 38, average log likelihood -1.406708
INFO: iteration 39, average log likelihood -1.406693
INFO: iteration 40, average log likelihood -1.406679
INFO: iteration 41, average log likelihood -1.406665
INFO: iteration 42, average log likelihood -1.406652
INFO: iteration 43, average log likelihood -1.406640
INFO: iteration 44, average log likelihood -1.406628
INFO: iteration 45, average log likelihood -1.406616
INFO: iteration 46, average log likelihood -1.406605
INFO: iteration 47, average log likelihood -1.406594
INFO: iteration 48, average log likelihood -1.406583
INFO: iteration 49, average log likelihood -1.406573
INFO: iteration 50, average log likelihood -1.406563
INFO: EM with 100000 data points 50 iterations avll -1.406563
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.147546    -0.0273226    0.0429395    0.0638464    0.097883      0.12268      0.0783898   -0.779154    -0.0203138   0.211319    -0.170797    -0.234598     0.243911    -0.0380649  -0.0436327    0.109089   -0.0948377   -0.20406     0.594003   -0.0669395    0.0118252  -0.116031    0.22295      0.0445579    0.316149   -0.38017  
  0.106957     0.76583     -0.192088     0.270589     0.152498      0.0928195    0.348481    -0.0798211    0.331736   -0.632394     0.460737     0.229943    -0.421602     0.353259    0.274719     0.295516   -0.197079    -0.32249    -0.39622    -0.284604    -0.239315    0.430424    0.484982    -0.015055     0.492858   -0.294175 
 -0.232543     0.0365199    0.371216     0.182796    -0.0325118    -0.234894    -0.175289     0.0277372   -0.135684   -0.0177757    0.175414    -3.26271e-5   0.0749864   -0.192612   -0.155404     0.0122057   0.351444     0.163263    0.118415   -0.00581055   0.110285    0.0480307  -0.054028     0.105525    -0.119795    0.0701777
 -0.753236     0.409535    -0.581256    -0.39863     -0.178653      0.485717     0.188836     0.0910024   -0.773658    0.406081    -0.584266     0.672626    -0.185983    -0.0396665  -0.660598     0.255469    0.00470816   0.0833127  -0.217314   -0.84594     -0.28811    -0.373913   -0.490404    -0.146806    -0.0830453   0.0237249
  0.550132    -0.138984    -0.87944      0.0144092   -0.408825      0.44726      0.476068    -0.150509    -0.0544711   0.0876174   -0.392667    -0.0999784    0.143208     0.107919    0.143743     0.0985838  -0.475971    -0.354607   -0.166553   -0.377461     0.360714   -0.0563212   0.0581688   -0.337562     0.246984   -0.184133 
  0.00484075   0.138433     0.0667641    0.0932397   -0.000555895   0.00422059   0.181475     0.572793    -0.0550518  -0.0813506    0.626646    -0.789076     0.116951     0.233295    0.51764     -0.0279819   0.326275     0.121547    0.54911     0.188455    -0.0259259   0.0306632  -0.333224    -0.219597    -0.525353    0.389009 
  0.0630243   -0.0114734    0.30492      0.301893    -0.451583      0.126759     0.340796     0.158668     1.04738     0.264354    -0.677973    -0.916538     0.118658     0.229461   -0.276289    -0.509551    0.397096     0.182931   -0.175269   -0.0214817    0.0999954   0.0640642   0.0797157   -0.288804    -0.163339   -0.398113 
  0.298409     0.689375    -0.181724    -0.0218544   -0.136827     -0.768235     0.542357    -0.263641     0.0509889  -0.281957     0.0503056   -0.041375    -0.291306     0.212024   -0.456737    -0.0393876   0.0214017   -0.268321   -0.692959   -0.32633      0.471847    0.406776   -0.0939086    0.932191    -0.854162    0.618527 
 -0.00661736  -0.487629    -0.102078     0.10123     -0.465328      0.0136888   -0.212833     0.238806    -0.0649555  -0.372713    -0.444052    -0.301794     0.228201    -0.329602    0.400119     0.3237      0.0895499   -0.3762      0.427374   -0.171847    -0.265397    0.132885    0.254165    -0.625374     0.24021     0.203846 
 -0.0829933   -0.278096     0.6549       0.544613     0.216542      0.302473     0.138814    -0.414481     0.108469   -0.63241     -0.0631852    0.033387     0.130362     0.0610798  -0.22571      0.0859004   0.311144    -0.158125    0.364888   -0.511727     0.112968   -0.780438    0.21544     -0.385437    -0.329224    0.0579319
  0.473323     0.214302     0.130308    -0.219304    -0.0400681    -0.575423    -0.188612     0.826128    -0.0583705   0.467403     0.141122     0.25908     -0.0193315   -0.588058   -0.123199     0.0319758   0.305685    -0.346126   -0.412746    0.176209    -0.405485    0.928798   -0.0971834    0.217639     0.0366767  -0.266217 
 -0.563215    -0.774874    -0.0822511    0.179531     0.122688      0.153187    -0.426787     0.406731     0.0971792  -0.47812      0.344183     0.43474     -0.275801    -0.189945   -0.154036     0.181028   -0.135455     0.0350252  -0.583315    0.156938    -0.182479    0.1476     -0.139381     0.0616676    0.0788409   0.6504   
 -0.27534     -0.481766     0.0163255   -0.912332    -0.363309     -0.202269     0.259219    -0.33026     -0.213201    0.207611     0.0621432    0.500617     0.0197305   -0.380449    0.682496    -0.168639   -0.022891    -0.507022    0.101642    0.0990658   -0.121778   -0.115229   -0.0329404    0.155754    -0.419786   -0.0234598
 -0.0936668   -0.695311    -0.277776     0.173566     0.0902825    -0.0437213    0.909109     0.412204     0.0741499  -0.207943    -0.346148    -0.467735     0.0157848   -0.340725    0.118854    -0.0947718  -0.102414    -0.348893    0.438631    0.349477     0.696446   -0.575792    0.441884     0.0750377    0.072625    0.609607 
 -0.265725     0.00851703   0.368238    -0.0423848   -0.055391     -0.140128    -0.257481     0.112047     0.0662803   0.07976      0.256864    -0.011548     0.0683217   -0.0629268   0.0733014   -0.0200828   0.264485     0.170069    0.139915    0.216241    -0.14814     0.0865997  -0.143515     0.275975    -0.154098    0.10241  
 -0.542086    -0.189895    -0.176259     0.00245175  -0.0622104     0.757066     0.21409     -0.743045     0.423786   -0.235638     0.0246937    0.073854    -0.284693     0.478064   -0.00891901   0.0581754  -0.234239     0.467943    0.118457    0.210852     0.312741   -0.586255   -0.00244134  -0.0775115   -0.253943    0.157001 
 -0.100231    -0.296538    -0.139856    -0.135421    -0.0158652     0.100762    -0.451869    -0.584543     0.0103413   0.131097    -0.400231     0.740851     0.255879    -0.183652   -0.579932     0.0597709  -0.40548     -0.212983   -0.375302   -0.0623823    0.124631   -0.183761    0.535368     0.11248      0.61265    -0.199951 
  0.212181     0.104994    -0.128201    -0.155004    -0.119091     -0.0148966    0.681068     0.154497    -0.361774    0.00445614   0.195692    -0.4513      -0.538774     0.40746    -0.323332     0.254096    0.155948     0.462692   -0.614737   -0.421285    -0.191187   -0.528993   -0.0903548    0.578933     0.135174    0.467883 
 -0.0181601    0.414361    -0.0246256   -0.213319     0.265393     -0.26303     -0.222393     0.00710701  -0.0330538   0.281356     0.33462      0.127226    -0.0936726    0.427487    0.056291     0.0708101  -0.101336     0.616562   -0.231563    0.480321    -0.0392477   0.444647   -0.635252     0.778634     0.158046   -0.0533988
  0.0178162   -0.321942     0.296969     0.197293     0.615316     -0.520659     0.00227943  -0.0427456   -0.652512    0.325913     0.86065      0.873365    -0.0833769    0.0213737   0.823674    -0.229305   -0.141862    -0.698793    0.242178   -0.160353    -0.183852    0.314954   -0.0182043   -0.333441     0.744415   -0.0706347
 -0.167668     0.336069     0.220395    -0.487706     0.375431     -0.0266063    0.632149    -0.31065     -0.492962    0.0482277   -0.133187    -0.066133    -0.8664      -0.428395   -0.354973     0.619548    0.0641809    0.0976588   0.0831142  -0.0155158    0.387581   -0.191808   -0.206849     0.00589052  -0.176037   -0.818676 
  0.0489772   -0.0351962   -0.206902    -0.134539     0.028378      0.071298     0.0412908    0.00495062   0.091753   -0.0154502   -0.00871008   0.0436939   -0.0998609    0.0711952  -0.0247453    0.0241152  -0.151761    -0.0717889  -0.204041    0.0220294    0.0109126   0.0170975   0.0179344    0.0849688    0.0122103   0.0110189
 -0.27785      0.274371    -0.466166     0.00899764   0.196957     -0.43148     -0.689664     0.271398    -0.0436169   0.586625     0.233535     0.0994672    0.380948    -0.0539805  -0.328313    -1.06707    -0.541452     0.0373321  -0.39546    -0.0843884    0.158678    0.336917   -0.563585    -0.120286    -0.295142   -0.205688 
  0.614656     0.110798     0.0411655   -0.121164     0.343699     -0.586242    -0.374723    -0.0714807    0.494064    0.030363     0.327614    -0.733256     0.152472    -0.107227    0.765518    -0.317944   -0.417003    -0.421067    0.204276    0.857165     0.0612275   0.195807    1.01226     -0.17054      0.568771   -0.21951  
  0.118622     0.183433    -0.250559     0.0160836    0.147278      0.20577      0.527235     0.716908    -0.162741    0.180222    -0.117355    -0.105672     0.00825025  -0.185633    0.28572      0.0876966  -0.514674     0.0905164   0.0214736   0.219083     0.715089   -0.0431098  -0.47975     -0.444473     0.339836   -0.203214 
 -0.10937      0.0180634    0.0277838    0.0905188   -0.424095     -0.394447    -0.0149972   -0.50411      0.255874   -0.436283    -0.259244    -0.219131    -0.166549    -0.222384   -0.354816     0.519726    0.311397     0.437002    0.337877    0.157779    -0.292455    0.154614    0.22293      0.939073    -0.07326     0.419522 
  0.0925968    0.869793    -0.24224      0.472195    -0.0194638    -0.415536    -0.773049     0.416182    -0.312739   -0.333724    -0.538301    -0.380965    -0.019079     0.504267   -0.703949     0.0490368  -0.0933828    0.563459   -0.398559   -0.213939    -0.0873698   0.525156   -0.28395     -0.286355     0.357635   -0.454403 
 -0.191478    -0.634451     0.0417649    0.011245     0.238107      0.504182    -0.808838     0.123374     0.200289    0.397628     0.0002373   -0.0102304    0.281487    -0.219389    0.332208    -0.24328    -0.0227063    0.294279    0.481993    0.677276    -0.160262   -0.241036   -0.211152    -0.590822     0.697811   -0.730507 
 -0.185956    -0.558653    -0.0915416   -0.215223    -0.324296      0.0600685   -0.180599    -0.291658    -0.22103     0.389872    -0.825424    -0.354718     0.288913    -0.235195   -0.383299    -0.256777    0.26347      0.244983    0.163102    0.398723     0.512375   -0.194456   -0.643057     0.0944601   -0.287707    0.511307 
  0.163967    -0.105231     0.00170671   0.0890466    0.238657      0.286606    -0.122393     0.208912     0.148377    0.0229423    0.197981     0.0739992    0.112529     0.265147    0.248731    -0.351535   -0.0851717   -0.406934   -0.0773889  -0.173399    -0.189354   -0.250357    0.113204    -0.623252    -0.162791   -0.142254 
  0.363095     1.2573       0.365146     0.164824    -0.303935     -0.527365     0.530337    -0.108518    -0.481204    0.637495    -0.0551472   -0.463545     0.307559     0.342608    0.217386    -0.352432    0.0714177   -0.195431    0.257165   -0.473827     0.255741    0.199854    0.0961914   -0.177235    -0.275077   -0.49635  
  0.528224     0.378801     0.310205    -0.0654783    0.987997      0.366477    -0.473265    -0.291857     0.376138    0.681832     0.460449     0.324974    -0.0413738    0.290732   -0.701362    -0.0980324   0.398544     0.356016   -0.650038   -0.335933    -0.172662   -0.242815    0.389573     0.724713    -0.275508   -0.272859 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.406553
INFO: iteration 2, average log likelihood -1.406544
INFO: iteration 3, average log likelihood -1.406534
INFO: iteration 4, average log likelihood -1.406525
INFO: iteration 5, average log likelihood -1.406517
INFO: iteration 6, average log likelihood -1.406508
INFO: iteration 7, average log likelihood -1.406500
INFO: iteration 8, average log likelihood -1.406491
INFO: iteration 9, average log likelihood -1.406483
INFO: iteration 10, average log likelihood -1.406475
INFO: EM with 100000 data points 10 iterations avll -1.406475
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
