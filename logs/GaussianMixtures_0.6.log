>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.6.6
INFO: Installing JLD v0.6.5
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.1
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.2.0
INFO: Installing StaticArrays v0.0.11
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1202
Commit 42a22b6 (2016-11-06 22:14 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-100-generic #147-Ubuntu SMP Tue Oct 18 16:48:51 UTC 2016 x86_64 x86_64
Memory: 2.939289093017578 GB (651.28515625 MB free)
Uptime: 25341.0 sec
Load Avg:  1.015625  1.0146484375  1.04541015625
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3499 MHz    1568185 s       3444 s     188716 s     519442 s         44 s
#2  3499 MHz     626345 s       3418 s      92882 s    1728294 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.3
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.6.6
 - JLD                           0.6.5
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.1
 - Rmath                         0.1.4
 - SHA                           0.2.1
 - ScikitLearnBase               0.2.0
 - StaticArrays                  0.0.11
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##773#775{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##773#775{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##773#775{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##773#775{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##773#775{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##773#775{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-930276.8060477403,[1051.39,98948.6],
[369.214 -299.156 2723.55; -474.655 -177.74 -3086.54],

Array{Float64,2}[
[1123.46 -162.484 825.601; -162.484 1119.52 -670.596; 825.601 -670.596 7205.11],

[99278.8 -180.772 -357.988; -180.772 99432.0 476.337; -357.988 476.337 93216.6]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.573823e+03
      1       1.263600e+03      -3.102233e+02 |        6
      2       1.219420e+03      -4.417975e+01 |        2
      3       1.211694e+03      -7.726513e+00 |        0
      4       1.211694e+03       0.000000e+00 |        0
K-means converged with 4 iterations (objv = 1211.693928218609)
INFO: K-means with 272 data points using 4 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.083088
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.626059
INFO: iteration 2, lowerbound -3.458741
INFO: iteration 3, lowerbound -3.284472
INFO: iteration 4, lowerbound -3.095588
INFO: dropping number of Gaussions to 7
INFO: iteration 5, lowerbound -2.900795
INFO: dropping number of Gaussions to 6
INFO: iteration 6, lowerbound -2.708629
INFO: iteration 7, lowerbound -2.551778
INFO: dropping number of Gaussions to 5
INFO: iteration 8, lowerbound -2.444297
INFO: dropping number of Gaussions to 4
INFO: iteration 9, lowerbound -2.373552
INFO: iteration 10, lowerbound -2.333628
INFO: dropping number of Gaussions to 3
INFO: iteration 11, lowerbound -2.313929
INFO: iteration 12, lowerbound -2.308202
INFO: dropping number of Gaussions to 2
INFO: iteration 13, lowerbound -2.302919
INFO: iteration 14, lowerbound -2.299260
INFO: iteration 15, lowerbound -2.299256
INFO: iteration 16, lowerbound -2.299255
INFO: iteration 17, lowerbound -2.299254
INFO: iteration 18, lowerbound -2.299253
INFO: iteration 19, lowerbound -2.299253
INFO: iteration 20, lowerbound -2.299253
INFO: iteration 21, lowerbound -2.299253
INFO: iteration 22, lowerbound -2.299253
INFO: iteration 23, lowerbound -2.299253
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Tue 08 Nov 2016 12:31:49 PM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Tue 08 Nov 2016 12:31:50 PM UTC: K-means with 272 data points using 4 iterations
11.3 data points per parameter
,Tue 08 Nov 2016 12:31:52 PM UTC: EM with 272 data points 0 iterations avll -2.083088
5.8 data points per parameter
,Tue 08 Nov 2016 12:31:53 PM UTC: GMM converted to Variational GMM
,Tue 08 Nov 2016 12:31:55 PM UTC: iteration 1, lowerbound -3.626059
,Tue 08 Nov 2016 12:31:55 PM UTC: iteration 2, lowerbound -3.458741
,Tue 08 Nov 2016 12:31:55 PM UTC: iteration 3, lowerbound -3.284472
,Tue 08 Nov 2016 12:31:55 PM UTC: iteration 4, lowerbound -3.095588
,Tue 08 Nov 2016 12:31:55 PM UTC: dropping number of Gaussions to 7
,Tue 08 Nov 2016 12:31:55 PM UTC: iteration 5, lowerbound -2.900795
,Tue 08 Nov 2016 12:31:55 PM UTC: dropping number of Gaussions to 6
,Tue 08 Nov 2016 12:31:55 PM UTC: iteration 6, lowerbound -2.708629
,Tue 08 Nov 2016 12:31:55 PM UTC: iteration 7, lowerbound -2.551778
,Tue 08 Nov 2016 12:31:56 PM UTC: dropping number of Gaussions to 5
,Tue 08 Nov 2016 12:31:56 PM UTC: iteration 8, lowerbound -2.444297
,Tue 08 Nov 2016 12:31:56 PM UTC: dropping number of Gaussions to 4
,Tue 08 Nov 2016 12:31:56 PM UTC: iteration 9, lowerbound -2.373552
,Tue 08 Nov 2016 12:31:56 PM UTC: iteration 10, lowerbound -2.333628
,Tue 08 Nov 2016 12:31:56 PM UTC: dropping number of Gaussions to 3
,Tue 08 Nov 2016 12:31:56 PM UTC: iteration 11, lowerbound -2.313929
,Tue 08 Nov 2016 12:31:56 PM UTC: iteration 12, lowerbound -2.308202
,Tue 08 Nov 2016 12:31:56 PM UTC: dropping number of Gaussions to 2
,Tue 08 Nov 2016 12:31:56 PM UTC: iteration 13, lowerbound -2.302919
,Tue 08 Nov 2016 12:31:56 PM UTC: iteration 14, lowerbound -2.299260
,Tue 08 Nov 2016 12:31:56 PM UTC: iteration 15, lowerbound -2.299256
,Tue 08 Nov 2016 12:31:56 PM UTC: iteration 16, lowerbound -2.299255
,Tue 08 Nov 2016 12:31:56 PM UTC: iteration 17, lowerbound -2.299254
,Tue 08 Nov 2016 12:31:56 PM UTC: iteration 18, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:56 PM UTC: iteration 19, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:56 PM UTC: iteration 20, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:56 PM UTC: iteration 21, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:56 PM UTC: iteration 22, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:57 PM UTC: iteration 23, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:57 PM UTC: iteration 24, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:57 PM UTC: iteration 25, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:57 PM UTC: iteration 26, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:57 PM UTC: iteration 27, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:57 PM UTC: iteration 28, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:57 PM UTC: iteration 29, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:57 PM UTC: iteration 30, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:57 PM UTC: iteration 31, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:57 PM UTC: iteration 32, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:57 PM UTC: iteration 33, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:57 PM UTC: iteration 34, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:57 PM UTC: iteration 35, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:57 PM UTC: iteration 36, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:57 PM UTC: iteration 37, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:57 PM UTC: iteration 38, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:57 PM UTC: iteration 39, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:57 PM UTC: iteration 40, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:58 PM UTC: iteration 41, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:58 PM UTC: iteration 42, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:58 PM UTC: iteration 43, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:58 PM UTC: iteration 44, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:58 PM UTC: iteration 45, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:58 PM UTC: iteration 46, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:58 PM UTC: iteration 47, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:58 PM UTC: iteration 48, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:58 PM UTC: iteration 49, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:58 PM UTC: iteration 50, lowerbound -2.299253
,Tue 08 Nov 2016 12:31:58 PM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.9549,178.045]
β = [95.9549,178.045]
m = [2.00023 53.852; 4.2503 79.2869]
ν = [97.9549,180.045]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.375876 -0.00895312; 0.0 0.0127487],

[0.184042 -0.00764405; 0.0 0.00858171]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -0.9819793698910542
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9819793698910536
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9819793698910536
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 99999.99999999999
avll from stats: -1.0226542007059234
avll from llpg:  -1.0226542007059234
avll direct:     -1.0226542007059234
sum posterior: 100000.0
32×26 Array{Float64,2}:
 -0.0427113    0.00645924   0.0729444   -0.124727     0.108123    -0.0603652   -0.0328183    0.0746394    0.15317      0.154606    -0.26994      -0.00529497    0.0183613   -0.152886     -0.0987948   -0.0723532    0.0105161    -0.138228     -0.0695627   -0.0651722  -0.214966     -0.108782     0.0556283    0.12781      0.102343    -0.0268658 
  0.079869     0.0804796   -0.0522136    0.0379085    0.0559797   -0.0939116   -0.0826976   -0.0235419   -0.0808255   -0.11583     -0.0107013    -0.0168077     0.0013299   -0.00212946    0.0542356    0.0825443    0.00317192   -0.205377      0.0189619   -0.106989    0.04025       0.0152787   -0.0803572    0.0658639    0.00310429  -0.0574382 
  0.106044     0.0513659   -0.00739317  -0.0401248    0.0344001   -0.0651437   -0.0548998   -0.0898444   -0.119195    -0.11206     -0.0102794    -0.0425726     0.0652356    0.0276068    -0.104434     0.0425582   -0.0282436    -0.0614206     0.0769279   -0.083943    0.10733       0.0147141    0.0718993   -0.211025    -0.0882985   -0.167981  
 -0.113645     0.207158     0.0760974   -0.0368434    0.0324876    0.0104629    0.014485     0.098432    -0.0225694   -0.0516453   -0.112037      0.0696653    -0.169406    -0.000689219   0.0342564    0.0502725   -0.117505      0.026117      0.051104    -0.0130102  -0.0783094    -0.101981    -0.180028    -0.14429      0.0881775    0.16378   
  0.0676069   -0.106919     0.0341998    0.0893981   -0.127519     0.11257      0.0685028    0.0156163   -0.0212774    0.0368225   -0.00112235   -0.0265151    -0.131116    -0.0173611    -0.0877404    0.0829027    0.130031      0.156944     -0.0665783    0.101954   -0.0555826    -0.048224     0.0791271   -0.0937613    0.0630161   -0.0701997 
 -0.24459      0.0183621   -0.150092    -0.0724628   -0.0680187   -0.14097     -0.00504066   0.0585269    0.0951528   -0.1124       0.0561871    -0.0818211     0.153082    -0.0845183     0.101089     0.0295841    0.0166706    -0.139825      0.00368482  -0.0537129   0.0527257    -0.126301    -0.0854698    0.0466102   -0.162187    -0.10014   
 -0.171962     0.0551834   -0.0241142   -0.0795402   -0.19429     -0.084753    -0.160147     0.0419727    0.0549586    0.0442799   -0.037956     -0.0905107     0.00956425  -0.13081      -0.0372349   -0.124001    -0.056836      0.0486818    -0.0606355   -0.0586518  -0.000615043  -0.0512798   -0.0123152   -0.0746567   -0.0969655   -0.0745913 
  0.102353    -0.0992681    0.117025     0.0239115    0.0533316    0.0616218    0.0480831   -0.0789339   -0.202948     0.20086     -0.0300455    -0.0649476    -0.137164     0.0533247     0.177306     0.00304813  -0.000662528   0.0770258    -0.159637     0.137096    0.035213     -0.0219892   -0.271093     0.108843    -0.0363258   -0.0135281 
 -0.209068     0.0230751    0.150605    -0.131502    -0.0365896   -0.155865    -0.129159     0.10114      0.120093    -0.0314699   -0.0333816     0.169661      0.0312383    0.226458     -0.0599345   -0.0220285   -0.137071     -0.0451382     0.0103645    0.0275266  -0.046733      0.0646661    0.035096     0.0349938    0.0135654   -0.0527309 
 -0.0890127   -0.0289579   -0.0541412   -0.0237111   -0.0949752    0.0999159    0.0665714   -0.114314     0.056097    -0.0131437   -0.0183444     0.130131      0.0299183    0.106964      0.146655     0.0773876    0.127187     -0.0106527    -0.0342954   -0.025271   -0.180275     -0.105753     0.00372011  -0.193591    -0.0712666    0.0996047 
  0.0605379    0.0398776    0.00974026  -0.0549167    0.0524077   -0.134724     0.127791     0.0161763    0.157997    -0.238681     0.183217      0.113967      0.0656676   -0.0900018    -0.0475667   -0.0851261    0.00215926   -0.0244347     0.0359847    0.0267334   0.112591      0.0936117    0.0264571   -0.0295697   -0.0297206    0.0201329 
 -0.0586142    0.123242    -0.0827587    0.011163     0.00707848  -0.0138471   -0.170389    -0.187887    -0.0674611    0.122091    -0.00302201    0.122404     -0.00347698  -0.0371693     0.0443889   -0.0779972   -0.0940274     0.070862     -0.0445819    0.152625   -0.045461     -0.00554273  -0.0683884   -0.122151    -0.0576757   -0.144331  
  0.0383012   -0.0568702   -0.0872125   -0.104678     0.0255701   -0.151918    -0.0436236    0.00903705  -0.0704449    0.142337     0.0774888    -0.0159023     0.0448004    0.0737286    -0.308468     0.209302    -0.0250991     0.0129133    -0.182872     0.0382181   0.0705749     0.0568465    0.0681164    0.110759     0.0275849    0.011548  
 -0.0888832    0.0100782   -0.0280955    0.0688599   -0.165592     0.074622    -0.054269     0.00386386   0.00392929  -0.0882051   -0.132065      0.0215715     0.232276     0.0552102    -0.20539     -0.199542    -0.0183788     0.0375321    -0.0517444    0.134539   -0.166788      0.0258924   -0.0813469    0.0500344    0.124663    -0.13345   
 -0.0170904    0.109326    -0.0101905   -0.0171504   -0.226822    -0.141584    -0.0359465    0.0102046   -0.0827193   -0.125229     0.0193086    -0.0933646     0.00381517  -0.0045386     0.150747    -0.0444558    0.0428685     0.134494     -0.0998284    0.150178   -0.0368501     0.118434    -0.0825561   -0.00995099  -0.0381656    0.00756373
 -0.00694567  -0.0316058    0.129255     0.0213795    0.0606211    0.00738601   0.00676038  -0.0524957   -0.132624     0.107658    -0.106637     -0.0925251    -0.108374     0.0499282    -0.187602    -0.192316     0.0538785     0.0403582    -0.0268918   -0.109025    0.164831      0.018809    -0.0913887    0.0430356   -0.0607846    0.0356491 
  0.0809182    0.0526143   -0.0332545    0.0372579   -0.00685147  -0.0930177    0.0424674   -0.00190367  -0.0268466   -0.0387715    0.0096253    -0.000268988   0.0182286   -0.0436563     0.103209     0.0699003    0.0454955    -0.0106869    -0.0354828   -0.132281    0.0667033     0.0439611   -0.0978863    0.0396835   -0.118813     0.100593  
 -0.0186048   -0.108364    -0.108223     0.0460012   -0.155435     0.0381767   -0.247342     0.0124521   -0.0461757    0.023046    -0.0164323    -0.137755      0.0137026    0.0820479    -0.049403    -0.0470967    0.00799323   -0.000731472   0.0957593   -0.0793922   0.0573656    -0.0735559   -0.0992553    0.103661    -0.219285     0.0452834 
 -0.125107     0.0486997   -0.0290575    0.0580495    0.0562892   -0.0100443   -0.0815253   -0.0984615   -0.0490874   -0.0451238    0.0843772     0.140019     -0.0207308   -0.094631      0.0347961    0.103517    -0.0455086    -0.0985929    -0.025658     0.0135268  -0.0771186    -0.116076     0.0728484   -0.0930287   -0.0960093    0.015716  
 -0.136794     0.0725759    0.0347833    0.00797685  -0.0642972    0.0230119    0.0179369    0.241762    -0.0404125    0.0422335   -0.166534     -0.0724484     0.00733199  -0.049102      0.0642053    0.010707    -0.0494598     0.183371      0.0115352    0.230838    0.216751     -0.0745777    0.104369     0.0974044   -0.0265421    0.0357121 
 -0.0491057   -0.00679999  -0.0657604   -0.0561946    0.0478005    0.0832791    0.157137    -0.0634288   -0.030845    -0.0936348    0.0560945    -0.13719      -0.00406052   0.0894733     0.00635272  -0.0660129   -0.0525088    -0.199535     -0.0850689    0.0654916   0.0328115     0.0403107   -0.111225    -0.0202365   -0.12865     -0.0569927 
  0.0155101    0.041525     0.11456      0.00339027  -0.0384032    0.0245783   -0.0216026    0.0858852   -0.071008    -0.0878138    0.0259465     0.110271      0.0267181   -0.0380734     0.109462     0.00138187   0.0513754    -0.0375123     0.0775059    0.0365279   0.0480765    -0.0168235    0.0323099    0.0429296   -0.088174    -0.124976  
  0.132336     0.069795    -0.0431356    0.104064     0.17632      0.068639    -0.0249095   -0.168754    -0.0190553   -0.164545     0.0165478    -0.0172207     0.0621316   -0.0637742    -0.232461     0.0453021    0.00465111    0.113815      0.0281947    0.0199501   0.0826222     0.124612    -0.123543     0.166331     0.0873882   -0.130928  
 -0.0434157    0.0232318   -0.0308769   -0.11741     -0.0535868   -0.0420123   -0.106319    -0.125491     0.0427177    0.0309852    0.0650459    -0.0457049    -0.0250049    0.0572436     0.080583    -0.0341737    0.134894     -0.000360592   0.0872863    0.0659421  -0.0519094    -0.0212522   -0.018055    -0.0540355   -0.155297     0.141584  
 -0.0279329    0.0179671   -0.0283927    0.112019    -0.0410708    0.0176556    0.027424    -0.0402046    0.132067     0.0610156   -0.000120315   0.0485554     0.0551371   -0.141278      0.0439342   -0.0361392    0.104914     -0.0736633     0.0407983   -0.135588   -0.0486242    -0.0620401   -0.15336      0.184725    -0.0529486   -0.123253  
  0.0744445   -0.0679242    0.0116266   -0.115349     0.0190236    0.119942     0.111112    -0.040171     0.0410988    0.120111    -0.0320794    -0.188441     -0.190784    -0.0395765     0.033821    -0.0602829    0.0457569    -0.0244205    -0.082142     0.0908086   0.0384203    -0.115753     0.0403219    0.171931     0.0581986    0.195281  
  0.0777134   -0.0492116   -0.0402126   -0.266112     0.122647     0.107368     0.0839803    0.153558     0.118829    -0.0651432   -0.118657     -0.00854307    0.206279    -0.0738695    -0.0104366    0.0268454    0.0922348    -0.0589995     0.107642     0.0141104  -0.136813      0.0821959   -0.11645     -0.00767688   0.19575      0.110136  
 -0.0602037   -0.139399    -0.166135     0.08354     -0.180064    -0.0751092   -0.0743171   -0.0223639   -0.00234355   0.0528369    0.00547035   -0.110189      0.129931    -0.119036     -0.0608876    0.0372771   -0.140989     -0.0139419    -0.0569905   -0.127517    0.1019       -0.145305     0.167513    -0.0677253   -0.103274     0.0897566 
 -0.0335851    0.0300951   -0.0673507    0.0189021    0.188797    -0.0480158   -0.0677638   -0.0802923   -0.039828     0.114317     0.0411561     0.104978      0.12438      0.122699     -0.0285703    0.052347     0.0377451     0.0656069     0.00883119   0.0130055  -0.203421     -0.0182162   -0.00673095   0.0916164    0.0614374    0.0618654 
 -0.124554     0.00380023   0.054102     0.148324    -0.103075    -0.0309867    0.00231319   0.164846    -0.105848    -0.00122045   0.0423675     0.177895     -0.0227099    0.116197      0.242448     0.219931     0.0014889    -0.107574     -0.060428     0.0574167  -0.166955      0.00891784  -0.0594037    0.174375    -0.102486    -0.0468201 
 -0.0505435    0.189687    -0.212849     0.264986     0.0872264    0.292109     0.0254801    0.087841     0.0196648   -0.0329256   -0.0572025     0.247989      0.133673     0.0369109     0.0174107    0.0671028   -0.0239103    -0.137332     -0.111037    -0.148662    0.0894812    -0.0469534    0.128847    -0.0414266    0.0265177    0.0447534 
  0.100749    -0.0248846    0.130644    -0.169821    -0.0999706    0.0252771   -0.0437724   -0.176142    -0.0708703    0.0161094   -0.0850028    -0.126207      0.0326417   -0.0220222     0.0895663    0.0924915   -0.0757986    -0.129457      0.0674914    0.139375    0.0557841     0.101398     0.0864964   -0.0799349   -0.0203395    0.123865  kind diag, method split
0: avll = -1.4088134710053686
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.408916
INFO: iteration 2, average log likelihood -1.408817
INFO: iteration 3, average log likelihood -1.408063
INFO: iteration 4, average log likelihood -1.400860
INFO: iteration 5, average log likelihood -1.384641
INFO: iteration 6, average log likelihood -1.377416
INFO: iteration 7, average log likelihood -1.374436
INFO: iteration 8, average log likelihood -1.372721
INFO: iteration 9, average log likelihood -1.371914
INFO: iteration 10, average log likelihood -1.371484
INFO: iteration 11, average log likelihood -1.371197
INFO: iteration 12, average log likelihood -1.370987
INFO: iteration 13, average log likelihood -1.370840
INFO: iteration 14, average log likelihood -1.370727
INFO: iteration 15, average log likelihood -1.370630
INFO: iteration 16, average log likelihood -1.370543
INFO: iteration 17, average log likelihood -1.370454
INFO: iteration 18, average log likelihood -1.370357
INFO: iteration 19, average log likelihood -1.370247
INFO: iteration 20, average log likelihood -1.370113
INFO: iteration 21, average log likelihood -1.369959
INFO: iteration 22, average log likelihood -1.369790
INFO: iteration 23, average log likelihood -1.369635
INFO: iteration 24, average log likelihood -1.369509
INFO: iteration 25, average log likelihood -1.369415
INFO: iteration 26, average log likelihood -1.369342
INFO: iteration 27, average log likelihood -1.369285
INFO: iteration 28, average log likelihood -1.369245
INFO: iteration 29, average log likelihood -1.369217
INFO: iteration 30, average log likelihood -1.369195
INFO: iteration 31, average log likelihood -1.369178
INFO: iteration 32, average log likelihood -1.369165
INFO: iteration 33, average log likelihood -1.369154
INFO: iteration 34, average log likelihood -1.369145
INFO: iteration 35, average log likelihood -1.369137
INFO: iteration 36, average log likelihood -1.369130
INFO: iteration 37, average log likelihood -1.369124
INFO: iteration 38, average log likelihood -1.369119
INFO: iteration 39, average log likelihood -1.369114
INFO: iteration 40, average log likelihood -1.369110
INFO: iteration 41, average log likelihood -1.369106
INFO: iteration 42, average log likelihood -1.369103
INFO: iteration 43, average log likelihood -1.369100
INFO: iteration 44, average log likelihood -1.369097
INFO: iteration 45, average log likelihood -1.369095
INFO: iteration 46, average log likelihood -1.369093
INFO: iteration 47, average log likelihood -1.369091
INFO: iteration 48, average log likelihood -1.369090
INFO: iteration 49, average log likelihood -1.369088
INFO: iteration 50, average log likelihood -1.369087
INFO: EM with 100000 data points 50 iterations avll -1.369087
952.4 data points per parameter
1: avll = [-1.40892,-1.40882,-1.40806,-1.40086,-1.38464,-1.37742,-1.37444,-1.37272,-1.37191,-1.37148,-1.3712,-1.37099,-1.37084,-1.37073,-1.37063,-1.37054,-1.37045,-1.37036,-1.37025,-1.37011,-1.36996,-1.36979,-1.36963,-1.36951,-1.36942,-1.36934,-1.36929,-1.36925,-1.36922,-1.36919,-1.36918,-1.36916,-1.36915,-1.36915,-1.36914,-1.36913,-1.36912,-1.36912,-1.36911,-1.36911,-1.36911,-1.3691,-1.3691,-1.3691,-1.36909,-1.36909,-1.36909,-1.36909,-1.36909,-1.36909]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.369253
INFO: iteration 2, average log likelihood -1.369129
INFO: iteration 3, average log likelihood -1.368795
INFO: iteration 4, average log likelihood -1.364717
INFO: iteration 5, average log likelihood -1.348001
INFO: iteration 6, average log likelihood -1.333084
INFO: iteration 7, average log likelihood -1.328042
INFO: iteration 8, average log likelihood -1.325882
INFO: iteration 9, average log likelihood -1.324855
INFO: iteration 10, average log likelihood -1.324313
INFO: iteration 11, average log likelihood -1.323988
INFO: iteration 12, average log likelihood -1.323768
INFO: iteration 13, average log likelihood -1.323599
INFO: iteration 14, average log likelihood -1.323453
INFO: iteration 15, average log likelihood -1.323316
INFO: iteration 16, average log likelihood -1.323178
INFO: iteration 17, average log likelihood -1.323031
INFO: iteration 18, average log likelihood -1.322865
INFO: iteration 19, average log likelihood -1.322677
INFO: iteration 20, average log likelihood -1.322469
INFO: iteration 21, average log likelihood -1.322250
INFO: iteration 22, average log likelihood -1.322020
INFO: iteration 23, average log likelihood -1.321789
INFO: iteration 24, average log likelihood -1.321574
INFO: iteration 25, average log likelihood -1.321376
INFO: iteration 26, average log likelihood -1.321200
INFO: iteration 27, average log likelihood -1.321039
INFO: iteration 28, average log likelihood -1.320903
INFO: iteration 29, average log likelihood -1.320791
INFO: iteration 30, average log likelihood -1.320699
INFO: iteration 31, average log likelihood -1.320625
INFO: iteration 32, average log likelihood -1.320563
INFO: iteration 33, average log likelihood -1.320509
INFO: iteration 34, average log likelihood -1.320460
INFO: iteration 35, average log likelihood -1.320418
INFO: iteration 36, average log likelihood -1.320384
INFO: iteration 37, average log likelihood -1.320361
INFO: iteration 38, average log likelihood -1.320347
INFO: iteration 39, average log likelihood -1.320337
INFO: iteration 40, average log likelihood -1.320331
INFO: iteration 41, average log likelihood -1.320325
INFO: iteration 42, average log likelihood -1.320321
INFO: iteration 43, average log likelihood -1.320318
INFO: iteration 44, average log likelihood -1.320315
INFO: iteration 45, average log likelihood -1.320313
INFO: iteration 46, average log likelihood -1.320311
INFO: iteration 47, average log likelihood -1.320310
INFO: iteration 48, average log likelihood -1.320308
INFO: iteration 49, average log likelihood -1.320307
INFO: iteration 50, average log likelihood -1.320306
INFO: EM with 100000 data points 50 iterations avll -1.320306
473.9 data points per parameter
2: avll = [-1.36925,-1.36913,-1.36879,-1.36472,-1.348,-1.33308,-1.32804,-1.32588,-1.32485,-1.32431,-1.32399,-1.32377,-1.3236,-1.32345,-1.32332,-1.32318,-1.32303,-1.32286,-1.32268,-1.32247,-1.32225,-1.32202,-1.32179,-1.32157,-1.32138,-1.3212,-1.32104,-1.3209,-1.32079,-1.3207,-1.32062,-1.32056,-1.32051,-1.32046,-1.32042,-1.32038,-1.32036,-1.32035,-1.32034,-1.32033,-1.32033,-1.32032,-1.32032,-1.32032,-1.32031,-1.32031,-1.32031,-1.32031,-1.32031,-1.32031]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.320496
INFO: iteration 2, average log likelihood -1.320316
INFO: iteration 3, average log likelihood -1.319615
INFO: iteration 4, average log likelihood -1.311945
INFO: iteration 5, average log likelihood -1.289887
INFO: iteration 6, average log likelihood -1.276727
INFO: iteration 7, average log likelihood -1.272535
INFO: iteration 8, average log likelihood -1.270813
INFO: iteration 9, average log likelihood -1.269874
INFO: iteration 10, average log likelihood -1.269269
INFO: iteration 11, average log likelihood -1.268851
INFO: iteration 12, average log likelihood -1.268544
INFO: iteration 13, average log likelihood -1.268302
INFO: iteration 14, average log likelihood -1.268076
INFO: iteration 15, average log likelihood -1.267814
INFO: iteration 16, average log likelihood -1.267484
INFO: iteration 17, average log likelihood -1.267074
INFO: iteration 18, average log likelihood -1.266601
INFO: iteration 19, average log likelihood -1.266100
INFO: iteration 20, average log likelihood -1.265636
INFO: iteration 21, average log likelihood -1.265283
INFO: iteration 22, average log likelihood -1.265068
INFO: iteration 23, average log likelihood -1.264944
INFO: iteration 24, average log likelihood -1.264869
INFO: iteration 25, average log likelihood -1.264823
INFO: iteration 26, average log likelihood -1.264793
INFO: iteration 27, average log likelihood -1.264775
INFO: iteration 28, average log likelihood -1.264764
INFO: iteration 29, average log likelihood -1.264756
INFO: iteration 30, average log likelihood -1.264752
INFO: iteration 31, average log likelihood -1.264749
INFO: iteration 32, average log likelihood -1.264747
INFO: iteration 33, average log likelihood -1.264746
INFO: iteration 34, average log likelihood -1.264745
INFO: iteration 35, average log likelihood -1.264744
INFO: iteration 36, average log likelihood -1.264744
INFO: iteration 37, average log likelihood -1.264743
INFO: iteration 38, average log likelihood -1.264743
INFO: iteration 39, average log likelihood -1.264742
INFO: iteration 40, average log likelihood -1.264742
INFO: iteration 41, average log likelihood -1.264742
INFO: iteration 42, average log likelihood -1.264741
INFO: iteration 43, average log likelihood -1.264741
INFO: iteration 44, average log likelihood -1.264741
INFO: iteration 45, average log likelihood -1.264741
INFO: iteration 46, average log likelihood -1.264740
INFO: iteration 47, average log likelihood -1.264740
INFO: iteration 48, average log likelihood -1.264740
INFO: iteration 49, average log likelihood -1.264739
INFO: iteration 50, average log likelihood -1.264739
INFO: EM with 100000 data points 50 iterations avll -1.264739
236.4 data points per parameter
3: avll = [-1.3205,-1.32032,-1.31962,-1.31194,-1.28989,-1.27673,-1.27254,-1.27081,-1.26987,-1.26927,-1.26885,-1.26854,-1.2683,-1.26808,-1.26781,-1.26748,-1.26707,-1.2666,-1.2661,-1.26564,-1.26528,-1.26507,-1.26494,-1.26487,-1.26482,-1.26479,-1.26477,-1.26476,-1.26476,-1.26475,-1.26475,-1.26475,-1.26475,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.264986
INFO: iteration 2, average log likelihood -1.264758
INFO: iteration 3, average log likelihood -1.264203
INFO: iteration 4, average log likelihood -1.257283
INFO: iteration 5, average log likelihood -1.228302
WARNING: Variances had to be floored 9 15
INFO: iteration 6, average log likelihood -1.200192
WARNING: Variances had to be floored 10
INFO: iteration 7, average log likelihood -1.203510
INFO: iteration 8, average log likelihood -1.195594
WARNING: Variances had to be floored 9 15
INFO: iteration 9, average log likelihood -1.184424
INFO: iteration 10, average log likelihood -1.195518
WARNING: Variances had to be floored 10
INFO: iteration 11, average log likelihood -1.183607
WARNING: Variances had to be floored 9 15
INFO: iteration 12, average log likelihood -1.184390
INFO: iteration 13, average log likelihood -1.195045
INFO: iteration 14, average log likelihood -1.183261
WARNING: Variances had to be floored 9 10 15
INFO: iteration 15, average log likelihood -1.177693
INFO: iteration 16, average log likelihood -1.197116
INFO: iteration 17, average log likelihood -1.189717
WARNING: Variances had to be floored 15
INFO: iteration 18, average log likelihood -1.180768
WARNING: Variances had to be floored 9 10
INFO: iteration 19, average log likelihood -1.184455
INFO: iteration 20, average log likelihood -1.191780
WARNING: Variances had to be floored 15
INFO: iteration 21, average log likelihood -1.187556
INFO: iteration 22, average log likelihood -1.187735
WARNING: Variances had to be floored 9 10
INFO: iteration 23, average log likelihood -1.179345
WARNING: Variances had to be floored 15
INFO: iteration 24, average log likelihood -1.189754
INFO: iteration 25, average log likelihood -1.194589
INFO: iteration 26, average log likelihood -1.182652
WARNING: Variances had to be floored 9 10 15
INFO: iteration 27, average log likelihood -1.177350
INFO: iteration 28, average log likelihood -1.196800
INFO: iteration 29, average log likelihood -1.189543
WARNING: Variances had to be floored 15
INFO: iteration 30, average log likelihood -1.180659
WARNING: Variances had to be floored 9 10
INFO: iteration 31, average log likelihood -1.184411
INFO: iteration 32, average log likelihood -1.191741
WARNING: Variances had to be floored 15
INFO: iteration 33, average log likelihood -1.187536
INFO: iteration 34, average log likelihood -1.187724
WARNING: Variances had to be floored 9 10
INFO: iteration 35, average log likelihood -1.179343
WARNING: Variances had to be floored 15
INFO: iteration 36, average log likelihood -1.189746
INFO: iteration 37, average log likelihood -1.194586
INFO: iteration 38, average log likelihood -1.182649
WARNING: Variances had to be floored 9 10 15
INFO: iteration 39, average log likelihood -1.177357
INFO: iteration 40, average log likelihood -1.196798
INFO: iteration 41, average log likelihood -1.189541
WARNING: Variances had to be floored 15
INFO: iteration 42, average log likelihood -1.180661
WARNING: Variances had to be floored 9 10
INFO: iteration 43, average log likelihood -1.184418
INFO: iteration 44, average log likelihood -1.191739
WARNING: Variances had to be floored 15
INFO: iteration 45, average log likelihood -1.187537
INFO: iteration 46, average log likelihood -1.187726
WARNING: Variances had to be floored 9 10
INFO: iteration 47, average log likelihood -1.179346
WARNING: Variances had to be floored 15
INFO: iteration 48, average log likelihood -1.189746
INFO: iteration 49, average log likelihood -1.194586
INFO: iteration 50, average log likelihood -1.182649
INFO: EM with 100000 data points 50 iterations avll -1.182649
118.1 data points per parameter
4: avll = [-1.26499,-1.26476,-1.2642,-1.25728,-1.2283,-1.20019,-1.20351,-1.19559,-1.18442,-1.19552,-1.18361,-1.18439,-1.19505,-1.18326,-1.17769,-1.19712,-1.18972,-1.18077,-1.18445,-1.19178,-1.18756,-1.18774,-1.17934,-1.18975,-1.19459,-1.18265,-1.17735,-1.1968,-1.18954,-1.18066,-1.18441,-1.19174,-1.18754,-1.18772,-1.17934,-1.18975,-1.19459,-1.18265,-1.17736,-1.1968,-1.18954,-1.18066,-1.18442,-1.19174,-1.18754,-1.18773,-1.17935,-1.18975,-1.19459,-1.18265]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 17 18 19 20 29 30
INFO: iteration 1, average log likelihood -1.177673
WARNING: Variances had to be floored 17 18 19 20 29 30
INFO: iteration 2, average log likelihood -1.177292
WARNING: Variances had to be floored 17 18 19 20 29 30
INFO: iteration 3, average log likelihood -1.175391
WARNING: Variances had to be floored 17 18 19 20 29 30
INFO: iteration 4, average log likelihood -1.154347
WARNING: Variances had to be floored 15 17 18 19 20 29 30
INFO: iteration 5, average log likelihood -1.099803
WARNING: Variances had to be floored 17 18 19 20 22 29 30
INFO: iteration 6, average log likelihood -1.075181
WARNING: Variances had to be floored 11 17 18 19 20 25 29 30
INFO: iteration 7, average log likelihood -1.065899
WARNING: Variances had to be floored 6 15 17 18 19 20 29 30
INFO: iteration 8, average log likelihood -1.067884
WARNING: Variances had to be floored 17 18 19 20 22 29 30
INFO: iteration 9, average log likelihood -1.070901
WARNING: Variances had to be floored 17 18 19 20 29 30
INFO: iteration 10, average log likelihood -1.060736
WARNING: Variances had to be floored 11 15 17 18 19 20 25 29 30
INFO: iteration 11, average log likelihood -1.051155
WARNING: Variances had to be floored 17 18 19 20 22 29 30
INFO: iteration 12, average log likelihood -1.069379
WARNING: Variances had to be floored 17 18 19 20 29 30
INFO: iteration 13, average log likelihood -1.062317
WARNING: Variances had to be floored 15 17 18 19 20 29 30
INFO: iteration 14, average log likelihood -1.052143
WARNING: Variances had to be floored 11 17 18 19 20 22 25 29 30
INFO: iteration 15, average log likelihood -1.056173
WARNING: Variances had to be floored 17 18 19 20 29 30
INFO: iteration 16, average log likelihood -1.069442
WARNING: Variances had to be floored 15 17 18 19 20 29 30
INFO: iteration 17, average log likelihood -1.055205
WARNING: Variances had to be floored 17 18 19 20 22 29 30
INFO: iteration 18, average log likelihood -1.057630
WARNING: Variances had to be floored 11 17 18 19 20 25 29 30
INFO: iteration 19, average log likelihood -1.056720
WARNING: Variances had to be floored 15 17 18 19 20 29 30
INFO: iteration 20, average log likelihood -1.062774
WARNING: Variances had to be floored 17 18 19 20 22 29 30
INFO: iteration 21, average log likelihood -1.061067
WARNING: Variances had to be floored 17 18 19 20 29 30
INFO: iteration 22, average log likelihood -1.058346
WARNING: Variances had to be floored 11 15 17 18 19 20 25 29 30
INFO: iteration 23, average log likelihood -1.050079
WARNING: Variances had to be floored 17 18 19 20 22 29 30
INFO: iteration 24, average log likelihood -1.068654
WARNING: Variances had to be floored 17 18 19 20 29 30
INFO: iteration 25, average log likelihood -1.061736
WARNING: Variances had to be floored 15 17 18 19 20 29 30
INFO: iteration 26, average log likelihood -1.051702
WARNING: Variances had to be floored 11 17 18 19 20 22 25 29 30
INFO: iteration 27, average log likelihood -1.055943
WARNING: Variances had to be floored 17 18 19 20 29 30
INFO: iteration 28, average log likelihood -1.069362
WARNING: Variances had to be floored 15 17 18 19 20 29 30
INFO: iteration 29, average log likelihood -1.055129
WARNING: Variances had to be floored 17 18 19 20 22 29 30
INFO: iteration 30, average log likelihood -1.057549
WARNING: Variances had to be floored 11 17 18 19 20 25 29 30
INFO: iteration 31, average log likelihood -1.056654
WARNING: Variances had to be floored 15 17 18 19 20 29 30
INFO: iteration 32, average log likelihood -1.062764
WARNING: Variances had to be floored 17 18 19 20 22 29 30
INFO: iteration 33, average log likelihood -1.061025
WARNING: Variances had to be floored 17 18 19 20 29 30
INFO: iteration 34, average log likelihood -1.058283
WARNING: Variances had to be floored 11 15 17 18 19 20 25 29 30
INFO: iteration 35, average log likelihood -1.050044
WARNING: Variances had to be floored 17 18 19 20 22 29 30
INFO: iteration 36, average log likelihood -1.068646
WARNING: Variances had to be floored 17 18 19 20 29 30
INFO: iteration 37, average log likelihood -1.061700
WARNING: Variances had to be floored 15 17 18 19 20 29 30
INFO: iteration 38, average log likelihood -1.051667
WARNING: Variances had to be floored 11 17 18 19 20 22 25 29 30
INFO: iteration 39, average log likelihood -1.055917
WARNING: Variances had to be floored 17 18 19 20 29 30
INFO: iteration 40, average log likelihood -1.069344
WARNING: Variances had to be floored 15 17 18 19 20 29 30
INFO: iteration 41, average log likelihood -1.055106
WARNING: Variances had to be floored 17 18 19 20 22 29 30
INFO: iteration 42, average log likelihood -1.057523
WARNING: Variances had to be floored 11 17 18 19 20 25 29 30
INFO: iteration 43, average log likelihood -1.056630
WARNING: Variances had to be floored 15 17 18 19 20 29 30
INFO: iteration 44, average log likelihood -1.062749
WARNING: Variances had to be floored 17 18 19 20 22 29 30
INFO: iteration 45, average log likelihood -1.061006
WARNING: Variances had to be floored 17 18 19 20 29 30
INFO: iteration 46, average log likelihood -1.058259
WARNING: Variances had to be floored 11 15 17 18 19 20 25 29 30
INFO: iteration 47, average log likelihood -1.050024
WARNING: Variances had to be floored 17 18 19 20 22 29 30
INFO: iteration 48, average log likelihood -1.068628
WARNING: Variances had to be floored 17 18 19 20 29 30
INFO: iteration 49, average log likelihood -1.061681
WARNING: Variances had to be floored 15 17 18 19 20 29 30
INFO: iteration 50, average log likelihood -1.051644
INFO: EM with 100000 data points 50 iterations avll -1.051644
59.0 data points per parameter
5: avll = [-1.17767,-1.17729,-1.17539,-1.15435,-1.0998,-1.07518,-1.0659,-1.06788,-1.0709,-1.06074,-1.05115,-1.06938,-1.06232,-1.05214,-1.05617,-1.06944,-1.0552,-1.05763,-1.05672,-1.06277,-1.06107,-1.05835,-1.05008,-1.06865,-1.06174,-1.0517,-1.05594,-1.06936,-1.05513,-1.05755,-1.05665,-1.06276,-1.06103,-1.05828,-1.05004,-1.06865,-1.0617,-1.05167,-1.05592,-1.06934,-1.05511,-1.05752,-1.05663,-1.06275,-1.06101,-1.05826,-1.05002,-1.06863,-1.06168,-1.05164]
[-1.40881,-1.40892,-1.40882,-1.40806,-1.40086,-1.38464,-1.37742,-1.37444,-1.37272,-1.37191,-1.37148,-1.3712,-1.37099,-1.37084,-1.37073,-1.37063,-1.37054,-1.37045,-1.37036,-1.37025,-1.37011,-1.36996,-1.36979,-1.36963,-1.36951,-1.36942,-1.36934,-1.36929,-1.36925,-1.36922,-1.36919,-1.36918,-1.36916,-1.36915,-1.36915,-1.36914,-1.36913,-1.36912,-1.36912,-1.36911,-1.36911,-1.36911,-1.3691,-1.3691,-1.3691,-1.36909,-1.36909,-1.36909,-1.36909,-1.36909,-1.36909,-1.36925,-1.36913,-1.36879,-1.36472,-1.348,-1.33308,-1.32804,-1.32588,-1.32485,-1.32431,-1.32399,-1.32377,-1.3236,-1.32345,-1.32332,-1.32318,-1.32303,-1.32286,-1.32268,-1.32247,-1.32225,-1.32202,-1.32179,-1.32157,-1.32138,-1.3212,-1.32104,-1.3209,-1.32079,-1.3207,-1.32062,-1.32056,-1.32051,-1.32046,-1.32042,-1.32038,-1.32036,-1.32035,-1.32034,-1.32033,-1.32033,-1.32032,-1.32032,-1.32032,-1.32031,-1.32031,-1.32031,-1.32031,-1.32031,-1.32031,-1.3205,-1.32032,-1.31962,-1.31194,-1.28989,-1.27673,-1.27254,-1.27081,-1.26987,-1.26927,-1.26885,-1.26854,-1.2683,-1.26808,-1.26781,-1.26748,-1.26707,-1.2666,-1.2661,-1.26564,-1.26528,-1.26507,-1.26494,-1.26487,-1.26482,-1.26479,-1.26477,-1.26476,-1.26476,-1.26475,-1.26475,-1.26475,-1.26475,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26474,-1.26499,-1.26476,-1.2642,-1.25728,-1.2283,-1.20019,-1.20351,-1.19559,-1.18442,-1.19552,-1.18361,-1.18439,-1.19505,-1.18326,-1.17769,-1.19712,-1.18972,-1.18077,-1.18445,-1.19178,-1.18756,-1.18774,-1.17934,-1.18975,-1.19459,-1.18265,-1.17735,-1.1968,-1.18954,-1.18066,-1.18441,-1.19174,-1.18754,-1.18772,-1.17934,-1.18975,-1.19459,-1.18265,-1.17736,-1.1968,-1.18954,-1.18066,-1.18442,-1.19174,-1.18754,-1.18773,-1.17935,-1.18975,-1.19459,-1.18265,-1.17767,-1.17729,-1.17539,-1.15435,-1.0998,-1.07518,-1.0659,-1.06788,-1.0709,-1.06074,-1.05115,-1.06938,-1.06232,-1.05214,-1.05617,-1.06944,-1.0552,-1.05763,-1.05672,-1.06277,-1.06107,-1.05835,-1.05008,-1.06865,-1.06174,-1.0517,-1.05594,-1.06936,-1.05513,-1.05755,-1.05665,-1.06276,-1.06103,-1.05828,-1.05004,-1.06865,-1.0617,-1.05167,-1.05592,-1.06934,-1.05511,-1.05752,-1.05663,-1.06275,-1.06101,-1.05826,-1.05002,-1.06863,-1.06168,-1.05164]
32×26 Array{Float64,2}:
 -0.247857     0.0392651    -0.173568    -0.0768066    -0.069221     -0.147821     -0.00513626   0.0552547     0.0959576   -0.11477      0.0593753   -0.0960354    0.148867     -0.0920744   0.109299     0.0533439    0.0156674    -0.130901      0.00295743  -0.0744435    0.0507271    -0.0821104  -0.0911615    0.0430501   -0.238055    -0.148322  
 -0.0426009   -0.154068     -0.154205     0.0919033    -0.198609     -0.0879679    -0.0769206   -0.0173733    -0.00385359   0.0237021    0.017072    -0.109963     0.129584     -0.110951   -0.0661493    0.0403536   -0.140334     -0.00908197   -0.0515823   -0.124505     0.107352     -0.134354    0.164595    -0.0635443   -0.0947172    0.10906   
 -0.0130134    0.0226837     0.0984856   -0.0108958    -0.0245738     0.0270736    -0.0274932    0.0518004    -0.0621055   -0.0708976    0.025078     0.105789     0.0228303    -0.0359141   0.095852     0.00429993   0.0529499    -0.030462      0.0771964    0.0556957    0.0439517    -0.0162748   0.0442219    0.03647     -0.0605509   -0.115864  
  0.066142     0.032734     -0.0325114   -0.0542263     0.0256359    -0.13658       0.129279    -0.00702816    0.153566    -0.229097     0.170172     0.117367     0.0570128    -0.0974602  -0.0338869   -0.0831825   -0.000718327  -0.0249329     0.0414015    0.0216318    0.11108       0.0841303   0.0102694   -0.0284233    0.019822     0.0140188 
 -0.0642658    0.0166334    -0.0530819   -0.0191172     0.053186      0.0589922     0.158294    -0.05246      -0.0098565   -0.0900838    0.0552828   -0.136282     5.13932e-5    0.0911573   0.0137741   -0.0824433   -0.0546575    -0.176892     -0.0769547    0.066674     0.022855      0.0404419  -0.111497    -0.068312    -0.156832    -0.0579357 
  0.0582868    0.0178778     0.0227666   -0.112919     -0.141837     -0.00732785   -0.0310966   -0.100148     -0.0907115   -0.0887265   -0.034544    -0.0944712    0.00691153   -0.029963    0.119341     0.0693556   -0.0226493    -0.0332352    -0.0100854    0.152887     0.0236044     0.166489    0.0184563   -0.0436637   -0.0292798    0.0801424 
 -0.163817     0.0760719    -0.0340723   -0.0563889    -0.174796     -0.077832     -0.173981     0.0250181     0.0150931    0.025599     0.00392584  -0.0868688    0.0387711    -0.120051   -0.038536    -0.132098    -0.0462406     0.0418644    -0.0511672   -0.0541835   -0.0365129    -0.0351118  -0.0284253   -0.0675844   -0.0984729   -0.00543202
  0.0525834    0.0372278    -0.0421626   -0.0082467     0.0924977     0.0607089    -0.0134866   -0.131036     -0.0204329    0.0243362    0.0088012   -0.0351573   -0.0557654    -0.0498996  -0.0693126   -0.0366572   -0.0217632     0.0356878    -0.0490369    0.075585     0.0139881    -0.0146596  -0.0504947    0.0671261    0.0235135   -0.0164556 
  0.0858715    0.101697      0.024715     0.115048      0.0790594    -0.0551715    -0.0611985   -0.0353853     0.00977475   0.0300787   -0.022483    -0.052929    -0.0130693     0.0424783   0.06986      0.103061    -0.07429      -0.145498      0.16796     -0.208558     0.045681     -0.115766   -0.481634     0.0472259    0.00889372  -0.203911  
  0.0708893    0.0945275    -0.103224    -0.00851512    0.0342816    -0.151861     -0.0742733   -0.0277477    -0.110565    -0.239741     0.00779159   0.0370165    0.0118379    -0.0319649   0.0493367    0.0661969    0.0992183    -0.213758     -0.124131    -0.0452604    0.0780592     0.307835    0.202715     0.087064    -0.00917417   0.0620402 
 -0.110298     0.222823      0.0727676   -0.0334784     0.0217818    -0.0116629     0.0055744    0.0960751    -0.0123092   -0.0514078   -0.114119     0.0689488   -0.178675      0.0131358   0.0118003    0.0400099   -0.128918      0.0259231     0.0480762   -0.00885421  -0.0830145    -0.10856    -0.133173    -0.143706     0.0879432    0.171016  
  0.0804339   -0.0953086    -0.0180506   -0.27126       0.110089      0.102382      0.0651702    0.150828      0.132594    -0.039625    -0.103601    -0.0170489    0.202777     -0.0785689  -0.0146574    0.0321391    0.0875937    -0.0393586     0.101091     0.0297869   -0.12662       0.118661   -0.129902    -0.00152673   0.197747     0.115267  
 -0.00689015  -0.0327199     0.121189     0.000374769   0.0627493     0.000893436   0.00279878  -0.0549696    -0.134067     0.114385    -0.0944217   -0.0926176   -0.106303      0.0574084  -0.191044    -0.186424     0.0494045     0.0592127    -0.014946    -0.103211     0.171509      0.0212439  -0.0806882    0.0381139   -0.0561149    0.0290139 
 -0.0900373    0.0275486    -0.0266899    0.0607645    -0.164657      0.064382     -0.0650343    0.017513     -0.00129488  -0.121012    -0.124087     0.019589     0.233169      0.0558718  -0.206947    -0.178388    -0.00209973    0.0353929     0.0282445    0.128673    -0.145327      0.0323602  -0.0836132    0.0278231    0.127344    -0.124987  
 -0.20954      0.0243677     0.148112    -0.126508     -0.0363172    -0.16013      -0.115084     0.101091      0.121349    -0.0160991   -0.0266497    0.172025     0.0784441     0.242429   -0.0658949   -0.0216672   -0.140214     -0.0428308     0.0244029    0.0681523   -0.0445025     0.0671268   0.056143     0.00881812   0.0133609   -0.0552882 
 -0.12368      0.122956     -0.0287369    0.0587345     0.0603165    -0.0297741    -0.0785384   -0.0788045    -0.0470291   -0.0467191    0.0906647    0.144507    -0.0294043    -0.0870027   0.042321     0.0973661   -0.0293685    -0.101191     -0.0267828    0.0325597   -0.0816485    -0.110384    0.0815514   -0.0994108   -0.0970843    0.0232381 
 -0.161314     0.0721788    -0.371813    -0.0356342     0.0568673     0.0234482     0.0166985    0.230146     -0.120048     0.0319888   -0.421479     0.037003     0.000231627  -0.0179736   0.0777657   -0.173151    -0.13544       0.257289      0.235115     0.238592     0.224196     -0.0724161   0.128116     0.206137     0.0196879    0.00487566
 -0.111525     0.0760313     0.271744     0.0181917    -0.165651      0.0234286     0.0159214    0.226283      0.00993045   0.0758031    0.175826    -0.210045    -0.01498      -0.0905448   0.0296648    0.182485    -0.0203148     0.118995     -0.165949     0.228706     0.220491     -0.0774711   0.0799871    0.02181     -0.0856392    0.055444  
 -0.0414138   -0.00326964    0.0493572   -0.249907      0.155458     -0.0729359    -0.0243307    0.0287763     0.152944     0.166441    -0.535312     0.0734057   -0.00768909   -0.139103   -0.089992    -0.166223    -0.049421     -0.388121     -0.0613057   -0.044398    -0.221145     -0.0409606   0.0486008    0.0741595    0.0642037    0.110358  
 -0.0458696    0.0159502     0.080038     0.098719      0.0633316    -0.0405404    -0.0648192    0.066476      0.152871     0.107728     0.0110046   -0.0864615    0.0174045    -0.187276   -0.112731     0.0135095   -0.0622052     0.175491     -0.0526079   -0.0471849   -0.21543      -0.157523    0.0616524    0.135808     0.140961    -0.167049  
 -0.0190521   -0.108217     -0.114339     0.0377717    -0.119455      0.0467839    -0.251643     0.0233351    -0.0439604    0.0251721   -0.01196     -0.152718     0.014169      0.0758196  -0.0682977   -0.0481025   -0.00344101   -0.000252412   0.101522    -0.10283      0.0562375    -0.070005   -0.109915     0.0868762   -0.218453     0.0333439 
 -0.0450656    0.0237858    -0.0312362   -0.102618     -0.0161869    -0.0324976    -0.113434    -0.12681       0.0596686    0.0287619    0.0663871   -0.043126    -0.0386756     0.0488799   0.0730971   -0.0414928    0.132164     -0.00442963    0.072504     0.0573374   -0.052604     -0.0179987  -0.00551739  -0.0295563   -0.172324     0.130404  
  0.00589172  -0.000251611  -0.0407374   -0.0583517    -0.000874074  -0.0515561    -0.0124532   -0.0770574    -0.0394544   -0.0162796    0.00654781   0.0195726    0.0561451     0.0685447  -0.0631011    0.0927173    0.0173837    -0.0219102    -0.0525761   -0.00176763  -0.000673932  -0.013454    0.0316464   -0.105424    -0.0560138   -0.0201046 
 -0.025844     0.10691      -0.139309     0.153488      0.114519      0.116909     -0.0276451    0.000264544  -0.0396585    0.0650279    0.0165393    0.161289     0.121089      0.081547    0.00823802   0.0739563    0.0113672    -0.0436269    -0.0676437   -0.0672497   -0.0824434    -0.0304474   0.0706289    0.0373848    0.0497204    0.0722021 
  0.0594115   -0.106842      0.0345607    0.0880489    -0.137275      0.100891      0.0722925   -0.0195102     0.00685966   0.0482131    0.00129545  -0.0316487   -0.129525     -0.0159405  -0.0830653    0.0693682    0.156064      0.148791     -0.0608712    0.089204    -0.0442339    -0.0326886   0.0779303   -0.0898606    0.0626981   -0.0675693 
 -0.141176     0.00404187    0.0489348    0.141194     -0.0990652    -0.0987319     0.00944255   0.164722     -0.114091     0.00366775   0.0395354    0.159129    -0.00955049    0.118275    0.201822     0.217581    -0.0232416    -0.0909428    -0.0548777    0.0700767   -0.193414      0.0117063  -0.0718333    0.173133    -0.0982118   -0.0522334 
 -0.175096     0.0667135     0.0230821    0.0357055    -0.00203972   -0.105392      0.0513167   -0.618458     -0.0370499   -0.119588    -0.018573     0.0126449    0.139937     -0.0239938   0.102762     0.154398     0.0533973    -0.00453758    0.0404364   -0.173289     0.0749025     0.0437849  -0.10101      0.0365217   -0.122473     0.0759797 
  0.220839     0.0465811    -0.0592889    0.0427546    -0.0115739    -0.052755      0.0318546    0.616689     -0.0209915   -0.0333938    0.00392154  -0.00754657  -0.0863227    -0.0582475   0.106681     0.0282122    0.0419101    -0.0154086    -0.095902    -0.151344     0.0606451     0.0373088  -0.105515     0.0269579   -0.109676     0.1443    
 -0.670192     0.0136078    -0.265695     0.110401     -0.0984422     0.0177852     0.00882003  -0.0716205     0.0675187    0.0610769   -0.0400637    0.136195     0.0132699    -0.160224    0.0480678   -0.0359114    0.121006     -0.121598      0.0398003   -0.163376     0.154596      0.0372659  -0.175065     0.158412    -0.0593755   -0.152624  
  0.702316     0.0179533     0.197572     0.111533      0.0148231     0.0169397     0.0343652    0.0377457     0.177049     0.0608716    0.0373829   -0.0602403    0.103019     -0.0980063   0.0514148   -0.0358296    0.100668     -0.0144658     0.0270541   -0.116878    -0.0784284    -0.123318   -0.151503     0.178648    -0.0744655   -0.121437  
  0.0979187   -0.102919      0.145803    -0.0117722    -0.141742      0.0603711     0.0542557   -0.0934483    -0.183321     0.189172    -0.0420711   -0.0778094   -0.140748      0.0551026   0.200209    -0.015572    -0.00531786    0.0101483    -0.125945     0.0857813    0.104324     -0.100815   -0.267161     0.103356    -0.015772     0.143416  
  0.110277    -0.0917652    -0.00271001   0.117067      0.310066      0.061582      0.0731272   -0.0229874    -0.206584     0.215764    -0.00384953  -0.0891928   -0.129383      0.0523742   0.126522     0.0353413   -0.00378398    0.128295     -0.196007     0.129017    -0.0735119     0.0979458  -0.252364     0.123517    -0.05026     -0.202385  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 11 17 18 19 20 22 25 29 30
INFO: iteration 1, average log likelihood -1.055895
WARNING: Variances had to be floored 11 17 18 19 20 22 25 29 30
INFO: iteration 2, average log likelihood -1.049438
WARNING: Variances had to be floored 11 15 17 18 19 20 22 25 29 30
INFO: iteration 3, average log likelihood -1.047915
WARNING: Variances had to be floored 11 17 18 19 20 22 25 29 30
INFO: iteration 4, average log likelihood -1.055868
WARNING: Variances had to be floored 11 17 18 19 20 22 25 29 30
INFO: iteration 5, average log likelihood -1.049386
WARNING: Variances had to be floored 11 15 17 18 19 20 22 25 29 30
INFO: iteration 6, average log likelihood -1.047830
WARNING: Variances had to be floored 11 17 18 19 20 22 25 29 30
INFO: iteration 7, average log likelihood -1.055614
WARNING: Variances had to be floored 11 17 18 19 20 22 25 29 30
INFO: iteration 8, average log likelihood -1.048079
WARNING: Variances had to be floored 11 15 17 18 19 20 22 25 29 30 32
INFO: iteration 9, average log likelihood -1.038478
WARNING: Variances had to be floored 11 17 18 19 20 22 25 29 30
INFO: iteration 10, average log likelihood -1.058609
INFO: EM with 100000 data points 10 iterations avll -1.058609
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.518585e+05
      1       6.753348e+05      -1.765237e+05 |       32
      2       6.465368e+05      -2.879799e+04 |       32
      3       6.318560e+05      -1.468081e+04 |       32
      4       6.216569e+05      -1.019902e+04 |       32
      5       6.150082e+05      -6.648792e+03 |       32
      6       6.111296e+05      -3.878598e+03 |       32
      7       6.091809e+05      -1.948627e+03 |       32
      8       6.080609e+05      -1.120024e+03 |       32
      9       6.073360e+05      -7.248819e+02 |       32
     10       6.067602e+05      -5.757917e+02 |       32
     11       6.063473e+05      -4.129286e+02 |       32
     12       6.060915e+05      -2.558164e+02 |       32
     13       6.059168e+05      -1.746646e+02 |       32
     14       6.057551e+05      -1.617281e+02 |       32
     15       6.055492e+05      -2.058851e+02 |       32
     16       6.053125e+05      -2.366738e+02 |       32
     17       6.050713e+05      -2.412432e+02 |       32
     18       6.048644e+05      -2.069244e+02 |       32
     19       6.047356e+05      -1.287494e+02 |       32
     20       6.046570e+05      -7.864219e+01 |       32
     21       6.046070e+05      -4.996553e+01 |       32
     22       6.045768e+05      -3.019194e+01 |       30
     23       6.045533e+05      -2.350444e+01 |       31
     24       6.045343e+05      -1.902753e+01 |       32
     25       6.045059e+05      -2.841449e+01 |       31
     26       6.044642e+05      -4.163579e+01 |       31
     27       6.044027e+05      -6.153951e+01 |       32
     28       6.043293e+05      -7.339130e+01 |       31
     29       6.042208e+05      -1.084684e+02 |       30
     30       6.041340e+05      -8.685836e+01 |       32
     31       6.040489e+05      -8.505848e+01 |       32
     32       6.039528e+05      -9.615363e+01 |       32
     33       6.038492e+05      -1.035603e+02 |       32
     34       6.037395e+05      -1.097177e+02 |       32
     35       6.036226e+05      -1.168922e+02 |       32
     36       6.034930e+05      -1.295602e+02 |       32
     37       6.033287e+05      -1.642964e+02 |       32
     38       6.031005e+05      -2.282726e+02 |       32
     39       6.027038e+05      -3.966537e+02 |       32
     40       6.020756e+05      -6.282014e+02 |       32
     41       6.015769e+05      -4.987211e+02 |       32
     42       6.013298e+05      -2.470594e+02 |       32
     43       6.012294e+05      -1.004557e+02 |       32
     44       6.011834e+05      -4.594244e+01 |       31
     45       6.011596e+05      -2.382884e+01 |       30
     46       6.011462e+05      -1.342388e+01 |       29
     47       6.011398e+05      -6.405943e+00 |       32
     48       6.011347e+05      -5.021805e+00 |       27
     49       6.011305e+05      -4.228317e+00 |       23
     50       6.011280e+05      -2.497324e+00 |       21
K-means terminated without convergence after 50 iterations (objv = 601128.0241279842)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.315736
INFO: iteration 2, average log likelihood -1.280927
INFO: iteration 3, average log likelihood -1.244348
INFO: iteration 4, average log likelihood -1.202923
INFO: iteration 5, average log likelihood -1.156706
WARNING: Variances had to be floored 3 12 32
INFO: iteration 6, average log likelihood -1.096092
WARNING: Variances had to be floored 1 4 24
INFO: iteration 7, average log likelihood -1.089673
WARNING: Variances had to be floored 13 27 28
INFO: iteration 8, average log likelihood -1.093060
WARNING: Variances had to be floored 15
INFO: iteration 9, average log likelihood -1.073279
WARNING: Variances had to be floored 1 3 12 29 32
INFO: iteration 10, average log likelihood -1.053087
WARNING: Variances had to be floored 4 24
INFO: iteration 11, average log likelihood -1.085088
WARNING: Variances had to be floored 21 27
INFO: iteration 12, average log likelihood -1.071102
WARNING: Variances had to be floored 1 15 32
INFO: iteration 13, average log likelihood -1.051292
WARNING: Variances had to be floored 3 12 28
INFO: iteration 14, average log likelihood -1.067910
WARNING: Variances had to be floored 4 13 24 29
INFO: iteration 15, average log likelihood -1.069762
INFO: iteration 16, average log likelihood -1.071784
WARNING: Variances had to be floored 1 3 15 27 32
INFO: iteration 17, average log likelihood -1.033837
WARNING: Variances had to be floored 12
INFO: iteration 18, average log likelihood -1.089299
WARNING: Variances had to be floored 4 24 28
INFO: iteration 19, average log likelihood -1.060610
WARNING: Variances had to be floored 1 3 21
INFO: iteration 20, average log likelihood -1.069475
WARNING: Variances had to be floored 27
INFO: iteration 21, average log likelihood -1.076252
WARNING: Variances had to be floored 12 15 29 32
INFO: iteration 22, average log likelihood -1.045268
WARNING: Variances had to be floored 1 3 4 24 28
INFO: iteration 23, average log likelihood -1.056633
WARNING: Variances had to be floored 13
INFO: iteration 24, average log likelihood -1.098059
WARNING: Variances had to be floored 21
INFO: iteration 25, average log likelihood -1.062230
WARNING: Variances had to be floored 3 12 15 27
INFO: iteration 26, average log likelihood -1.035911
WARNING: Variances had to be floored 1 4 24 28 31 32
INFO: iteration 27, average log likelihood -1.063941
INFO: iteration 28, average log likelihood -1.099358
WARNING: Variances had to be floored 3 12 13 21 29
INFO: iteration 29, average log likelihood -1.047187
WARNING: Variances had to be floored 15
INFO: iteration 30, average log likelihood -1.076093
WARNING: Variances had to be floored 1 4 24 28 32
INFO: iteration 31, average log likelihood -1.048218
INFO: iteration 32, average log likelihood -1.094333
WARNING: Variances had to be floored 3 12 27 31
INFO: iteration 33, average log likelihood -1.046025
WARNING: Variances had to be floored 13 15
INFO: iteration 34, average log likelihood -1.062233
WARNING: Variances had to be floored 1 4 24 28 29 32
INFO: iteration 35, average log likelihood -1.053696
WARNING: Variances had to be floored 21
INFO: iteration 36, average log likelihood -1.090147
WARNING: Variances had to be floored 3 12 27
INFO: iteration 37, average log likelihood -1.057992
WARNING: Variances had to be floored 31
INFO: iteration 38, average log likelihood -1.065985
WARNING: Variances had to be floored 1 4 15 24 28
INFO: iteration 39, average log likelihood -1.036838
WARNING: Variances had to be floored 3 21
INFO: iteration 40, average log likelihood -1.081120
WARNING: Variances had to be floored 12 13 27 29
INFO: iteration 41, average log likelihood -1.061727
WARNING: Variances had to be floored 1 32
INFO: iteration 42, average log likelihood -1.076414
WARNING: Variances had to be floored 4 24
INFO: iteration 43, average log likelihood -1.060056
WARNING: Variances had to be floored 3 21 27 28
INFO: iteration 44, average log likelihood -1.053537
WARNING: Variances had to be floored 1 12 15
INFO: iteration 45, average log likelihood -1.069407
WARNING: Variances had to be floored 31
INFO: iteration 46, average log likelihood -1.075506
WARNING: Variances had to be floored 3 4 13 24 29
INFO: iteration 47, average log likelihood -1.040249
WARNING: Variances had to be floored 12 21 32
INFO: iteration 48, average log likelihood -1.080269
WARNING: Variances had to be floored 1 15
INFO: iteration 49, average log likelihood -1.066657
WARNING: Variances had to be floored 3 27
INFO: iteration 50, average log likelihood -1.068548
INFO: EM with 100000 data points 50 iterations avll -1.068548
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.207462     0.018892     0.129376   -0.114386     -0.0582437    -0.175399     -0.142953     0.0861824    0.111214     -0.0416303  -0.0270752     0.165418      0.100751      0.278333    -0.0502134    -0.0155666   -0.122914     -0.0422892     0.00325561   0.0841363   -0.0526208    0.0619956   0.0675441    -0.00799761   0.00992923  -0.0540238 
 -0.0559482    0.00374574   0.0349287  -0.0181171    -0.0624192     0.0542324     0.00935906  -0.0217985   -0.000320442  -0.0395037   0.00446731    0.114957      0.0188269     0.0344635    0.106471      0.0315456    0.0788988    -0.0227976     0.0207128    0.0375895   -0.0690491   -0.0558195   0.00717903   -0.0634422   -0.0600633   -0.0217854 
 -0.111884     0.230815     0.0745242  -0.0396639     0.0243055    -0.0221256     0.004079     0.0983194   -0.00855716   -0.0546237  -0.11216       0.0711317    -0.17991       0.0141952    0.000439766   0.0395014   -0.130718      0.0261777     0.0496453   -0.0083339   -0.0883994   -0.108368   -0.133034     -0.145104     0.0888273    0.17862   
  0.108118    -0.0389884    0.0177228  -0.155412     -0.0994568     0.0659246    -0.0164646   -0.17502     -0.0694772    -0.0613587  -0.0754548    -0.0950563     0.0171528    -0.0464861    0.0883318     0.140319    -0.064806     -0.131437      0.0583971    0.137826     0.0576051    0.131099    0.073875     -0.0766662   -0.0384322    0.122107  
  0.0187146    0.0568599   -0.015334    0.039491     -0.00664759   -0.0820851     0.0426545   -0.0156559   -0.0284952    -0.0755465  -0.00560664    0.00327583    0.0305153    -0.0405659    0.104689      0.0944049    0.0481282    -0.0111812    -0.0249711   -0.162046     0.0651669    0.0402703  -0.102891      0.034218    -0.116005     0.109892  
 -0.0411873   -0.053752     0.0393312   0.114852     -0.117563      0.00310722    0.0398636    0.0694672   -0.0505554     0.027339    0.0165791     0.0630924    -0.0671742     0.0480575    0.0578362     0.144602     0.072719      0.0309862    -0.0587443    0.0772894   -0.122365    -0.0147028   0.00645861    0.0399517   -0.013862    -0.0591394 
 -0.0297731    0.113606     0.0219755  -0.0320028    -0.227806     -0.138529     -0.0436713    0.0122178   -0.106428     -0.117097    0.0217208    -0.0945185     0.010246     -0.00603675   0.136227     -0.0458439    0.0395934     0.134886     -0.110871     0.160864    -0.0354219    0.157291   -0.0866653     0.00543938  -0.0264906    0.0109477 
 -0.123639     0.121501    -0.0289659   0.0589836     0.0603346    -0.030458     -0.0788772   -0.0788276   -0.0467037    -0.0466607   0.0906017     0.144574     -0.029294     -0.0870851    0.0426246     0.0974338   -0.0295483    -0.101216     -0.0268173    0.0320356   -0.0817071   -0.110482    0.0818746    -0.0994125   -0.0971243    0.0230216 
 -0.0641089    0.0169979   -0.0518534  -0.0176989     0.0517077     0.0575898     0.158513    -0.0518123   -0.00945735   -0.0882339   0.0529032    -0.136371      0.000392852   0.0910974    0.0145146    -0.0820951   -0.0540828    -0.17623      -0.0773498    0.0659876    0.0235105    0.0403721  -0.111341     -0.067731    -0.153071    -0.0579903 
 -0.0903112    0.0271805   -0.0253882   0.0618322    -0.165263      0.0641512    -0.0672789    0.0210391   -0.00426342   -0.117713   -0.126314      0.0199955     0.234742      0.0560894   -0.20827      -0.180766    -0.000454805   0.0352616     0.0295025    0.127887    -0.147053     0.0326751  -0.0839681     0.0276726    0.126862    -0.125717  
  0.0662759    0.0330914   -0.0328938  -0.0548039     0.0267523    -0.138508      0.129501    -0.00619754   0.153684     -0.229185    0.16907       0.117647      0.0545635    -0.0980228   -0.0326803    -0.0840491   -0.00111442   -0.0250621     0.0418631    0.0210501    0.111094     0.0878724   0.010189     -0.0277741    0.0199922    0.0153097 
 -0.0470062    0.0219683   -0.0306902  -0.106266     -0.00867966   -0.0342584    -0.110461    -0.128067     0.0613824     0.0369588   0.0696252    -0.0493096    -0.0399019     0.0500056    0.0713903    -0.0467235    0.131772     -0.00473692    0.0840554    0.0598283   -0.0550341   -0.0171985  -0.00723149   -0.0176325   -0.173559     0.133442  
  0.0528476    0.105273    -0.0635731   0.0723653     0.0866867     0.366607     -0.0564981   -0.0435707   -0.0139251     0.097622    0.0630802     0.225245      0.10421       0.072188     0.0142798     0.110611     0.025367     -0.176037     -0.0686123   -0.0361951   -0.0695925   -0.0121188   0.0272021     0.0631293    0.0550405    0.095844  
 -0.251857     0.040148    -0.167338   -0.0776759    -0.0708451    -0.148533     -0.00191765   0.0571792    0.096329     -0.114692    0.0608423    -0.0947222     0.153697     -0.094593     0.107999      0.0534615    0.01518      -0.134349      0.0029273   -0.0835948    0.0502018   -0.0792749  -0.0917358     0.0446541   -0.241348    -0.147664  
 -0.0195922    0.0425487   -0.0650762   0.0292108     0.196708     -0.112793     -0.0556835   -0.0744943   -0.0665037     0.109626    0.0673158     0.0909806     0.113436      0.109867    -0.0129332     0.0800072    0.0301495     0.0798051    -0.0163798    0.022095    -0.265376    -0.0170703   0.00224872    0.0851617    0.0862576    0.0408984 
 -0.174249     0.0584127   -0.0241383  -0.0783464    -0.186544     -0.0823219    -0.183139     0.0372157    0.0245673     0.0410916   0.000144022  -0.0899724     0.0262788    -0.131138    -0.035622     -0.152493    -0.0570987     0.0409822    -0.057194    -0.0561809   -0.0329448   -0.0433151  -0.0113519    -0.0749958   -0.10549     -0.00726517
  0.0668471    0.0992133   -0.0273376   0.0452604     0.0560685    -0.104928     -0.0691268   -0.0241829   -0.0435435    -0.0950066  -0.0107854    -0.00457309   -0.0112322     0.0288079    0.0591996     0.0797595    0.0023973    -0.171296      0.025712    -0.122673     0.0532409    0.088079   -0.133725      0.0652658   -0.00339757  -0.0744615 
 -0.00708348  -0.0324151    0.121181   -0.000182253   0.0629527     0.000852929   0.00286903  -0.0545993   -0.133501      0.114296   -0.0954008    -0.0927161    -0.105913      0.0583141   -0.191094     -0.185342     0.0489551     0.0616978    -0.0143047   -0.103053     0.169782     0.0209993  -0.0819021     0.0379291   -0.0558542    0.0283132 
 -0.0475302   -0.157195    -0.158764    0.0951201    -0.196263     -0.087132     -0.0761128   -0.0179249   -0.00364152    0.0246988   0.0188754    -0.110318      0.132354     -0.114349    -0.0618438     0.0372678   -0.141897     -0.011512     -0.0524766   -0.125723     0.105862    -0.137738    0.163383     -0.0666315   -0.0975722    0.108664  
  0.0802727   -0.0808195    0.0111273  -0.115104      0.075376      0.117038      0.117176    -0.0623781    0.0365952     0.0969542  -0.0228881    -0.202441     -0.203104     -0.0518483    0.00618889   -0.0606281    0.0165525    -0.0434364    -0.105047     0.0829638    0.0191745   -0.142758    0.0350847     0.170238     0.0377558    0.19565   
  0.268867     0.166208    -0.204876    0.132545      0.167017      0.0595916     0.0269798   -0.159575    -0.0401354    -0.257993    0.0100725    -0.00471503    0.0555387    -0.0592977   -0.236174      0.0713668   -0.00661532    0.269223      0.0178245   -0.0515466    0.0754049    0.135198   -0.270545      0.219234     0.0804955   -0.143227  
 -0.0190251   -0.108254    -0.114434    0.0381913    -0.117517      0.0470156    -0.252771     0.0236552   -0.0429207     0.0251499  -0.011991     -0.15299       0.0141187     0.0765959   -0.0669856    -0.0490115   -0.00330004   -0.000297023   0.102289    -0.102575     0.0548142   -0.070249   -0.111159      0.0863806   -0.218825     0.0341278 
  0.0766072    0.0453995   -0.0164951  -0.0443378     0.0504371    -0.0689139    -0.0493886   -0.0867392   -0.107372     -0.103111   -0.0237488    -0.0204832     0.0916845     0.0299295   -0.0743016     0.0325033   -0.0268474    -0.0791661     0.0733108   -0.0758792    0.0965015    0.0108122   0.0765604    -0.20384     -0.103254    -0.150749  
 -0.132932     0.0732434   -0.0588052  -0.00909173   -0.0510359     0.0237264     0.0157304    0.226522    -0.0503866     0.05452    -0.119218     -0.0818192    -0.00602406   -0.0487899    0.0518059     0.00588472  -0.0809402     0.189133      0.0451696    0.231837     0.218473    -0.0746257   0.104141      0.113925    -0.0356789    0.0317565 
  0.0805889   -0.107914    -0.015812   -0.276483      0.11675       0.0974814     0.062782     0.147482     0.141853     -0.0419282  -0.112531     -0.0151028     0.205445     -0.0779105   -0.00833084    0.0296767    0.0900682    -0.0487577     0.106639     0.0233283   -0.131961     0.112806   -0.128183     -0.00436871   0.20622      0.115267  
 -0.00771187   0.0614358   -0.0563163   0.0660124    -0.000127033   0.00279227   -0.0593837   -0.0955942    0.0322979     0.100954    0.0089194     0.0786731     0.0223558    -0.083499     0.0393777    -0.0677662    0.0114979    -0.00301888   -0.0032996   -0.0133208   -0.00401548  -0.028105   -0.126226      0.031188    -0.0638867   -0.140904  
 -0.0488186    0.177566    -0.220753    0.301484      0.0656713     0.324481      0.0127859    0.0782173   -0.00469032   -0.0012225  -0.0202863     0.250823      0.127928      0.0492717    0.0587985     0.0723513   -0.0164274    -0.139271     -0.111707    -0.16856      0.06325     -0.0468989   0.128546     -0.0300586    0.022782     0.0752966 
  0.0717246    0.0464598    0.106856    0.127217      0.167724      0.060815     -0.0638425   -0.15392     -0.0033963     0.026049    0.035728      0.000199423   0.040136     -0.0433675   -0.126144      0.0272746   -0.0161834    -0.0412746     0.0418291    0.00435186   0.041279     0.021775    0.0197226     0.139187     0.0767174   -0.121043  
 -0.0509849    0.0057121    0.0700969  -0.0800976     0.104657     -0.0642932    -0.0504421    0.0484339    0.150846      0.144053   -0.253431      0.00118373    0.0126288    -0.171285    -0.0988009    -0.0755878   -0.0624132    -0.101431     -0.0553636   -0.0405448   -0.214967    -0.0918141   0.0564438     0.100433     0.0975255   -0.0285125 
  0.0675155   -0.0772834    0.0149116  -0.0394934     0.0478082    -0.0519262     0.00630181  -0.0306815   -0.132498      0.162361    0.0214682    -0.051186     -0.0371653     0.0776556   -0.0673972     0.10924     -0.0159717     0.0361272    -0.17741      0.0665171    0.0557663    0.0144073  -0.0988151     0.110847     0.00197046   0.0321272 
  0.0683596    0.0903489   -0.127041    0.0814542     0.173642      0.0667156    -0.0689046   -0.164526    -0.117661     -0.160542    0.0107728    -0.00804288    0.0719636    -0.0753401   -0.245889      0.0341754    0.0394382     0.116212      0.0253937    0.111693     0.0763317    0.115724   -0.0406475     0.118322     0.0713566   -0.233563  
 -0.223464    -0.046631     0.275602   -0.0503755     0.13504       0.0368619    -0.0121233   -0.0836018    0.103233     -0.0173473   0.00321248    0.0438773     0.0742101    -0.00850412  -0.0991933     0.0128906   -0.0748716    -0.333836      0.00357821   0.0413622    0.0403952    0.0941248  -0.000922963   0.00890617   0.0660129    0.048131  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 4 24 28 29 31
INFO: iteration 1, average log likelihood -1.055954
WARNING: Variances had to be floored 1 4 12 24 28 29 31
INFO: iteration 2, average log likelihood -1.029255
WARNING: Variances had to be floored 3 4 15 21 24 29 31
INFO: iteration 3, average log likelihood -1.024714
WARNING: Variances had to be floored 1 4 12 24 27 28 29 31
INFO: iteration 4, average log likelihood -1.033436
WARNING: Variances had to be floored 4 13 21 24 29 31 32
INFO: iteration 5, average log likelihood -1.039575
WARNING: Variances had to be floored 1 3 4 12 15 24 28 29 31
INFO: iteration 6, average log likelihood -1.022834
WARNING: Variances had to be floored 4 12 21 24 28 29 31
INFO: iteration 7, average log likelihood -1.043488
WARNING: Variances had to be floored 1 4 24 27 28 29 31
INFO: iteration 8, average log likelihood -1.022110
WARNING: Variances had to be floored 4 12 15 24 29 31
INFO: iteration 9, average log likelihood -1.033678
WARNING: Variances had to be floored 1 3 4 13 21 24 28 29 31
INFO: iteration 10, average log likelihood -1.027838
INFO: EM with 100000 data points 10 iterations avll -1.027838
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.204314      0.123541    -0.0883746   -0.0674957    0.115709     0.128671     0.0143961    0.012546    -0.135098    -0.139509     0.0129279    0.0870352    0.0360011    0.0498347    0.228479    -0.0573311   -0.0796313   -0.342533     0.0531702    0.172369     0.0849332   -0.0144814   -0.093048     0.0990917    0.0273596   -0.00821276
 -0.0910836    -0.123447    -0.00840618   0.0789482   -0.0317828    0.120819     0.073488     0.109639     0.0439147   -0.030155    -0.0999822    0.229814    -0.0669923   -0.00243879  -0.0452586    0.0101327   -0.0843368   -0.084955    -0.013485     0.0799876   -0.133056     0.0880975   -0.056025    -0.0718902    0.168393    -0.00756225
  0.0295607     0.0573165    0.0180794   -0.0239082   -0.153716    -0.0681981    0.132472     0.0126504   -0.0177169   -0.0706161    0.166856    -0.112751    -0.0383725    0.105517    -0.0294729   -0.13457     -0.0195536    0.0484262   -0.0777277   -0.0197365   -0.246413    -0.146681    -0.00804444  -0.111549     0.00658972   0.123777  
 -0.152973      0.204503     0.0108813    0.079971     0.0322262    0.171428     0.00662341   0.247997     0.0397844   -0.049611    -0.0036364   -0.132229     0.136839    -0.0540563   -0.114143    -0.045414    -0.0283536    0.0474985    0.122309    -0.0128329    0.147688     0.182466    -0.0188155   -0.0563606    0.0543289   -0.0910881 
 -0.0972997    -0.0908299    0.183752    -0.00311553  -0.119549     0.02854      0.156565     0.0503091   -0.0281044   -0.400389     0.0318648    0.0139059   -0.0625703   -0.069857    -0.0337892   -0.108608    -0.0864113    0.196664     0.00381568  -0.0420627   -0.0680168   -0.0980205   -0.0710646   -0.00371501   0.104713    -0.107929  
 -0.171498     -0.0826385    0.090183    -0.0698056   -0.00432502   0.0659844    0.030768    -0.0473543    0.161705     0.0938836    0.0886599    0.0352379    0.0040249   -0.148665    -0.0134781   -0.0724359    0.0695823    0.112454     0.133053    -0.0939638    0.100662     0.0265559    0.187893     0.0916216   -0.0362066    0.0759182 
  0.0469391     0.0115418    0.0436469    0.0722359   -0.121343     0.0446054    0.0853053   -0.0507346   -0.0469499    0.135476    -0.0390871   -0.0420559   -0.0151901    0.115578    -0.124764    -0.0482443    0.038733    -0.0465453   -0.0726902   -0.00846354   0.00915849  -0.0166608    0.0264407   -0.0139905    0.0280177   -0.0365468 
 -0.111899      0.0529964   -0.146047     0.030111     0.108005     0.0313519    0.0738464    0.00956432  -0.0845089   -0.0220524   -0.0225482    0.119485     0.148337     0.222685     0.0848377   -0.0833528   -0.0523813   -0.0632969   -0.0813891   -0.0389136   -0.116167     0.113504    -0.0974876    0.103113     0.0558734   -0.156425  
  0.134253     -0.0911778   -0.0199351   -0.115639    -0.0747258   -0.0173455   -0.115787     0.0120584   -0.103026    -0.0698103   -0.0147123    0.236893     0.11278     -0.133882     0.091524    -0.0591063    0.0723878   -0.245935     0.00304392   0.00180351  -0.127451     0.0515355    0.176414    -0.162592     0.0875959   -0.0438001 
  0.122376      0.0876763   -0.0734855    0.0609862   -0.189733    -0.262918    -0.0774847   -0.11938      0.223562     0.00432054   0.0202724    0.0472134   -0.213067     0.185879     0.0413995   -0.00421618   0.0929913   -0.194249     0.0576465   -0.205036    -0.07049     -0.0584999    0.0472631   -0.0664421   -0.101762     0.0151724 
 -0.0643573    -0.050091     0.146081    -0.11806      0.0990891    0.0949833   -0.0442539    0.253558    -0.07136     -0.0142836   -0.0853705   -0.030835     0.0603151    0.0605486    0.013909     0.0471059    0.160411    -0.0522508   -0.282879    -0.00978103   0.171529     0.108335     0.0595166    0.186942    -0.0367051    0.119462  
 -0.0158072    -0.041381    -0.0200267    0.0489498   -0.122621     0.0742526   -0.0943319    0.0764356   -0.229977    -0.033986     0.0275058    0.199954     0.0215831   -0.0810824   -0.0143859   -0.116804     0.0278024   -0.0136037   -0.0150483    0.0368222    0.0602044   -0.205081    -0.0240743    0.282272     0.116132     0.118521  
 -0.18538       0.033158    -0.0522344    0.0617555   -0.0553566   -0.049484     0.0241465   -0.127496     0.0045705    0.18852      0.0831967   -0.163706     0.0163811    0.187502     0.0408428   -0.0752227   -0.165802    -0.112903     0.155758    -0.0194317   -0.0817405   -0.0369175    0.0617901    0.0939748    0.0654575    0.0990044 
  0.0530176     0.113279    -0.102796    -0.144487     0.0404669   -0.0187422   -0.0284082    0.0581067   -0.0698837    0.12691     -0.0589551   -0.0814072    0.176134     0.0311134   -0.12881      0.148829     0.0634132    0.0757955    0.083792    -0.0706648   -0.0768817    0.183058    -0.0553778    0.0683044    0.116949     0.00933175
 -0.0163795     0.0240177   -0.120802     0.0109088    0.0388703   -0.129103     0.132021     0.0371682    0.00126055  -0.0531475   -0.00839475   0.00963075   0.170315     0.0384436    0.138418    -0.106302     0.0592539   -0.0105988   -0.078824    -0.0100632    0.0450555   -0.0440725    0.105061    -0.152313     0.0442327   -0.0406338 
 -0.238686      0.156601     0.105246    -0.0427922    0.141249     0.123256    -0.080174     0.0182165    0.197164    -0.0558079    0.0291256    0.0473117    0.0891104    0.059083     0.0464093    0.0177893    0.143595    -0.0586047   -0.0307936   -0.0852647   -0.0285563   -0.147988    -0.012978     0.0768565    0.0938892   -0.0399546 
  0.057009     -0.112446    -0.0982079   -0.187121     0.0721313   -0.0575983    0.00811044   0.131184     0.0825306   -0.0530691   -0.0297903    0.131654    -0.0463128    0.0310862    0.0468691    0.0945987    0.0954515    0.00750142  -0.0779829    0.137217     0.202812     0.0543028    0.0201521    0.127788     0.0139926   -0.15645   
  0.0289541     0.0220329    0.0388404   -0.160404     0.043986    -0.132615    -0.0876749   -0.0326108   -0.122687     0.163834    -0.00488274  -0.173901    -0.151908    -0.0288142    0.123339     0.0851983    0.090538    -0.0663102   -0.0522236    0.0591276    0.0773635   -0.139264    -0.077357    -0.0104536   -0.0778546   -0.136931  
  0.0388166    -0.132502    -0.0304316    0.0457631   -0.261676     0.156034    -0.121254     0.0227566   -0.0583687   -0.0227504    0.038529    -0.0712524   -0.211438    -0.0380644    0.0449498    0.0355694    0.0120168   -0.00721197  -0.021878    -0.0194829   -0.0472164    0.00522195   0.0683883   -0.0171884    0.159469    -0.0784639 
 -0.182401     -0.0986161   -0.0442164    0.0520804   -0.092421     0.210058    -0.035946    -0.0624864   -0.219888    -0.144888    -0.00659311   0.0180504    0.0454565   -0.0774203   -0.0224176    0.136878    -0.0425881   -0.00637218  -0.0229526    0.0265057    0.174023    -0.118977    -0.0438594   -0.0150393    0.0081352    0.126843  
 -0.0716414     0.00619484  -0.0173844    0.0431107    0.0137312   -0.127806     0.0607168    0.176846     0.128002     0.0433633   -0.171719     0.0594807    0.0213054    0.169118    -0.0828468   -0.0226807    0.148668    -0.169514    -0.062926    -0.0155224   -0.206667    -0.140653    -0.219399     0.0703678    0.0386447    0.0284049 
  0.120831      0.0681604   -0.0712324   -0.132331    -0.0384444   -0.00232161   0.0859547   -0.119468     0.180023     0.177027     0.0700728   -0.0170857    0.0081844    0.0754827   -0.211251    -0.0273189   -0.078689    -0.0065228    0.14837      0.0256278    0.0398621   -0.0843557    0.0914542    0.127953    -0.137258     0.0846301 
 -0.123173      0.13237      0.0607778    0.0103197   -0.0129583   -0.0366528    0.0409159   -0.055669     0.0917767   -0.090715    -0.0917539    0.109225    -0.00942079   0.109164     0.0846974   -0.0588534   -0.0666544   -0.00643092  -0.00449103  -0.0317853   -0.0757019    0.202775    -0.0635321    0.110452     0.0299778    0.115126  
  0.068038     -0.00694027   0.0109217    0.0380873    0.0970413    0.0345944   -0.112208    -0.0242       0.0191471   -0.125505     0.0619579   -0.0336504   -0.116769    -0.179263    -0.00422405  -0.107694     0.0962969   -0.00611234   0.152329     0.00971542   0.187496     0.0287755   -0.194414    -0.0503871    0.167729    -0.0718525 
  0.000300088   0.069056     0.0225198    0.0381846    0.103053    -0.0366232   -0.0122954   -0.172811    -0.131748    -0.0868101   -0.00583031   0.149459     0.0830518    0.0611036    0.193256    -0.196632    -0.0908773    0.0876847    0.120711     0.0661193    0.012297     0.132027     0.108537     0.0449075   -0.0136323    0.0874477 
 -0.0278134    -0.00443527  -0.10596     -0.0940479   -0.0819437   -0.257648    -0.0711171    0.202673     0.174337     0.0804636   -0.133138     0.155922     0.00834      0.0720834    0.0309765    0.109724    -0.103495     0.010444    -0.286466    -0.18545      0.0612632   -0.0126048   -0.00371317  -0.0319012    0.0367379   -0.0585097 
 -0.126179     -0.0944129    0.0642732   -0.147797    -0.0788878    0.0507563   -0.0888012    0.100335     0.0475728   -0.0724415    0.0527199    0.0953029    0.00451017  -0.222391    -0.0359643   -0.0618177   -0.0608111    0.0738169   -0.0599112    0.0132435   -0.0284804   -0.0129363    0.0954325   -0.0368432   -0.0178497   -0.0259673 
 -0.113744     -0.0423181    0.121013    -0.0778464    0.0618092    0.171242    -0.00871505  -0.0300917    0.0301994    0.0213104   -0.078826     0.103424    -0.142584    -0.0449196   -0.066264     0.0152645    0.0607529   -0.033395    -0.10158      0.160231     0.0413328    0.0272809   -0.00577196   0.17201      0.0340334   -0.0685326 
 -0.1319        0.0866355    0.0539249    0.0210188    0.00503457  -0.0232252   -0.104926     0.122731     0.0442276   -0.0286463   -0.172949    -0.0191252    0.144201     0.0485878    0.00491994  -0.00492297  -0.152403    -0.112866     0.0559673    0.0490939    0.17495      0.0522152   -0.0858001   -0.125193     0.0226744   -0.025968  
  0.0478126     0.0607703   -0.0471818    0.0641275   -0.177773    -0.0611125    0.0948652    0.0824292   -0.133906     0.0900199   -0.199711    -0.0689999   -0.185757    -0.115048     0.109079    -0.0747822   -0.00076635  -0.132433     0.0147492   -0.0947904   -0.0181591   -0.0317633   -0.216377     0.0391709   -0.14366     -0.0713005 
 -0.00204929    0.0787616   -0.162074     0.050914     0.0118519   -0.063707    -0.0181117    0.0938112    0.243075     0.112195     0.0187433    0.0587203    0.0245271   -0.127452    -0.0939069    0.0514431   -0.0383136   -0.0989441    0.0434909    0.0582495   -0.0848179    0.182678    -0.210101    -0.0488613    0.111294     0.196648  
  0.083811      0.154477     0.0585251    0.0561562   -0.152719    -0.105755     0.10238      0.0131768    0.102392     0.0961652    0.134885    -0.00105884   0.0574933    0.16392     -0.111075     0.0685935   -0.13482     -0.0138049    0.0413784    0.0204593   -0.0527669    0.11498      0.046577     0.0856819   -0.0941644   -0.184956  kind full, method split
0: avll = -1.4219529596960188
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.421972
INFO: iteration 2, average log likelihood -1.421912
INFO: iteration 3, average log likelihood -1.421869
INFO: iteration 4, average log likelihood -1.421818
INFO: iteration 5, average log likelihood -1.421753
INFO: iteration 6, average log likelihood -1.421657
INFO: iteration 7, average log likelihood -1.421488
INFO: iteration 8, average log likelihood -1.421143
INFO: iteration 9, average log likelihood -1.420455
INFO: iteration 10, average log likelihood -1.419340
INFO: iteration 11, average log likelihood -1.418112
INFO: iteration 12, average log likelihood -1.417278
INFO: iteration 13, average log likelihood -1.416902
INFO: iteration 14, average log likelihood -1.416763
INFO: iteration 15, average log likelihood -1.416714
INFO: iteration 16, average log likelihood -1.416695
INFO: iteration 17, average log likelihood -1.416687
INFO: iteration 18, average log likelihood -1.416684
INFO: iteration 19, average log likelihood -1.416682
INFO: iteration 20, average log likelihood -1.416681
INFO: iteration 21, average log likelihood -1.416680
INFO: iteration 22, average log likelihood -1.416679
INFO: iteration 23, average log likelihood -1.416679
INFO: iteration 24, average log likelihood -1.416678
INFO: iteration 25, average log likelihood -1.416678
INFO: iteration 26, average log likelihood -1.416677
INFO: iteration 27, average log likelihood -1.416677
INFO: iteration 28, average log likelihood -1.416677
INFO: iteration 29, average log likelihood -1.416676
INFO: iteration 30, average log likelihood -1.416676
INFO: iteration 31, average log likelihood -1.416676
INFO: iteration 32, average log likelihood -1.416675
INFO: iteration 33, average log likelihood -1.416675
INFO: iteration 34, average log likelihood -1.416675
INFO: iteration 35, average log likelihood -1.416675
INFO: iteration 36, average log likelihood -1.416675
INFO: iteration 37, average log likelihood -1.416675
INFO: iteration 38, average log likelihood -1.416674
INFO: iteration 39, average log likelihood -1.416674
INFO: iteration 40, average log likelihood -1.416674
INFO: iteration 41, average log likelihood -1.416674
INFO: iteration 42, average log likelihood -1.416674
INFO: iteration 43, average log likelihood -1.416674
INFO: iteration 44, average log likelihood -1.416674
INFO: iteration 45, average log likelihood -1.416674
INFO: iteration 46, average log likelihood -1.416674
INFO: iteration 47, average log likelihood -1.416674
INFO: iteration 48, average log likelihood -1.416674
INFO: iteration 49, average log likelihood -1.416674
INFO: iteration 50, average log likelihood -1.416674
INFO: EM with 100000 data points 50 iterations avll -1.416674
952.4 data points per parameter
1: avll = [-1.42197,-1.42191,-1.42187,-1.42182,-1.42175,-1.42166,-1.42149,-1.42114,-1.42045,-1.41934,-1.41811,-1.41728,-1.4169,-1.41676,-1.41671,-1.41669,-1.41669,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.416689
INFO: iteration 2, average log likelihood -1.416630
INFO: iteration 3, average log likelihood -1.416583
INFO: iteration 4, average log likelihood -1.416530
INFO: iteration 5, average log likelihood -1.416467
INFO: iteration 6, average log likelihood -1.416396
INFO: iteration 7, average log likelihood -1.416319
INFO: iteration 8, average log likelihood -1.416241
INFO: iteration 9, average log likelihood -1.416169
INFO: iteration 10, average log likelihood -1.416105
INFO: iteration 11, average log likelihood -1.416050
INFO: iteration 12, average log likelihood -1.416005
INFO: iteration 13, average log likelihood -1.415969
INFO: iteration 14, average log likelihood -1.415942
INFO: iteration 15, average log likelihood -1.415921
INFO: iteration 16, average log likelihood -1.415905
INFO: iteration 17, average log likelihood -1.415892
INFO: iteration 18, average log likelihood -1.415883
INFO: iteration 19, average log likelihood -1.415875
INFO: iteration 20, average log likelihood -1.415868
INFO: iteration 21, average log likelihood -1.415862
INFO: iteration 22, average log likelihood -1.415856
INFO: iteration 23, average log likelihood -1.415851
INFO: iteration 24, average log likelihood -1.415846
INFO: iteration 25, average log likelihood -1.415842
INFO: iteration 26, average log likelihood -1.415838
INFO: iteration 27, average log likelihood -1.415834
INFO: iteration 28, average log likelihood -1.415830
INFO: iteration 29, average log likelihood -1.415826
INFO: iteration 30, average log likelihood -1.415823
INFO: iteration 31, average log likelihood -1.415820
INFO: iteration 32, average log likelihood -1.415817
INFO: iteration 33, average log likelihood -1.415814
INFO: iteration 34, average log likelihood -1.415812
INFO: iteration 35, average log likelihood -1.415809
INFO: iteration 36, average log likelihood -1.415807
INFO: iteration 37, average log likelihood -1.415805
INFO: iteration 38, average log likelihood -1.415802
INFO: iteration 39, average log likelihood -1.415800
INFO: iteration 40, average log likelihood -1.415798
INFO: iteration 41, average log likelihood -1.415796
INFO: iteration 42, average log likelihood -1.415794
INFO: iteration 43, average log likelihood -1.415792
INFO: iteration 44, average log likelihood -1.415790
INFO: iteration 45, average log likelihood -1.415789
INFO: iteration 46, average log likelihood -1.415787
INFO: iteration 47, average log likelihood -1.415785
INFO: iteration 48, average log likelihood -1.415783
INFO: iteration 49, average log likelihood -1.415781
INFO: iteration 50, average log likelihood -1.415779
INFO: EM with 100000 data points 50 iterations avll -1.415779
473.9 data points per parameter
2: avll = [-1.41669,-1.41663,-1.41658,-1.41653,-1.41647,-1.4164,-1.41632,-1.41624,-1.41617,-1.4161,-1.41605,-1.41601,-1.41597,-1.41594,-1.41592,-1.4159,-1.41589,-1.41588,-1.41587,-1.41587,-1.41586,-1.41586,-1.41585,-1.41585,-1.41584,-1.41584,-1.41583,-1.41583,-1.41583,-1.41582,-1.41582,-1.41582,-1.41581,-1.41581,-1.41581,-1.41581,-1.4158,-1.4158,-1.4158,-1.4158,-1.4158,-1.41579,-1.41579,-1.41579,-1.41579,-1.41579,-1.41578,-1.41578,-1.41578,-1.41578]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.415787
INFO: iteration 2, average log likelihood -1.415729
INFO: iteration 3, average log likelihood -1.415678
INFO: iteration 4, average log likelihood -1.415619
INFO: iteration 5, average log likelihood -1.415547
INFO: iteration 6, average log likelihood -1.415462
INFO: iteration 7, average log likelihood -1.415365
INFO: iteration 8, average log likelihood -1.415264
INFO: iteration 9, average log likelihood -1.415167
INFO: iteration 10, average log likelihood -1.415080
INFO: iteration 11, average log likelihood -1.415006
INFO: iteration 12, average log likelihood -1.414943
INFO: iteration 13, average log likelihood -1.414890
INFO: iteration 14, average log likelihood -1.414846
INFO: iteration 15, average log likelihood -1.414809
INFO: iteration 16, average log likelihood -1.414777
INFO: iteration 17, average log likelihood -1.414750
INFO: iteration 18, average log likelihood -1.414726
INFO: iteration 19, average log likelihood -1.414706
INFO: iteration 20, average log likelihood -1.414688
INFO: iteration 21, average log likelihood -1.414673
INFO: iteration 22, average log likelihood -1.414659
INFO: iteration 23, average log likelihood -1.414646
INFO: iteration 24, average log likelihood -1.414635
INFO: iteration 25, average log likelihood -1.414625
INFO: iteration 26, average log likelihood -1.414615
INFO: iteration 27, average log likelihood -1.414606
INFO: iteration 28, average log likelihood -1.414598
INFO: iteration 29, average log likelihood -1.414589
INFO: iteration 30, average log likelihood -1.414582
INFO: iteration 31, average log likelihood -1.414574
INFO: iteration 32, average log likelihood -1.414567
INFO: iteration 33, average log likelihood -1.414560
INFO: iteration 34, average log likelihood -1.414554
INFO: iteration 35, average log likelihood -1.414547
INFO: iteration 36, average log likelihood -1.414541
INFO: iteration 37, average log likelihood -1.414535
INFO: iteration 38, average log likelihood -1.414529
INFO: iteration 39, average log likelihood -1.414523
INFO: iteration 40, average log likelihood -1.414517
INFO: iteration 41, average log likelihood -1.414512
INFO: iteration 42, average log likelihood -1.414507
INFO: iteration 43, average log likelihood -1.414501
INFO: iteration 44, average log likelihood -1.414496
INFO: iteration 45, average log likelihood -1.414491
INFO: iteration 46, average log likelihood -1.414486
INFO: iteration 47, average log likelihood -1.414482
INFO: iteration 48, average log likelihood -1.414477
INFO: iteration 49, average log likelihood -1.414473
INFO: iteration 50, average log likelihood -1.414468
INFO: EM with 100000 data points 50 iterations avll -1.414468
236.4 data points per parameter
3: avll = [-1.41579,-1.41573,-1.41568,-1.41562,-1.41555,-1.41546,-1.41536,-1.41526,-1.41517,-1.41508,-1.41501,-1.41494,-1.41489,-1.41485,-1.41481,-1.41478,-1.41475,-1.41473,-1.41471,-1.41469,-1.41467,-1.41466,-1.41465,-1.41464,-1.41462,-1.41462,-1.41461,-1.4146,-1.41459,-1.41458,-1.41457,-1.41457,-1.41456,-1.41455,-1.41455,-1.41454,-1.41453,-1.41453,-1.41452,-1.41452,-1.41451,-1.41451,-1.4145,-1.4145,-1.41449,-1.41449,-1.41448,-1.41448,-1.41447,-1.41447]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.414474
INFO: iteration 2, average log likelihood -1.414422
INFO: iteration 3, average log likelihood -1.414375
INFO: iteration 4, average log likelihood -1.414321
INFO: iteration 5, average log likelihood -1.414257
INFO: iteration 6, average log likelihood -1.414179
INFO: iteration 7, average log likelihood -1.414089
INFO: iteration 8, average log likelihood -1.413989
INFO: iteration 9, average log likelihood -1.413885
INFO: iteration 10, average log likelihood -1.413782
INFO: iteration 11, average log likelihood -1.413685
INFO: iteration 12, average log likelihood -1.413595
INFO: iteration 13, average log likelihood -1.413514
INFO: iteration 14, average log likelihood -1.413442
INFO: iteration 15, average log likelihood -1.413378
INFO: iteration 16, average log likelihood -1.413322
INFO: iteration 17, average log likelihood -1.413272
INFO: iteration 18, average log likelihood -1.413228
INFO: iteration 19, average log likelihood -1.413189
INFO: iteration 20, average log likelihood -1.413154
INFO: iteration 21, average log likelihood -1.413122
INFO: iteration 22, average log likelihood -1.413093
INFO: iteration 23, average log likelihood -1.413065
INFO: iteration 24, average log likelihood -1.413040
INFO: iteration 25, average log likelihood -1.413016
INFO: iteration 26, average log likelihood -1.412993
INFO: iteration 27, average log likelihood -1.412971
INFO: iteration 28, average log likelihood -1.412950
INFO: iteration 29, average log likelihood -1.412929
INFO: iteration 30, average log likelihood -1.412909
INFO: iteration 31, average log likelihood -1.412890
INFO: iteration 32, average log likelihood -1.412872
INFO: iteration 33, average log likelihood -1.412854
INFO: iteration 34, average log likelihood -1.412836
INFO: iteration 35, average log likelihood -1.412819
INFO: iteration 36, average log likelihood -1.412802
INFO: iteration 37, average log likelihood -1.412786
INFO: iteration 38, average log likelihood -1.412771
INFO: iteration 39, average log likelihood -1.412756
INFO: iteration 40, average log likelihood -1.412741
INFO: iteration 41, average log likelihood -1.412727
INFO: iteration 42, average log likelihood -1.412714
INFO: iteration 43, average log likelihood -1.412701
INFO: iteration 44, average log likelihood -1.412688
INFO: iteration 45, average log likelihood -1.412676
INFO: iteration 46, average log likelihood -1.412665
INFO: iteration 47, average log likelihood -1.412654
INFO: iteration 48, average log likelihood -1.412643
INFO: iteration 49, average log likelihood -1.412633
INFO: iteration 50, average log likelihood -1.412623
INFO: EM with 100000 data points 50 iterations avll -1.412623
118.1 data points per parameter
4: avll = [-1.41447,-1.41442,-1.41437,-1.41432,-1.41426,-1.41418,-1.41409,-1.41399,-1.41389,-1.41378,-1.41368,-1.4136,-1.41351,-1.41344,-1.41338,-1.41332,-1.41327,-1.41323,-1.41319,-1.41315,-1.41312,-1.41309,-1.41307,-1.41304,-1.41302,-1.41299,-1.41297,-1.41295,-1.41293,-1.41291,-1.41289,-1.41287,-1.41285,-1.41284,-1.41282,-1.4128,-1.41279,-1.41277,-1.41276,-1.41274,-1.41273,-1.41271,-1.4127,-1.41269,-1.41268,-1.41266,-1.41265,-1.41264,-1.41263,-1.41262]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.412622
INFO: iteration 2, average log likelihood -1.412559
INFO: iteration 3, average log likelihood -1.412499
INFO: iteration 4, average log likelihood -1.412431
INFO: iteration 5, average log likelihood -1.412348
INFO: iteration 6, average log likelihood -1.412246
INFO: iteration 7, average log likelihood -1.412123
INFO: iteration 8, average log likelihood -1.411978
INFO: iteration 9, average log likelihood -1.411818
INFO: iteration 10, average log likelihood -1.411650
INFO: iteration 11, average log likelihood -1.411482
INFO: iteration 12, average log likelihood -1.411323
INFO: iteration 13, average log likelihood -1.411176
INFO: iteration 14, average log likelihood -1.411045
INFO: iteration 15, average log likelihood -1.410928
INFO: iteration 16, average log likelihood -1.410825
INFO: iteration 17, average log likelihood -1.410733
INFO: iteration 18, average log likelihood -1.410652
INFO: iteration 19, average log likelihood -1.410581
INFO: iteration 20, average log likelihood -1.410517
INFO: iteration 21, average log likelihood -1.410460
INFO: iteration 22, average log likelihood -1.410409
INFO: iteration 23, average log likelihood -1.410364
INFO: iteration 24, average log likelihood -1.410322
INFO: iteration 25, average log likelihood -1.410285
INFO: iteration 26, average log likelihood -1.410250
INFO: iteration 27, average log likelihood -1.410219
INFO: iteration 28, average log likelihood -1.410189
INFO: iteration 29, average log likelihood -1.410162
INFO: iteration 30, average log likelihood -1.410136
INFO: iteration 31, average log likelihood -1.410111
INFO: iteration 32, average log likelihood -1.410087
INFO: iteration 33, average log likelihood -1.410065
INFO: iteration 34, average log likelihood -1.410043
INFO: iteration 35, average log likelihood -1.410021
INFO: iteration 36, average log likelihood -1.410000
INFO: iteration 37, average log likelihood -1.409980
INFO: iteration 38, average log likelihood -1.409960
INFO: iteration 39, average log likelihood -1.409940
INFO: iteration 40, average log likelihood -1.409921
INFO: iteration 41, average log likelihood -1.409903
INFO: iteration 42, average log likelihood -1.409885
INFO: iteration 43, average log likelihood -1.409867
INFO: iteration 44, average log likelihood -1.409850
INFO: iteration 45, average log likelihood -1.409834
INFO: iteration 46, average log likelihood -1.409818
INFO: iteration 47, average log likelihood -1.409803
INFO: iteration 48, average log likelihood -1.409788
INFO: iteration 49, average log likelihood -1.409774
INFO: iteration 50, average log likelihood -1.409761
INFO: EM with 100000 data points 50 iterations avll -1.409761
59.0 data points per parameter
5: avll = [-1.41262,-1.41256,-1.4125,-1.41243,-1.41235,-1.41225,-1.41212,-1.41198,-1.41182,-1.41165,-1.41148,-1.41132,-1.41118,-1.41104,-1.41093,-1.41082,-1.41073,-1.41065,-1.41058,-1.41052,-1.41046,-1.41041,-1.41036,-1.41032,-1.41028,-1.41025,-1.41022,-1.41019,-1.41016,-1.41014,-1.41011,-1.41009,-1.41006,-1.41004,-1.41002,-1.41,-1.40998,-1.40996,-1.40994,-1.40992,-1.4099,-1.40988,-1.40987,-1.40985,-1.40983,-1.40982,-1.4098,-1.40979,-1.40977,-1.40976]
[-1.42195,-1.42197,-1.42191,-1.42187,-1.42182,-1.42175,-1.42166,-1.42149,-1.42114,-1.42045,-1.41934,-1.41811,-1.41728,-1.4169,-1.41676,-1.41671,-1.41669,-1.41669,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41668,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667,-1.41667,-1.41669,-1.41663,-1.41658,-1.41653,-1.41647,-1.4164,-1.41632,-1.41624,-1.41617,-1.4161,-1.41605,-1.41601,-1.41597,-1.41594,-1.41592,-1.4159,-1.41589,-1.41588,-1.41587,-1.41587,-1.41586,-1.41586,-1.41585,-1.41585,-1.41584,-1.41584,-1.41583,-1.41583,-1.41583,-1.41582,-1.41582,-1.41582,-1.41581,-1.41581,-1.41581,-1.41581,-1.4158,-1.4158,-1.4158,-1.4158,-1.4158,-1.41579,-1.41579,-1.41579,-1.41579,-1.41579,-1.41578,-1.41578,-1.41578,-1.41578,-1.41579,-1.41573,-1.41568,-1.41562,-1.41555,-1.41546,-1.41536,-1.41526,-1.41517,-1.41508,-1.41501,-1.41494,-1.41489,-1.41485,-1.41481,-1.41478,-1.41475,-1.41473,-1.41471,-1.41469,-1.41467,-1.41466,-1.41465,-1.41464,-1.41462,-1.41462,-1.41461,-1.4146,-1.41459,-1.41458,-1.41457,-1.41457,-1.41456,-1.41455,-1.41455,-1.41454,-1.41453,-1.41453,-1.41452,-1.41452,-1.41451,-1.41451,-1.4145,-1.4145,-1.41449,-1.41449,-1.41448,-1.41448,-1.41447,-1.41447,-1.41447,-1.41442,-1.41437,-1.41432,-1.41426,-1.41418,-1.41409,-1.41399,-1.41389,-1.41378,-1.41368,-1.4136,-1.41351,-1.41344,-1.41338,-1.41332,-1.41327,-1.41323,-1.41319,-1.41315,-1.41312,-1.41309,-1.41307,-1.41304,-1.41302,-1.41299,-1.41297,-1.41295,-1.41293,-1.41291,-1.41289,-1.41287,-1.41285,-1.41284,-1.41282,-1.4128,-1.41279,-1.41277,-1.41276,-1.41274,-1.41273,-1.41271,-1.4127,-1.41269,-1.41268,-1.41266,-1.41265,-1.41264,-1.41263,-1.41262,-1.41262,-1.41256,-1.4125,-1.41243,-1.41235,-1.41225,-1.41212,-1.41198,-1.41182,-1.41165,-1.41148,-1.41132,-1.41118,-1.41104,-1.41093,-1.41082,-1.41073,-1.41065,-1.41058,-1.41052,-1.41046,-1.41041,-1.41036,-1.41032,-1.41028,-1.41025,-1.41022,-1.41019,-1.41016,-1.41014,-1.41011,-1.41009,-1.41006,-1.41004,-1.41002,-1.41,-1.40998,-1.40996,-1.40994,-1.40992,-1.4099,-1.40988,-1.40987,-1.40985,-1.40983,-1.40982,-1.4098,-1.40979,-1.40977,-1.40976]
32×26 Array{Float64,2}:
 -0.532664   -0.0773211    0.379293     0.865648    0.115379     0.079098   -0.553874    -0.259387   -0.908925     0.211008    0.32243     0.361729    -0.19334       0.249611     0.140917   -0.65385     -0.211497   -0.317556    -0.0194313   -0.0465355   -0.568538    -0.404179    -0.191905   -0.385282     0.0935956  -0.32058   
 -0.134907    0.0620013    0.536017     0.38616     0.0607322   -0.819575   -0.470219    -0.721899   -0.698349    -0.154091   -0.0492585   0.0710313    0.905354      0.103025     0.514171    0.314556     0.674598   -0.452162    -0.640746    -0.386291    -0.00719331  -0.518263     0.21074    -0.612124     0.489047   -0.323393  
 -0.749958    0.559652     0.219814     0.186436   -0.196881    -0.0976228  -0.33423     -0.953631    0.507627     0.501343   -0.300693   -0.352217    -0.18427       0.32002     -0.214549   -0.115999     0.364226    0.232181    -0.569531    -0.197097    -0.231242     0.120632    -0.033466   -0.179764    -0.232245    0.201394  
  0.392151    0.843017    -0.13245      0.106269    0.134198    -0.0130764  -0.340866    -0.548932    0.186188     0.373425   -0.609931    0.538525    -0.287662      0.0351317    0.0817219   0.0273544   -0.0123472   0.0938303    0.144428     0.121663    -0.456806     0.560954    -0.43972    -0.194773    -0.134432   -0.00857272
 -0.124941    0.00254969  -0.0426647    0.279554   -0.311033    -0.0513137   0.275099    -0.225422   -0.173481    -0.605616    0.209392    0.126286     0.730575      0.129617     0.094593    0.15718      0.214267    0.0978068    0.141758    -0.0816794   -0.481864    -0.134253    -0.689449    0.370872    -0.0560316   0.0390321 
 -0.153576    0.0105774    0.541408     0.752375   -0.714698    -0.137229    0.237808     0.06336    -0.361256    -0.631922   -0.471007    0.327921     0.155135      0.0766134    0.0867933  -0.0435665    0.249795    0.287106     0.822234     0.154262    -0.024291     0.127192     0.11633    -0.0894534    0.112975   -0.407147  
  0.229245   -0.669573    -0.0309203    0.314556   -0.353804     0.0428591   0.502719     0.423759   -0.23647     -0.423486    0.409823   -0.714574    -0.14088       0.485892     0.146041    0.137036     0.437957   -0.510039    -0.0684038   -0.0818878    0.372518    -0.00468128   0.243168    0.140155    -0.334254   -0.0707128 
 -0.521541   -0.660729    -0.104525     0.380978   -0.0166373    0.402019    0.277923     0.268111   -0.324599    -0.5194      0.22976    -0.309219     0.650764     -0.316205    -0.248051   -0.132124    -0.0901812  -0.148532     0.0104252    0.134501     0.912098    -0.146147     0.569503    0.3222       0.324825   -0.0822999 
 -0.837386   -0.00922545  -0.284099    -0.0481634  -0.134005    -0.701671    0.0691575    0.199348    0.00199946   0.207692    0.169336    0.0231469    0.0188962     0.135405    -0.0922053  -0.0712734   -0.569859    0.140263    -0.18086      0.214637    -0.0235794    0.346683    -0.443915   -0.156962     0.381576    0.190495  
 -0.229336   -0.00473701  -0.19534      0.589873    0.138122     0.783101    0.234994     0.554513    0.520007     0.332101    0.027045    0.342033     0.0276194    -0.24572     -0.286806   -0.663847     0.26703     0.216055     0.00837382   0.0374234    0.515697     0.243354    -0.486066   -0.00809963  -0.561261   -0.483876  
  0.278497   -0.0396743    0.00656712  -0.151172    0.00549725   0.123043   -0.0245219    0.0620955   0.074111    -0.0643275  -0.0341867   0.0179759    0.000536893   0.0340868   -0.0173523  -0.0196922    0.167912   -0.135168    -0.0324481   -0.13823      0.00333095  -0.187179     0.131102    0.187345    -0.0723347  -0.00793932
  0.139843   -0.0453534    0.333385    -0.114454   -0.270815    -0.286705    0.179198     0.398547    0.133618    -0.161395   -0.337949    0.456115    -0.0650792    -0.505189     0.255444   -0.204554    -0.514771    0.991486    -0.806023    -0.198448     0.744904    -0.223777     0.327292   -0.522809    -0.143403   -0.37081   
  0.222067    0.271332     0.0944438   -0.90869     0.354756    -0.376028   -0.469063     0.0304895  -0.318665    -0.0906834  -0.173424   -0.187065    -0.033909      0.0343786    0.043423    0.745862    -0.241066   -0.151458     0.323914     0.140696    -0.427739    -0.131254    -0.030413    0.0871899    0.27661     0.116786  
  0.355311   -0.164714    -1.19879     -0.742086    0.339505     0.439601    0.69782      0.558161    0.400629     0.02605     0.144599    0.0749908   -0.413689     -0.254131    -0.299748    0.679824    -0.262607   -0.13154      0.0272414   -0.132885     0.454713     0.348069    -0.0445887   0.359349     0.224702    0.346624  
  0.0538674  -0.412656    -0.0269663    0.182408    0.109886     0.042258    0.407531    -0.160985    0.666287    -0.368719   -0.534435   -0.0944949    0.153957     -0.531146     0.363876   -0.0719361    0.219884   -0.663679     0.534939    -0.11217     -0.0381633    0.351518    -0.190549   -0.0684865    0.316726    0.377934  
  0.325147    0.279118    -0.383075    -0.0509407  -0.0809375    0.46733     0.482999     0.156276    0.179799    -0.163156   -0.303047    0.151235    -0.0398048    -0.14178      0.115167   -0.13284     -0.212177    0.302461     0.422052     0.399622     0.190665     0.681887    -0.184942    0.50755      0.229883    0.249083  
  0.206011   -0.0769027    0.896595    -0.264621   -0.168826    -0.2389     -0.246861     0.183238   -0.133785     0.253689    0.107598    0.285898     0.139661     -0.297164    -0.0771904  -0.571889    -0.31264     0.00433432   0.119304     0.837806     0.176053     0.0686488    0.110821    0.175604    -0.0247343   0.0748266 
 -0.354585   -0.384617     0.159936    -0.391067   -0.452398    -0.0386551   0.0527512    0.228886    0.407533    -0.186045    0.195461    0.199948    -0.640203     -0.140099    -0.342006   -0.56368     -0.535151    0.374027    -0.00824153  -0.116673    -0.92474      0.0518797    0.299981    0.57021     -0.170487    0.133529  
 -0.321689   -0.148915    -0.332111    -0.51075     0.210833    -0.223524   -0.177562     0.135255    0.218066     0.123956    0.47858    -0.00601856   0.26939      -0.35562      0.168334   -0.207939    -0.594139   -0.346023    -0.905663    -0.206476    -0.0397145   -0.349297    -0.0764687   0.0354219    0.26351     0.40363   
 -0.176838   -0.429264     0.0145459   -0.121236    0.0638294    0.221438    0.200733     0.452206   -0.617479     0.0209859   0.442038    0.367294     0.110346      0.204263    -0.0973544   0.0360799   -0.575019    0.196085     0.50439     -0.194431     0.257284    -0.226037     0.0917872  -0.00881553   0.669364   -0.00934401
 -0.667695   -0.0432464    0.126787     0.576804   -0.234281    -0.0297849   0.0766789    0.0571316  -0.0902511   -0.0956004   0.0434998   0.187223     0.176799     -0.00786547  -0.204032   -0.381379    -0.037839    0.117044    -0.0842496    0.237245     0.0874895    0.198882    -0.265909    0.0102036   -0.10927    -0.147277  
  0.144186   -0.0606834    0.0303024    0.330293    0.0303694    0.0516607   0.194356    -0.356186    0.238945    -0.103627   -0.170476    0.122814    -0.246582      0.00381597  -0.017417   -0.01694      0.370875   -0.186443    -0.0250686   -0.0108245   -0.0882084    0.25336     -0.0643239  -0.319619    -0.375511    0.065623  
 -0.0245935   0.0439472    0.102051    -0.105113   -0.342922    -0.128912   -0.00323341   0.0255683  -0.0124822    0.0134425   0.125387   -0.0477347    0.0138087     0.0727919    0.0455363   0.0633504    0.0543439   0.0672125   -0.113028    -0.0532614   -0.108098    -0.238817     0.107436    0.0486879    0.0563281  -0.0256018 
  0.115018    0.0573527   -0.175336    -0.211499    0.407692     0.136985   -0.0651019    0.170358    0.0982223   -0.0747906  -0.103984    0.0743605   -0.00360593   -0.0888501    0.0573448   0.121689    -0.0613818  -0.130858     0.210466     0.161669     0.0489396    0.187442    -0.0537632   0.0895938    0.160196   -0.0449535 
 -0.386567    0.0297017   -0.29419     -0.609557    0.168454    -0.177708    0.103156     0.357474    0.159568     0.317021    0.0640614   0.0152097   -0.656463     -0.764177     0.0432628   0.00113485   0.190164   -0.0431186    0.00407701  -0.200776     0.026909    -0.367825    -0.0608221  -0.821246    -0.0290559  -0.0639535 
  0.29765     0.0227536   -0.29649     -0.432849   -0.318432    -0.0436769  -0.264472     0.22611     0.112667     0.0109148  -0.373877   -0.174795    -0.2941        0.35163      0.143292   -0.0277427    0.449446    0.0420973   -0.112914    -0.546991    -0.276562    -0.374791    -0.170797   -0.033308    -0.0683746  -0.570225  
  0.675466   -0.0330012    0.0295612   -0.248789   -0.555998     0.68383    -0.754367    -0.310945    0.104402    -0.227226    0.301616    0.0740166   -0.23727      -0.183812    -0.0298815  -0.281361     0.506225   -0.431891    -0.636607    -0.0168542   -0.165989    -0.297249     0.744332    0.565253    -0.135536    0.257699  
  1.03028     0.0896247    0.408769    -0.290339    0.411156     0.93622     0.0869357   -0.420517    0.0718499   -0.344626    0.0859375   0.782495    -0.392958     -0.654317     0.348889    0.52635      0.610623   -0.347263     0.519211    -0.331956    -0.161398    -0.649027     0.564353   -0.356204     0.3429     -0.277643  
  0.250629   -0.0351802    0.00564576  -0.24403     0.632242    -0.0488083  -0.400947     0.0276899   0.331395     0.684606   -0.177338   -0.643755    -0.467557      0.441181    -0.216853    0.00319199  -0.0835329  -0.507794    -0.423313    -0.0567053    0.468335     0.452348     0.285257   -0.0175831    0.131347    0.169808  
  0.05914     0.544218    -0.122679     0.163854    0.0731201   -0.527058    0.0302227    0.0603745  -0.142364     0.209744   -0.66283    -0.142148     0.649469      0.408836     0.0418703  -0.0387779   -0.256009   -0.38688     -0.0559126   -0.225287     0.414383    -0.186818     0.243887    0.150403    -0.108649   -0.108958  
 -0.171806    0.692658    -0.064786    -0.0614347  -0.0224432    0.0929215   0.0731929    0.123658   -0.245404     0.376679    1.02909     0.122852    -0.102868      0.490933    -0.0117755   0.433527    -0.0171305   0.67303     -0.750444     0.184276     0.288338    -0.189034     0.383217   -0.112644    -0.472016   -0.10968   
 -0.0252681   0.45123      0.118908     0.172688    0.312622     0.462001   -0.290831    -0.175845   -0.296057     0.594971    0.550594   -0.412563     0.316728      1.09164     -0.231446    0.409225     0.104496    0.062162     0.583333    -0.00323639  -0.103654    -0.0463288    0.0502954   0.472541     0.0862121   0.206695  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.409748
INFO: iteration 2, average log likelihood -1.409735
INFO: iteration 3, average log likelihood -1.409723
INFO: iteration 4, average log likelihood -1.409711
INFO: iteration 5, average log likelihood -1.409700
INFO: iteration 6, average log likelihood -1.409689
INFO: iteration 7, average log likelihood -1.409678
INFO: iteration 8, average log likelihood -1.409668
INFO: iteration 9, average log likelihood -1.409657
INFO: iteration 10, average log likelihood -1.409648
INFO: EM with 100000 data points 10 iterations avll -1.409648
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.128012e+05
      1       7.039639e+05      -2.088372e+05 |       32
      2       6.917891e+05      -1.217485e+04 |       32
      3       6.867148e+05      -5.074299e+03 |       32
      4       6.839813e+05      -2.733440e+03 |       32
      5       6.821815e+05      -1.799806e+03 |       32
      6       6.808218e+05      -1.359732e+03 |       32
      7       6.797909e+05      -1.030932e+03 |       32
      8       6.789865e+05      -8.044110e+02 |       32
      9       6.783031e+05      -6.834053e+02 |       32
     10       6.777125e+05      -5.905511e+02 |       32
     11       6.771968e+05      -5.156888e+02 |       32
     12       6.767985e+05      -3.983377e+02 |       32
     13       6.764804e+05      -3.180600e+02 |       32
     14       6.761997e+05      -2.807632e+02 |       32
     15       6.759471e+05      -2.525828e+02 |       32
     16       6.757364e+05      -2.107113e+02 |       32
     17       6.755483e+05      -1.880604e+02 |       32
     18       6.753660e+05      -1.822580e+02 |       32
     19       6.752076e+05      -1.584608e+02 |       32
     20       6.750760e+05      -1.316247e+02 |       32
     21       6.749595e+05      -1.164964e+02 |       32
     22       6.748544e+05      -1.050280e+02 |       32
     23       6.747638e+05      -9.059471e+01 |       32
     24       6.746968e+05      -6.702665e+01 |       32
     25       6.746389e+05      -5.793338e+01 |       32
     26       6.745837e+05      -5.519452e+01 |       32
     27       6.745319e+05      -5.182245e+01 |       32
     28       6.744760e+05      -5.584448e+01 |       32
     29       6.744267e+05      -4.936544e+01 |       32
     30       6.743826e+05      -4.403788e+01 |       32
     31       6.743399e+05      -4.273238e+01 |       32
     32       6.743023e+05      -3.762090e+01 |       32
     33       6.742676e+05      -3.465941e+01 |       32
     34       6.742353e+05      -3.226295e+01 |       32
     35       6.742050e+05      -3.030050e+01 |       32
     36       6.741708e+05      -3.421561e+01 |       32
     37       6.741309e+05      -3.990472e+01 |       32
     38       6.740927e+05      -3.822456e+01 |       32
     39       6.740602e+05      -3.254717e+01 |       32
     40       6.740316e+05      -2.850271e+01 |       32
     41       6.740063e+05      -2.535388e+01 |       32
     42       6.739792e+05      -2.707336e+01 |       32
     43       6.739558e+05      -2.343863e+01 |       32
     44       6.739346e+05      -2.119425e+01 |       32
     45       6.739131e+05      -2.151708e+01 |       32
     46       6.738881e+05      -2.498373e+01 |       32
     47       6.738653e+05      -2.277929e+01 |       32
     48       6.738449e+05      -2.037620e+01 |       32
     49       6.738255e+05      -1.948009e+01 |       32
     50       6.738071e+05      -1.836493e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 673807.0859581415)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.421705
INFO: iteration 2, average log likelihood -1.416660
INFO: iteration 3, average log likelihood -1.415321
INFO: iteration 4, average log likelihood -1.414366
INFO: iteration 5, average log likelihood -1.413362
INFO: iteration 6, average log likelihood -1.412379
INFO: iteration 7, average log likelihood -1.411635
INFO: iteration 8, average log likelihood -1.411188
INFO: iteration 9, average log likelihood -1.410935
INFO: iteration 10, average log likelihood -1.410779
INFO: iteration 11, average log likelihood -1.410668
INFO: iteration 12, average log likelihood -1.410581
INFO: iteration 13, average log likelihood -1.410508
INFO: iteration 14, average log likelihood -1.410446
INFO: iteration 15, average log likelihood -1.410390
INFO: iteration 16, average log likelihood -1.410341
INFO: iteration 17, average log likelihood -1.410295
INFO: iteration 18, average log likelihood -1.410254
INFO: iteration 19, average log likelihood -1.410216
INFO: iteration 20, average log likelihood -1.410180
INFO: iteration 21, average log likelihood -1.410148
INFO: iteration 22, average log likelihood -1.410117
INFO: iteration 23, average log likelihood -1.410089
INFO: iteration 24, average log likelihood -1.410062
INFO: iteration 25, average log likelihood -1.410037
INFO: iteration 26, average log likelihood -1.410014
INFO: iteration 27, average log likelihood -1.409992
INFO: iteration 28, average log likelihood -1.409971
INFO: iteration 29, average log likelihood -1.409952
INFO: iteration 30, average log likelihood -1.409934
INFO: iteration 31, average log likelihood -1.409917
INFO: iteration 32, average log likelihood -1.409900
INFO: iteration 33, average log likelihood -1.409884
INFO: iteration 34, average log likelihood -1.409869
INFO: iteration 35, average log likelihood -1.409855
INFO: iteration 36, average log likelihood -1.409841
INFO: iteration 37, average log likelihood -1.409828
INFO: iteration 38, average log likelihood -1.409815
INFO: iteration 39, average log likelihood -1.409802
INFO: iteration 40, average log likelihood -1.409790
INFO: iteration 41, average log likelihood -1.409778
INFO: iteration 42, average log likelihood -1.409767
INFO: iteration 43, average log likelihood -1.409756
INFO: iteration 44, average log likelihood -1.409745
INFO: iteration 45, average log likelihood -1.409734
INFO: iteration 46, average log likelihood -1.409724
INFO: iteration 47, average log likelihood -1.409714
INFO: iteration 48, average log likelihood -1.409704
INFO: iteration 49, average log likelihood -1.409694
INFO: iteration 50, average log likelihood -1.409685
INFO: EM with 100000 data points 50 iterations avll -1.409685
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.296202      0.32809      0.192676    0.12394     0.133131    -0.500735    -0.573166    -0.803469   -1.06095    -0.160215   -0.113755    0.423265     0.672579   -0.0659426    0.729988    0.883828    0.358557    -0.368207   -0.612698    -0.244639    -0.0272457  -0.169426     0.383966    -0.654917    0.93269    -0.157259  
 -0.719038     -0.0257032   -0.168016   -0.138545   -0.110682     0.316519     0.182619     0.805486    1.01057    -0.0349244  -0.019741   -0.056474    -0.116924    0.134932    -0.759874   -0.808781   -0.307812     0.272484    0.314995     0.096903    -0.176823    0.126881    -0.237061     0.596285   -0.800713    0.0174455 
  0.0654857     0.281317    -0.243168   -0.0378437   0.240543    -0.30987      0.00994944   0.125002    0.205305    0.293849   -0.540094   -0.178928     0.0337754   0.075848     0.11958    -0.004531   -0.124737    -0.444828   -0.216563    -0.0887804    0.484347    0.075148     0.34228     -0.0487901  -0.0418649  -0.104341  
 -0.0558649    -0.0328008   -0.353189    0.336693    0.186774     0.239874     0.59234     -0.356974    0.221934   -0.265794    0.17429    -0.355431     0.25759     0.407727     0.550337    0.351787    0.525484    -0.450526    0.120346    -0.367326    -0.121213    0.201241    -0.00235588   0.0911945   0.0190211   0.260749  
  0.791161     -0.0791026    0.207469   -0.291984   -0.396327     0.787194    -0.581349    -0.24096    -0.104312   -0.128791    0.344802    0.194638    -0.230263   -0.264901    -0.122531   -0.242908    0.485146    -0.444955   -0.388123    -0.0608331   -0.0231991  -0.456877     0.918006     0.469762   -0.0565848   0.078671  
 -0.321396      0.387717     0.652154    0.834556    0.0832614    0.00995729  -0.186106    -0.412268   -0.580451    0.404614    0.622577    0.0224774    0.490767    0.585218     0.0543132  -0.278099    0.144304    -0.232013   -0.0748428    0.240074    -0.0544943  -0.351149    -0.294762    -0.199154   -0.171137    0.0128633 
  0.623329     -0.0109032   -0.103537   -0.516632   -0.0128498    0.278287    -0.147141    -0.115928    0.602312    0.0261892  -0.361281   -0.144027    -0.445332   -0.124223     0.557056    0.0876009   0.413718    -0.0266192   0.075148    -0.341922    -0.269521   -0.188632     0.249025    -0.102404    0.058196   -0.273382  
  0.0062014    -0.317356    -0.358111    0.0797524  -0.311859    -0.274012    -0.123996     0.358665   -0.542271   -0.29978    -0.268871    0.00624833   0.170698    0.543684     0.162802   -0.216076    0.49182     -0.294198   -0.368161    -0.730275    -0.109363   -0.717946    -0.271152     0.356358   -0.385469   -0.438797  
 -0.518479     -0.100741    -0.162234    0.540878   -0.361526     0.226805     0.38905      0.155552   -0.367671   -0.492826    0.108561    0.125837     0.89377    -0.0462919   -0.457038    0.144932    0.296076     0.332329    0.448708     0.123059     0.474875   -0.301975    -0.0267617    0.089003    0.365977   -0.347666  
  0.0463809    -0.209066     0.0339168   0.496772   -0.140392     0.220906     0.428953     0.519981   -0.455373   -0.136248    0.0644219  -0.13508     -0.228571    0.0427716   -0.272862   -0.307959    0.226446    -0.130088   -0.0328007    0.718219     0.673549    0.540348    -0.0462704   -0.0204038  -0.305694   -0.538656  
 -0.675646     -0.262422    -0.18747    -0.0810946   0.0633567   -1.04445      0.117796     0.194889   -0.0970927   0.430139    0.197102   -0.240423    -0.0678689   0.162463    -0.171566    0.11372    -0.397026    -0.172999   -0.415759     0.251097     0.268744    0.462997    -0.352594    -0.290804    0.61994     0.379082  
 -0.170133     -0.605693     0.329984    0.20965    -0.406132     0.0130518    0.289881     0.182276    0.0858307  -0.222169    0.781419   -0.339114     0.348935    0.0724844   -0.017974   -0.121347   -0.0666016   -0.0901605  -0.353666    -0.131452     0.285769   -0.271769     0.342898     0.423228   -0.209215    0.360601  
  0.0577054     0.782324    -0.485445   -0.182313   -0.115718    -0.203469    -0.585868    -0.201953    0.672477    0.346477    0.078952    0.208249    -0.041516   -0.0764361    0.434569   -0.15176    -0.442081     0.0573242  -0.531276     0.258439    -0.497764    0.25201     -0.671374     0.278317    0.211728    0.300628  
 -0.0760299     0.0874128    0.105872    0.193334   -0.0464617   -0.00356152   0.0389191   -0.346248    0.169887   -0.0971401  -0.08343     0.129819    -0.0312609  -0.0241318   -0.03541    -0.0372105   0.10383      0.0121661  -0.0431825    0.124593    -0.157521    0.1659      -0.125807    -0.0754473  -0.144192    0.126644  
  0.164481     -0.0419669   -0.0256992  -0.226753    0.325287    -0.0383182    0.0592552    0.455604   -0.253068   -0.320961    0.0206041   0.319147     0.482214   -0.184157     0.161017    0.161581   -0.348688    -0.0932055   0.495385     0.309342     0.206489    0.00650397  -0.0236385    0.311326    0.410088   -0.0749154 
 -0.409211     -0.197595     0.157609   -0.229789   -0.136256     0.0104524    0.0750915   -0.0536095   0.446221    0.234084    0.838438    0.404374    -0.430324   -0.856598    -0.0492124  -0.235461    0.428891     0.219571    0.179559     0.345999    -0.899124    0.394805    -0.495106    -0.15169    -0.187027   -0.163579  
 -0.292664      0.41609      0.163067    0.256414    0.28115      0.191763    -0.0476477   -0.413288    0.809096    0.53728    -0.44341     0.0698406   -0.0166078   0.0579745   -0.28382    -0.45465     0.115694     0.297442   -0.483554    -0.217846     0.772729    0.519725     0.143115    -0.42777    -0.388701   -0.0333704 
 -0.0671847    -0.786464    -0.213525    0.070785    0.123524     0.347889     0.269617     0.0381179   0.500455   -0.183971   -0.254674    0.0695839   -0.0575528  -0.585349    -0.183893   -0.283638    0.0260427   -0.552289    0.187724    -0.0949362    0.152722    0.477915    -0.0238299   -0.0716349   0.493199    0.458318  
  0.412197      0.301269    -0.0412379  -0.570713    0.5925       0.132048    -0.486479     0.0429758  -0.176613    0.396394    0.174675   -0.596009     0.0294527   0.638867    -0.307172    0.647938   -0.113014    -0.216339    0.175269     0.00458291   0.0194461  -0.00708144   0.0165367    0.334959    0.148098    0.248338  
  0.519977      0.408406     0.273684    0.527559   -0.216482     0.488967     0.167481    -0.215061    0.178615   -0.276812   -0.672783    0.162784     0.192018    0.166917     0.0611514  -0.0641167   0.10632      0.20929     0.700473     0.227816    -0.305553    0.484334    -0.426862     0.685872   -0.0131348  -8.07421e-5
 -0.31391       0.0484687    0.950727    0.0624347  -0.226159    -0.754933    -0.817194    -0.182571    0.290008    0.185777   -0.356153   -0.393037     0.190849    0.251498    -0.180559   -0.107079    0.206218    -0.104337   -0.310769    -0.26968     -0.26946    -0.355344     0.160814    -0.277879   -0.268656   -0.140745  
  0.409339      0.308127     0.0495352  -0.198306    0.128109    -0.170142     0.111566     0.140345    0.0244552  -0.183147   -0.516281    0.445007    -0.375506   -0.441861     0.0705372   0.591829    0.205875    -0.185133    0.648346    -0.0347068   -0.338447   -0.0644415   -0.361761    -0.645381   -0.441897    0.0268324 
 -0.0111793     0.0018682    0.154464    0.0151878  -0.0163697   -0.0970258   -0.0583354    0.0375549  -0.083692   -0.0350911  -0.0506988   0.0213909    0.0881901  -0.00991209   0.0250516  -0.0455381   0.0848028   -0.0385121   0.0343845    0.0425261   -0.0330265  -0.0935792   -0.0119167    0.0590477  -0.0203259  -0.0972095 
 -0.251263      0.609233    -0.0427177  -0.198079   -0.357054    -0.0812667    0.100049     0.0653207  -0.139117    0.331427    0.797333    0.231181    -0.227833    0.344515    -0.177762    0.437527   -0.235726     1.09814    -0.586397     0.00367958  -0.0704835  -0.263232     0.602098     0.0830688  -0.476658   -0.119184  
 -0.38626       0.476891    -0.370954    0.0356637   0.00341264  -0.0554373   -0.558433    -0.601839   -0.294971    0.405345   -0.553364    0.121317    -0.559707    0.429878    -0.251627   -0.0556646   0.116314     0.0546474  -0.0482244   -0.0391988   -0.72673     0.258216    -0.397178    -0.263101    0.143248   -0.28949   
 -0.33943      -0.427513     0.201681    0.20097     0.0593257    0.411425     0.0312142    0.339597   -0.953651    0.331606    0.49866     0.48206     -0.312749    0.408092    -0.0195756  -0.377604   -0.816264     0.255466    0.262753    -0.173751     0.162533   -0.253222     0.215759    -0.388564    0.554946   -0.169488  
 -0.0334563     0.00172186  -0.075827   -0.182442   -0.0945216    0.0882466    0.0258382    0.328437   -0.0912836   0.0644475   0.197692   -0.0106783   -0.0444313   0.124845    -0.0387099  -0.0111838   0.00249266   0.0570807  -0.00569191  -0.0694616    0.0290459  -0.120913    -0.017637     0.0245055   0.133937   -0.135676  
 -0.442202     -0.473994    -0.100454   -0.383699   -0.530229    -0.470839    -0.0975482    0.088524   -0.0438782  -0.60505    -0.0264162  -0.0486512   -0.178137   -0.124711    -0.173038    0.0571981  -0.448539    -0.10296     0.317686    -0.469145    -0.798821   -0.447663     0.00808239   0.377177    0.683524    0.307981  
 -0.276814     -0.100361    -0.360719   -0.890367    0.299734     0.0418383   -0.098428     0.380126    0.178617    0.205046    0.248523    0.0442715   -0.269321   -0.710279     0.162053   -0.0467897  -0.285464     0.0584716  -0.583491    -0.452618     0.289805   -0.70138      0.00118326  -0.520203   -0.0409459   0.0234011 
 -0.350813     -0.338711     0.421423    0.473031   -0.579046    -0.397826     0.398791    -0.169596   -0.207246   -0.699746   -0.294131    0.248634     0.335169   -0.56354      0.57176    -0.605538   -0.0680835    0.271875   -0.115289     0.00385887   0.0686657  -0.0235286   -0.0769819   -0.491559   -0.0132455  -0.390861  
 -0.000281323   0.0411492    0.55383    -0.35002     0.00322356  -0.207238    -0.359854     0.0208009  -0.154757    0.0601048  -0.185138    0.436043    -0.237061   -0.164168     0.0183819  -0.517681   -0.706749     0.0196023   0.0873124    0.770397    -0.148729    0.244947     0.514738     0.598056    0.0584968   0.231865  
  0.4079       -0.0629908   -0.892864   -0.456992    0.012156     0.373331     0.796401     0.492059    0.33672    -0.101199   -0.016853    0.131507    -0.394835   -0.332941    -0.114821    0.330912   -0.254029     0.0368437   0.0788213   -0.0507469    0.550685    0.494892     0.0471474    0.210846    0.274273    0.35353   INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.409676
INFO: iteration 2, average log likelihood -1.409667
INFO: iteration 3, average log likelihood -1.409658
INFO: iteration 4, average log likelihood -1.409650
INFO: iteration 5, average log likelihood -1.409641
INFO: iteration 6, average log likelihood -1.409633
INFO: iteration 7, average log likelihood -1.409625
INFO: iteration 8, average log likelihood -1.409617
INFO: iteration 9, average log likelihood -1.409609
INFO: iteration 10, average log likelihood -1.409601
INFO: EM with 100000 data points 10 iterations avll -1.409601
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
