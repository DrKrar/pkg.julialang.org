>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.7.0
INFO: Installing JLD v0.6.6
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.1
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.3.0
INFO: Installing ScikitLearnBase v0.2.0
INFO: Installing StaticArrays v0.1.0
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
WARNING: takebuf_string(b) is deprecated, use String(take!(b)) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in takebuf_string(::Base.AbstractIOBuffer{Array{UInt8,1}}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/BinDeps/src/dependencies.jl:887 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::SubString{String}) at ./sysimg.jl:14
 in evalfile(::SubString{String}, ::Array{String,1}) at ./loading.jl:572 (repeats 2 times)
 in cd(::##2#4, ::String) at ./file.jl:69
 in (::##1#3)(::IOStream) at ./none:13
 in open(::##1#3, ::String, ::String) at ./iostream.jl:152
 in eval(::Module, ::Any) at ./boot.jl:236
 in process_options(::Base.JLOptions) at ./client.jl:248
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/Rmath/deps/build.jl, in expression starting on line 39
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1283
Commit 581a034 (2016-11-19 20:28 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-101-generic #148-Ubuntu SMP Thu Oct 20 22:08:32 UTC 2016 x86_64 x86_64
Memory: 2.939281463623047 GB (651.8046875 MB free)
Uptime: 27910.0 sec
Load Avg:  1.0390625  0.9853515625  1.01416015625
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3499 MHz    1586293 s      10424 s     160296 s     733049 s         86 s
#2  3499 MHz     866794 s         90 s      98615 s    1700599 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.4
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.7.0
 - JLD                           0.6.6
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.1
 - Rmath                         0.1.4
 - SHA                           0.3.0
 - ScikitLearnBase               0.2.0
 - StaticArrays                  0.1.0
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: takebuf_string(b) is deprecated, use String(take!(b)) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in takebuf_string(::Base.AbstractIOBuffer{Array{UInt8,1}}) at ./deprecated.jl:50
 in #_write#17(::Array{Any,1}, ::Function, ::JLD.JldFile, ::String, ::Array{Any,1}, ::JLD.JldWriteSession) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:587
 in #write#14(::Array{Any,1}, ::Function, ::JLD.JldFile, ::String, ::Array{Any,1}, ::JLD.JldWriteSession) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:529
 in #jldopen#9(::Bool, ::Bool, ::Bool, ::Function, ::String, ::Bool, ::Bool, ::Bool, ::Bool, ::Bool) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:198
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::String, ::Bool, ::Bool, ::Bool, ::Bool, ::Bool) at ./<missing>:0
 in #jldopen#10(::Bool, ::Bool, ::Bool, ::Function, ::String, ::String) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:253
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::String, ::String) at ./<missing>:0
 in #jldopen#11(::Array{Any,1}, ::Function, ::JLD.##34#35{String,Array{Float64,2},Tuple{}}, ::String, ::Vararg{String,N}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:263
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::Function, ::String, ::String) at ./<missing>:0
 in #save#33(::Bool, ::Bool, ::Function, ::FileIO.File{FileIO.DataFormat{:JLD}}, ::String, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:1217
 in save(::FileIO.File{FileIO.DataFormat{:JLD}}, ::String, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:1214
 in #save#14(::Array{Any,1}, ::Function, ::String, ::String, ::Vararg{Any,N}) at /home/vagrant/.julia/v0.6/FileIO/src/loadsave.jl:54
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:8 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-2.4198920936391383e7,[792.52,99207.5],
[-1891.26 127.328 1030.92; 1654.59 -202.023 -697.904],

Array{Float64,2}[
[4747.7 -271.942 -2169.67; -271.942 900.799 143.319; -2169.67 143.319 1951.83],

[95084.6 312.719 2332.17; 312.719 99857.0 -128.826; 2332.17 -128.826 97930.5]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.580351e+03
      1       1.106641e+03      -4.737107e+02 |        8
      2       1.005823e+03      -1.008179e+02 |        8
      3       9.495263e+02      -5.629650e+01 |        2
      4       9.480092e+02      -1.517132e+00 |        0
      5       9.480092e+02       0.000000e+00 |        0
K-means converged with 5 iterations (objv = 948.0091535686843)
INFO: K-means with 272 data points using 5 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.072915
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.687160
INFO: iteration 2, lowerbound -3.544085
INFO: iteration 3, lowerbound -3.422273
INFO: iteration 4, lowerbound -3.303405
INFO: dropping number of Gaussions to 7
INFO: iteration 5, lowerbound -3.176075
INFO: iteration 6, lowerbound -3.043729
INFO: iteration 7, lowerbound -2.934478
INFO: dropping number of Gaussions to 6
INFO: iteration 8, lowerbound -2.853917
INFO: dropping number of Gaussions to 5
INFO: iteration 9, lowerbound -2.803115
INFO: dropping number of Gaussions to 4
INFO: iteration 10, lowerbound -2.781404
INFO: dropping number of Gaussions to 3
INFO: iteration 11, lowerbound -2.767879
INFO: iteration 12, lowerbound -2.755723
INFO: iteration 13, lowerbound -2.743794
INFO: iteration 14, lowerbound -2.727362
INFO: iteration 15, lowerbound -2.705465
INFO: iteration 16, lowerbound -2.677482
INFO: iteration 17, lowerbound -2.643417
INFO: iteration 18, lowerbound -2.604121
INFO: iteration 19, lowerbound -2.561355
INFO: iteration 20, lowerbound -2.517585
INFO: iteration 21, lowerbound -2.475338
INFO: iteration 22, lowerbound -2.436307
INFO: iteration 23, lowerbound -2.400822
INFO: iteration 24, lowerbound -2.368393
INFO: iteration 25, lowerbound -2.339485
INFO: iteration 26, lowerbound -2.317521
INFO: iteration 27, lowerbound -2.307674
INFO: dropping number of Gaussions to 2
INFO: iteration 28, lowerbound -2.303002
INFO: iteration 29, lowerbound -2.299262
INFO: iteration 30, lowerbound -2.299257
INFO: iteration 31, lowerbound -2.299255
INFO: iteration 32, lowerbound -2.299254
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Sun 20 Nov 2016 01:14:49 PM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Sun 20 Nov 2016 01:14:51 PM UTC: K-means with 272 data points using 5 iterations
11.3 data points per parameter
,Sun 20 Nov 2016 01:14:52 PM UTC: EM with 272 data points 0 iterations avll -2.072915
5.8 data points per parameter
,Sun 20 Nov 2016 01:14:53 PM UTC: GMM converted to Variational GMM
,Sun 20 Nov 2016 01:14:55 PM UTC: iteration 1, lowerbound -3.687160
,Sun 20 Nov 2016 01:14:55 PM UTC: iteration 2, lowerbound -3.544085
,Sun 20 Nov 2016 01:14:55 PM UTC: iteration 3, lowerbound -3.422273
,Sun 20 Nov 2016 01:14:55 PM UTC: iteration 4, lowerbound -3.303405
,Sun 20 Nov 2016 01:14:55 PM UTC: dropping number of Gaussions to 7
,Sun 20 Nov 2016 01:14:55 PM UTC: iteration 5, lowerbound -3.176075
,Sun 20 Nov 2016 01:14:55 PM UTC: iteration 6, lowerbound -3.043729
,Sun 20 Nov 2016 01:14:55 PM UTC: iteration 7, lowerbound -2.934478
,Sun 20 Nov 2016 01:14:56 PM UTC: dropping number of Gaussions to 6
,Sun 20 Nov 2016 01:14:56 PM UTC: iteration 8, lowerbound -2.853917
,Sun 20 Nov 2016 01:14:56 PM UTC: dropping number of Gaussions to 5
,Sun 20 Nov 2016 01:14:56 PM UTC: iteration 9, lowerbound -2.803115
,Sun 20 Nov 2016 01:14:56 PM UTC: dropping number of Gaussions to 4
,Sun 20 Nov 2016 01:14:56 PM UTC: iteration 10, lowerbound -2.781404
,Sun 20 Nov 2016 01:14:56 PM UTC: dropping number of Gaussions to 3
,Sun 20 Nov 2016 01:14:56 PM UTC: iteration 11, lowerbound -2.767879
,Sun 20 Nov 2016 01:14:56 PM UTC: iteration 12, lowerbound -2.755723
,Sun 20 Nov 2016 01:14:56 PM UTC: iteration 13, lowerbound -2.743794
,Sun 20 Nov 2016 01:14:56 PM UTC: iteration 14, lowerbound -2.727362
,Sun 20 Nov 2016 01:14:56 PM UTC: iteration 15, lowerbound -2.705465
,Sun 20 Nov 2016 01:14:56 PM UTC: iteration 16, lowerbound -2.677482
,Sun 20 Nov 2016 01:14:56 PM UTC: iteration 17, lowerbound -2.643417
,Sun 20 Nov 2016 01:14:56 PM UTC: iteration 18, lowerbound -2.604121
,Sun 20 Nov 2016 01:14:56 PM UTC: iteration 19, lowerbound -2.561355
,Sun 20 Nov 2016 01:14:56 PM UTC: iteration 20, lowerbound -2.517585
,Sun 20 Nov 2016 01:14:56 PM UTC: iteration 21, lowerbound -2.475338
,Sun 20 Nov 2016 01:14:56 PM UTC: iteration 22, lowerbound -2.436307
,Sun 20 Nov 2016 01:14:56 PM UTC: iteration 23, lowerbound -2.400822
,Sun 20 Nov 2016 01:14:57 PM UTC: iteration 24, lowerbound -2.368393
,Sun 20 Nov 2016 01:14:57 PM UTC: iteration 25, lowerbound -2.339485
,Sun 20 Nov 2016 01:14:57 PM UTC: iteration 26, lowerbound -2.317521
,Sun 20 Nov 2016 01:14:57 PM UTC: iteration 27, lowerbound -2.307674
,Sun 20 Nov 2016 01:14:57 PM UTC: dropping number of Gaussions to 2
,Sun 20 Nov 2016 01:14:57 PM UTC: iteration 28, lowerbound -2.303002
,Sun 20 Nov 2016 01:14:57 PM UTC: iteration 29, lowerbound -2.299262
,Sun 20 Nov 2016 01:14:57 PM UTC: iteration 30, lowerbound -2.299257
,Sun 20 Nov 2016 01:14:57 PM UTC: iteration 31, lowerbound -2.299255
,Sun 20 Nov 2016 01:14:57 PM UTC: iteration 32, lowerbound -2.299254
,Sun 20 Nov 2016 01:14:57 PM UTC: iteration 33, lowerbound -2.299253
,Sun 20 Nov 2016 01:14:57 PM UTC: iteration 34, lowerbound -2.299253
,Sun 20 Nov 2016 01:14:57 PM UTC: iteration 35, lowerbound -2.299253
,Sun 20 Nov 2016 01:14:57 PM UTC: iteration 36, lowerbound -2.299253
,Sun 20 Nov 2016 01:14:57 PM UTC: iteration 37, lowerbound -2.299253
,Sun 20 Nov 2016 01:14:57 PM UTC: iteration 38, lowerbound -2.299253
,Sun 20 Nov 2016 01:14:58 PM UTC: iteration 39, lowerbound -2.299253
,Sun 20 Nov 2016 01:14:58 PM UTC: iteration 40, lowerbound -2.299253
,Sun 20 Nov 2016 01:14:58 PM UTC: iteration 41, lowerbound -2.299253
,Sun 20 Nov 2016 01:14:58 PM UTC: iteration 42, lowerbound -2.299253
,Sun 20 Nov 2016 01:14:58 PM UTC: iteration 43, lowerbound -2.299253
,Sun 20 Nov 2016 01:14:58 PM UTC: iteration 44, lowerbound -2.299253
,Sun 20 Nov 2016 01:14:58 PM UTC: iteration 45, lowerbound -2.299253
,Sun 20 Nov 2016 01:14:58 PM UTC: iteration 46, lowerbound -2.299253
,Sun 20 Nov 2016 01:14:58 PM UTC: iteration 47, lowerbound -2.299253
,Sun 20 Nov 2016 01:14:58 PM UTC: iteration 48, lowerbound -2.299253
,Sun 20 Nov 2016 01:14:58 PM UTC: iteration 49, lowerbound -2.299253
,Sun 20 Nov 2016 01:14:58 PM UTC: iteration 50, lowerbound -2.299253
,Sun 20 Nov 2016 01:14:58 PM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.9549,178.045]
β = [95.9549,178.045]
m = [2.00023 53.852; 4.2503 79.2869]
ν = [97.9549,180.045]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.375876 -0.00895312; 0.0 0.0127487],

[0.184042 -0.00764405; 0.0 0.00858171]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.00000000004
avll from stats: -0.9776481303266858
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9776481303266847
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9776481303266847
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.00000000001
avll from stats: -0.9695407232663918
avll from llpg:  -0.9695407232663916
avll direct:     -0.9695407232663916
sum posterior: 100000.0
32×26 Array{Float64,2}:
  0.185703     -0.0270256   -0.00413692  -0.0918641    0.0348301     0.0867789   -0.0827804   -0.0098616   -0.0288904   0.0546803    0.0391629   -0.0609405   -0.180792    -0.0833328   0.0651585     0.178222     -0.115442    -0.0924125  -0.179004   -0.0524282    0.115796    0.0353085    0.00774678  -0.106992     -0.123784    -0.0349382 
  0.123856      0.0851544   -0.0998371    0.0645278   -0.0270774    -0.0597202    0.00550486  -0.0366881    0.106988    0.0864867   -0.128602     0.111       -0.114056    -0.0984802  -0.161058      0.143729      0.0923369    0.193371   -0.116197   -0.0914131   -0.058919   -0.0841246   -0.090813    -0.0720445     0.0552214    0.0177765 
 -0.1559       -0.0414851    0.142077     0.0594854   -0.13829       0.0807767   -0.289862    -0.113307    -0.177214   -0.108667    -0.0101178    0.144409     0.00280462  -0.221076    0.0484878     0.0724575    -0.0703555    0.0585655  -0.0368501   0.17312     -0.0190979  -0.0567959   -0.0858399    0.193955      0.0638956    0.0971375 
 -0.12589       0.0304144    0.215942     0.00433746   0.23127      -0.130932     0.071066     0.084165     0.0873898  -0.0655474    0.0324011    0.0131718    0.0580063   -0.0402447   0.0135857     0.000792205  -0.149191     0.0936844   0.0124387   0.00325902  -0.0836642  -0.0746009    0.0557363    0.032737      0.0575914    0.0235961 
 -0.152355     -0.0663797   -0.0525047   -0.136183    -0.00294329    0.139732     0.0274263   -0.0460051   -0.109683   -0.130068    -0.111508     0.139969     0.0043398    0.174899   -0.213631      0.0160855    -0.0476343    0.10374    -0.225107    0.109464     0.170775   -0.0713379   -0.00818613  -0.03981       0.0176427   -0.054406  
  0.0493918     0.152402    -0.0818827   -0.0155859    0.000415975  -0.0450315    0.00390898  -0.0584781    0.097766   -0.0416547    0.0484844    0.112843    -0.0706366   -0.0239791  -0.000777041   0.0490313     0.0359984   -0.0649499  -0.0756648  -0.0872122   -0.0563623  -0.0254161   -0.0175927    0.0561139     0.176878    -0.14498   
 -0.0285959    -0.0559413   -0.0107335   -0.128174    -0.113407      0.0883372    0.18135     -0.130307     0.141703   -0.0170934    0.0274678    0.2036      -0.120593     0.056027   -0.000121028  -0.0908426    -0.0234308   -0.0585577   0.0345129  -0.0677606    0.268049   -0.10385      0.0158696    0.0188864     0.0640868    0.0607807 
 -0.0534836    -0.0189594    0.0668247   -0.00746363  -0.0292346    -0.072908    -0.110427     0.0832646    0.0744964   0.0618922    0.094464    -0.0170668   -0.0808569   -0.0507187   0.071639     -0.201698     -0.0695055   -0.111604   -0.0976575  -0.0383389    0.0393124   0.0278192   -0.107819    -0.0542504    -0.0150429   -0.207024  
 -0.176602     -0.0223023    0.11695      0.136572    -0.0973327    -0.0644062   -0.0190976   -0.165813     0.225826    0.0878753    0.0419607    0.105453    -0.0217372   -0.0253673  -0.0625835     0.0775818    -0.00900309   0.0767789  -0.0874186   0.0246859   -0.0958989   0.00908765  -0.0169672   -0.102338     -0.0285139    0.0927053 
 -0.0493541    -0.0742026   -0.0700181    0.071252     0.129886      0.131092    -0.117554     0.248077     0.054265   -0.0776513    0.0348593   -0.0864699   -0.187653     0.041018    0.011851      0.0392748     0.0700406   -0.0967194  -0.0851495  -0.062307    -0.0263864  -0.0319829    0.136327    -0.083554     -0.14551      0.0777752 
 -0.0950293     0.152583    -0.157601    -0.0716791   -0.00465742   -0.214306     0.122963     0.111775    -0.0281384  -0.0211722    0.0467991    0.0375183    0.1236       0.206529    0.0403717    -0.0139036    -0.175068     0.116396    0.15418     0.197664    -0.135385   -0.0425164   -0.0411353    0.0205839    -0.184343    -0.0629804 
  0.164172      0.0966484    0.0863662   -0.127637     0.117257     -0.0189611   -0.0570014    0.0319148   -0.013649   -0.168219    -0.00597236   0.0715242   -0.106821    -0.073662   -0.00419559   -0.0177369    -0.113771     0.0180611   0.099525   -0.0019035   -0.150695    0.0988716   -0.0976149   -0.143125      0.0146646    0.130273  
 -0.0843811    -0.0296002    0.0833874    0.015904    -0.102607     -0.0171128   -0.113716     0.0129106   -0.101993    0.0588756   -0.0477404    0.14256      0.0371537    0.0146222   0.00931677    0.121386      0.0184368    0.139657    0.0581886  -0.0738002   -0.0156404  -0.0313049   -0.0617907   -0.0311853     0.112164    -0.105331  
 -0.131636      0.078336     0.0237078    0.190189    -0.106872      0.0520493    0.163524    -0.0480188    0.057924   -0.17318     -0.0278336   -0.0547516    0.00571834   0.0174527  -0.0199362    -0.0173172     0.00230656  -0.059377   -0.0211028   0.0306763   -0.0279978  -0.0746752    0.0506624    0.08675       0.075806    -0.132482  
 -0.0435367    -0.083639    -0.128577    -0.0433318    0.0519213     0.0527376   -0.00176651  -0.158073     0.0398666  -0.121124    -0.109109     0.171478     0.00574868  -0.0330675  -4.73191e-5   -0.0476447    -0.0345028    0.0359167  -0.0708467  -0.0439396   -0.0441491   0.107935    -0.160388     0.00204191    0.119871     0.0349188 
  0.036946      0.163201     0.043355    -0.0219455   -0.0574808     0.111831     0.0611345   -0.00787069  -0.0239723   0.107929     0.0359643   -0.0475937    0.00734622   0.267653    0.0500778    -0.0213173     0.0012053   -0.190878   -0.131051   -0.0139346    0.0109375  -0.171221     0.117378    -0.0195405     0.0712165    0.0813333 
 -0.123305      0.070334    -0.216063    -0.122828    -0.0927787     0.0817498   -0.0526264   -0.0651923   -0.0994814  -0.0394503    0.115403    -0.0398606    0.0176414    0.043675    0.0800008     0.144164     -0.0639133    0.0394926   0.0143104  -0.0363593    0.0684289  -0.00204979  -0.00563773   0.0465734     0.0216888    0.0858713 
  0.0807227    -0.00716828  -0.118203     0.0303258   -0.103661      0.0866118    0.264572    -0.0983753    0.102854   -0.0776351    0.0911862   -0.0561512    0.0141394    0.228809   -0.0365431    -0.0663755     0.191807    -0.0987443  -0.19616    -0.14866     -0.120417    0.00358261   0.00184428   0.0878591    -0.0780564   -0.0657542 
  0.134107      0.0238153    0.203857     0.0881811   -0.0445613    -0.187482     0.0394913   -0.0670382   -0.0246742  -0.136349    -0.0820009    0.160718     0.0672084   -0.0227197   0.103985     -0.137294      0.0864887    0.218556   -0.165193   -0.148259    -0.185236   -0.028122    -0.073154    -0.154353     -0.0533088   -0.018804  
 -0.0826229    -0.0481224    0.11197      0.115781    -0.150842      0.116268    -0.101383     0.124299    -0.082444    0.116705    -0.0308286    0.0220184    0.192995    -0.166187    0.113086     -0.0543889    -0.0872595   -0.109618   -0.114026   -0.222833    -0.156553   -0.13125      0.00734782  -0.000804039  -0.194817    -0.0225283 
  0.100694     -0.129329    -0.0886783   -0.157987     0.10101      -0.0673981   -0.0556208    0.0955778    0.0408842   0.0255514    0.0396275    0.0629014    0.130651    -0.120172   -0.173839     -0.38304      -0.0533102   -0.0238838  -0.25202    -0.00446506   0.0589553  -0.19997     -0.0958262   -0.0234553    -0.148802     0.0843087 
 -0.134655      0.160851    -0.0617994   -0.0895671   -0.0332117    -0.24856     -0.0276259    0.0869135    0.042914   -0.0515438    0.0737493    0.10064      0.131147    -0.119673    0.0491576     0.0349744    -0.0637904   -0.296206   -0.088348    0.101555     0.0284997   0.0861037    0.0457432   -0.0428919    -0.0957659   -0.105356  
 -0.290088     -0.187023     0.00390485  -0.0189734   -0.115265      0.0762589   -0.0850299   -0.00639034   0.112685    0.0724469    0.0636552   -0.0331949    0.00360407  -0.0618922   0.0643895    -0.173128      0.0477532    0.0995418  -0.0170244  -0.156436     0.216838    0.100528    -0.0584243   -0.0393931     0.121633    -0.00524317
 -0.119029      0.0230016   -0.100647    -0.0317408   -0.102199      0.087319     0.080863     0.00350223  -0.0145897  -0.00198293  -0.107432    -0.11038     -0.0714261    0.15663    -0.0634836    -0.0429802     0.0122235    0.0756846   0.0111794   0.025683     0.0131917   0.0686321   -0.0468405    0.0646575    -0.072522    -0.0322213 
 -0.0738706    -0.0476416   -0.0452444   -0.0252764    0.0452578     0.102448     0.117388    -0.148824     0.0834503   0.00697279   0.114201     0.0470236    0.0795517    0.0545018   0.0419022    -0.0830879     0.108563    -0.0143834   0.141478    0.0793736    0.187082   -0.116233     0.0600193    0.185802      0.153435     0.138575  
 -0.0364561    -0.110713    -0.185244     0.0231996   -0.203442      0.00224678   0.12038     -0.0772996   -0.19912    -0.02416     -0.0663513   -0.0459237    0.0401251    0.113963    0.161081     -0.172996      0.00542335   0.0617199   0.0458917  -0.0296136    0.027062   -0.0643314   -0.0706269   -0.0508502    -0.0677157    0.0699188 
  0.0694219     0.12307     -0.118249    -0.0548123   -0.0584923     0.0319353   -0.113667    -0.0574635    0.11026    -0.0529627    0.0573923   -0.00279983  -0.169512    -0.155752    0.223548      0.0229726    -0.017005     0.0559252   0.0341985  -0.0680244    0.150628   -0.371175     0.159922    -0.0193762    -0.00923041  -0.145675  
 -0.0140024     0.0117959   -0.0506459   -0.186844    -0.00391733   -0.0573683   -0.106953     0.0231768   -0.0135665  -0.149239     0.140568     0.0778137   -0.0974054    0.0742688  -0.182377      0.0375879    -0.143622     0.192338   -0.0084404  -0.114675     0.128175    0.273044    -0.143504     0.0496871     0.0412288    0.0409837 
 -0.0616265    -0.0215319   -0.0656503   -0.104777     0.0208125    -0.347254    -0.0829884    0.00719638  -0.0672717  -0.00244156  -0.107768     0.0298064   -0.0763761    0.11531     0.031852      0.051646     -0.155673    -0.0689311  -0.0131752  -0.0027278    0.118437   -0.129089     0.0949977    0.0488285     0.210012    -0.0912735 
 -0.000676752  -0.00621652   0.13169      0.069529    -0.0128433    -0.15312      0.0150898    0.118431    -0.0899897  -0.0053542    0.180067    -0.0905618    0.00297632   0.0475722  -0.133491      0.0819257    -0.0674729   -0.0089163  -0.126406    0.140879     0.0148786   0.154228    -0.073988    -0.0287243     0.0748111    0.0194598 
  0.175049     -0.0487715   -0.125655    -0.0274041   -0.100911      0.00531393   0.0978744   -0.0179575   -0.0802885   0.0455315   -0.0647942   -0.00118283   0.00352921   0.0528585  -0.0969895    -0.139305      0.0406936   -0.0580593   0.179145    0.123675     0.0399105  -0.0289599   -0.0828656   -0.0535899    -0.0636212   -0.0857946 
  0.103178      0.099445     0.0513401   -0.0234427    0.19763       0.0646936   -0.147972     0.102148     0.113496    0.0549633   -0.119894    -0.0227782    0.127707     0.128913    0.0359425    -0.016751      0.0701212   -0.0394321   0.0878002  -0.00235255   0.0482683  -0.155692     0.0985473    0.0283392     0.0233177   -0.206395  kind diag, method split
0: avll = -1.3713533449256448
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.371446
INFO: iteration 2, average log likelihood -1.371335
INFO: iteration 3, average log likelihood -1.370310
INFO: iteration 4, average log likelihood -1.362058
INFO: iteration 5, average log likelihood -1.346807
INFO: iteration 6, average log likelihood -1.339607
INFO: iteration 7, average log likelihood -1.336328
INFO: iteration 8, average log likelihood -1.333538
INFO: iteration 9, average log likelihood -1.331175
INFO: iteration 10, average log likelihood -1.330074
INFO: iteration 11, average log likelihood -1.329697
INFO: iteration 12, average log likelihood -1.329548
INFO: iteration 13, average log likelihood -1.329477
INFO: iteration 14, average log likelihood -1.329439
INFO: iteration 15, average log likelihood -1.329416
INFO: iteration 16, average log likelihood -1.329401
INFO: iteration 17, average log likelihood -1.329392
INFO: iteration 18, average log likelihood -1.329385
INFO: iteration 19, average log likelihood -1.329380
INFO: iteration 20, average log likelihood -1.329377
INFO: iteration 21, average log likelihood -1.329374
INFO: iteration 22, average log likelihood -1.329373
INFO: iteration 23, average log likelihood -1.329371
INFO: iteration 24, average log likelihood -1.329370
INFO: iteration 25, average log likelihood -1.329369
INFO: iteration 26, average log likelihood -1.329368
INFO: iteration 27, average log likelihood -1.329368
INFO: iteration 28, average log likelihood -1.329367
INFO: iteration 29, average log likelihood -1.329367
INFO: iteration 30, average log likelihood -1.329366
INFO: iteration 31, average log likelihood -1.329366
INFO: iteration 32, average log likelihood -1.329366
INFO: iteration 33, average log likelihood -1.329366
INFO: iteration 34, average log likelihood -1.329366
INFO: iteration 35, average log likelihood -1.329365
INFO: iteration 36, average log likelihood -1.329365
INFO: iteration 37, average log likelihood -1.329365
INFO: iteration 38, average log likelihood -1.329365
INFO: iteration 39, average log likelihood -1.329365
INFO: iteration 40, average log likelihood -1.329365
INFO: iteration 41, average log likelihood -1.329365
INFO: iteration 42, average log likelihood -1.329365
INFO: iteration 43, average log likelihood -1.329365
INFO: iteration 44, average log likelihood -1.329365
INFO: iteration 45, average log likelihood -1.329365
INFO: iteration 46, average log likelihood -1.329365
INFO: iteration 47, average log likelihood -1.329365
INFO: iteration 48, average log likelihood -1.329365
INFO: iteration 49, average log likelihood -1.329365
INFO: iteration 50, average log likelihood -1.329365
INFO: EM with 100000 data points 50 iterations avll -1.329365
952.4 data points per parameter
1: avll = [-1.37145,-1.37134,-1.37031,-1.36206,-1.34681,-1.33961,-1.33633,-1.33354,-1.33118,-1.33007,-1.3297,-1.32955,-1.32948,-1.32944,-1.32942,-1.3294,-1.32939,-1.32939,-1.32938,-1.32938,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32936,-1.32936,-1.32936,-1.32936,-1.32936,-1.32936,-1.32936,-1.32936,-1.32936,-1.32936,-1.32936]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.329485
INFO: iteration 2, average log likelihood -1.329378
INFO: iteration 3, average log likelihood -1.329006
INFO: iteration 4, average log likelihood -1.326053
INFO: iteration 5, average log likelihood -1.315844
INFO: iteration 6, average log likelihood -1.304174
INFO: iteration 7, average log likelihood -1.298101
INFO: iteration 8, average log likelihood -1.294464
INFO: iteration 9, average log likelihood -1.291707
INFO: iteration 10, average log likelihood -1.289657
INFO: iteration 11, average log likelihood -1.288120
INFO: iteration 12, average log likelihood -1.286897
INFO: iteration 13, average log likelihood -1.285900
INFO: iteration 14, average log likelihood -1.285049
INFO: iteration 15, average log likelihood -1.284276
INFO: iteration 16, average log likelihood -1.283560
INFO: iteration 17, average log likelihood -1.282854
INFO: iteration 18, average log likelihood -1.282146
INFO: iteration 19, average log likelihood -1.281463
INFO: iteration 20, average log likelihood -1.280908
INFO: iteration 21, average log likelihood -1.280499
INFO: iteration 22, average log likelihood -1.280187
INFO: iteration 23, average log likelihood -1.279916
INFO: iteration 24, average log likelihood -1.279638
INFO: iteration 25, average log likelihood -1.279288
INFO: iteration 26, average log likelihood -1.278897
INFO: iteration 27, average log likelihood -1.278578
INFO: iteration 28, average log likelihood -1.278233
INFO: iteration 29, average log likelihood -1.277787
INFO: iteration 30, average log likelihood -1.277244
INFO: iteration 31, average log likelihood -1.276720
INFO: iteration 32, average log likelihood -1.276350
INFO: iteration 33, average log likelihood -1.276145
INFO: iteration 34, average log likelihood -1.276040
INFO: iteration 35, average log likelihood -1.275980
INFO: iteration 36, average log likelihood -1.275934
INFO: iteration 37, average log likelihood -1.275890
INFO: iteration 38, average log likelihood -1.275847
INFO: iteration 39, average log likelihood -1.275807
INFO: iteration 40, average log likelihood -1.275772
INFO: iteration 41, average log likelihood -1.275746
INFO: iteration 42, average log likelihood -1.275726
INFO: iteration 43, average log likelihood -1.275713
INFO: iteration 44, average log likelihood -1.275703
INFO: iteration 45, average log likelihood -1.275696
INFO: iteration 46, average log likelihood -1.275691
INFO: iteration 47, average log likelihood -1.275687
INFO: iteration 48, average log likelihood -1.275684
INFO: iteration 49, average log likelihood -1.275682
INFO: iteration 50, average log likelihood -1.275680
INFO: EM with 100000 data points 50 iterations avll -1.275680
473.9 data points per parameter
2: avll = [-1.32949,-1.32938,-1.32901,-1.32605,-1.31584,-1.30417,-1.2981,-1.29446,-1.29171,-1.28966,-1.28812,-1.2869,-1.2859,-1.28505,-1.28428,-1.28356,-1.28285,-1.28215,-1.28146,-1.28091,-1.2805,-1.28019,-1.27992,-1.27964,-1.27929,-1.2789,-1.27858,-1.27823,-1.27779,-1.27724,-1.27672,-1.27635,-1.27614,-1.27604,-1.27598,-1.27593,-1.27589,-1.27585,-1.27581,-1.27577,-1.27575,-1.27573,-1.27571,-1.2757,-1.2757,-1.27569,-1.27569,-1.27568,-1.27568,-1.27568]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.275846
INFO: iteration 2, average log likelihood -1.275670
INFO: iteration 3, average log likelihood -1.274891
INFO: iteration 4, average log likelihood -1.268986
INFO: iteration 5, average log likelihood -1.252691
INFO: iteration 6, average log likelihood -1.238695
INFO: iteration 7, average log likelihood -1.232327
INFO: iteration 8, average log likelihood -1.228893
INFO: iteration 9, average log likelihood -1.227104
INFO: iteration 10, average log likelihood -1.226102
INFO: iteration 11, average log likelihood -1.225402
INFO: iteration 12, average log likelihood -1.224854
INFO: iteration 13, average log likelihood -1.224409
INFO: iteration 14, average log likelihood -1.224042
INFO: iteration 15, average log likelihood -1.223742
INFO: iteration 16, average log likelihood -1.223500
INFO: iteration 17, average log likelihood -1.223313
INFO: iteration 18, average log likelihood -1.223170
INFO: iteration 19, average log likelihood -1.223053
INFO: iteration 20, average log likelihood -1.222954
INFO: iteration 21, average log likelihood -1.222871
INFO: iteration 22, average log likelihood -1.222801
INFO: iteration 23, average log likelihood -1.222736
INFO: iteration 24, average log likelihood -1.222671
INFO: iteration 25, average log likelihood -1.222606
INFO: iteration 26, average log likelihood -1.222537
INFO: iteration 27, average log likelihood -1.222462
INFO: iteration 28, average log likelihood -1.222379
INFO: iteration 29, average log likelihood -1.222286
INFO: iteration 30, average log likelihood -1.222182
INFO: iteration 31, average log likelihood -1.222064
INFO: iteration 32, average log likelihood -1.221926
INFO: iteration 33, average log likelihood -1.221753
INFO: iteration 34, average log likelihood -1.221524
INFO: iteration 35, average log likelihood -1.221217
INFO: iteration 36, average log likelihood -1.220802
INFO: iteration 37, average log likelihood -1.220250
INFO: iteration 38, average log likelihood -1.219563
INFO: iteration 39, average log likelihood -1.218865
INFO: iteration 40, average log likelihood -1.218394
INFO: iteration 41, average log likelihood -1.218119
INFO: iteration 42, average log likelihood -1.217931
INFO: iteration 43, average log likelihood -1.217789
INFO: iteration 44, average log likelihood -1.217677
INFO: iteration 45, average log likelihood -1.217589
INFO: iteration 46, average log likelihood -1.217524
INFO: iteration 47, average log likelihood -1.217478
INFO: iteration 48, average log likelihood -1.217447
INFO: iteration 49, average log likelihood -1.217427
INFO: iteration 50, average log likelihood -1.217413
INFO: EM with 100000 data points 50 iterations avll -1.217413
236.4 data points per parameter
3: avll = [-1.27585,-1.27567,-1.27489,-1.26899,-1.25269,-1.2387,-1.23233,-1.22889,-1.2271,-1.2261,-1.2254,-1.22485,-1.22441,-1.22404,-1.22374,-1.2235,-1.22331,-1.22317,-1.22305,-1.22295,-1.22287,-1.2228,-1.22274,-1.22267,-1.22261,-1.22254,-1.22246,-1.22238,-1.22229,-1.22218,-1.22206,-1.22193,-1.22175,-1.22152,-1.22122,-1.2208,-1.22025,-1.21956,-1.21886,-1.21839,-1.21812,-1.21793,-1.21779,-1.21768,-1.21759,-1.21752,-1.21748,-1.21745,-1.21743,-1.21741]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.217619
INFO: iteration 2, average log likelihood -1.217348
INFO: iteration 3, average log likelihood -1.215771
INFO: iteration 4, average log likelihood -1.201074
INFO: iteration 5, average log likelihood -1.164068
WARNING: Variances had to be floored 14
INFO: iteration 6, average log likelihood -1.134946
WARNING: Variances had to be floored 5
INFO: iteration 7, average log likelihood -1.127645
INFO: iteration 8, average log likelihood -1.123876
WARNING: Variances had to be floored 14 16
INFO: iteration 9, average log likelihood -1.116177
WARNING: Variances had to be floored 5
INFO: iteration 10, average log likelihood -1.127967
INFO: iteration 11, average log likelihood -1.123411
INFO: iteration 12, average log likelihood -1.115217
WARNING: Variances had to be floored 5 14
INFO: iteration 13, average log likelihood -1.111170
WARNING: Variances had to be floored 2 16
INFO: iteration 14, average log likelihood -1.124030
INFO: iteration 15, average log likelihood -1.130501
WARNING: Variances had to be floored 5 14
INFO: iteration 16, average log likelihood -1.116552
INFO: iteration 17, average log likelihood -1.126290
INFO: iteration 18, average log likelihood -1.115931
WARNING: Variances had to be floored 5 16
INFO: iteration 19, average log likelihood -1.111880
WARNING: Variances had to be floored 14
INFO: iteration 20, average log likelihood -1.125014
INFO: iteration 21, average log likelihood -1.121629
WARNING: Variances had to be floored 5
INFO: iteration 22, average log likelihood -1.113253
INFO: iteration 23, average log likelihood -1.117540
WARNING: Variances had to be floored 14 16
INFO: iteration 24, average log likelihood -1.111861
WARNING: Variances had to be floored 2 5
INFO: iteration 25, average log likelihood -1.124733
INFO: iteration 26, average log likelihood -1.129113
INFO: iteration 27, average log likelihood -1.117167
WARNING: Variances had to be floored 5 14
INFO: iteration 28, average log likelihood -1.112057
WARNING: Variances had to be floored 16
INFO: iteration 29, average log likelihood -1.125124
INFO: iteration 30, average log likelihood -1.123233
WARNING: Variances had to be floored 5 14
INFO: iteration 31, average log likelihood -1.113550
INFO: iteration 32, average log likelihood -1.124811
INFO: iteration 33, average log likelihood -1.114829
WARNING: Variances had to be floored 5 16
INFO: iteration 34, average log likelihood -1.111022
WARNING: Variances had to be floored 14
INFO: iteration 35, average log likelihood -1.124353
INFO: iteration 36, average log likelihood -1.120907
WARNING: Variances had to be floored 2 5
INFO: iteration 37, average log likelihood -1.112795
INFO: iteration 38, average log likelihood -1.125593
WARNING: Variances had to be floored 14 16
INFO: iteration 39, average log likelihood -1.115641
WARNING: Variances had to be floored 5
INFO: iteration 40, average log likelihood -1.127066
INFO: iteration 41, average log likelihood -1.122493
WARNING: Variances had to be floored 14
INFO: iteration 42, average log likelihood -1.114493
WARNING: Variances had to be floored 5
INFO: iteration 43, average log likelihood -1.117994
WARNING: Variances had to be floored 16
INFO: iteration 44, average log likelihood -1.119378
INFO: iteration 45, average log likelihood -1.121099
WARNING: Variances had to be floored 5 14
INFO: iteration 46, average log likelihood -1.112057
INFO: iteration 47, average log likelihood -1.124226
WARNING: Variances had to be floored 2
INFO: iteration 48, average log likelihood -1.114283
WARNING: Variances had to be floored 5 16
INFO: iteration 49, average log likelihood -1.119074
WARNING: Variances had to be floored 14
INFO: iteration 50, average log likelihood -1.127968
INFO: EM with 100000 data points 50 iterations avll -1.127968
118.1 data points per parameter
4: avll = [-1.21762,-1.21735,-1.21577,-1.20107,-1.16407,-1.13495,-1.12765,-1.12388,-1.11618,-1.12797,-1.12341,-1.11522,-1.11117,-1.12403,-1.1305,-1.11655,-1.12629,-1.11593,-1.11188,-1.12501,-1.12163,-1.11325,-1.11754,-1.11186,-1.12473,-1.12911,-1.11717,-1.11206,-1.12512,-1.12323,-1.11355,-1.12481,-1.11483,-1.11102,-1.12435,-1.12091,-1.11279,-1.12559,-1.11564,-1.12707,-1.12249,-1.11449,-1.11799,-1.11938,-1.1211,-1.11206,-1.12423,-1.11428,-1.11907,-1.12797]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.123275
WARNING: Variances had to be floored 9 10
INFO: iteration 2, average log likelihood -1.114261
WARNING: Variances had to be floored 28
INFO: iteration 3, average log likelihood -1.112556
WARNING: Variances had to be floored 9 10 25 27 31 32
INFO: iteration 4, average log likelihood -1.091974
WARNING: Variances had to be floored 23 24
INFO: iteration 5, average log likelihood -1.069214
WARNING: Variances had to be floored 3 9 10 25 27 32
INFO: iteration 6, average log likelihood -1.033415
WARNING: Variances had to be floored 1 16 24 30 31
INFO: iteration 7, average log likelihood -1.019471
WARNING: Variances had to be floored 2 4 9 10 23 25 27
INFO: iteration 8, average log likelihood -1.020000
WARNING: Variances had to be floored 14 24 30 32
INFO: iteration 9, average log likelihood -1.023653
WARNING: Variances had to be floored 1 3 9 10 16 23 25 27 31
INFO: iteration 10, average log likelihood -1.009391
WARNING: Variances had to be floored 24 30
INFO: iteration 11, average log likelihood -1.037767
WARNING: Variances had to be floored 4 9 10 23 25 27 31
INFO: iteration 12, average log likelihood -1.009979
WARNING: Variances had to be floored 1 16 24 30 32
INFO: iteration 13, average log likelihood -1.015417
WARNING: Variances had to be floored 2 3 9 10 14 23 25 27 31
INFO: iteration 14, average log likelihood -1.018475
WARNING: Variances had to be floored 24 30 32
INFO: iteration 15, average log likelihood -1.033592
WARNING: Variances had to be floored 1 9 10 16 23 25 27 31
INFO: iteration 16, average log likelihood -1.007358
WARNING: Variances had to be floored 24 30
INFO: iteration 17, average log likelihood -1.033447
WARNING: Variances had to be floored 3 9 10 23 25 27 32
INFO: iteration 18, average log likelihood -1.007165
WARNING: Variances had to be floored 1 4 14 16 24 30 31
INFO: iteration 19, average log likelihood -1.012517
WARNING: Variances had to be floored 2 9 10 23 25 27
INFO: iteration 20, average log likelihood -1.029801
WARNING: Variances had to be floored 24 30 32
INFO: iteration 21, average log likelihood -1.027444
WARNING: Variances had to be floored 1 3 9 10 16 23 25 27 31
INFO: iteration 22, average log likelihood -1.004328
WARNING: Variances had to be floored 24 30
INFO: iteration 23, average log likelihood -1.033623
WARNING: Variances had to be floored 4 9 10 14 23 25 27 31
INFO: iteration 24, average log likelihood -1.006274
WARNING: Variances had to be floored 1 16 24 30 32
INFO: iteration 25, average log likelihood -1.023129
WARNING: Variances had to be floored 2 9 10 23 25 27 31
INFO: iteration 26, average log likelihood -1.024106
WARNING: Variances had to be floored 4 24 30 32
INFO: iteration 27, average log likelihood -1.025351
WARNING: Variances had to be floored 1 9 10 16 23 25 27 31
INFO: iteration 28, average log likelihood -1.003793
WARNING: Variances had to be floored 3 14 24 30
INFO: iteration 29, average log likelihood -1.023333
WARNING: Variances had to be floored 4 9 10 23 25 27 32
INFO: iteration 30, average log likelihood -1.012927
WARNING: Variances had to be floored 1 16 24 30 31
INFO: iteration 31, average log likelihood -1.017275
WARNING: Variances had to be floored 2 9 10 23 25 27
INFO: iteration 32, average log likelihood -1.018523
WARNING: Variances had to be floored 3 4 24 30 32
INFO: iteration 33, average log likelihood -1.014639
WARNING: Variances had to be floored 1 9 10 14 16 23 25 27 31
INFO: iteration 34, average log likelihood -1.004056
WARNING: Variances had to be floored 24 30
INFO: iteration 35, average log likelihood -1.039908
WARNING: Variances had to be floored 9 10 23 25 27 31
INFO: iteration 36, average log likelihood -1.008275
WARNING: Variances had to be floored 1 3 4 16 24 30
INFO: iteration 37, average log likelihood -1.008603
WARNING: Variances had to be floored 2 9 10 23 25 27 31
INFO: iteration 38, average log likelihood -1.021168
WARNING: Variances had to be floored 14 24 30 32
INFO: iteration 39, average log likelihood -1.025302
WARNING: Variances had to be floored 1 9 10 23 25 27 31
INFO: iteration 40, average log likelihood -1.009115
WARNING: Variances had to be floored 3 4 16 24 30
INFO: iteration 41, average log likelihood -1.019437
WARNING: Variances had to be floored 9 10 13 23 25 27 32
INFO: iteration 42, average log likelihood -1.012728
WARNING: Variances had to be floored 1 24 30 31
INFO: iteration 43, average log likelihood -1.022251
WARNING: Variances had to be floored 2 4 9 10 16 23 25 27
INFO: iteration 44, average log likelihood -1.016163
WARNING: Variances had to be floored 3 24 30 32
INFO: iteration 45, average log likelihood -1.024364
WARNING: Variances had to be floored 1 9 10 23 25 27 31
INFO: iteration 46, average log likelihood -1.004744
WARNING: Variances had to be floored 4 14 16 24 30
INFO: iteration 47, average log likelihood -1.022560
WARNING: Variances had to be floored 9 10 23 25 27 31
INFO: iteration 48, average log likelihood -1.012774
WARNING: Variances had to be floored 1 3 13 24 30 32
INFO: iteration 49, average log likelihood -1.003515
WARNING: Variances had to be floored 2 4 9 10 16 23 25 27 31
INFO: iteration 50, average log likelihood -1.019299
INFO: EM with 100000 data points 50 iterations avll -1.019299
59.0 data points per parameter
5: avll = [-1.12328,-1.11426,-1.11256,-1.09197,-1.06921,-1.03342,-1.01947,-1.02,-1.02365,-1.00939,-1.03777,-1.00998,-1.01542,-1.01847,-1.03359,-1.00736,-1.03345,-1.00716,-1.01252,-1.0298,-1.02744,-1.00433,-1.03362,-1.00627,-1.02313,-1.02411,-1.02535,-1.00379,-1.02333,-1.01293,-1.01728,-1.01852,-1.01464,-1.00406,-1.03991,-1.00828,-1.0086,-1.02117,-1.0253,-1.00911,-1.01944,-1.01273,-1.02225,-1.01616,-1.02436,-1.00474,-1.02256,-1.01277,-1.00351,-1.0193]
[-1.37135,-1.37145,-1.37134,-1.37031,-1.36206,-1.34681,-1.33961,-1.33633,-1.33354,-1.33118,-1.33007,-1.3297,-1.32955,-1.32948,-1.32944,-1.32942,-1.3294,-1.32939,-1.32939,-1.32938,-1.32938,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32937,-1.32936,-1.32936,-1.32936,-1.32936,-1.32936,-1.32936,-1.32936,-1.32936,-1.32936,-1.32936,-1.32936,-1.32949,-1.32938,-1.32901,-1.32605,-1.31584,-1.30417,-1.2981,-1.29446,-1.29171,-1.28966,-1.28812,-1.2869,-1.2859,-1.28505,-1.28428,-1.28356,-1.28285,-1.28215,-1.28146,-1.28091,-1.2805,-1.28019,-1.27992,-1.27964,-1.27929,-1.2789,-1.27858,-1.27823,-1.27779,-1.27724,-1.27672,-1.27635,-1.27614,-1.27604,-1.27598,-1.27593,-1.27589,-1.27585,-1.27581,-1.27577,-1.27575,-1.27573,-1.27571,-1.2757,-1.2757,-1.27569,-1.27569,-1.27568,-1.27568,-1.27568,-1.27585,-1.27567,-1.27489,-1.26899,-1.25269,-1.2387,-1.23233,-1.22889,-1.2271,-1.2261,-1.2254,-1.22485,-1.22441,-1.22404,-1.22374,-1.2235,-1.22331,-1.22317,-1.22305,-1.22295,-1.22287,-1.2228,-1.22274,-1.22267,-1.22261,-1.22254,-1.22246,-1.22238,-1.22229,-1.22218,-1.22206,-1.22193,-1.22175,-1.22152,-1.22122,-1.2208,-1.22025,-1.21956,-1.21886,-1.21839,-1.21812,-1.21793,-1.21779,-1.21768,-1.21759,-1.21752,-1.21748,-1.21745,-1.21743,-1.21741,-1.21762,-1.21735,-1.21577,-1.20107,-1.16407,-1.13495,-1.12765,-1.12388,-1.11618,-1.12797,-1.12341,-1.11522,-1.11117,-1.12403,-1.1305,-1.11655,-1.12629,-1.11593,-1.11188,-1.12501,-1.12163,-1.11325,-1.11754,-1.11186,-1.12473,-1.12911,-1.11717,-1.11206,-1.12512,-1.12323,-1.11355,-1.12481,-1.11483,-1.11102,-1.12435,-1.12091,-1.11279,-1.12559,-1.11564,-1.12707,-1.12249,-1.11449,-1.11799,-1.11938,-1.1211,-1.11206,-1.12423,-1.11428,-1.11907,-1.12797,-1.12328,-1.11426,-1.11256,-1.09197,-1.06921,-1.03342,-1.01947,-1.02,-1.02365,-1.00939,-1.03777,-1.00998,-1.01542,-1.01847,-1.03359,-1.00736,-1.03345,-1.00716,-1.01252,-1.0298,-1.02744,-1.00433,-1.03362,-1.00627,-1.02313,-1.02411,-1.02535,-1.00379,-1.02333,-1.01293,-1.01728,-1.01852,-1.01464,-1.00406,-1.03991,-1.00828,-1.0086,-1.02117,-1.0253,-1.00911,-1.01944,-1.01273,-1.02225,-1.01616,-1.02436,-1.00474,-1.02256,-1.01277,-1.00351,-1.0193]
32×26 Array{Float64,2}:
 -0.00557901  -0.00911477   0.133584     0.036905    -0.0214672  -0.153162     0.00771521   0.118669     -0.0913456   -0.0492439    0.185038    -0.109277     0.00393481   0.0556758   -0.126039     0.0685131   -0.0664986    0.0120499   -0.134409      0.139254     0.0248704    0.169181   -0.0586395   -0.0262663    0.0666214    0.0214178 
 -0.133276     0.163138    -0.0680912   -0.104532    -0.020818   -0.256537    -0.0393481    0.0934238     0.0470836   -0.0448963    0.073603     0.182958     0.136674    -0.107212     0.0599521    0.0534729   -0.0653138   -0.300605    -0.0796731     0.0970357    0.0411751    0.120124    0.0709315   -0.0355711   -0.102482    -0.121987  
 -0.0524011    0.0277442   -0.167984    -0.0966554   -0.0884155   0.010173    -0.019766    -0.0239578    -0.102981     0.0125976    0.124257    -0.0849205    0.0646748    0.0236243   -0.00288744   0.108429    -0.0481955   -0.00168961  -0.0383807    -0.00391129   0.0438216    0.0352172  -0.0174766    0.00175856   0.0256301    0.0605509 
  0.176415     0.0275009   -0.142371    -0.0232559   -0.0996372  -0.0283691    0.101264    -0.00788488   -0.0858961    0.0286397   -0.0608625    0.0183618   -0.0133168    0.0145966   -0.123152    -0.154132     0.0302587   -0.0593623    0.183275      0.129077     0.0440647   -0.0159198  -0.0822866   -0.0721922   -0.0720205   -0.078198  
  0.0707211   -0.0229449    0.198868     0.158869    -0.0423958  -0.206374    -0.0109785   -0.0694911    -0.0345701   -0.251588    -0.0935572    0.177592     0.0585323   -0.0293576    0.10678     -0.20977      0.139029     0.199652    -0.180649     -0.109845    -0.184596     0.13214    -0.0755147   -0.24148     -0.297299    -0.0173226 
  0.18981      0.0397215    0.212551     0.0806548   -0.0447843  -0.171981     0.135429    -0.0658887    -0.0132904   -0.0272445   -0.0740079    0.120921     0.0749729   -0.00380788   0.102758    -0.062796     0.017761     0.261225    -0.145434     -0.167658    -0.143446    -0.141589   -0.0704912   -0.0301727    0.248178    -0.0279012 
  0.0370475    0.0612708   -0.122919     0.0790871   -0.0843423   0.0963822    0.274603    -0.109525      0.103399    -0.230934     0.0863833   -0.0229275    0.0109633    0.286197     0.00659467  -0.0843685    0.161623    -0.0990667   -0.22721      -0.163499    -0.229602     0.0717834   0.0219643    0.122109    -0.0147478    0.0584641 
  0.133384    -0.0160944   -0.112973    -0.0362428   -0.113757    0.0590251    0.228348    -0.0881087     0.0993527    0.172719     0.0900951   -0.0818684    0.0223233    0.149451    -0.0412132   -0.0439492    0.187162    -0.0864309   -0.0600573    -0.131843     0.0315637   -0.0100138  -0.0164082    0.0600643   -0.197938    -0.15499   
 -0.13229     -1.44705      0.113604     0.134491    -0.077105   -0.0506886   -0.00282783  -0.12843       0.241563     0.08793      0.0252208    0.162396     0.0158718   -0.0218052   -0.0571312   -0.0558618    0.0333751    0.0971418   -0.063278      0.0552448    0.00833012  -0.097577    0.0335899   -0.120112     0.0962804    0.094211  
 -0.192826     0.447571     0.118605     0.13491     -0.105048   -0.0722871   -0.0174937   -0.167497      0.222166     0.087854     0.0411612    0.0832995   -0.0420273   -0.0232712   -0.0600794    0.0932439   -0.0236184    0.0646552   -0.0868748     0.0398722   -0.125381     0.0606435  -0.0289724   -0.105767    -0.095852     0.101291  
 -0.0246396   -0.656061    -0.073983    -0.177019    -0.01295    -0.016417    -0.110885     0.023301     -0.0980915   -0.150455     0.131728     0.0815436   -0.0680006    0.0597033   -0.178962     0.0786773   -0.135964     0.263183    -0.0124207    -0.0772253    0.117399     0.268863   -0.19272     -0.0272977    0.0409479   -0.0825379 
  0.00186831   0.736055    -0.0992935   -0.19706      0.0179768  -0.208519    -0.108219     0.0246614     0.102344    -0.162383     0.150973     0.079478    -0.135767     0.128365    -0.178828     0.0223134   -0.148143     0.159113    -0.000245792  -0.1209       0.144526     0.275472   -0.113758     0.104783     0.0410214    0.172098  
 -0.0907801    0.0592435   -0.00860718   0.111177    -0.0628981   0.0753274    0.0632402   -0.0197127     0.0318758   -0.109611    -0.0149803   -0.0530722    0.0467189    0.0323507   -0.00788156   0.00361242   0.00730599  -0.0507978   -0.00348353   -0.00322553  -0.0318388   -0.0763999   0.0489636    0.0594246    0.0677658   -0.122677  
 -0.0828812    0.042279     0.112586     0.0142053   -0.0398849   0.0750456   -0.264741    -0.0418408    -0.086992    -0.0489104   -0.0552185    0.0618328    0.0494153   -0.108829     0.0450484    0.0599311   -0.0455922    0.022221     0.00256321    0.131503    -0.00143145  -0.0662222  -0.0374854    0.153953     0.059473     0.0374752 
  0.175979    -0.0119572    0.00300122  -0.0884569    0.0391277   0.0877128   -0.0698498   -0.000826629  -0.0309175    0.057323     0.0541551   -0.0373901   -0.166412    -0.0841649    0.0456663    0.173555    -0.125837    -0.0929323   -0.176089     -0.0639473    0.129723     0.0663139   0.00792643  -0.0907968   -0.123421    -0.0159927 
 -0.01717     -0.0521952   -0.00355812  -0.119332    -0.114965    0.0571818    0.169243    -0.129995      0.139999    -0.0155288    0.0516339    0.170988    -0.108052     0.0535845   -0.0144075   -0.0910504   -0.0258323   -0.0622735    0.0348589    -0.0745518    0.271181    -0.100603    0.0419169   -0.0230651    0.0546948    0.0753777 
  0.0887819    0.126505    -0.089383     0.0466091   -0.0108353  -0.0599278   -0.00580574  -0.0345907     0.104879     0.0458508   -0.0389044    0.113946    -0.0747148   -0.032094    -0.0761216    0.0810119    0.0548479    0.0601129   -0.0884598    -0.0873697   -0.0576848   -0.0414091  -0.0535081   -0.0111735    0.111073    -0.0725012 
  0.0579976    0.055289    -0.0828166   -0.0789323    0.016026    0.00889553  -0.0462755    0.00967055    0.0582953    0.0374768    0.0304012    0.00125571   0.0270724   -0.0033979    0.0701315   -0.115739    -0.0201071   -0.049623    -0.097764     -0.0290023    0.0869627   -0.247206    0.0474228   -0.0196783    0.00228546  -0.0119168 
 -0.0994407    0.146655    -0.168788    -0.0652343    0.0225868  -0.192132     0.149883     0.137259     -0.0359342   -0.00785235   0.0498406    0.0331104    0.136901     0.197602     0.0514834   -0.0183259   -0.18707      0.112194     0.171252      0.19383     -0.135777    -0.0457326  -0.0571424    0.00891977  -0.181999    -0.0428882 
 -0.118583     0.0057033    0.157403     0.0658543    0.0312677  -0.0141194   -0.0121883    0.108594      0.00160317   0.0197862   -0.0193501    0.0145437    0.114123    -0.0985062    0.0585436   -0.0266702   -0.113463     0.00353709  -0.0353253    -0.0977628   -0.114742    -0.0957955   0.00181156   0.0226307   -0.063561    -0.00194789
 -0.0834887   -0.0217223   -0.0609891   -0.104226     0.0297219  -0.323324    -0.091144     0.010065     -0.0750118    0.0138564   -0.134478     0.0346726   -0.064735     0.117138     0.0817048    0.0516061   -0.200656    -0.0861875    0.000387652  -0.0241615    0.118822    -0.135709    0.0949895    0.037845     0.205963    -0.0933755 
 -0.0331233   -0.0831176   -0.126724     0.00405368   0.015777    0.0425647   -0.00651659  -0.160739      0.0502678   -0.120444    -0.0942507    0.168419     0.0140618   -0.0340251   -0.0293953   -0.0505708   -0.0525882    0.00596995  -0.0908062    -0.031738    -0.0427771    0.110603   -0.160814     0.0521985    0.137972     0.0408061 
  0.137352     0.0985969    0.0882426   -0.148458     0.15168    -0.022319    -0.0581618    0.017263     -0.00701746  -0.1716       0.0244095    0.0722938   -0.0771421   -0.0732741   -0.0158589   -0.0204461   -0.114809     0.0210851    0.0765583    -0.0189152   -0.144078     0.0979622  -0.0956517   -0.100232     0.0379131    0.129681  
 -0.071246    -0.072554    -0.0336064   -0.0766719    0.072951    0.103882     0.113633    -0.137676      0.0811781    0.00872002   0.121958     0.0451612    0.0754425    0.0627086    0.0691344   -0.104067     0.103227    -0.00938415   0.128292      0.0689556    0.203145    -0.108487    0.0176819    0.176377     0.170298     0.139847  
 -0.0823326   -0.0334179    0.0857193    0.0090324   -0.100585   -0.0166745   -0.11373      0.0216074    -0.101225     0.0505111   -0.0722956    0.13983      0.0478929    0.0117177    0.0156134    0.121121     0.0179265    0.134542     0.055844     -0.0742811   -0.0048171   -0.0443316  -0.0968841   -0.0357736    0.132015    -0.162886  
 -0.132692    -0.0578867   -0.0775122   -0.136777     0.0237333   0.145726     0.0123363   -0.0496743    -0.114277    -0.125535    -0.112847     0.143953     0.0101817    0.179986    -0.232803     0.0316912   -0.0504205    0.109485    -0.22844       0.10264      0.131811    -0.0630097   0.00308291  -0.0163586    0.0235357   -0.0481584 
 -0.0460184   -0.0182164    0.0378907   -0.00424684  -0.0311021  -0.0731211   -0.0667603    0.0762512     0.0513984    0.0819466    0.0638996    0.00822365  -0.06858     -0.025715     0.035808    -0.198062    -0.0673958   -0.106583    -0.128463     -0.0106096    0.0418016    0.0233819  -0.0981303   -0.0724678   -0.0038461   -0.206669  
 -0.095438     0.0250304   -0.105791    -0.0328808   -0.101484    0.0810178    0.103645     0.00154434   -0.00874016  -0.00817838  -0.112551    -0.0998671   -0.0363371    0.162994    -0.0658884   -0.0413084    0.011279     0.0824212    0.00629596    0.0210212    0.00354034   0.0595923  -0.0380663    0.0609495   -0.0708265   -0.0306348 
 -0.0254051   -0.073885    -0.0634343    0.081005     0.130047    0.130264    -0.120375     0.251857      0.0746127   -0.0765805   -0.0198116   -0.0918961   -0.197882     0.0408983    0.0274297    0.016604     0.0365592   -0.0981702   -0.0868199    -0.0153783   -0.0279698   -0.0350962   0.136055    -0.0834987   -0.143193     0.0787395 
 -0.277729    -0.191271     0.00304506  -0.015686    -0.105957    0.0775398   -0.10375      0.0289389     0.137416     0.0560203    0.0711054   -0.0350026   -0.00649823  -0.0846907    0.0704588   -0.178414     0.0599416    0.0908411   -0.0168779    -0.142155     0.212002     0.119192   -0.0769414   -0.0415769    0.118247    -0.00546427
 -0.0417111   -0.102349    -0.192036     0.0198157   -0.212149    0.00263862   0.199986    -0.050101     -0.188521    -0.0305093   -0.0689423   -0.0225793    0.0487642    0.0996489    0.272697    -0.194121    -0.0104188    0.0613919    0.0715502    -0.0283097    0.00810073  -0.0655498  -0.110849    -0.054583    -0.0414186    0.093164  
 -0.0899502   -0.0848421   -0.115305     0.0213766   -0.177417    0.0162691   -0.0610169   -0.0539675    -0.198393    -0.0158775   -0.00777002  -0.101827     0.044035     0.193327     0.031735    -0.107758    -0.00835352   0.06456      0.0226231    -0.0403074    0.125582    -0.0465091  -0.0275731   -0.0493391   -0.056453     0.0128346 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 24 30
INFO: iteration 1, average log likelihood -1.033420
WARNING: Variances had to be floored 1 9 10 23 24 25 27 30 32
INFO: iteration 2, average log likelihood -0.986767
WARNING: Variances had to be floored 3 4 16 24 30 31
INFO: iteration 3, average log likelihood -1.007696
WARNING: Variances had to be floored 1 9 10 23 24 25 27 30 32
INFO: iteration 4, average log likelihood -0.993902
WARNING: Variances had to be floored 14 24 30 31
INFO: iteration 5, average log likelihood -1.012178
WARNING: Variances had to be floored 1 4 10 16 23 24 25 27 30 32
INFO: iteration 6, average log likelihood -0.976813
WARNING: Variances had to be floored 3 9 13 24 30 31
INFO: iteration 7, average log likelihood -1.011180
WARNING: Variances had to be floored 1 10 14 23 24 25 27 30 32
INFO: iteration 8, average log likelihood -0.989921
WARNING: Variances had to be floored 4 9 16 24 30
INFO: iteration 9, average log likelihood -1.014575
WARNING: Variances had to be floored 1 3 10 23 24 25 27 30 32
INFO: iteration 10, average log likelihood -0.985683
INFO: EM with 100000 data points 10 iterations avll -0.985683
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.547938e+05
      1       6.333993e+05      -2.213945e+05 |       32
      2       6.064706e+05      -2.692864e+04 |       32
      3       5.902544e+05      -1.621625e+04 |       32
      4       5.804854e+05      -9.768955e+03 |       32
      5       5.743589e+05      -6.126523e+03 |       32
      6       5.701424e+05      -4.216541e+03 |       32
      7       5.673615e+05      -2.780843e+03 |       32
      8       5.649783e+05      -2.383242e+03 |       32
      9       5.630756e+05      -1.902686e+03 |       32
     10       5.615867e+05      -1.488909e+03 |       32
     11       5.604107e+05      -1.176004e+03 |       32
     12       5.595859e+05      -8.247485e+02 |       32
     13       5.591183e+05      -4.676154e+02 |       32
     14       5.588832e+05      -2.351710e+02 |       32
     15       5.587648e+05      -1.183802e+02 |       32
     16       5.586993e+05      -6.552270e+01 |       32
     17       5.586567e+05      -4.254031e+01 |       32
     18       5.586237e+05      -3.300153e+01 |       31
     19       5.585925e+05      -3.119739e+01 |       32
     20       5.585662e+05      -2.632000e+01 |       31
     21       5.585449e+05      -2.125912e+01 |       30
     22       5.585245e+05      -2.041578e+01 |       32
     23       5.585062e+05      -1.834401e+01 |       30
     24       5.584858e+05      -2.034290e+01 |       30
     25       5.584653e+05      -2.053190e+01 |       31
     26       5.584445e+05      -2.075634e+01 |       31
     27       5.584246e+05      -1.994681e+01 |       31
     28       5.584071e+05      -1.748061e+01 |       31
     29       5.583909e+05      -1.618526e+01 |       30
     30       5.583739e+05      -1.702551e+01 |       29
     31       5.583539e+05      -1.999640e+01 |       32
     32       5.583296e+05      -2.433395e+01 |       31
     33       5.583000e+05      -2.955354e+01 |       31
     34       5.582666e+05      -3.340709e+01 |       31
     35       5.582222e+05      -4.438559e+01 |       32
     36       5.581755e+05      -4.670980e+01 |       32
     37       5.581169e+05      -5.864030e+01 |       32
     38       5.580567e+05      -6.021221e+01 |       31
     39       5.579821e+05      -7.459552e+01 |       31
     40       5.578719e+05      -1.101667e+02 |       32
     41       5.577532e+05      -1.187437e+02 |       32
     42       5.576255e+05      -1.276475e+02 |       32
     43       5.574855e+05      -1.400286e+02 |       32
     44       5.573576e+05      -1.278618e+02 |       32
     45       5.572364e+05      -1.212517e+02 |       32
     46       5.571027e+05      -1.337088e+02 |       32
     47       5.569734e+05      -1.292781e+02 |       32
     48       5.568527e+05      -1.207169e+02 |       32
     49       5.567296e+05      -1.230848e+02 |       32
     50       5.566376e+05      -9.193866e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 556637.6422584059)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.278580
INFO: iteration 2, average log likelihood -1.247434
INFO: iteration 3, average log likelihood -1.214572
INFO: iteration 4, average log likelihood -1.173959
WARNING: Variances had to be floored 29
INFO: iteration 5, average log likelihood -1.119204
WARNING: Variances had to be floored 6 18
INFO: iteration 6, average log likelihood -1.074291
WARNING: Variances had to be floored 30
INFO: iteration 7, average log likelihood -1.072222
WARNING: Variances had to be floored 24 26 28 32
INFO: iteration 8, average log likelihood -1.018658
WARNING: Variances had to be floored 3 6 18
INFO: iteration 9, average log likelihood -1.034955
WARNING: Variances had to be floored 2 7 11 19 20
INFO: iteration 10, average log likelihood -1.045606
WARNING: Variances had to be floored 30
INFO: iteration 11, average log likelihood -1.037388
WARNING: Variances had to be floored 6 18 26 28 32
INFO: iteration 12, average log likelihood -1.007094
WARNING: Variances had to be floored 8 24
INFO: iteration 13, average log likelihood -1.055310
WARNING: Variances had to be floored 7
INFO: iteration 14, average log likelihood -1.008569
WARNING: Variances had to be floored 2 3 6 11 18 19 20 26 30 32
INFO: iteration 15, average log likelihood -0.956405
WARNING: Variances had to be floored 8 24 28
INFO: iteration 16, average log likelihood -1.072535
WARNING: Variances had to be floored 22
INFO: iteration 17, average log likelihood -1.061606
WARNING: Variances had to be floored 6 7
INFO: iteration 18, average log likelihood -1.015445
WARNING: Variances had to be floored 2 8 18 19 24 25 26 28 30 32
INFO: iteration 19, average log likelihood -0.974369
WARNING: Variances had to be floored 6 20 22
INFO: iteration 20, average log likelihood -1.061972
WARNING: Variances had to be floored 7 11
INFO: iteration 21, average log likelihood -1.052685
WARNING: Variances had to be floored 3
INFO: iteration 22, average log likelihood -1.002093
WARNING: Variances had to be floored 6 8 19 20 22 24 25 26 28 32
INFO: iteration 23, average log likelihood -0.937417
WARNING: Variances had to be floored 2 7 30
INFO: iteration 24, average log likelihood -1.072983
INFO: iteration 25, average log likelihood -1.042791
WARNING: Variances had to be floored 3 6 11 18
INFO: iteration 26, average log likelihood -0.977670
WARNING: Variances had to be floored 8 19 24 25 26 28 32
INFO: iteration 27, average log likelihood -0.984426
WARNING: Variances had to be floored 2 7 20 22
INFO: iteration 28, average log likelihood -1.041166
WARNING: Variances had to be floored 6 18 30
INFO: iteration 29, average log likelihood -1.029230
WARNING: Variances had to be floored 25
INFO: iteration 30, average log likelihood -1.011958
WARNING: Variances had to be floored 3 6 7 8 11 19 24 26 28 32
INFO: iteration 31, average log likelihood -0.953457
WARNING: Variances had to be floored 2 18 20
INFO: iteration 32, average log likelihood -1.061633
WARNING: Variances had to be floored 22 25
INFO: iteration 33, average log likelihood -1.028167
WARNING: Variances had to be floored 6 26 30
INFO: iteration 34, average log likelihood -1.001498
WARNING: Variances had to be floored 7 8 18 19 20 24 28 32
INFO: iteration 35, average log likelihood -0.979302
WARNING: Variances had to be floored 2 3 25
INFO: iteration 36, average log likelihood -1.050607
WARNING: Variances had to be floored 6 11 22
INFO: iteration 37, average log likelihood -1.028230
WARNING: Variances had to be floored 26
INFO: iteration 38, average log likelihood -1.015370
WARNING: Variances had to be floored 6 7 8 18 19 24 25 28 30 32
INFO: iteration 39, average log likelihood -0.955768
WARNING: Variances had to be floored 2 20
INFO: iteration 40, average log likelihood -1.073081
WARNING: Variances had to be floored 3 11 26
INFO: iteration 41, average log likelihood -1.020552
WARNING: Variances had to be floored 6 22 25
INFO: iteration 42, average log likelihood -1.004087
WARNING: Variances had to be floored 7 8 18 19 24 28 32
INFO: iteration 43, average log likelihood -0.978838
WARNING: Variances had to be floored 2 6 20 26
INFO: iteration 44, average log likelihood -1.036666
WARNING: Variances had to be floored 3 25 30
INFO: iteration 45, average log likelihood -1.018575
WARNING: Variances had to be floored 8 28
INFO: iteration 46, average log likelihood -1.008338
WARNING: Variances had to be floored 6 7 11 18 22 24 32
INFO: iteration 47, average log likelihood -0.974256
WARNING: Variances had to be floored 2 19 20 25 26
INFO: iteration 48, average log likelihood -1.013964
WARNING: Variances had to be floored 8 28 30
INFO: iteration 49, average log likelihood -1.030715
WARNING: Variances had to be floored 6 22
INFO: iteration 50, average log likelihood -1.028749
INFO: EM with 100000 data points 50 iterations avll -1.028749
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.120307     0.160659    -0.0663986   -0.10165     -0.0231121    -0.252189   -0.037881      0.0957092     0.043637    -0.0423271    0.0737101    0.164215     0.131973     -0.100874    0.054469      0.0612352   -0.0591482   -0.28986     -0.0814463    0.0984216    0.0440039    0.120048     0.0659178   -0.043558    -0.0938664   -0.115919 
  0.0864841    0.0748948    0.00719857  -0.0742048    0.162612      0.132068   -0.145884      0.0629905     0.171881     0.0296157   -0.119459    -0.0329452    0.139899      0.106298    0.0353944     0.0233348    0.0519735   -0.0719576    0.0745857   -0.0655513    0.0620818   -0.136714     0.143397    -0.00209387   0.0189807   -0.313523 
 -0.137475     0.0276318    0.122813     0.0520824   -0.116418      0.0816466  -0.259028     -0.0905183    -0.14364     -0.0906629   -0.0354453    0.100388     0.0275723    -0.178092    0.0411996     0.0694657   -0.0763938    0.0435764   -0.0260882    0.158141    -0.0221642   -0.0575173   -0.0734924    0.188549     0.0670827    0.0832709
 -0.0871315   -0.0485364    0.101775     0.117229    -0.161035      0.111304   -0.0990941     0.124583     -0.0925354    0.115315    -0.0731834    0.00463988   0.198314     -0.175581    0.112917     -0.0639781   -0.0753707   -0.107823    -0.0780432   -0.219725    -0.152673    -0.117673    -0.00881937  -0.00533358  -0.194959    -0.0227105
 -0.0820181   -0.0219887   -0.060598    -0.104425     0.0292877    -0.322303   -0.0891527     0.0107197    -0.0753406    0.0118889   -0.135015     0.0351415   -0.0652002     0.1173      0.089086      0.0517328   -0.197826    -0.0839906   -3.70439e-5  -0.0191538    0.118473    -0.133405     0.0946674    0.036335     0.206908    -0.0926001
 -0.0451347   -0.0191669    0.0404671   -0.00432513  -0.0275814    -0.072768   -0.0673452     0.0762919     0.0511707    0.0809853    0.0639765    0.00512654  -0.0719658    -0.0235921   0.0378114    -0.20107     -0.065203    -0.10991     -0.126834    -0.00460543   0.0436638    0.0250881   -0.100978    -0.071641    -0.0030959   -0.207779 
 -0.16372     -0.00782595   0.114986     0.135198    -0.0897528    -0.068192   -0.0159635    -0.14785       0.22016      0.0853901    0.0312936    0.0996987   -0.0236421    -0.0211781  -0.0589277     0.0533724   -0.0107582    0.0661015   -0.0719598    0.0360605   -0.0893374    0.018192    -0.0127481   -0.109432    -0.0417318    0.0981065
 -0.1272       0.0400096   -0.00414948   0.133982    -0.113859      0.0612402   0.15717      -0.0327471     0.0373569   -0.124199    -0.0374204   -0.0530081   -0.00222405    0.0170342  -0.0182276    -0.0597372    0.0147933   -0.0740462    0.02991      0.0461716   -0.0281564   -0.107528     0.0230205    0.0620245    0.0637196   -0.13323  
  0.0350057    0.155133     0.0426319   -0.0372328   -0.0552794     0.099692    0.0554198    -0.00927432   -0.029861     0.15628      0.02397     -0.0617449    0.0172854     0.246751    0.0567816    -0.0316633    0.00466081  -0.215165    -0.130402    -0.0259608    0.0119793   -0.163531     0.111825    -0.00802862   0.108617     0.0496375
  0.132267     0.0110293    0.205659     0.118506    -0.0434677    -0.188642    0.0664648    -0.0675712    -0.0234774   -0.133361    -0.0833002    0.148436     0.0673124    -0.0157171   0.104616     -0.133034     0.0777287    0.232363    -0.161922    -0.140052    -0.162718    -0.0125409   -0.0728663   -0.130638    -0.0110099   -0.0234037
 -0.102854     0.0725398   -0.150443    -0.0920777   -0.0393149     0.0434871  -0.0423234    -0.0520121    -0.0657285   -0.0415147    0.0970183   -0.075559     0.0475261     0.0333282   0.0752815     0.0821444   -0.0437958    0.0335795    0.0158142   -0.0257143    0.0370265   -0.00216844   0.00587118   0.0110125    0.00187465   0.107379 
 -0.0988335    0.150389    -0.163902    -0.069421     0.0365555    -0.200368    0.155509      0.138295     -0.0415408   -0.00943529   0.0497649    0.033627     0.134764      0.201878    0.0574313    -0.0153942   -0.191502     0.112362     0.187125     0.20692     -0.144255    -0.0420639   -0.0582856    0.00956517  -0.180596    -0.0438278
  0.0605142    0.123649    -0.150581    -0.0467074   -0.0606958     0.0389235  -0.116712     -0.0479272     0.113531    -0.0738624    0.0470104   -0.00261463  -0.0702981    -0.151361    0.275258      0.0233317    0.00617596   0.0452895    0.0367905   -0.0438833    0.159008    -0.373837     0.109925    -0.0305107   -0.0145282   -0.140079 
  0.0832078    0.0238046   -0.119237     0.0249539   -0.0971111     0.079656    0.252099     -0.0992529     0.10119     -0.0489267    0.0884977   -0.0482519    0.015758      0.221488   -0.0192268    -0.0637795    0.170595    -0.0934647   -0.149427    -0.148433    -0.109452     0.0361563    0.00299149   0.0927288   -0.100779    -0.038787 
 -0.165315    -0.144527    -0.0911032    0.0032245   -0.158551      0.0393229   0.0175509    -0.014325     -0.0394713    0.00969998   0.00527119  -0.0360416    0.0226548     0.0268056   0.141523     -0.176169     0.0169794    0.0766301    0.0248414   -0.0823674    0.127576     0.0194984   -0.0789525   -0.0491562    0.0236922    0.0334562
 -0.0163005   -0.0821094   -0.112949    -0.00773089   0.0241338     0.0376114  -0.011465     -0.152937      0.043738    -0.122988    -0.0863704    0.162345     0.0132991    -0.0354581  -0.0260239    -0.0517485   -0.0617701    0.00636368  -0.0803582   -0.0341802   -0.0489096    0.110486    -0.156271     0.0579179    0.142397     0.0477796
 -0.0111217    0.0107215   -0.0850384   -0.187204     0.000754306  -0.107284   -0.10943       0.0239103    -0.00128237  -0.156536     0.140921     0.0806214   -0.101494      0.0930174  -0.17912       0.0523412   -0.142033     0.213162    -0.00700068  -0.0981699    0.129789     0.27157     -0.154313     0.0375932    0.0410399    0.0394831
 -0.0105584    0.113683     0.175013     0.0648181   -0.0281307    -0.0636958   0.023215     -0.0368166     0.0257835   -0.0613797    0.0689771    0.0156869    0.00123946   -0.0560884   0.00500359    0.0118276   -0.00793225  -0.0359916    0.0596356    0.0354993   -0.0337377   -0.161165    -0.0625726   -0.0600686    0.0695904    0.1069   
 -0.055935    -0.0857754   -0.0211846   -0.0617733    0.074865      0.0853158   0.0876201    -0.112184      0.0667577   -0.00923762   0.133084     0.0124572    0.0540105     0.0568957   0.109407     -0.0810281    0.103436    -0.00677809   0.163604     0.125171     0.170896    -0.09296     -0.00334387   0.205178     0.373517     0.12753  
 -0.0300656   -0.0295514    0.00405953  -0.203531    -0.0824876     0.0531698   0.131677     -0.115385      0.107444    -0.0199661    0.0590717    0.182875    -0.0699205     0.0474349  -0.072737     -0.0789579   -0.00578355  -0.0412326    0.0127571   -0.0945052    0.260892    -0.056834     0.0501623   -0.0376841   -0.142639     0.0874456
  0.175767    -0.0123287    0.00352404  -0.0877781    0.0371455     0.0883188  -0.0679126     0.000332251  -0.0307685    0.0575571    0.0576136   -0.034071    -0.165041     -0.0833951   0.0478904     0.17255     -0.125628    -0.0926279   -0.174112    -0.0636402    0.127447     0.064593     0.00782778  -0.0914995   -0.123429    -0.012387 
 -0.148935     0.0460898    0.212081     0.0233481    0.202488     -0.128361    0.069266      0.105669      0.0884395   -0.0644117    0.0203239    0.0131926    0.0591248    -0.0244872   0.00459977    0.00274014  -0.146091     0.131503     0.0115728    0.00911158  -0.0849262   -0.0740608    0.0074141    0.045621     0.0497743    0.0250328
 -0.0303141   -0.0740844   -0.0638653    0.0806846    0.127895      0.129547   -0.120306      0.253011      0.0709045   -0.0760773   -0.0152886   -0.0916232   -0.194624      0.0392353   0.0289701     0.0173642    0.0380386   -0.0943163   -0.0866325   -0.0192326   -0.0246357   -0.0317902    0.135033    -0.083496    -0.140602     0.0784353
  0.0981733   -0.122905    -0.0655637   -0.125321     0.127406     -0.105936   -0.0535416     0.094799      0.069246     0.0141887    0.026069     0.0608913    0.110308     -0.0922843  -0.164652     -0.372505    -0.054386    -0.0123993   -0.227539    -0.00852431   0.0632624   -0.177213    -0.0927451   -0.0223578   -0.104717     0.0407737
 -0.0798284   -0.0294621    0.064791     0.0106238   -0.0970891    -0.0181708  -0.0854139    -0.0031707    -0.0808854    0.0529322   -0.0904555    0.162749     0.0338105     0.0169742   0.0168951     0.0962373    0.0141063    0.118555     0.0526814   -0.0621825    0.024018    -0.066707    -0.0949891   -0.0420892    0.139892    -0.163335 
 -0.00244513   0.00138586   0.17216      0.109097    -0.0329258    -0.143369    0.0122108     0.101465     -0.0906976   -0.073789     0.224104    -0.109571     0.000236584   0.0590618  -0.118927      0.0777899   -0.105294     0.012129    -0.0884682    0.114625     0.0077562    0.218686    -0.0565011   -0.0207003    0.0596416    0.0289849
 -0.134466    -0.0589173   -0.0814474   -0.134891     0.0209001     0.143646    0.0146319    -0.0564394    -0.112977    -0.125614    -0.11511      0.143509     0.0102173     0.177451   -0.232748      0.0302965   -0.0495388    0.107897    -0.228049     0.102508     0.12945     -0.0639629    0.00197905  -0.0167805    0.0236599   -0.0460234
  0.214253     0.128174     0.0715893   -0.140535     0.194336     -0.0365835  -0.0418727     0.0233343    -0.00725036  -0.125507    -0.00847458   0.0639465   -0.0901722    -0.0567652   0.000508337  -0.00305375  -0.107339     0.011571     0.115928    -0.0117893   -0.105099     0.0962219   -0.0904687   -0.142135     0.0497808    0.112259 
 -0.0953356    0.0241788   -0.104679    -0.0327295   -0.101742      0.0803359   0.0998873     0.0030695    -0.00826783  -0.00715667  -0.112562    -0.0920302   -0.036009      0.161583   -0.0653852    -0.0402246    0.0112172    0.08225      0.00681402   0.018379     0.00554391   0.0627459   -0.0377141    0.0607194   -0.0702081   -0.0312105
  0.329059     0.0139594   -0.246658    -0.0439018   -0.128429     -0.0298234   0.085523      0.00594538   -0.0804382    0.0379275   -0.0727445    0.0564977   -0.0179085     0.0261851  -0.230703     -0.151364     0.0356814   -0.0901843    0.184016     0.14192      0.0426183   -0.0009903   -0.0843469   -0.0781568   -0.0825681   -0.0551595
  0.126659     0.107605    -0.112252     0.0796973   -0.0114136    -0.0757765  -0.00163      -0.0430019     0.098291     0.106751    -0.117964     0.114163    -0.070627     -0.0978408  -0.165927      0.120331     0.0738895    0.22048     -0.114363    -0.0816011   -0.0574011   -0.0777665   -0.0927445   -0.0717308    0.0455042    0.0178285
  0.0535329    0.151133    -0.0817768    0.00429495  -0.0084586    -0.0461253  -0.000542432  -0.0362993     0.115405    -0.0205715    0.0354804    0.117853    -0.0942853     0.0389231   0.00181363    0.0476629    0.0327067   -0.0986542   -0.0518719   -0.082011    -0.047555    -0.00222661  -0.013442     0.0525308    0.178849    -0.168096 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 3 7 24 32
INFO: iteration 1, average log likelihood -1.007117
WARNING: Variances had to be floored 2 3 6 7 8 11 18 19 20 24 25 26 28 32
INFO: iteration 2, average log likelihood -0.926895
WARNING: Variances had to be floored 3 7 22 24 30 32
INFO: iteration 3, average log likelihood -0.996471
WARNING: Variances had to be floored 2 3 6 7 8 11 18 19 20 24 25 26 28 32
INFO: iteration 4, average log likelihood -0.929524
WARNING: Variances had to be floored 3 7 24 32
INFO: iteration 5, average log likelihood -0.999743
WARNING: Variances had to be floored 2 3 6 7 8 11 18 19 20 24 25 26 28 30 32
INFO: iteration 6, average log likelihood -0.920588
WARNING: Variances had to be floored 3 7 24 32
INFO: iteration 7, average log likelihood -1.003301
WARNING: Variances had to be floored 2 3 6 7 8 11 18 19 20 24 25 26 28 32
INFO: iteration 8, average log likelihood -0.927286
WARNING: Variances had to be floored 3 7 24 30 32
INFO: iteration 9, average log likelihood -0.999340
WARNING: Variances had to be floored 2 3 6 7 8 11 18 19 20 24 25 26 28 32
INFO: iteration 10, average log likelihood -0.926375
INFO: EM with 100000 data points 10 iterations avll -0.926375
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0116458   -0.0711775   -0.0156936   -0.00434428   -0.119794    -0.0409519    0.0781532    -0.0770607    0.0461561    0.105367    0.0110557    0.00161979  -0.126807      0.00780132   0.0957053   -0.0806576    0.125794     -0.168171     0.123477     -0.150782     0.101911     -0.118085    -0.0326495   -0.089685   -0.0787958    0.110271   
 -0.168288    -0.0249443    0.00207564   0.0278807    -0.00963848   0.107397     0.0822414    -0.106212     0.0184118    0.182484   -0.076598    -0.0494458   -0.264881     -0.0454858   -0.116398    -0.0638098    0.13247      -0.043395     0.0150529     0.135158     0.12131      -0.0960652   -0.0544729    0.0448177  -0.00472279  -0.158159   
 -0.0158147   -0.0563218   -0.0214545   -0.0068664     0.0937203   -0.0981506    0.0285714     0.109519    -0.149562     0.0567429  -0.169056    -0.0848407    0.120998     -0.132958     0.0670321    0.0854227   -0.147569     -0.0074112    0.0739913    -0.0538512   -0.0723928    -0.128329    -0.118393     0.161115   -0.110461    -0.0484775  
 -0.196377     0.0631104   -0.0200784    0.0338447     0.0560806    0.125825    -0.0379644    -0.167113    -0.0124827    0.21486     0.101334    -0.0863686    0.045547      0.0341705    0.0978024   -0.110269     0.096179     -0.110831    -0.157363     -0.0676565    0.0680406    -0.05034      0.0876559    0.05598     0.00222213   0.016074   
 -0.0254087    0.0505764   -0.0927604    0.230608     -0.0503412   -0.0470744   -0.123117     -0.138896     0.0416594    0.0101672   0.0755911    0.0427731   -0.175404     -0.0508863    0.115339     0.104531    -0.177478      0.13189     -0.0810856    -0.0350032    0.00345566    0.0296766    0.0896574   -0.0833301  -0.00616513  -0.306582   
 -0.083342    -0.031545     0.0497648   -0.18041      -0.0851622    0.00494937   0.0252871    -0.0643786   -0.0169066    0.0834999  -0.0620455    0.243918    -0.0101329     0.041609    -0.0499872    0.0837749    0.00947485    0.0429925    0.0434611     0.140171    -0.0922256     0.00364455   0.0512767    0.209399   -0.140085    -0.0115864  
 -0.0771404    0.0150264    0.122695     0.0838886    -0.0581782    0.0462736    0.152539      0.0616766   -0.0632213    0.0400213   0.0812914   -0.138337     0.00429235   -0.00413208  -0.17852     -0.0317355   -0.0783361    -0.0264704    0.0394772    -0.070134     0.0646532    -0.00827927   0.149426    -0.0664218  -0.0294388    0.199632   
 -0.157671    -0.0945283    0.209362    -0.0216185    -0.0536721    0.0881087   -0.0373171    -0.00752114   0.0276835   -0.0923337   0.0555808    0.0218733   -0.0696687    -0.16936     -0.00576555  -0.0945584    0.0369855    -0.184948     0.101412      0.0372457    0.000215948   0.058739    -0.102641     0.212089    0.0182319   -0.00918333 
  0.136865    -0.0359283    0.109994     0.0419009    -0.14258      0.0946182    0.0603011    -0.0078286   -0.149697    -0.0832321   0.12559     -0.0336012   -0.0403893    -0.0339936    0.135679     0.0951113   -0.0242206    -0.00457098   0.11431       0.0953644   -0.0454416    -0.0305573    0.0454229   -0.0152832  -0.0162695    0.141546   
  0.105591     0.0355136   -0.156225    -0.000288078   0.0592133    0.00571322   0.0958758     0.134531     0.0940474   -0.0158749   0.00920911   0.0652763    0.1852       -0.108533     0.171265     0.0455551   -0.133862     -0.017966    -0.000770977  -0.130306     0.0380608     0.154158     0.0188461   -0.0311472  -0.0566929   -0.0192928  
  0.191418    -0.0480913   -0.0252524    0.00497346    0.0357629   -0.169064     0.0594517     0.119944     0.115071     0.0660939   0.0445338   -0.193611     0.114998      0.0533171    0.00840248  -0.0174386    0.00987756    0.0211242    0.027558     -0.0767494    0.0776497    -0.0796451   -0.0572897   -0.125579   -0.0615163    0.0583357  
  0.0692464   -0.140789    -0.156054    -0.0542603    -0.00474331   0.0789073    0.0670338     0.0721845   -0.0922454   -0.0784619  -0.0319731    0.0464461   -0.109357      0.0237113   -0.0136041   -0.120532    -0.133386     -0.0248913   -0.0230482     0.10207     -0.159698     -0.132096    -0.12651     -0.0674769   0.110446     0.0602061  
  0.181795     0.00414827   0.0278933    0.189667      0.0608281    0.0814019   -0.014433      0.13036      0.246849     0.0176215   0.0248017    0.0854757   -0.0624465     0.149389    -0.039913     0.0212575   -0.00192473   -0.0199969    0.122223      0.0528396    0.152829     -0.0612135   -0.203202    -0.169888   -0.0256728   -0.134667   
  0.107294    -0.10795     -0.0788848   -0.0403993    -0.094906     0.0888851   -0.156407      0.0245228   -0.0537193    0.093431    0.12572      0.0995233   -0.0997249     0.0227562    0.201957    -0.132069    -0.0434568    -0.0108959   -0.106137      0.180631     0.121976     -0.00299298  -0.00467238   0.119729   -0.0586908    0.0608008  
  0.080476    -0.015903     0.0411918    0.000715203   0.178222     0.0613995    0.166273     -0.100429    -0.0658033    0.0529245  -0.106254     0.0556249    0.160529     -0.150338     0.13721      0.0151149    0.225494      0.0263268    0.122257      0.0518412   -0.175115      0.10436     -0.0920456   -0.142345    0.00905929  -0.000227714
 -0.167641     0.112515     0.036895     0.12441      -0.148013     0.102914     0.0620707    -0.0830948   -0.142901    -0.0383913  -0.0577454   -0.00100359  -0.0524207     0.156415     0.0399574   -0.0592727    0.0198697    -0.138634     0.0647045    -0.0909877   -0.0208344     0.0931478    0.054778    -0.234788    0.165814    -0.0352556  
  0.116142     0.0282585   -0.197384     0.0496212    -0.114252     0.217686     0.0320955    -0.0346172   -0.00906606  -0.08496    -0.117534     0.0729241    0.000313239   0.0347039    0.0975409    0.0147482   -0.0537084    -0.0303668    0.04722      -0.0394128    0.0691128     0.0591468   -0.133106     0.0435037  -0.0507086    0.010099   
 -0.112946     0.0621731   -0.0908626   -0.0435308    -0.0539193   -0.0787796    0.0216365     0.116848     0.0154522    0.0384728   0.0703502   -0.045747    -0.117432      0.13354     -0.147912     0.00129757   0.0329765    -0.0911798   -0.00534348    0.089531     0.000343229  -0.144116     0.0371129    0.0284666  -0.0489877    0.259573   
 -0.0851709    0.0740289   -0.032905    -0.117385     -0.0932739    0.0397858   -0.0755374     0.0497785   -0.0198778   -0.224433   -0.132195    -0.00816706  -0.0333393    -0.0271609    0.105063    -0.0649827   -0.108808      0.290979    -0.0180278    -0.154171     0.120424      0.0951219   -0.0736752   -0.143093   -0.0378755   -0.03927    
 -0.0214996    0.0660988    0.106156    -0.125009     -0.137291    -0.0472203    0.0189959    -0.027252    -0.158062     0.0134947   0.0120711   -0.109934     0.103884      0.00741549   0.0657439    0.0181103   -0.124233     -0.0934848   -0.0267284     0.133659    -0.0721164     0.152827     0.0189042   -0.1934     -0.0817478   -0.0823113  
  0.0310614   -0.0102617   -0.0087988    0.215443      0.0821936   -0.0430324   -0.00969989    0.0230387    0.00879568   0.0994841   0.177207     0.17671      0.131981     -0.00296165   0.0984897   -0.0554725   -0.0776337    -0.0514133   -0.0730419    -0.0226174   -0.043466     -0.0582941   -0.0238461   -0.027895   -0.0387343   -0.140369   
 -0.186658     0.0545093   -0.0860647   -0.0858478     0.0266304   -0.0858137   -0.0877816    -0.240697    -0.00243618   0.0118473   0.0455319   -0.229364     0.152482     -0.149112     0.0278477   -0.102446    -0.0403963     0.042802     0.0452489     0.00773909  -0.038235     -0.0294438   -0.0799733    0.153563    0.0535651    0.115576   
 -0.0357166    0.0244156   -0.0265429    0.0146726    -0.0959196    0.12731     -0.18836      -0.0831692   -0.152243     0.0875876  -0.0654756   -0.058458    -0.080392     -0.0393176   -0.105798    -0.0285282    0.124239      0.15446      0.00536675   -0.0752078   -0.0801911     0.0587174    0.105756    -0.109374    0.0681201   -0.187843   
  0.15104     -0.008008    -0.097483    -0.057418      0.0949162   -0.0671845    0.0903134    -0.0145724    0.0417668   -0.162404   -0.0212196   -0.0196764    0.0352528    -0.0289302    0.0734663    0.0257234   -0.210842      0.084732     0.0266501    -0.192973    -0.0825435    -0.0991795   -0.0455957    0.143943    0.0554081    0.0519337  
  0.132919     0.215295    -0.0760034    0.0119083    -0.124176    -0.0889125    0.0260513     0.11932      0.0563363    0.195268    0.0726811   -0.0231784    0.160983      0.044755    -0.116167     0.0446084   -0.0695791    -0.10437     -0.08685       0.0376576    0.213447      0.033156    -0.0436532   -0.0579555  -0.0195861   -0.0547067  
  0.162754    -0.0787414   -0.188687     0.0291569    -0.069783    -0.0177381   -0.159252     -0.0346506   -0.0748058   -0.131241    0.0400038    0.136836     0.124587      0.132746    -0.0122913    0.020069     0.0357563    -0.0341201    0.0921673     0.0810583   -0.00726873    0.0562756    0.0664105    0.0533556  -0.0235097   -0.124723   
 -0.11942      0.0751931   -0.0678211    0.0271244    -0.136333    -0.129203    -0.0345858    -0.0732144    0.0370296   -0.092822    0.0292054    0.0113436   -0.173935     -0.0717119    0.152748     0.122099    -0.0623171     0.0742297   -0.0918898     0.00584955   0.0246274     0.0975643   -0.104112    -0.135444   -0.152197     0.0477446  
 -0.00991171   0.0964076   -0.0296035   -0.217563     -0.0399068   -0.0718242    0.182655      0.0761744   -0.0151489    0.17403    -0.073289    -0.0110545    0.0372275    -0.0918714   -0.105889     0.0585757    0.0575663     0.00533804  -0.190029      0.181686    -0.0998598    -0.144179     0.0491512    0.0959697   0.160614    -0.0223844  
 -0.0160205    0.0966384    0.0284954   -0.0158444    -0.167489    -0.0779033    0.000611233  -0.0609298    0.102931     0.070253   -0.130961     0.247213     0.0894442    -0.0793359    0.0123701    0.062472    -0.000348015  -0.0279421    0.0318676     0.0735136    0.0233507     0.0792804    0.0514822    0.210973   -0.0620954   -0.120546   
 -0.0858968   -0.0557198   -0.00966983   0.15547      -0.00980236  -0.046855    -0.0687162    -0.0972862    0.262954    -0.126563    0.10132      0.169595     0.171835     -0.18183      0.0611771    0.0367524    0.0736203    -0.0861249    0.161089     -0.156244    -0.0738384     0.143075     0.0582123   -0.0370966   0.0867855   -0.0174322  
  0.0461849    0.0759264    0.225111     0.0600891    -0.0984023   -0.153391     0.0189394    -0.0992529   -0.0056429   -0.0139797  -0.00433854  -0.0447739    0.0268237    -0.00445272  -0.0596986   -0.127542     0.174001     -0.154912     0.040946     -0.00981959  -0.066106      0.108252    -0.0958889    0.0157405   0.0892536    0.0182433  
 -0.0516051    0.145634     0.0593294    0.0587622    -0.0156117    0.0216816   -0.0347015    -0.0439002    0.0892108    0.0344909   0.0772543   -0.144181    -0.0563749    -0.0439562   -0.137902    -0.00534013  -0.0233248     0.00743562   0.107892      0.0147135    0.0160984     0.125311     0.0486291   -0.0239713  -0.0984291    0.0199955  kind full, method split
0: avll = -1.4228540940044978
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.422874
INFO: iteration 2, average log likelihood -1.422818
INFO: iteration 3, average log likelihood -1.422783
INFO: iteration 4, average log likelihood -1.422746
INFO: iteration 5, average log likelihood -1.422701
INFO: iteration 6, average log likelihood -1.422635
INFO: iteration 7, average log likelihood -1.422522
INFO: iteration 8, average log likelihood -1.422289
INFO: iteration 9, average log likelihood -1.421784
INFO: iteration 10, average log likelihood -1.420831
INFO: iteration 11, average log likelihood -1.419528
INFO: iteration 12, average log likelihood -1.418414
INFO: iteration 13, average log likelihood -1.417825
INFO: iteration 14, average log likelihood -1.417599
INFO: iteration 15, average log likelihood -1.417521
INFO: iteration 16, average log likelihood -1.417493
INFO: iteration 17, average log likelihood -1.417482
INFO: iteration 18, average log likelihood -1.417478
INFO: iteration 19, average log likelihood -1.417476
INFO: iteration 20, average log likelihood -1.417475
INFO: iteration 21, average log likelihood -1.417475
INFO: iteration 22, average log likelihood -1.417474
INFO: iteration 23, average log likelihood -1.417474
INFO: iteration 24, average log likelihood -1.417474
INFO: iteration 25, average log likelihood -1.417473
INFO: iteration 26, average log likelihood -1.417473
INFO: iteration 27, average log likelihood -1.417473
INFO: iteration 28, average log likelihood -1.417473
INFO: iteration 29, average log likelihood -1.417472
INFO: iteration 30, average log likelihood -1.417472
INFO: iteration 31, average log likelihood -1.417472
INFO: iteration 32, average log likelihood -1.417472
INFO: iteration 33, average log likelihood -1.417472
INFO: iteration 34, average log likelihood -1.417472
INFO: iteration 35, average log likelihood -1.417472
INFO: iteration 36, average log likelihood -1.417472
INFO: iteration 37, average log likelihood -1.417472
INFO: iteration 38, average log likelihood -1.417471
INFO: iteration 39, average log likelihood -1.417471
INFO: iteration 40, average log likelihood -1.417471
INFO: iteration 41, average log likelihood -1.417471
INFO: iteration 42, average log likelihood -1.417471
INFO: iteration 43, average log likelihood -1.417471
INFO: iteration 44, average log likelihood -1.417471
INFO: iteration 45, average log likelihood -1.417471
INFO: iteration 46, average log likelihood -1.417471
INFO: iteration 47, average log likelihood -1.417471
INFO: iteration 48, average log likelihood -1.417471
INFO: iteration 49, average log likelihood -1.417471
INFO: iteration 50, average log likelihood -1.417471
INFO: EM with 100000 data points 50 iterations avll -1.417471
952.4 data points per parameter
1: avll = [-1.42287,-1.42282,-1.42278,-1.42275,-1.4227,-1.42264,-1.42252,-1.42229,-1.42178,-1.42083,-1.41953,-1.41841,-1.41783,-1.4176,-1.41752,-1.41749,-1.41748,-1.41748,-1.41748,-1.41748,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.417487
INFO: iteration 2, average log likelihood -1.417434
INFO: iteration 3, average log likelihood -1.417397
INFO: iteration 4, average log likelihood -1.417357
INFO: iteration 5, average log likelihood -1.417311
INFO: iteration 6, average log likelihood -1.417257
INFO: iteration 7, average log likelihood -1.417196
INFO: iteration 8, average log likelihood -1.417131
INFO: iteration 9, average log likelihood -1.417066
INFO: iteration 10, average log likelihood -1.417003
INFO: iteration 11, average log likelihood -1.416945
INFO: iteration 12, average log likelihood -1.416893
INFO: iteration 13, average log likelihood -1.416847
INFO: iteration 14, average log likelihood -1.416806
INFO: iteration 15, average log likelihood -1.416771
INFO: iteration 16, average log likelihood -1.416740
INFO: iteration 17, average log likelihood -1.416713
INFO: iteration 18, average log likelihood -1.416690
INFO: iteration 19, average log likelihood -1.416669
INFO: iteration 20, average log likelihood -1.416649
INFO: iteration 21, average log likelihood -1.416631
INFO: iteration 22, average log likelihood -1.416615
INFO: iteration 23, average log likelihood -1.416599
INFO: iteration 24, average log likelihood -1.416585
INFO: iteration 25, average log likelihood -1.416572
INFO: iteration 26, average log likelihood -1.416559
INFO: iteration 27, average log likelihood -1.416547
INFO: iteration 28, average log likelihood -1.416537
INFO: iteration 29, average log likelihood -1.416527
INFO: iteration 30, average log likelihood -1.416518
INFO: iteration 31, average log likelihood -1.416509
INFO: iteration 32, average log likelihood -1.416501
INFO: iteration 33, average log likelihood -1.416494
INFO: iteration 34, average log likelihood -1.416488
INFO: iteration 35, average log likelihood -1.416482
INFO: iteration 36, average log likelihood -1.416476
INFO: iteration 37, average log likelihood -1.416471
INFO: iteration 38, average log likelihood -1.416467
INFO: iteration 39, average log likelihood -1.416462
INFO: iteration 40, average log likelihood -1.416458
INFO: iteration 41, average log likelihood -1.416455
INFO: iteration 42, average log likelihood -1.416451
INFO: iteration 43, average log likelihood -1.416448
INFO: iteration 44, average log likelihood -1.416445
INFO: iteration 45, average log likelihood -1.416443
INFO: iteration 46, average log likelihood -1.416440
INFO: iteration 47, average log likelihood -1.416438
INFO: iteration 48, average log likelihood -1.416436
INFO: iteration 49, average log likelihood -1.416434
INFO: iteration 50, average log likelihood -1.416432
INFO: EM with 100000 data points 50 iterations avll -1.416432
473.9 data points per parameter
2: avll = [-1.41749,-1.41743,-1.4174,-1.41736,-1.41731,-1.41726,-1.4172,-1.41713,-1.41707,-1.417,-1.41695,-1.41689,-1.41685,-1.41681,-1.41677,-1.41674,-1.41671,-1.41669,-1.41667,-1.41665,-1.41663,-1.41661,-1.4166,-1.41659,-1.41657,-1.41656,-1.41655,-1.41654,-1.41653,-1.41652,-1.41651,-1.4165,-1.41649,-1.41649,-1.41648,-1.41648,-1.41647,-1.41647,-1.41646,-1.41646,-1.41645,-1.41645,-1.41645,-1.41645,-1.41644,-1.41644,-1.41644,-1.41644,-1.41643,-1.41643]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.416443
INFO: iteration 2, average log likelihood -1.416385
INFO: iteration 3, average log likelihood -1.416338
INFO: iteration 4, average log likelihood -1.416284
INFO: iteration 5, average log likelihood -1.416218
INFO: iteration 6, average log likelihood -1.416140
INFO: iteration 7, average log likelihood -1.416049
INFO: iteration 8, average log likelihood -1.415951
INFO: iteration 9, average log likelihood -1.415853
INFO: iteration 10, average log likelihood -1.415762
INFO: iteration 11, average log likelihood -1.415682
INFO: iteration 12, average log likelihood -1.415613
INFO: iteration 13, average log likelihood -1.415555
INFO: iteration 14, average log likelihood -1.415505
INFO: iteration 15, average log likelihood -1.415462
INFO: iteration 16, average log likelihood -1.415422
INFO: iteration 17, average log likelihood -1.415385
INFO: iteration 18, average log likelihood -1.415351
INFO: iteration 19, average log likelihood -1.415318
INFO: iteration 20, average log likelihood -1.415286
INFO: iteration 21, average log likelihood -1.415256
INFO: iteration 22, average log likelihood -1.415226
INFO: iteration 23, average log likelihood -1.415199
INFO: iteration 24, average log likelihood -1.415174
INFO: iteration 25, average log likelihood -1.415150
INFO: iteration 26, average log likelihood -1.415129
INFO: iteration 27, average log likelihood -1.415110
INFO: iteration 28, average log likelihood -1.415092
INFO: iteration 29, average log likelihood -1.415077
INFO: iteration 30, average log likelihood -1.415063
INFO: iteration 31, average log likelihood -1.415051
INFO: iteration 32, average log likelihood -1.415040
INFO: iteration 33, average log likelihood -1.415030
INFO: iteration 34, average log likelihood -1.415021
INFO: iteration 35, average log likelihood -1.415013
INFO: iteration 36, average log likelihood -1.415006
INFO: iteration 37, average log likelihood -1.414999
INFO: iteration 38, average log likelihood -1.414993
INFO: iteration 39, average log likelihood -1.414987
INFO: iteration 40, average log likelihood -1.414982
INFO: iteration 41, average log likelihood -1.414977
INFO: iteration 42, average log likelihood -1.414973
INFO: iteration 43, average log likelihood -1.414968
INFO: iteration 44, average log likelihood -1.414964
INFO: iteration 45, average log likelihood -1.414960
INFO: iteration 46, average log likelihood -1.414957
INFO: iteration 47, average log likelihood -1.414953
INFO: iteration 48, average log likelihood -1.414950
INFO: iteration 49, average log likelihood -1.414946
INFO: iteration 50, average log likelihood -1.414943
INFO: EM with 100000 data points 50 iterations avll -1.414943
236.4 data points per parameter
3: avll = [-1.41644,-1.41639,-1.41634,-1.41628,-1.41622,-1.41614,-1.41605,-1.41595,-1.41585,-1.41576,-1.41568,-1.41561,-1.41555,-1.41551,-1.41546,-1.41542,-1.41539,-1.41535,-1.41532,-1.41529,-1.41526,-1.41523,-1.4152,-1.41517,-1.41515,-1.41513,-1.41511,-1.41509,-1.41508,-1.41506,-1.41505,-1.41504,-1.41503,-1.41502,-1.41501,-1.41501,-1.415,-1.41499,-1.41499,-1.41498,-1.41498,-1.41497,-1.41497,-1.41496,-1.41496,-1.41496,-1.41495,-1.41495,-1.41495,-1.41494]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.414949
INFO: iteration 2, average log likelihood -1.414899
INFO: iteration 3, average log likelihood -1.414857
INFO: iteration 4, average log likelihood -1.414812
INFO: iteration 5, average log likelihood -1.414758
INFO: iteration 6, average log likelihood -1.414695
INFO: iteration 7, average log likelihood -1.414621
INFO: iteration 8, average log likelihood -1.414537
INFO: iteration 9, average log likelihood -1.414447
INFO: iteration 10, average log likelihood -1.414353
INFO: iteration 11, average log likelihood -1.414258
INFO: iteration 12, average log likelihood -1.414165
INFO: iteration 13, average log likelihood -1.414076
INFO: iteration 14, average log likelihood -1.413992
INFO: iteration 15, average log likelihood -1.413914
INFO: iteration 16, average log likelihood -1.413843
INFO: iteration 17, average log likelihood -1.413779
INFO: iteration 18, average log likelihood -1.413722
INFO: iteration 19, average log likelihood -1.413671
INFO: iteration 20, average log likelihood -1.413626
INFO: iteration 21, average log likelihood -1.413586
INFO: iteration 22, average log likelihood -1.413550
INFO: iteration 23, average log likelihood -1.413517
INFO: iteration 24, average log likelihood -1.413486
INFO: iteration 25, average log likelihood -1.413458
INFO: iteration 26, average log likelihood -1.413431
INFO: iteration 27, average log likelihood -1.413405
INFO: iteration 28, average log likelihood -1.413380
INFO: iteration 29, average log likelihood -1.413356
INFO: iteration 30, average log likelihood -1.413333
INFO: iteration 31, average log likelihood -1.413311
INFO: iteration 32, average log likelihood -1.413290
INFO: iteration 33, average log likelihood -1.413269
INFO: iteration 34, average log likelihood -1.413249
INFO: iteration 35, average log likelihood -1.413230
INFO: iteration 36, average log likelihood -1.413212
INFO: iteration 37, average log likelihood -1.413194
INFO: iteration 38, average log likelihood -1.413177
INFO: iteration 39, average log likelihood -1.413161
INFO: iteration 40, average log likelihood -1.413145
INFO: iteration 41, average log likelihood -1.413131
INFO: iteration 42, average log likelihood -1.413117
INFO: iteration 43, average log likelihood -1.413103
INFO: iteration 44, average log likelihood -1.413090
INFO: iteration 45, average log likelihood -1.413078
INFO: iteration 46, average log likelihood -1.413067
INFO: iteration 47, average log likelihood -1.413056
INFO: iteration 48, average log likelihood -1.413046
INFO: iteration 49, average log likelihood -1.413036
INFO: iteration 50, average log likelihood -1.413027
INFO: EM with 100000 data points 50 iterations avll -1.413027
118.1 data points per parameter
4: avll = [-1.41495,-1.4149,-1.41486,-1.41481,-1.41476,-1.41469,-1.41462,-1.41454,-1.41445,-1.41435,-1.41426,-1.41417,-1.41408,-1.41399,-1.41391,-1.41384,-1.41378,-1.41372,-1.41367,-1.41363,-1.41359,-1.41355,-1.41352,-1.41349,-1.41346,-1.41343,-1.4134,-1.41338,-1.41336,-1.41333,-1.41331,-1.41329,-1.41327,-1.41325,-1.41323,-1.41321,-1.41319,-1.41318,-1.41316,-1.41315,-1.41313,-1.41312,-1.4131,-1.41309,-1.41308,-1.41307,-1.41306,-1.41305,-1.41304,-1.41303]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.413027
INFO: iteration 2, average log likelihood -1.412964
INFO: iteration 3, average log likelihood -1.412904
INFO: iteration 4, average log likelihood -1.412835
INFO: iteration 5, average log likelihood -1.412749
INFO: iteration 6, average log likelihood -1.412641
INFO: iteration 7, average log likelihood -1.412512
INFO: iteration 8, average log likelihood -1.412365
INFO: iteration 9, average log likelihood -1.412209
INFO: iteration 10, average log likelihood -1.412052
INFO: iteration 11, average log likelihood -1.411901
INFO: iteration 12, average log likelihood -1.411761
INFO: iteration 13, average log likelihood -1.411633
INFO: iteration 14, average log likelihood -1.411521
INFO: iteration 15, average log likelihood -1.411422
INFO: iteration 16, average log likelihood -1.411336
INFO: iteration 17, average log likelihood -1.411262
INFO: iteration 18, average log likelihood -1.411196
INFO: iteration 19, average log likelihood -1.411139
INFO: iteration 20, average log likelihood -1.411088
INFO: iteration 21, average log likelihood -1.411042
INFO: iteration 22, average log likelihood -1.411000
INFO: iteration 23, average log likelihood -1.410962
INFO: iteration 24, average log likelihood -1.410926
INFO: iteration 25, average log likelihood -1.410892
INFO: iteration 26, average log likelihood -1.410861
INFO: iteration 27, average log likelihood -1.410831
INFO: iteration 28, average log likelihood -1.410802
INFO: iteration 29, average log likelihood -1.410775
INFO: iteration 30, average log likelihood -1.410749
INFO: iteration 31, average log likelihood -1.410725
INFO: iteration 32, average log likelihood -1.410701
INFO: iteration 33, average log likelihood -1.410678
INFO: iteration 34, average log likelihood -1.410656
INFO: iteration 35, average log likelihood -1.410636
INFO: iteration 36, average log likelihood -1.410616
INFO: iteration 37, average log likelihood -1.410597
INFO: iteration 38, average log likelihood -1.410579
INFO: iteration 39, average log likelihood -1.410562
INFO: iteration 40, average log likelihood -1.410546
INFO: iteration 41, average log likelihood -1.410530
INFO: iteration 42, average log likelihood -1.410516
INFO: iteration 43, average log likelihood -1.410502
INFO: iteration 44, average log likelihood -1.410488
INFO: iteration 45, average log likelihood -1.410475
INFO: iteration 46, average log likelihood -1.410463
INFO: iteration 47, average log likelihood -1.410451
INFO: iteration 48, average log likelihood -1.410440
INFO: iteration 49, average log likelihood -1.410429
INFO: iteration 50, average log likelihood -1.410418
INFO: EM with 100000 data points 50 iterations avll -1.410418
59.0 data points per parameter
5: avll = [-1.41303,-1.41296,-1.4129,-1.41283,-1.41275,-1.41264,-1.41251,-1.41237,-1.41221,-1.41205,-1.4119,-1.41176,-1.41163,-1.41152,-1.41142,-1.41134,-1.41126,-1.4112,-1.41114,-1.41109,-1.41104,-1.411,-1.41096,-1.41093,-1.41089,-1.41086,-1.41083,-1.4108,-1.41078,-1.41075,-1.41072,-1.4107,-1.41068,-1.41066,-1.41064,-1.41062,-1.4106,-1.41058,-1.41056,-1.41055,-1.41053,-1.41052,-1.4105,-1.41049,-1.41048,-1.41046,-1.41045,-1.41044,-1.41043,-1.41042]
[-1.42285,-1.42287,-1.42282,-1.42278,-1.42275,-1.4227,-1.42264,-1.42252,-1.42229,-1.42178,-1.42083,-1.41953,-1.41841,-1.41783,-1.4176,-1.41752,-1.41749,-1.41748,-1.41748,-1.41748,-1.41748,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41747,-1.41749,-1.41743,-1.4174,-1.41736,-1.41731,-1.41726,-1.4172,-1.41713,-1.41707,-1.417,-1.41695,-1.41689,-1.41685,-1.41681,-1.41677,-1.41674,-1.41671,-1.41669,-1.41667,-1.41665,-1.41663,-1.41661,-1.4166,-1.41659,-1.41657,-1.41656,-1.41655,-1.41654,-1.41653,-1.41652,-1.41651,-1.4165,-1.41649,-1.41649,-1.41648,-1.41648,-1.41647,-1.41647,-1.41646,-1.41646,-1.41645,-1.41645,-1.41645,-1.41645,-1.41644,-1.41644,-1.41644,-1.41644,-1.41643,-1.41643,-1.41644,-1.41639,-1.41634,-1.41628,-1.41622,-1.41614,-1.41605,-1.41595,-1.41585,-1.41576,-1.41568,-1.41561,-1.41555,-1.41551,-1.41546,-1.41542,-1.41539,-1.41535,-1.41532,-1.41529,-1.41526,-1.41523,-1.4152,-1.41517,-1.41515,-1.41513,-1.41511,-1.41509,-1.41508,-1.41506,-1.41505,-1.41504,-1.41503,-1.41502,-1.41501,-1.41501,-1.415,-1.41499,-1.41499,-1.41498,-1.41498,-1.41497,-1.41497,-1.41496,-1.41496,-1.41496,-1.41495,-1.41495,-1.41495,-1.41494,-1.41495,-1.4149,-1.41486,-1.41481,-1.41476,-1.41469,-1.41462,-1.41454,-1.41445,-1.41435,-1.41426,-1.41417,-1.41408,-1.41399,-1.41391,-1.41384,-1.41378,-1.41372,-1.41367,-1.41363,-1.41359,-1.41355,-1.41352,-1.41349,-1.41346,-1.41343,-1.4134,-1.41338,-1.41336,-1.41333,-1.41331,-1.41329,-1.41327,-1.41325,-1.41323,-1.41321,-1.41319,-1.41318,-1.41316,-1.41315,-1.41313,-1.41312,-1.4131,-1.41309,-1.41308,-1.41307,-1.41306,-1.41305,-1.41304,-1.41303,-1.41303,-1.41296,-1.4129,-1.41283,-1.41275,-1.41264,-1.41251,-1.41237,-1.41221,-1.41205,-1.4119,-1.41176,-1.41163,-1.41152,-1.41142,-1.41134,-1.41126,-1.4112,-1.41114,-1.41109,-1.41104,-1.411,-1.41096,-1.41093,-1.41089,-1.41086,-1.41083,-1.4108,-1.41078,-1.41075,-1.41072,-1.4107,-1.41068,-1.41066,-1.41064,-1.41062,-1.4106,-1.41058,-1.41056,-1.41055,-1.41053,-1.41052,-1.4105,-1.41049,-1.41048,-1.41046,-1.41045,-1.41044,-1.41043,-1.41042]
32×26 Array{Float64,2}:
 -0.0304917   0.0763002  -0.347569     0.0294431   0.345124   -0.563823     0.102466    -0.120265    -0.0460335   -0.201508     0.557538   -0.523243   -0.77972      0.216908      0.699216      0.17387     -0.635213    0.0611802   0.103789    0.414603   -0.233374    -0.43031      0.550859   -0.552344    0.0161047    0.514059 
  0.380144   -0.377017   -0.115752    -0.270546   -0.455193   -0.0564412   -0.0493566   -0.119044     0.324569    -0.655631    -0.0798302   0.36866     0.326256     0.0148569    -0.15423      -0.444612    -0.132505    0.15297    -0.324948    0.318304   -0.345251     0.207848     0.334729   -0.418736    0.0356289    0.225583 
  0.573869   -0.304289   -0.119407     0.438863   -0.0734266   0.207792     3.88737e-5   0.20929     -0.254471     0.354475    -0.179805    0.154966    0.36358      0.0772098    -0.296828      0.088168    -0.191695    0.428719   -0.352916    0.116677    0.127177     0.18155     -0.0114348   0.369136   -0.0953541   -0.582526 
  0.119434    0.142812   -0.501208     0.154968    0.380549    0.24646     -0.0702422   -0.120339     0.578059     0.0186781   -0.190414   -0.0856933   0.225961     0.0904101    -0.334396      0.0131022   -0.0153889   0.177036   -0.421736    0.195354    0.00711258   0.0987722    0.324412    0.213631    0.371061     0.314869 
 -0.270167    0.535455   -0.127103     0.0456437  -0.0667234  -0.369936    -0.108264    -0.582706     0.136756     0.224063    -0.433507    0.0162473   0.538961     0.279513      0.272983     -0.0593549   -0.113436   -0.623247    0.250295   -0.0779411   0.42779     -0.397068    -0.459711    0.319198   -0.00608543  -0.0761434
  0.0266243   0.560495   -0.148071     0.0501412   0.227934   -0.243276    -0.215925    -0.361044     0.830536     0.260445    -0.17138    -0.0349755   0.00794737   0.0913474    -0.16713       0.0649015   -0.29878     0.704914   -0.134679   -0.192293    0.207037    -0.524932    -0.561736   -0.100238   -0.55436      0.168001 
 -0.861881   -0.189402   -0.487464    -0.667296    0.15978     0.0769441    0.323083    -0.54002      0.242787    -0.163361     0.293928    0.139422    0.090895     0.317423     -0.231928      0.555738     0.170376   -0.561452   -0.28258     0.522542    0.0663281   -0.152044    -0.485315   -0.10663    -0.164482     0.608392 
 -0.95149     0.0160644  -0.478689     0.269754   -0.201037    0.352998    -0.0200095    0.343066     0.262685    -0.512733    -0.153767   -0.162098    0.324221     0.406184      0.112882      0.665567     0.196118    0.109161   -0.143847    0.149849    0.0396239    0.325027     0.114706   -0.0337497   0.257733     0.0698982
  0.105317   -0.0380488   0.00143278  -0.101333   -0.0229429   0.00866915  -0.0582564   -0.253926    -0.0452368    0.0533614    0.0806021  -0.0336758  -0.0120566    0.0954545     0.000520668   0.0378281   -0.0777862  -0.130487   -0.138738   -0.0494205  -0.0577337   -0.00276449  -0.0881307   0.0072697  -0.109449     0.040059 
 -0.162103   -0.0982017   0.0479241    0.0365484  -0.276095    0.0774448    0.13277      0.261175    -0.0102979   -0.0678031   -0.145345    0.0049574  -0.0711511   -0.178212      0.252175     -0.122363     0.0303434  -0.033303    0.140745   -0.0392422   0.0049957    0.0644082    0.0120779  -0.103883   -0.00682674  -0.10725  
  0.573636   -0.0194279  -0.31572     -0.68818     0.334244    0.165414    -0.198167    -0.152523     0.0982298    0.236078     0.270356    0.323273   -0.308519    -0.139736      0.245108     -0.37026     -0.178519    0.182713    0.139577    0.068      -0.165267    -0.321551    -0.204632    0.456451    0.365259    -0.167469 
 -0.3565      0.420016   -0.0300075    0.0581033   0.185855    0.184138     0.233824     0.504833    -0.286718     0.00516096   0.0295555   0.1589     -0.210914    -0.465627      0.192691     -0.169762     0.252297   -0.260993    0.332656    0.110621    0.223744    -0.574346    -0.14334     0.229008    0.398155    -0.228144 
 -0.147468    0.23691     0.216128     0.261705    0.0869125  -0.392696    -0.179074     0.0349822    0.0616592    0.202539     0.112764   -0.154667    0.075093    -0.0974108    -0.54139       0.104625     0.130026    0.379833    0.631233    0.105785    0.00481166   0.204841     0.251133   -0.157264   -0.137189     0.118698 
 -0.28137     0.420367   -0.284728     0.673617    0.0912446  -0.465034    -0.358672    -0.230927    -0.340539    -0.0461371    0.576102   -0.731531   -0.340978     0.229477      0.47139       0.147844     0.125513   -0.081615   -0.130906   -0.266032    0.245079    -0.141539     0.22973    -0.375325   -0.1972      -0.430124 
  0.16503    -0.225031    0.183921     0.118743    0.140605    0.0668195   -0.211823     0.255521    -0.923702     0.685766     0.612501   -0.39359     0.0449121   -0.357185      0.159338      0.34379     -0.144588   -0.423739    0.123714    0.11453     0.306762     0.120122    -0.188222   -0.157235   -0.100214     0.270625 
 -0.0875702   0.0101266   0.659479     0.328795   -0.316191    0.350466     0.0297207   -0.207911    -0.90914     -0.111081     0.490118    0.258486    0.337052     0.14946      -0.212513      0.557099    -0.301719   -0.453784    0.1343     -0.0238592  -0.139786     0.414615     0.190816   -0.264186   -0.0554019    0.496443 
  0.450144   -0.175111    0.422691    -0.33008    -0.429088   -0.578065    -0.166132    -0.399067    -0.36579      0.135304     0.20425     0.145636   -0.550336    -0.132921     -0.0874268    -0.219343    -0.0996397  -0.0301204   0.50018    -0.732934   -0.164465    -0.338533    -0.656996   -0.049694   -0.582982    -0.272935 
  0.542641    0.264822    0.147485    -0.191463   -0.100325   -0.519313     0.12012      0.4943      -0.237887    -0.0492659    0.0443032  -0.343816   -0.456448    -0.493634     -0.100927     -0.555274    -0.510406    0.147818    0.405259   -0.117287   -0.557712    -0.581808     0.350775    0.461688   -0.238567    -0.239415 
  0.408146    0.0294425  -0.048867    -0.144187   -0.263773   -0.892433    -0.476033    -0.404928    -0.216151     0.541316    -0.278501    0.0182251   0.403794     0.2419       -0.569446     -0.00477988  -1.28646     0.319698    0.741299    0.0783707  -0.110981     0.171761     0.159785   -0.108793   -0.430452     0.206386 
  0.387645   -0.300911    0.0096907    0.0800384   0.0127869  -0.654389     0.15992     -0.8397       0.289802     0.283465     0.0723517  -0.0512912   0.249151     0.179622     -0.411319      0.0778574    0.252421   -0.432209   -0.636834   -0.132079   -0.283652     0.820978     0.231578   -0.0358492  -0.235007     0.179913 
  0.197853   -0.0733613  -0.321344    -0.0914912  -0.0186432  -0.102491     0.105923    -0.57648      0.52024     -0.0777705   -0.0837675  -0.423478   -0.263801     0.390199      0.208032     -0.153217    -0.193107    0.171518   -0.415728   -0.0145731  -0.0860709   -0.0449127   -0.0155848  -0.304218   -0.19607     -0.0155057
 -0.0680199   0.18292    -0.0576072    0.133849    0.137816    0.0362376   -0.128258     0.294565    -0.136786     0.150967     0.0711723   0.220949    0.269312    -0.260173     -0.236018      0.223448     0.0709871   0.105488    0.191123    0.235379    0.157434    -0.0103625    0.124595    0.21871     0.239516     0.0101721
 -0.308604    0.210526   -0.580032     0.195346   -0.357131    0.233238    -0.288293     0.317832     0.0505599    0.0345138    0.0909481  -0.0465379  -0.331158    -0.370746     -0.288582      0.200363     0.611213    0.449826    0.31553     0.10178     0.664466    -0.439986    -0.0653458  -0.355183    0.0475014   -0.0741382
  0.179122    0.607736   -0.275624     0.823175    0.0568099  -0.282686    -0.246642     0.231796     0.530022     0.389012    -0.317944   -0.382429   -0.315086    -0.041942     -0.0692321    -0.528762     0.368485    0.429851   -0.0035705  -0.515833    0.139599     0.496788     0.0521478  -0.421939    0.225597    -0.256376 
 -0.409154   -0.260822    0.0511294   -0.68907    -0.29093     0.327772     0.555787     0.0102132    0.0606193   -0.591635    -0.22609     0.0551994  -0.0573788    0.0503503     0.863178      0.305673    -0.431484   -0.046396    0.158627   -0.145494    0.0389329   -0.291449     0.0643836  -0.0631172  -0.0274687   -0.283499 
  0.173305   -0.126971   -0.0415734   -0.237398   -0.107323    0.815951     0.244824    -0.00324266  -0.00539135  -0.464693    -0.0182017   0.319648   -0.420276     0.183645      0.849451     -0.140943     0.415791   -0.563989   -0.996091   -0.0131898  -0.0185149   -0.280171    -0.187262   -0.0381209   0.0907909   -0.102252 
 -0.254377   -0.0564845   0.458281    -0.22375    -0.433475    0.0583021    0.00667665  -0.225001    -0.551222    -0.128325     0.62236     0.202704   -0.60763     -0.218971      0.580283     -0.84389      0.520275   -1.27369     0.514607   -0.20224     0.0561354   -0.0115222   -0.69765     0.0570591  -0.0515307    0.449819 
 -0.420987   -0.249644    0.613659     0.0710482  -0.148045    0.399317     0.360556     0.412797    -0.227369    -0.196617     0.700366   -0.160204   -0.663796    -0.2931        0.160701      0.230123     0.646066   -0.122903   -0.0752754  -0.12126    -0.358657     0.884038     0.195489   -0.257624   -0.490985    -0.137155 
 -0.179782    0.364873    0.465147    -0.246887   -0.224193    0.762658    -0.281421     0.0775595    0.031032     0.376974    -0.856267    0.653133    0.556753    -0.000407209  -0.79892      -0.815993     0.345756    0.636605   -0.861093   -0.426504   -0.188837     0.618213    -0.76519     0.261138   -0.286493     0.0974399
 -0.0188688  -0.306819    0.515046    -0.474591   -0.489084    0.578874     0.550773     0.286214     0.297764    -0.1282      -0.607566    0.791237    0.344895    -0.171298     -0.689056      0.0118118    0.165201    0.0440336   0.513291   -0.254066   -0.184163     0.55611     -0.476713    0.283075    0.130839    -0.0150702
  0.37761    -0.733455    0.423315     0.291701   -0.382919   -0.00495869   0.199799     0.206926    -0.269136     0.269832    -0.720223   -0.196917    0.586987    -0.189467      0.231104     -0.609029     0.0878365  -0.47734     0.0931087   0.124552    0.260572    -0.152216     0.16125     0.475177    0.1347      -0.835057 
 -0.0222169   0.0715871   0.896517     0.0220789   1.07948    -0.19043      0.357247     0.153322     0.269904     0.233006    -0.454121    0.496777    0.494041    -0.32533       0.294605     -0.0719527   -0.323461    0.152973    0.328852   -0.387293   -0.317988     0.363458    -0.0214043   0.509805    0.20766     -0.124523 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.410408
INFO: iteration 2, average log likelihood -1.410398
INFO: iteration 3, average log likelihood -1.410388
INFO: iteration 4, average log likelihood -1.410378
INFO: iteration 5, average log likelihood -1.410369
INFO: iteration 6, average log likelihood -1.410360
INFO: iteration 7, average log likelihood -1.410351
INFO: iteration 8, average log likelihood -1.410342
INFO: iteration 9, average log likelihood -1.410333
INFO: iteration 10, average log likelihood -1.410325
INFO: EM with 100000 data points 10 iterations avll -1.410325
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.558218e+05
      1       7.038113e+05      -2.520104e+05 |       32
      2       6.908068e+05      -1.300456e+04 |       32
      3       6.857640e+05      -5.042752e+03 |       32
      4       6.831410e+05      -2.623038e+03 |       32
      5       6.815923e+05      -1.548727e+03 |       32
      6       6.805092e+05      -1.083040e+03 |       32
      7       6.796541e+05      -8.551405e+02 |       32
      8       6.789722e+05      -6.818491e+02 |       32
      9       6.783660e+05      -6.061989e+02 |       32
     10       6.778404e+05      -5.256763e+02 |       32
     11       6.773577e+05      -4.826879e+02 |       32
     12       6.769020e+05      -4.556412e+02 |       32
     13       6.764713e+05      -4.306815e+02 |       32
     14       6.761263e+05      -3.450370e+02 |       32
     15       6.758051e+05      -3.212127e+02 |       32
     16       6.755179e+05      -2.872051e+02 |       32
     17       6.752884e+05      -2.295432e+02 |       32
     18       6.750731e+05      -2.152109e+02 |       32
     19       6.748753e+05      -1.978793e+02 |       32
     20       6.746914e+05      -1.838261e+02 |       32
     21       6.745388e+05      -1.525970e+02 |       32
     22       6.743807e+05      -1.580921e+02 |       32
     23       6.742430e+05      -1.377320e+02 |       32
     24       6.741265e+05      -1.164961e+02 |       32
     25       6.740092e+05      -1.172829e+02 |       32
     26       6.739120e+05      -9.720243e+01 |       32
     27       6.738189e+05      -9.314195e+01 |       32
     28       6.737439e+05      -7.500663e+01 |       32
     29       6.736766e+05      -6.728574e+01 |       32
     30       6.736138e+05      -6.277234e+01 |       32
     31       6.735505e+05      -6.336818e+01 |       32
     32       6.734906e+05      -5.982848e+01 |       32
     33       6.734287e+05      -6.190237e+01 |       32
     34       6.733695e+05      -5.922914e+01 |       32
     35       6.733090e+05      -6.051598e+01 |       32
     36       6.732498e+05      -5.922305e+01 |       32
     37       6.731935e+05      -5.625280e+01 |       32
     38       6.731449e+05      -4.861498e+01 |       32
     39       6.731082e+05      -3.665748e+01 |       32
     40       6.730736e+05      -3.459276e+01 |       32
     41       6.730382e+05      -3.540192e+01 |       32
     42       6.730047e+05      -3.354277e+01 |       32
     43       6.729686e+05      -3.610392e+01 |       32
     44       6.729327e+05      -3.586761e+01 |       32
     45       6.729004e+05      -3.232946e+01 |       32
     46       6.728697e+05      -3.070601e+01 |       32
     47       6.728392e+05      -3.048587e+01 |       32
     48       6.728081e+05      -3.105942e+01 |       32
     49       6.727797e+05      -2.845205e+01 |       32
     50       6.727475e+05      -3.220475e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 672747.4872893048)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.422101
INFO: iteration 2, average log likelihood -1.417057
INFO: iteration 3, average log likelihood -1.415724
INFO: iteration 4, average log likelihood -1.414758
INFO: iteration 5, average log likelihood -1.413733
INFO: iteration 6, average log likelihood -1.412758
INFO: iteration 7, average log likelihood -1.412071
INFO: iteration 8, average log likelihood -1.411686
INFO: iteration 9, average log likelihood -1.411474
INFO: iteration 10, average log likelihood -1.411340
INFO: iteration 11, average log likelihood -1.411242
INFO: iteration 12, average log likelihood -1.411164
INFO: iteration 13, average log likelihood -1.411098
INFO: iteration 14, average log likelihood -1.411041
INFO: iteration 15, average log likelihood -1.410989
INFO: iteration 16, average log likelihood -1.410943
INFO: iteration 17, average log likelihood -1.410900
INFO: iteration 18, average log likelihood -1.410861
INFO: iteration 19, average log likelihood -1.410823
INFO: iteration 20, average log likelihood -1.410788
INFO: iteration 21, average log likelihood -1.410755
INFO: iteration 22, average log likelihood -1.410724
INFO: iteration 23, average log likelihood -1.410694
INFO: iteration 24, average log likelihood -1.410665
INFO: iteration 25, average log likelihood -1.410637
INFO: iteration 26, average log likelihood -1.410611
INFO: iteration 27, average log likelihood -1.410585
INFO: iteration 28, average log likelihood -1.410561
INFO: iteration 29, average log likelihood -1.410538
INFO: iteration 30, average log likelihood -1.410515
INFO: iteration 31, average log likelihood -1.410493
INFO: iteration 32, average log likelihood -1.410472
INFO: iteration 33, average log likelihood -1.410452
INFO: iteration 34, average log likelihood -1.410433
INFO: iteration 35, average log likelihood -1.410414
INFO: iteration 36, average log likelihood -1.410396
INFO: iteration 37, average log likelihood -1.410378
INFO: iteration 38, average log likelihood -1.410362
INFO: iteration 39, average log likelihood -1.410346
INFO: iteration 40, average log likelihood -1.410330
INFO: iteration 41, average log likelihood -1.410315
INFO: iteration 42, average log likelihood -1.410301
INFO: iteration 43, average log likelihood -1.410288
INFO: iteration 44, average log likelihood -1.410274
INFO: iteration 45, average log likelihood -1.410262
INFO: iteration 46, average log likelihood -1.410250
INFO: iteration 47, average log likelihood -1.410238
INFO: iteration 48, average log likelihood -1.410227
INFO: iteration 49, average log likelihood -1.410217
INFO: iteration 50, average log likelihood -1.410207
INFO: EM with 100000 data points 50 iterations avll -1.410207
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0745683  -0.0283283    0.381442    -0.323893    -0.400731     0.689407     0.0790562    0.394692     0.317477    0.0906282    -0.913257    0.76928     0.588108    -0.0482135    -0.720659   -0.50986      0.149274    0.367553    -0.393011   -0.438224    -0.337995    0.649217   -0.754155     0.227213   -0.147088    0.147728  
  0.3258     -0.0922932   -0.0491906   -0.0347778    0.210165     0.274509    -0.586971     0.827446    -0.76149     0.473147      0.515813   -0.130246    0.0428766   -0.135845     -0.0571184  -0.091814     0.163378   -0.64168      0.101232    0.105433    -0.0375602   0.300742    0.237569     0.451551    0.688588    0.457623  
  0.119126    0.933235    -0.500765    -0.0996017   -0.173893    -0.611011    -0.260806    -0.563641     0.350739    0.413726     -0.313563    0.50475     1.08467      0.449998     -0.185066    0.147767    -0.797189   -0.0387575    0.420387    0.00761476   0.319716   -0.683137   -0.681029     0.0903018   0.0693583   0.0683189 
 -0.0251057   0.566948     0.0868924   -0.185603     0.225093    -0.284719    -0.0100087    0.188817    -0.0359722   0.212348     -0.22035     0.148071   -0.259095    -0.388719      0.0586012  -0.327383    -0.235724    0.102508     0.555076   -0.318519    -0.123812   -0.445118   -0.371172     0.492036   -0.0634434  -0.157627  
  0.588119   -0.326342     0.620813     0.406201     0.0308805   -0.0974341    0.278159     0.568651    -0.207878    0.461433     -0.474031   -0.326801    0.342219    -0.179356      0.300715   -0.745646    -0.36447    -0.0888744    0.0265505  -0.0438987   -0.0930843  -0.102743    0.399172     0.593114    0.187321   -0.841775  
 -0.183242    0.776297    -0.190528     0.225856     0.514496     0.366814    -0.233815     0.321758     0.513795    0.0523006     0.338708    0.323068   -0.774138     0.404032     -0.354484    0.375072     0.377362    0.454972     0.374671   -0.30496      0.546801   -0.325362   -0.188332    -0.542442   -0.215776    0.278214  
 -0.0669765   0.406427    -0.00133354   0.535999     0.0284463   -0.331441    -0.476194    -0.0770692   -0.158698    0.336707      0.178001   -0.641981   -0.210003     0.542218     -0.149685    0.131441     0.354083   -0.3982      -0.294549   -0.468425    -0.0219372   0.568534   -0.360924    -0.160645   -0.43875    -0.028949  
  0.0398115   0.0988719   -0.542717     0.192653    -0.241244     0.180775    -0.0805561   -0.473488     0.20928     0.342699     -0.364941   -0.761841    0.00854807  -0.0272888     0.383716   -0.359924    -0.105944    0.251447    -0.503957    0.0113299    0.681543   -0.49822    -0.445251    -0.26634    -0.238326   -0.149176  
 -0.0589711   0.0294539   -0.00297152   0.264665     0.241419    -0.126948     0.00906325   0.115896     0.172116    0.373908     -0.318062    0.068898    0.500195    -0.182448     -0.445122    0.305278    -0.0845556   0.427818     0.0261066   0.210435     0.275963    0.181755    0.240841     0.227314    0.0148058  -0.113758  
 -0.0994622  -0.00865676   0.522632     0.0571684   -0.156669    -0.173453    -0.195171    -0.0262132   -0.780817    0.360265      0.390986   -0.320656    0.363591    -0.489419     -0.237783    0.387502    -0.482817   -0.119682     0.549715    0.15973      0.157673   -0.107481    0.280362    -0.397734   -0.373878    0.578476  
 -0.0973642   0.0202295   -0.0900757    0.353808     0.0189159    0.295708    -0.0209801   -0.120417    -0.2946     -0.307452      0.433766    0.786367    0.770923     0.161332     -0.143845    0.198369     0.0188074  -0.00309893  -0.403975    0.411427    -0.297107    0.496928    0.229978    -0.826575    0.453396    0.414201  
 -0.398223    0.0538837   -0.121794     0.037861    -0.120606     0.350259     0.255026     0.426536    -0.164113   -0.202151     -0.291987    0.223588   -0.0360392   -0.54835       0.0662425  -0.215571     0.581551   -0.227538     0.291447    0.325026     0.553212   -0.421395    0.0817806    0.116996    0.57986    -0.313222  
 -0.119783   -0.69795      0.548752    -0.0251495   -0.362186     0.111029     0.445107     0.24247      0.0337276  -0.70361       0.26198    -0.181374   -0.381303    -0.178162     -0.0516485   0.00157872   0.565326   -0.0964533   -0.0701361   0.129247    -0.420803    0.859223    0.684774    -0.323488   -0.348699    0.121774  
 -0.160037    0.206694    -0.239075     0.357391     0.319817    -0.37995     -0.0994821   -0.302152     0.750198   -0.0706789    -0.184591   -0.0715493   0.540906    -0.000117903  -0.49581    -0.0847634    0.399061    0.139284    -0.0289419  -0.0111795   -0.0281208   0.248796    0.276673     0.129528    0.240174    0.044866  
 -0.0588722  -0.0900917    0.0695928   -0.373904    -0.405823     0.554537     0.243933    -0.068307    -0.176292   -0.609657      0.137414    0.354552   -0.503348     0.0153572     0.831698   -0.123806     0.299066   -0.626128    -0.597732   -0.12951      0.0126732  -0.196037   -0.202477    -0.231121   -0.155816   -0.0564757 
  0.113927   -0.0352058   -0.402336    -0.243194     0.147827     0.128098     0.162008    -0.173317     1.02264    -0.2898       -0.302395    0.0314066  -0.172152     0.245671      0.336513   -0.38969      0.148154    0.149199    -0.4787     -0.0317073   -0.284583   -0.0851453   0.0297847   -0.0646067   0.226632   -0.211338  
  0.436605   -0.495334     0.150917    -0.381967    -0.661425    -0.706175    -0.00225151  -0.197086     0.0415035  -0.000221741  -0.0634708  -0.222394   -0.562086     0.126187     -0.0837623   0.0278728   -0.249567    0.163604     0.264219   -0.425808    -0.33599    -0.359913   -0.356581     0.02014    -0.82948    -0.484545  
 -0.201582    0.0650038    0.0166826    0.157364    -0.135237    -0.144611    -0.0533747   -0.0274067   -0.128248   -0.0176675     0.0868206  -0.177518   -0.0202208    0.0151996     0.0779237   0.11208      0.0369269  -0.104898     0.115263   -0.023137     0.0682274   0.0504085   0.00349005  -0.148837   -0.165542   -0.0743523 
 -0.343916    0.259093     1.39823      0.00855744  -0.163569     0.409186     0.28436     -0.209328    -0.542363    0.206275     -0.0428393   0.394198    0.223579    -0.155954     -0.54308     0.0441879   -0.0030918  -0.0273161    0.428903   -0.30963     -0.211738    0.740543   -0.135345     0.239994   -0.191442    0.0666882 
  0.429876   -0.0594093   -0.218046    -0.236937     0.101339    -0.0237337    0.021348    -0.00352918  -0.14401    -0.0233346     0.551992    0.032804   -0.451736    -0.279551      0.110113   -0.278111    -0.200051    0.0619277   -0.082536    0.120454    -0.282384   -0.278511    0.0686412    0.0178536   0.161906   -0.00566596
 -0.495004   -0.0468384   -0.498525    -0.516685    -0.0712211    0.100935     0.0532153   -0.901714     0.402698   -0.207081      0.569448   -0.0366209  -0.221075     0.158433     -0.496077    0.601712     0.235123   -0.011371    -0.177409    0.422559     0.0640459   0.147419   -0.400725    -0.339439   -0.217863    0.803934  
 -0.297405   -0.416427     0.11172     -0.644706     0.395285     0.317535     0.32818     -0.382253     0.055411    0.00685874   -0.140822    0.292154    0.352372     0.437832      0.51333     0.123801    -0.0744673  -0.775363    -0.0557389   0.172015     0.046474   -0.191364   -0.417626     0.532449    0.0890538   0.170702  
  0.623759   -0.349183     0.0280116   -0.313451     0.109756    -0.717775    -0.105237    -0.634202     0.290242    0.216805     -0.173592    0.123674    0.265924     0.0318483    -0.288145   -0.372711    -0.548011    0.0276906    0.0322351  -0.0227779   -0.216884    0.202869    0.414728    -0.146371   -0.311699    0.37195   
 -0.29891    -0.0806539   -0.690106     0.239495    -0.00798579   0.466125     0.157694     0.185439     0.273189   -0.229716     -0.244534   -0.336605    0.295173     0.284458      0.0932382   0.556609    -0.156981    0.0829398   -0.606123    0.192278     0.0838201   0.102673    0.253415     0.0965085   0.227923    0.133204  
  0.19527     0.442235    -0.33123      0.643127    -0.397012    -0.24035     -0.411763     0.275694     0.0876976   0.352533      0.147135   -0.384698   -0.410625    -0.541297     -0.520782   -0.165316     0.107371    0.988131     0.16171     0.117956    -0.0242093   0.23672     0.222848    -0.498923    0.143834   -0.13936   
 -0.773428   -0.00296312   0.0103446   -0.373879    -0.379414     0.00476458   0.188328     0.410708     0.0759782  -0.912855     -0.127298    0.0983758   0.194149     0.155971      0.468404    0.763173    -0.113838    0.168044     0.721012   -0.108184    -0.0933385  -0.0663649   0.284404     0.151766    0.157265   -0.37094   
 -0.578736   -0.14105      0.374069     0.466727     0.15197      0.330716     0.456034     0.220482    -0.558659    0.532532      0.648987   -0.264728   -0.246554    -0.412115      0.801009    0.414637     0.413028   -0.445547     0.223294   -0.1256       0.207588    0.247647   -0.332372    -0.234324   -0.110683   -0.270116  
  0.135214   -0.0417437    0.315741    -0.0471068   -0.29648     -0.0406906   -0.109583    -0.140365    -0.730708    0.0574046     0.47971     0.0135634  -0.459714    -0.34089       0.312102   -0.395178     0.0439034  -0.487337     0.395277   -0.170104     0.0293405  -0.209265   -0.540227    -0.0168307  -0.195091    0.165449  
  0.204496   -0.109325     0.0349104   -0.126903    -0.0603667    0.229161     0.0278602    0.0162707    0.231198    0.0891895    -0.32965     0.229815    0.105001     0.0286107    -0.0968857  -0.16938     -0.0756841   0.167961    -0.0846639   0.0133239   -0.055156    0.120586   -0.0512812    0.133802    0.11952     0.0279407 
  0.624464   -0.553041     0.0123305    0.306738    -0.375559     0.184454    -0.284024    -0.13064     -0.605341    0.293917     -0.0747559   0.555126    0.731673    -0.0252545    -0.431802    0.281353     0.192459   -0.1164      -0.0284544   0.11115      0.38853     0.13479    -0.141635     0.310822   -0.194994   -0.661423  
  0.0734867   0.306601     0.108151     0.201342    -0.118813    -0.0357945    0.179169    -0.0453328   -0.347016   -0.13226       0.415514   -0.109371   -0.446091     0.77623       0.628219    0.220971    -1.37404     0.106728    -0.176435    0.253139    -0.418849    0.132523    0.420791    -0.457258   -0.130634    0.382648  
 -0.222323    0.432858    -0.582844     0.362637     0.534083    -1.07887     -0.159834    -0.374711    -0.152524   -0.179913      0.596286   -0.851842   -0.660683     0.180281      0.756845    0.19828     -0.126102   -0.00359658   0.232227    0.176323     0.375743   -0.640559    0.721081    -0.437917    0.0979718  -0.0448644 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.410197
INFO: iteration 2, average log likelihood -1.410188
INFO: iteration 3, average log likelihood -1.410179
INFO: iteration 4, average log likelihood -1.410171
INFO: iteration 5, average log likelihood -1.410163
INFO: iteration 6, average log likelihood -1.410155
INFO: iteration 7, average log likelihood -1.410148
INFO: iteration 8, average log likelihood -1.410141
INFO: iteration 9, average log likelihood -1.410134
INFO: iteration 10, average log likelihood -1.410127
INFO: EM with 100000 data points 10 iterations avll -1.410127
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
