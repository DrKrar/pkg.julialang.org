>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.7.0
INFO: Installing JLD v0.6.6
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.1
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.2.0
INFO: Installing StaticArrays v0.0.11
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated
INFO: METADATA is out-of-date â€” you may not have the latest version of GaussianMixtures
INFO: Use `Pkg.update()` to get the latest versions of your packages

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1211
Commit 1ac30c3 (2016-11-08 21:35 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-100-generic #147-Ubuntu SMP Tue Oct 18 16:48:51 UTC 2016 x86_64 x86_64
Memory: 2.939289093017578 GB (660.7578125 MB free)
Uptime: 24226.0 sec
Load Avg:  0.94580078125  0.97119140625  1.02392578125
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3500 MHz    1403716 s       6276 s     156886 s     580458 s         61 s
#2  3500 MHz     721057 s         54 s      81694 s    1550877 s          2 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.3
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.7.0
 - JLD                           0.6.6
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.1
 - Rmath                         0.1.4
 - SHA                           0.2.1
 - ScikitLearnBase               0.2.0
 - StaticArrays                  0.0.11
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-885800.0426477842,[15652.7,84347.3],
[-4363.38 -14579.5 18194.3; 3897.82 14720.4 -18031.3],

Array{Float64,2}[
[16274.0 1804.83 -4363.69; 1804.83 25156.4 -11257.5; -4363.69 -11257.5 29144.7],

[84037.4 -1106.45 4553.06; -1106.45 75297.0 10981.2; 4553.06 10981.2 71372.2]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       2.260980e+03
      1       1.370597e+03      -8.903827e+02 |        8
      2       1.178595e+03      -1.920025e+02 |        5
      3       1.098086e+03      -8.050893e+01 |        6
      4       1.052260e+03      -4.582559e+01 |        2
      5       1.045727e+03      -6.533107e+00 |        3
      6       1.007907e+03      -3.782038e+01 |        2
      7       9.851260e+02      -2.278073e+01 |        2
      8       9.682276e+02      -1.689835e+01 |        3
      9       9.533518e+02      -1.487579e+01 |        2
     10       9.522258e+02      -1.126050e+00 |        2
     11       9.495082e+02      -2.717628e+00 |        2
     12       9.468045e+02      -2.703657e+00 |        2
     13       9.404948e+02      -6.309701e+00 |        0
     14       9.404948e+02       0.000000e+00 |        0
K-means converged with 14 iterations (objv = 940.4947941199716)
INFO: K-means with 272 data points using 14 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.074098
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.676709
INFO: iteration 2, lowerbound -3.521729
INFO: iteration 3, lowerbound -3.383645
INFO: iteration 4, lowerbound -3.245749
INFO: dropping number of Gaussions to 7
INFO: iteration 5, lowerbound -3.105555
INFO: iteration 6, lowerbound -2.976590
INFO: iteration 7, lowerbound -2.888021
INFO: dropping number of Gaussions to 6
INFO: iteration 8, lowerbound -2.835820
INFO: dropping number of Gaussions to 4
INFO: iteration 9, lowerbound -2.795365
INFO: iteration 10, lowerbound -2.777609
INFO: dropping number of Gaussions to 3
INFO: iteration 11, lowerbound -2.767863
INFO: iteration 12, lowerbound -2.755925
INFO: iteration 13, lowerbound -2.744196
INFO: iteration 14, lowerbound -2.727997
INFO: iteration 15, lowerbound -2.706367
INFO: iteration 16, lowerbound -2.678669
INFO: iteration 17, lowerbound -2.644879
INFO: iteration 18, lowerbound -2.605807
INFO: iteration 19, lowerbound -2.563176
INFO: iteration 20, lowerbound -2.519425
INFO: iteration 21, lowerbound -2.477094
INFO: iteration 22, lowerbound -2.437924
INFO: iteration 23, lowerbound -2.402304
INFO: iteration 24, lowerbound -2.369751
INFO: iteration 25, lowerbound -2.340643
INFO: iteration 26, lowerbound -2.318263
INFO: iteration 27, lowerbound -2.307807
INFO: dropping number of Gaussions to 2
INFO: iteration 28, lowerbound -2.303023
INFO: iteration 29, lowerbound -2.299262
INFO: iteration 30, lowerbound -2.299257
INFO: iteration 31, lowerbound -2.299255
INFO: iteration 32, lowerbound -2.299254
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Wed 09 Nov 2016 12:13:24 PM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Wed 09 Nov 2016 12:13:26 PM UTC: K-means with 272 data points using 14 iterations
11.3 data points per parameter
,Wed 09 Nov 2016 12:13:27 PM UTC: EM with 272 data points 0 iterations avll -2.074098
5.8 data points per parameter
,Wed 09 Nov 2016 12:13:28 PM UTC: GMM converted to Variational GMM
,Wed 09 Nov 2016 12:13:30 PM UTC: iteration 1, lowerbound -3.676709
,Wed 09 Nov 2016 12:13:30 PM UTC: iteration 2, lowerbound -3.521729
,Wed 09 Nov 2016 12:13:30 PM UTC: iteration 3, lowerbound -3.383645
,Wed 09 Nov 2016 12:13:31 PM UTC: iteration 4, lowerbound -3.245749
,Wed 09 Nov 2016 12:13:31 PM UTC: dropping number of Gaussions to 7
,Wed 09 Nov 2016 12:13:31 PM UTC: iteration 5, lowerbound -3.105555
,Wed 09 Nov 2016 12:13:31 PM UTC: iteration 6, lowerbound -2.976590
,Wed 09 Nov 2016 12:13:31 PM UTC: iteration 7, lowerbound -2.888021
,Wed 09 Nov 2016 12:13:31 PM UTC: dropping number of Gaussions to 6
,Wed 09 Nov 2016 12:13:31 PM UTC: iteration 8, lowerbound -2.835820
,Wed 09 Nov 2016 12:13:31 PM UTC: dropping number of Gaussions to 4
,Wed 09 Nov 2016 12:13:31 PM UTC: iteration 9, lowerbound -2.795365
,Wed 09 Nov 2016 12:13:31 PM UTC: iteration 10, lowerbound -2.777609
,Wed 09 Nov 2016 12:13:31 PM UTC: dropping number of Gaussions to 3
,Wed 09 Nov 2016 12:13:31 PM UTC: iteration 11, lowerbound -2.767863
,Wed 09 Nov 2016 12:13:31 PM UTC: iteration 12, lowerbound -2.755925
,Wed 09 Nov 2016 12:13:31 PM UTC: iteration 13, lowerbound -2.744196
,Wed 09 Nov 2016 12:13:31 PM UTC: iteration 14, lowerbound -2.727997
,Wed 09 Nov 2016 12:13:31 PM UTC: iteration 15, lowerbound -2.706367
,Wed 09 Nov 2016 12:13:31 PM UTC: iteration 16, lowerbound -2.678669
,Wed 09 Nov 2016 12:13:31 PM UTC: iteration 17, lowerbound -2.644879
,Wed 09 Nov 2016 12:13:31 PM UTC: iteration 18, lowerbound -2.605807
,Wed 09 Nov 2016 12:13:32 PM UTC: iteration 19, lowerbound -2.563176
,Wed 09 Nov 2016 12:13:32 PM UTC: iteration 20, lowerbound -2.519425
,Wed 09 Nov 2016 12:13:32 PM UTC: iteration 21, lowerbound -2.477094
,Wed 09 Nov 2016 12:13:32 PM UTC: iteration 22, lowerbound -2.437924
,Wed 09 Nov 2016 12:13:32 PM UTC: iteration 23, lowerbound -2.402304
,Wed 09 Nov 2016 12:13:32 PM UTC: iteration 24, lowerbound -2.369751
,Wed 09 Nov 2016 12:13:32 PM UTC: iteration 25, lowerbound -2.340643
,Wed 09 Nov 2016 12:13:32 PM UTC: iteration 26, lowerbound -2.318263
,Wed 09 Nov 2016 12:13:32 PM UTC: iteration 27, lowerbound -2.307807
,Wed 09 Nov 2016 12:13:32 PM UTC: dropping number of Gaussions to 2
,Wed 09 Nov 2016 12:13:32 PM UTC: iteration 28, lowerbound -2.303023
,Wed 09 Nov 2016 12:13:32 PM UTC: iteration 29, lowerbound -2.299262
,Wed 09 Nov 2016 12:13:32 PM UTC: iteration 30, lowerbound -2.299257
,Wed 09 Nov 2016 12:13:32 PM UTC: iteration 31, lowerbound -2.299255
,Wed 09 Nov 2016 12:13:32 PM UTC: iteration 32, lowerbound -2.299254
,Wed 09 Nov 2016 12:13:33 PM UTC: iteration 33, lowerbound -2.299253
,Wed 09 Nov 2016 12:13:33 PM UTC: iteration 34, lowerbound -2.299253
,Wed 09 Nov 2016 12:13:33 PM UTC: iteration 35, lowerbound -2.299253
,Wed 09 Nov 2016 12:13:33 PM UTC: iteration 36, lowerbound -2.299253
,Wed 09 Nov 2016 12:13:33 PM UTC: iteration 37, lowerbound -2.299253
,Wed 09 Nov 2016 12:13:33 PM UTC: iteration 38, lowerbound -2.299253
,Wed 09 Nov 2016 12:13:33 PM UTC: iteration 39, lowerbound -2.299253
,Wed 09 Nov 2016 12:13:33 PM UTC: iteration 40, lowerbound -2.299253
,Wed 09 Nov 2016 12:13:33 PM UTC: iteration 41, lowerbound -2.299253
,Wed 09 Nov 2016 12:13:33 PM UTC: iteration 42, lowerbound -2.299253
,Wed 09 Nov 2016 12:13:33 PM UTC: iteration 43, lowerbound -2.299253
,Wed 09 Nov 2016 12:13:33 PM UTC: iteration 44, lowerbound -2.299253
,Wed 09 Nov 2016 12:13:33 PM UTC: iteration 45, lowerbound -2.299253
,Wed 09 Nov 2016 12:13:34 PM UTC: iteration 46, lowerbound -2.299253
,Wed 09 Nov 2016 12:13:34 PM UTC: iteration 47, lowerbound -2.299253
,Wed 09 Nov 2016 12:13:34 PM UTC: iteration 48, lowerbound -2.299253
,Wed 09 Nov 2016 12:13:34 PM UTC: iteration 49, lowerbound -2.299253
,Wed 09 Nov 2016 12:13:34 PM UTC: iteration 50, lowerbound -2.299253
,Wed 09 Nov 2016 12:13:34 PM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
Î± = [178.045,95.9549]
Î² = [178.045,95.9549]
m = [4.2503 79.2869; 2.00023 53.852]
Î½ = [180.045,97.9549]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.184042 -0.00764405; 0.0 0.00858171],

[0.375876 -0.00895312; 0.0 0.0127487]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.00000000003
avll from stats: -0.9700966727596919
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.970096672759694
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9700966727596942
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -1.0176816325342655
avll from llpg:  -1.0176816325342655
avll direct:     -1.0176816325342655
sum posterior: 100000.0
32Ã—26 Array{Float64,2}:
 -0.155218    0.0122387    0.0542678     0.0333305   -0.0972194   -0.234439     0.0321145    -0.0195745   -0.103704    -0.134388     0.236136      0.0743575    0.101327    0.0477882     0.0344506    0.0657864    0.0368257    0.243464   -0.0742803   -0.0111718     0.0170649   0.0615422   -0.0250404     0.12527      0.161055     0.257342  
  0.0773743   0.225595    -0.0746882    -0.0344809   -0.161622    -0.0784154    0.0234538    -0.0343855   -0.0303993   -0.0661884   -0.000603211  -0.0357007    0.17563    -0.219666      0.14891     -0.197512    -0.0521847    0.0575712  -0.0396947    0.100086      0.127173    0.123363     0.0592227     0.0126941    0.0663347    0.152904  
 -0.0509917  -0.108071    -0.126121      0.077707     0.137634     0.0326962   -0.0421906    -0.0940678    0.0325813   -0.00594264  -0.032567      0.0729354   -0.0780378   0.129788     -0.0596942   -0.0997438    0.145994     0.141587    0.0293958    0.0233294     0.0456936   0.200998    -0.0959503    -0.0078029    0.0696462   -0.0943109 
 -0.0242596   0.14544     -0.109584     -0.0594017   -0.0588695    0.147527     0.0424888     0.0697393   -0.00660657   0.0663222   -0.0292485    -0.137437     0.0310748  -0.096004      0.0376263   -0.0514197    0.208446     0.156566   -0.0567138    0.105327     -0.047634   -0.0332799    0.0293354     0.0514116   -0.00587608   0.123398  
 -0.20076     0.11617      0.000674462   0.0318108    0.11949      0.110038    -0.0900727    -0.19298      0.194215    -0.16227     -0.0191266     0.202459    -0.0574801  -0.0271388    -0.00771221  -0.139527     0.0799044    0.0190622   0.0319317   -0.100268      0.153127    0.0302031    0.0430538    -0.181704     0.0215472    0.00443381
 -0.111171   -0.0172532   -0.032249     -0.1165      -0.0387127   -0.0858755   -0.0529788     0.00948512  -0.0548116   -0.0571753   -0.0717246    -0.0406847   -0.0211687  -0.0950226     0.104683    -0.101855    -0.110999    -0.176042    0.0721672   -0.0206144    -0.214543   -0.0156135   -0.180708     -0.0819647    0.0734192   -0.173642  
 -0.0668252   0.128309    -0.0454847     0.0969062   -0.119542    -0.0916296    0.0495618    -0.172375    -0.0561575   -0.0754617    0.0650464    -0.101932    -0.0388063   0.0183798    -0.0337168   -0.00942273  -0.0699711   -0.136743    0.0689396   -0.0191214     0.130364   -0.0449956    0.0669489    -0.0557096   -0.0969006   -0.0294025 
 -0.110202    0.102912    -0.0231071     0.164031     0.0622481    0.131698     0.0411358    -0.0448354    0.0121487   -0.0139617   -0.0463125     0.10462     -0.0690164   0.0593571     0.0185276    0.214279     0.0389632   -0.0499814  -0.0217765    0.0305307    -0.0363825  -0.0273998   -0.0368572     0.12397     -0.0582016    0.181315  
  0.133633    0.102333    -0.150177      0.0162102   -0.156585     0.0371925    0.141197     -0.00433107   0.0256678    0.00906305  -0.310307      0.00822506   0.0716669  -0.112769     -0.0938048   -0.140675     0.00758272  -0.171497    0.210269     0.0743931     0.0986196  -0.0459032    0.167412     -0.141449     0.0771814   -0.186123  
 -0.0115656  -0.016556     0.0104912    -0.0557612    0.100475     0.0929025    0.157698     -0.118234     0.0874598   -0.0570479   -0.0670808     0.138232     0.136969   -0.00327715    0.00480189   0.109676     0.031977     0.0629829  -0.0188143   -0.068442      0.043226    0.0260617   -0.0736477    -0.148806    -0.0931611   -0.0411942 
 -0.0468976  -0.0822163    0.030609      0.065343     0.178584    -0.0629981   -0.122382      0.022755     0.0689088   -0.0728009   -0.0131259     0.095899    -0.0961136   0.000780658   0.0213517    0.16374     -0.0100333    0.179665   -0.0954355    0.121319      0.0316689  -0.0255874    0.0323723     0.168819    -0.0188838   -0.0496469 
 -0.111036    0.0203618   -0.0660632    -0.0930692   -0.101807    -0.0272244    0.0743519    -0.0474166    0.147592    -0.123324    -0.04837      -0.1805       0.0302834   0.0194284     0.17062      0.0677725    0.144753    -0.101417   -0.239028    -0.0143415     0.124961   -0.00238937  -0.093401      0.0188892    0.0543886    0.116873  
  0.0105191  -0.111865     0.00543469   -0.0288828   -0.0624201   -0.0539666   -0.125324     -0.115874     0.0891284    0.17925     -0.109637     -0.131966     0.030521   -0.11969       0.070897    -0.0577601    0.0342841   -0.0127956   0.00290364  -0.195348     -0.0405775   0.30351      0.0787733    -0.161275    -0.0535242   -0.190806  
  0.202241   -0.116309    -0.0999322    -0.0786643    0.0366103    0.108391    -0.0625572    -0.0648312    0.0317374    0.0550761   -0.117262      0.158453    -0.110987    0.00672635   -0.0367247    0.0379376    0.0576815    0.0078972  -0.148222    -0.0189689     0.234309   -0.0260499   -0.168916      0.100015    -0.0902333    0.0704211 
 -0.0405468   0.0951444    0.0230946     0.0614785    0.0654983   -0.0841757   -0.00238595   -0.205281    -0.00814912  -0.148096    -0.115733     -0.018709    -0.106628    0.0407039     0.0927677   -0.021753    -0.0448113   -0.118288   -0.0603653   -0.0555209     0.111436    0.0163532    0.0589484    -0.0665876    0.0913309    0.0731694 
  0.0256413   0.165605     0.0566087     0.0904316    0.0386853    0.0383322   -0.0249522    -0.0677886   -0.129287    -0.0111709    0.0347154    -0.0105506   -0.189695   -0.175508      0.0492481    0.0893929    0.0925206   -0.0610906  -0.115785    -0.0117695     0.0416969   0.0624036    0.104434     -0.249758    -0.0320821    0.040513  
 -0.0710432  -0.0840583   -0.00965669   -0.152222     0.0410638    0.0697145    0.00795858    0.0419961    0.246804    -0.0116176    0.160999      0.112948    -0.0724251  -0.00133595   -0.0554631   -0.115736    -0.0071975    0.0752531  -0.0528589    0.0280546     0.227798    0.162389    -0.112897     -0.138633    -0.0371118    0.0715093 
 -0.0868788   0.024286     0.248902      0.154159    -0.0234397    0.0250582    0.025211     -0.0479692   -0.0155602    0.157564     0.111308     -0.0379388    0.0598289  -0.137663      0.0287419    0.169625    -0.0901486    0.0728948   0.137476     0.0814057    -0.0765707   0.152034    -0.0325678    -0.065415     0.0726323    0.143211  
 -0.113471   -0.100245     0.0583956     0.0590581   -0.268913    -0.147472    -0.0988755    -0.183895    -0.0410728    0.0132601   -0.0685028     0.0961657    0.12183    -0.126496     -0.0835472    0.180529    -0.194406    -0.110639    0.151561    -0.0199779     0.272525   -0.0621144    0.0822836     0.029589     0.0463742    0.0150818 
  0.1881      0.0856965    0.0973647     0.0161056    0.0559082    0.121762    -0.0976153    -0.0914009    0.0985808    0.0630854   -0.130942      0.184444     0.012671    0.0468337    -0.145979    -0.041999    -0.017937     0.129149   -0.024747     0.150678      0.0555562   0.121934     0.00287532   -0.0447098   -0.215365    -0.0850017 
  0.0395586  -0.00459296  -0.0508365    -0.215623    -0.103775     0.00306111   0.111955      0.0825023   -0.0999416    0.150621    -0.201739     -0.00221603   0.284964    0.0943939    -0.113186    -0.0990273   -0.139307    -0.225365    0.135861     0.0187402     0.0815106   0.124446     0.0700054    -0.167844     0.131897     0.00289073
  0.145772    0.148746     0.00688235   -0.0723556    0.028561     0.197699    -0.151639     -0.0685444   -0.0110836   -0.107192     0.0027083    -0.0476199   -0.0237919  -0.00782765   -0.197862     0.0413321   -0.00369496  -0.261792    0.0455951    0.000640952   0.070525   -0.0151535    0.080872      0.0942755    0.113819     0.212633  
 -0.0405839   0.0235162    0.111639     -0.0134741    0.183257     0.13364      0.140688     -0.0746611    0.094805    -0.071538     0.0463924    -0.118801     0.1171     -0.0410952     0.192646    -0.0849331   -0.0940383   -0.0127751   0.0418214   -0.088418      0.0773258   0.0121229   -0.0225181    -0.0641732   -0.0467323   -0.0579618 
  0.104758   -0.132254     0.0753348     0.0246263   -0.0657414    0.0865219    0.140078      0.0401423   -0.0810749   -0.0133881   -0.0379622    -0.0112479    0.204169    0.00980053    0.0232011    0.0670173    0.277972    -0.0191928   0.0740196   -0.111911      0.0462176   0.141231     0.072819      0.0182436   -0.166471    -0.0542957 
 -0.110496   -0.0254968   -0.0460691     0.0409739    0.14515     -0.186039     0.113721     -0.0531437    0.00503858  -0.0280436    0.0238       -0.0985376    0.0054205   0.0262442     0.141744    -0.0606921   -0.0458855    0.0393701  -0.0756965   -0.0877542     0.0462305  -0.104507     0.0371632    -0.0950019    0.00487819  -0.0159301 
  0.0276224  -0.0067443    0.0462709    -0.0704796    0.0797639    0.0556077    0.000131851  -0.166984    -0.148703     0.0583237   -0.0494924     0.0678362    0.0636595   0.117031     -0.00826265   0.191155     0.057316     0.155076    0.0480071   -0.01106       0.15024     0.0074882   -0.000706763   0.0179915   -0.0562497    0.0209405 
 -0.0105258  -0.154031     0.0957485    -0.0859134   -0.148398    -0.121097    -0.018763      0.0578609   -0.0714112    0.10648      0.0718561    -0.154426    -0.114781   -0.068069     -0.0918496   -0.259733     0.00385012   0.0866311  -0.152598     0.0580935     0.0165338  -0.0942561   -0.0252244     0.151616    -0.100935    -0.0522222 
 -0.0296587  -0.191442     0.0538936    -0.0271398   -0.0405711    0.0335467   -0.0142058    -0.258601    -0.0673064    0.0599704   -0.117939      0.0145245    0.0210778  -0.0202329    -0.0455997   -0.0807131   -0.0507635    0.116584   -0.0339051   -0.052552      0.0859085   0.0409548    0.0541413     0.142947     0.201912     0.0284856 
 -0.212498    0.118164    -0.0484583    -0.00813158  -0.00290936  -0.0881284    0.0473424    -0.0915969   -0.0186391    0.0435009   -0.0207633     0.0659973   -0.176014   -0.0148516     0.00661653   0.19323     -0.0627804   -0.111827   -0.0471875   -0.120109     -0.0997693  -0.0685407   -0.101182      0.0480702   -0.10412      0.106987  
  0.245675   -0.164245     0.0637124     0.0146914    0.0263255   -0.0312078   -0.0504261     0.00592258  -0.00178599  -0.124991     0.110292     -0.0175866   -0.0952464   0.0625089    -0.10226      0.0168674    0.192323     0.0042527   0.111307     0.0969732    -0.0209399   0.131759     0.0220125    -0.0431765    0.0842906   -0.0756763 
  0.0687451  -0.0743556   -0.111244      0.27514     -0.0723428    0.0622882    0.031635     -0.168902     0.14915     -0.117069    -0.156262     -0.0742396    0.0909665   0.0787363     0.25134     -0.0707892   -0.0739859   -0.108503   -0.0522695   -0.132443     -0.0308463   0.110493     0.11415       0.00250013  -0.0297739   -0.107935  
 -0.137227   -0.0095735   -0.137186      0.030251    -0.0384803    0.0378719    0.0778386     0.0335131   -0.0524206    0.0126534   -0.146544     -0.120089     0.138515   -0.0664384    -0.096236     0.129459    -0.0492591   -0.0803299   0.0295525    0.107618     -0.182811   -0.0307311   -0.0507953     0.0744308   -0.0970498   -0.101544  kind diag, method split
0: avll = -1.487289009449442
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.487389
INFO: iteration 2, average log likelihood -1.487296
INFO: iteration 3, average log likelihood -1.486626
INFO: iteration 4, average log likelihood -1.481586
INFO: iteration 5, average log likelihood -1.468045
INFO: iteration 6, average log likelihood -1.458177
INFO: iteration 7, average log likelihood -1.455595
INFO: iteration 8, average log likelihood -1.454451
INFO: iteration 9, average log likelihood -1.453457
INFO: iteration 10, average log likelihood -1.452372
INFO: iteration 11, average log likelihood -1.451360
INFO: iteration 12, average log likelihood -1.450517
INFO: iteration 13, average log likelihood -1.449833
INFO: iteration 14, average log likelihood -1.449291
INFO: iteration 15, average log likelihood -1.448878
INFO: iteration 16, average log likelihood -1.448570
INFO: iteration 17, average log likelihood -1.448338
INFO: iteration 18, average log likelihood -1.448165
INFO: iteration 19, average log likelihood -1.448031
INFO: iteration 20, average log likelihood -1.447918
INFO: iteration 21, average log likelihood -1.447809
INFO: iteration 22, average log likelihood -1.447699
INFO: iteration 23, average log likelihood -1.447597
INFO: iteration 24, average log likelihood -1.447500
INFO: iteration 25, average log likelihood -1.447408
INFO: iteration 26, average log likelihood -1.447319
INFO: iteration 27, average log likelihood -1.447233
INFO: iteration 28, average log likelihood -1.447151
INFO: iteration 29, average log likelihood -1.447074
INFO: iteration 30, average log likelihood -1.447001
INFO: iteration 31, average log likelihood -1.446933
INFO: iteration 32, average log likelihood -1.446875
INFO: iteration 33, average log likelihood -1.446828
INFO: iteration 34, average log likelihood -1.446793
INFO: iteration 35, average log likelihood -1.446767
INFO: iteration 36, average log likelihood -1.446748
INFO: iteration 37, average log likelihood -1.446734
INFO: iteration 38, average log likelihood -1.446723
INFO: iteration 39, average log likelihood -1.446716
INFO: iteration 40, average log likelihood -1.446710
INFO: iteration 41, average log likelihood -1.446706
INFO: iteration 42, average log likelihood -1.446703
INFO: iteration 43, average log likelihood -1.446701
INFO: iteration 44, average log likelihood -1.446699
INFO: iteration 45, average log likelihood -1.446698
INFO: iteration 46, average log likelihood -1.446697
INFO: iteration 47, average log likelihood -1.446697
INFO: iteration 48, average log likelihood -1.446696
INFO: iteration 49, average log likelihood -1.446696
INFO: iteration 50, average log likelihood -1.446695
INFO: EM with 100000 data points 50 iterations avll -1.446695
952.4 data points per parameter
1: avll = [-1.48739,-1.4873,-1.48663,-1.48159,-1.46805,-1.45818,-1.4556,-1.45445,-1.45346,-1.45237,-1.45136,-1.45052,-1.44983,-1.44929,-1.44888,-1.44857,-1.44834,-1.44816,-1.44803,-1.44792,-1.44781,-1.4477,-1.4476,-1.4475,-1.44741,-1.44732,-1.44723,-1.44715,-1.44707,-1.447,-1.44693,-1.44687,-1.44683,-1.44679,-1.44677,-1.44675,-1.44673,-1.44672,-1.44672,-1.44671,-1.44671,-1.4467,-1.4467,-1.4467,-1.4467,-1.4467,-1.4467,-1.4467,-1.4467,-1.4467]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.446896
INFO: iteration 2, average log likelihood -1.446715
INFO: iteration 3, average log likelihood -1.446191
INFO: iteration 4, average log likelihood -1.441686
INFO: iteration 5, average log likelihood -1.427511
INFO: iteration 6, average log likelihood -1.414144
INFO: iteration 7, average log likelihood -1.407322
INFO: iteration 8, average log likelihood -1.403792
INFO: iteration 9, average log likelihood -1.401913
INFO: iteration 10, average log likelihood -1.400974
INFO: iteration 11, average log likelihood -1.400457
INFO: iteration 12, average log likelihood -1.400132
INFO: iteration 13, average log likelihood -1.399897
INFO: iteration 14, average log likelihood -1.399705
INFO: iteration 15, average log likelihood -1.399534
INFO: iteration 16, average log likelihood -1.399373
INFO: iteration 17, average log likelihood -1.399219
INFO: iteration 18, average log likelihood -1.399072
INFO: iteration 19, average log likelihood -1.398930
INFO: iteration 20, average log likelihood -1.398794
INFO: iteration 21, average log likelihood -1.398664
INFO: iteration 22, average log likelihood -1.398537
INFO: iteration 23, average log likelihood -1.398410
INFO: iteration 24, average log likelihood -1.398280
INFO: iteration 25, average log likelihood -1.398148
INFO: iteration 26, average log likelihood -1.398001
INFO: iteration 27, average log likelihood -1.397842
INFO: iteration 28, average log likelihood -1.397673
INFO: iteration 29, average log likelihood -1.397488
INFO: iteration 30, average log likelihood -1.397288
INFO: iteration 31, average log likelihood -1.397116
INFO: iteration 32, average log likelihood -1.396996
INFO: iteration 33, average log likelihood -1.396920
INFO: iteration 34, average log likelihood -1.396876
INFO: iteration 35, average log likelihood -1.396851
INFO: iteration 36, average log likelihood -1.396836
INFO: iteration 37, average log likelihood -1.396826
INFO: iteration 38, average log likelihood -1.396818
INFO: iteration 39, average log likelihood -1.396813
INFO: iteration 40, average log likelihood -1.396808
INFO: iteration 41, average log likelihood -1.396805
INFO: iteration 42, average log likelihood -1.396802
INFO: iteration 43, average log likelihood -1.396799
INFO: iteration 44, average log likelihood -1.396797
INFO: iteration 45, average log likelihood -1.396796
INFO: iteration 46, average log likelihood -1.396794
INFO: iteration 47, average log likelihood -1.396793
INFO: iteration 48, average log likelihood -1.396791
INFO: iteration 49, average log likelihood -1.396790
INFO: iteration 50, average log likelihood -1.396789
INFO: EM with 100000 data points 50 iterations avll -1.396789
473.9 data points per parameter
2: avll = [-1.4469,-1.44672,-1.44619,-1.44169,-1.42751,-1.41414,-1.40732,-1.40379,-1.40191,-1.40097,-1.40046,-1.40013,-1.3999,-1.3997,-1.39953,-1.39937,-1.39922,-1.39907,-1.39893,-1.39879,-1.39866,-1.39854,-1.39841,-1.39828,-1.39815,-1.398,-1.39784,-1.39767,-1.39749,-1.39729,-1.39712,-1.397,-1.39692,-1.39688,-1.39685,-1.39684,-1.39683,-1.39682,-1.39681,-1.39681,-1.3968,-1.3968,-1.3968,-1.3968,-1.3968,-1.39679,-1.39679,-1.39679,-1.39679,-1.39679]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.397027
INFO: iteration 2, average log likelihood -1.396817
INFO: iteration 3, average log likelihood -1.396347
INFO: iteration 4, average log likelihood -1.392021
INFO: iteration 5, average log likelihood -1.375302
INFO: iteration 6, average log likelihood -1.356663
INFO: iteration 7, average log likelihood -1.345538
INFO: iteration 8, average log likelihood -1.337741
INFO: iteration 9, average log likelihood -1.331707
WARNING: Variances had to be floored 3
INFO: iteration 10, average log likelihood -1.327163
INFO: iteration 11, average log likelihood -1.334991
INFO: iteration 12, average log likelihood -1.328891
INFO: iteration 13, average log likelihood -1.325905
INFO: iteration 14, average log likelihood -1.323260
INFO: iteration 15, average log likelihood -1.321045
INFO: iteration 16, average log likelihood -1.320195
INFO: iteration 17, average log likelihood -1.319830
INFO: iteration 18, average log likelihood -1.319541
INFO: iteration 19, average log likelihood -1.319210
INFO: iteration 20, average log likelihood -1.318815
INFO: iteration 21, average log likelihood -1.318368
INFO: iteration 22, average log likelihood -1.317935
INFO: iteration 23, average log likelihood -1.317580
INFO: iteration 24, average log likelihood -1.317346
INFO: iteration 25, average log likelihood -1.317213
INFO: iteration 26, average log likelihood -1.317141
INFO: iteration 27, average log likelihood -1.317101
INFO: iteration 28, average log likelihood -1.317076
INFO: iteration 29, average log likelihood -1.317060
INFO: iteration 30, average log likelihood -1.317048
INFO: iteration 31, average log likelihood -1.317039
INFO: iteration 32, average log likelihood -1.317032
INFO: iteration 33, average log likelihood -1.317027
INFO: iteration 34, average log likelihood -1.317022
INFO: iteration 35, average log likelihood -1.317018
INFO: iteration 36, average log likelihood -1.317015
INFO: iteration 37, average log likelihood -1.317012
INFO: iteration 38, average log likelihood -1.317009
INFO: iteration 39, average log likelihood -1.317007
INFO: iteration 40, average log likelihood -1.317005
INFO: iteration 41, average log likelihood -1.317003
INFO: iteration 42, average log likelihood -1.317001
INFO: iteration 43, average log likelihood -1.316999
INFO: iteration 44, average log likelihood -1.316998
INFO: iteration 45, average log likelihood -1.316997
INFO: iteration 46, average log likelihood -1.316996
INFO: iteration 47, average log likelihood -1.316995
INFO: iteration 48, average log likelihood -1.316994
INFO: iteration 49, average log likelihood -1.316993
INFO: iteration 50, average log likelihood -1.316992
INFO: EM with 100000 data points 50 iterations avll -1.316992
236.4 data points per parameter
3: avll = [-1.39703,-1.39682,-1.39635,-1.39202,-1.3753,-1.35666,-1.34554,-1.33774,-1.33171,-1.32716,-1.33499,-1.32889,-1.3259,-1.32326,-1.32105,-1.3202,-1.31983,-1.31954,-1.31921,-1.31881,-1.31837,-1.31793,-1.31758,-1.31735,-1.31721,-1.31714,-1.3171,-1.31708,-1.31706,-1.31705,-1.31704,-1.31703,-1.31703,-1.31702,-1.31702,-1.31701,-1.31701,-1.31701,-1.31701,-1.317,-1.317,-1.317,-1.317,-1.317,-1.317,-1.317,-1.31699,-1.31699,-1.31699,-1.31699]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.317298
INFO: iteration 2, average log likelihood -1.316939
INFO: iteration 3, average log likelihood -1.314531
INFO: iteration 4, average log likelihood -1.294063
INFO: iteration 5, average log likelihood -1.261342
INFO: iteration 6, average log likelihood -1.238361
WARNING: Variances had to be floored 5 11 12
INFO: iteration 7, average log likelihood -1.224588
INFO: iteration 8, average log likelihood -1.240151
WARNING: Variances had to be floored 5
INFO: iteration 9, average log likelihood -1.227628
INFO: iteration 10, average log likelihood -1.226811
WARNING: Variances had to be floored 11
INFO: iteration 11, average log likelihood -1.217645
WARNING: Variances had to be floored 5 12
INFO: iteration 12, average log likelihood -1.218194
INFO: iteration 13, average log likelihood -1.228410
WARNING: Variances had to be floored 5
INFO: iteration 14, average log likelihood -1.216810
WARNING: Variances had to be floored 11
INFO: iteration 15, average log likelihood -1.219163
WARNING: Variances had to be floored 5 12
INFO: iteration 16, average log likelihood -1.217595
INFO: iteration 17, average log likelihood -1.226334
WARNING: Variances had to be floored 5
INFO: iteration 18, average log likelihood -1.216140
WARNING: Variances had to be floored 11
INFO: iteration 19, average log likelihood -1.219380
WARNING: Variances had to be floored 5
INFO: iteration 20, average log likelihood -1.217843
WARNING: Variances had to be floored 12
INFO: iteration 21, average log likelihood -1.219303
WARNING: Variances had to be floored 5
INFO: iteration 22, average log likelihood -1.219762
INFO: iteration 23, average log likelihood -1.220947
WARNING: Variances had to be floored 5 11
INFO: iteration 24, average log likelihood -1.213250
INFO: iteration 25, average log likelihood -1.224005
WARNING: Variances had to be floored 5
INFO: iteration 26, average log likelihood -1.214429
WARNING: Variances had to be floored 12
INFO: iteration 27, average log likelihood -1.218364
WARNING: Variances had to be floored 5
INFO: iteration 28, average log likelihood -1.218737
INFO: iteration 29, average log likelihood -1.219888
WARNING: Variances had to be floored 5 11
INFO: iteration 30, average log likelihood -1.212654
INFO: iteration 31, average log likelihood -1.224135
WARNING: Variances had to be floored 5
INFO: iteration 32, average log likelihood -1.214903
WARNING: Variances had to be floored 12
INFO: iteration 33, average log likelihood -1.218931
WARNING: Variances had to be floored 5
INFO: iteration 34, average log likelihood -1.218196
WARNING: Variances had to be floored 11
INFO: iteration 35, average log likelihood -1.219365
WARNING: Variances had to be floored 5
INFO: iteration 36, average log likelihood -1.218800
INFO: iteration 37, average log likelihood -1.220852
WARNING: Variances had to be floored 5
INFO: iteration 38, average log likelihood -1.213460
WARNING: Variances had to be floored 12
INFO: iteration 39, average log likelihood -1.217070
WARNING: Variances had to be floored 5 11
INFO: iteration 40, average log likelihood -1.216562
INFO: iteration 41, average log likelihood -1.225304
WARNING: Variances had to be floored 5
INFO: iteration 42, average log likelihood -1.215404
INFO: iteration 43, average log likelihood -1.219171
WARNING: Variances had to be floored 5 12
INFO: iteration 44, average log likelihood -1.211226
WARNING: Variances had to be floored 11
INFO: iteration 45, average log likelihood -1.221792
WARNING: Variances had to be floored 5
INFO: iteration 46, average log likelihood -1.219595
INFO: iteration 47, average log likelihood -1.220958
WARNING: Variances had to be floored 5
INFO: iteration 48, average log likelihood -1.213128
WARNING: Variances had to be floored 12
INFO: iteration 49, average log likelihood -1.216418
WARNING: Variances had to be floored 5 11
INFO: iteration 50, average log likelihood -1.216018
INFO: EM with 100000 data points 50 iterations avll -1.216018
118.1 data points per parameter
4: avll = [-1.3173,-1.31694,-1.31453,-1.29406,-1.26134,-1.23836,-1.22459,-1.24015,-1.22763,-1.22681,-1.21765,-1.21819,-1.22841,-1.21681,-1.21916,-1.21759,-1.22633,-1.21614,-1.21938,-1.21784,-1.2193,-1.21976,-1.22095,-1.21325,-1.224,-1.21443,-1.21836,-1.21874,-1.21989,-1.21265,-1.22414,-1.2149,-1.21893,-1.2182,-1.21936,-1.2188,-1.22085,-1.21346,-1.21707,-1.21656,-1.2253,-1.2154,-1.21917,-1.21123,-1.22179,-1.21959,-1.22096,-1.21313,-1.21642,-1.21602]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.225632
WARNING: Variances had to be floored 9 10
INFO: iteration 2, average log likelihood -1.214928
INFO: iteration 3, average log likelihood -1.215717
WARNING: Variances had to be floored 9 10 22 23 24
INFO: iteration 4, average log likelihood -1.181012
WARNING: Variances had to be floored 15 16 21
INFO: iteration 5, average log likelihood -1.140743
WARNING: Variances had to be floored 4 8 9 10 29 30
INFO: iteration 6, average log likelihood -1.124402
WARNING: Variances had to be floored 16 23 24
INFO: iteration 7, average log likelihood -1.153861
WARNING: Variances had to be floored 9 10 15 21
INFO: iteration 8, average log likelihood -1.134556
WARNING: Variances had to be floored 4 8 16 23 24
INFO: iteration 9, average log likelihood -1.118183
WARNING: Variances had to be floored 7 9 10 15 21 22 29 30
INFO: iteration 10, average log likelihood -1.123137
WARNING: Variances had to be floored 8 16 19 23 24
INFO: iteration 11, average log likelihood -1.148252
WARNING: Variances had to be floored 4 9 10 15
INFO: iteration 12, average log likelihood -1.144333
WARNING: Variances had to be floored 16 21 23
INFO: iteration 13, average log likelihood -1.132007
WARNING: Variances had to be floored 7 8 9 10 15 22 24 29 30
INFO: iteration 14, average log likelihood -1.110520
WARNING: Variances had to be floored 4 16 21
INFO: iteration 15, average log likelihood -1.144209
WARNING: Variances had to be floored 9 10 15 24
INFO: iteration 16, average log likelihood -1.145453
WARNING: Variances had to be floored 16 21 23
INFO: iteration 17, average log likelihood -1.118667
WARNING: Variances had to be floored 4 7 8 9 10 15 22 29 30
INFO: iteration 18, average log likelihood -1.099386
WARNING: Variances had to be floored 16 21 24 28
INFO: iteration 19, average log likelihood -1.144416
WARNING: Variances had to be floored 9 10 15 23
INFO: iteration 20, average log likelihood -1.149163
WARNING: Variances had to be floored 4 7 16 21
INFO: iteration 21, average log likelihood -1.116227
WARNING: Variances had to be floored 8 9 10 15 22 24 29 30
INFO: iteration 22, average log likelihood -1.112966
WARNING: Variances had to be floored 7 16 21 23 28
INFO: iteration 23, average log likelihood -1.137125
WARNING: Variances had to be floored 4 9 10 15
INFO: iteration 24, average log likelihood -1.144043
WARNING: Variances had to be floored 7 16 21 24
INFO: iteration 25, average log likelihood -1.124691
WARNING: Variances had to be floored 8 9 10 15 22 29 30
INFO: iteration 26, average log likelihood -1.110529
WARNING: Variances had to be floored 4 7 16 21 24 28
INFO: iteration 27, average log likelihood -1.127863
WARNING: Variances had to be floored 9 10 15
INFO: iteration 28, average log likelihood -1.156452
WARNING: Variances had to be floored 7 16 21 24
INFO: iteration 29, average log likelihood -1.117166
WARNING: Variances had to be floored 4 8 9 10 15 22 29 30
INFO: iteration 30, average log likelihood -1.100522
WARNING: Variances had to be floored 3 7 16 21 24 28
INFO: iteration 31, average log likelihood -1.133163
WARNING: Variances had to be floored 9 10 15
INFO: iteration 32, average log likelihood -1.148624
WARNING: Variances had to be floored 3 4 7 8 16 21 24
INFO: iteration 33, average log likelihood -1.103592
WARNING: Variances had to be floored 9 10 15 22 29 30
INFO: iteration 34, average log likelihood -1.123708
WARNING: Variances had to be floored 3 16 21 24 28
INFO: iteration 35, average log likelihood -1.133088
WARNING: Variances had to be floored 4 7 8 9 10 15
INFO: iteration 36, average log likelihood -1.135831
WARNING: Variances had to be floored 3 16 21 24
INFO: iteration 37, average log likelihood -1.126883
WARNING: Variances had to be floored 9 10 15 22 29 30
INFO: iteration 38, average log likelihood -1.110039
WARNING: Variances had to be floored 3 4 7 8 16 21 24 28
INFO: iteration 39, average log likelihood -1.110199
WARNING: Variances had to be floored 9 10 15
INFO: iteration 40, average log likelihood -1.157182
WARNING: Variances had to be floored 3 16 21 24
INFO: iteration 41, average log likelihood -1.121768
WARNING: Variances had to be floored 4 7 8 9 10 15 22 29 30
INFO: iteration 42, average log likelihood -1.099816
WARNING: Variances had to be floored 3 16 21 24 28
INFO: iteration 43, average log likelihood -1.136427
WARNING: Variances had to be floored 9 10 15
INFO: iteration 44, average log likelihood -1.147730
WARNING: Variances had to be floored 3 4 7 8 16 21 24
INFO: iteration 45, average log likelihood -1.102918
WARNING: Variances had to be floored 9 10 15 22 29 30
INFO: iteration 46, average log likelihood -1.123631
WARNING: Variances had to be floored 3 16 21 24 28
INFO: iteration 47, average log likelihood -1.133040
WARNING: Variances had to be floored 4 7 8 9 10 15
INFO: iteration 48, average log likelihood -1.135802
WARNING: Variances had to be floored 3 16 21 24
INFO: iteration 49, average log likelihood -1.126871
WARNING: Variances had to be floored 9 10 15 22 29 30
INFO: iteration 50, average log likelihood -1.110023
INFO: EM with 100000 data points 50 iterations avll -1.110023
59.0 data points per parameter
5: avll = [-1.22563,-1.21493,-1.21572,-1.18101,-1.14074,-1.1244,-1.15386,-1.13456,-1.11818,-1.12314,-1.14825,-1.14433,-1.13201,-1.11052,-1.14421,-1.14545,-1.11867,-1.09939,-1.14442,-1.14916,-1.11623,-1.11297,-1.13712,-1.14404,-1.12469,-1.11053,-1.12786,-1.15645,-1.11717,-1.10052,-1.13316,-1.14862,-1.10359,-1.12371,-1.13309,-1.13583,-1.12688,-1.11004,-1.1102,-1.15718,-1.12177,-1.09982,-1.13643,-1.14773,-1.10292,-1.12363,-1.13304,-1.1358,-1.12687,-1.11002]
[-1.48729,-1.48739,-1.4873,-1.48663,-1.48159,-1.46805,-1.45818,-1.4556,-1.45445,-1.45346,-1.45237,-1.45136,-1.45052,-1.44983,-1.44929,-1.44888,-1.44857,-1.44834,-1.44816,-1.44803,-1.44792,-1.44781,-1.4477,-1.4476,-1.4475,-1.44741,-1.44732,-1.44723,-1.44715,-1.44707,-1.447,-1.44693,-1.44687,-1.44683,-1.44679,-1.44677,-1.44675,-1.44673,-1.44672,-1.44672,-1.44671,-1.44671,-1.4467,-1.4467,-1.4467,-1.4467,-1.4467,-1.4467,-1.4467,-1.4467,-1.4467,-1.4469,-1.44672,-1.44619,-1.44169,-1.42751,-1.41414,-1.40732,-1.40379,-1.40191,-1.40097,-1.40046,-1.40013,-1.3999,-1.3997,-1.39953,-1.39937,-1.39922,-1.39907,-1.39893,-1.39879,-1.39866,-1.39854,-1.39841,-1.39828,-1.39815,-1.398,-1.39784,-1.39767,-1.39749,-1.39729,-1.39712,-1.397,-1.39692,-1.39688,-1.39685,-1.39684,-1.39683,-1.39682,-1.39681,-1.39681,-1.3968,-1.3968,-1.3968,-1.3968,-1.3968,-1.39679,-1.39679,-1.39679,-1.39679,-1.39679,-1.39703,-1.39682,-1.39635,-1.39202,-1.3753,-1.35666,-1.34554,-1.33774,-1.33171,-1.32716,-1.33499,-1.32889,-1.3259,-1.32326,-1.32105,-1.3202,-1.31983,-1.31954,-1.31921,-1.31881,-1.31837,-1.31793,-1.31758,-1.31735,-1.31721,-1.31714,-1.3171,-1.31708,-1.31706,-1.31705,-1.31704,-1.31703,-1.31703,-1.31702,-1.31702,-1.31701,-1.31701,-1.31701,-1.31701,-1.317,-1.317,-1.317,-1.317,-1.317,-1.317,-1.317,-1.31699,-1.31699,-1.31699,-1.31699,-1.3173,-1.31694,-1.31453,-1.29406,-1.26134,-1.23836,-1.22459,-1.24015,-1.22763,-1.22681,-1.21765,-1.21819,-1.22841,-1.21681,-1.21916,-1.21759,-1.22633,-1.21614,-1.21938,-1.21784,-1.2193,-1.21976,-1.22095,-1.21325,-1.224,-1.21443,-1.21836,-1.21874,-1.21989,-1.21265,-1.22414,-1.2149,-1.21893,-1.2182,-1.21936,-1.2188,-1.22085,-1.21346,-1.21707,-1.21656,-1.2253,-1.2154,-1.21917,-1.21123,-1.22179,-1.21959,-1.22096,-1.21313,-1.21642,-1.21602,-1.22563,-1.21493,-1.21572,-1.18101,-1.14074,-1.1244,-1.15386,-1.13456,-1.11818,-1.12314,-1.14825,-1.14433,-1.13201,-1.11052,-1.14421,-1.14545,-1.11867,-1.09939,-1.14442,-1.14916,-1.11623,-1.11297,-1.13712,-1.14404,-1.12469,-1.11053,-1.12786,-1.15645,-1.11717,-1.10052,-1.13316,-1.14862,-1.10359,-1.12371,-1.13309,-1.13583,-1.12688,-1.11004,-1.1102,-1.15718,-1.12177,-1.09982,-1.13643,-1.14773,-1.10292,-1.12363,-1.13304,-1.1358,-1.12687,-1.11002]
32Ã—26 Array{Float64,2}:
 -0.00932425  -0.118778     -0.125769     0.0726929    0.128733     0.0578627  -0.0368028   -0.0800425    0.0318145   -0.0256813   -0.0330791    0.091486     -0.082569     0.129639    -0.06248     -0.0955613    0.140897     0.124586     0.0449823   0.0318916     0.0447065    0.135555    -0.0933982    0.00540811   0.0679445   -0.0418236 
  0.0632587   -0.0691317    -0.0802767    0.280707    -0.101115     0.0607736   0.0389538   -0.175457     0.150776    -0.113533    -0.143363    -0.0684534     0.0837322    0.0804837    0.244918    -0.0697055   -0.0692705   -0.103341    -0.0551108  -0.129296     -0.0325329    0.0908561    0.117776    -0.0336981   -0.0304883   -0.117753  
 -0.0431593    0.0863881     0.00620354   0.0975802    0.0687798   -0.0571869   0.00647705  -0.192616    -0.00773739  -0.144586    -0.11359     -0.0228577    -0.117673     0.0334111    0.0829947   -0.0151064   -0.0419615   -0.1228      -0.0599618  -0.0358669     0.113153    -0.00713885   0.0473388   -0.0973005    0.0992447    0.0411493 
 -0.0200532    0.170978     -0.0991975   -0.059301    -0.0375272    0.16824     0.0455238    0.0718734   -0.00744292   0.06995     -0.0328599   -0.136449     -0.0078937   -0.089958     0.0375114   -0.0578822    0.190742     0.13657     -0.0567514   0.116394     -0.0472076   -0.0235341    0.0403796    0.0472954   -0.0242888    0.123279  
 -0.165314    -0.0177325    -0.0302617   -0.143265     0.063139     0.0113091   0.0100073    0.00373039   0.00333923  -0.0592394   -0.0217383   -0.0412788     0.00453517  -0.0933036    0.117402    -0.117487    -0.124204    -0.130798     0.02651    -0.0777782    -0.226122     0.0884602   -0.183883    -0.814919     0.173142    -0.243834  
 -0.0914287   -0.0177556    -0.0335606   -0.127501    -0.0544924   -0.203795   -0.0908721    0.100119    -0.0926848   -0.0897624   -0.109055    -0.0400491    -0.0798652   -0.123016     0.0984056   -0.0938739   -0.0987742   -0.330437     0.0958015  -0.0060529    -0.205175    -0.012654    -0.16923      0.851738    -0.0248194   -0.107042  
 -0.0453421   -0.151838      0.103752    -0.0844817   -0.151061    -0.121342   -0.0193815    0.054061    -0.0712049    0.0588534    0.0699373   -0.174052     -0.142852    -0.0968145   -0.100233    -0.259097    -6.78514e-5   0.0877186   -0.163214    0.023412      0.0222726   -0.0949787   -0.0253667    0.160349    -0.119024    -0.0816987 
  0.0413119   -0.10753       0.0142659   -0.0480124   -0.0623331   -0.0607349  -0.128813    -0.117296     0.0693976    0.18479     -0.107613    -0.132045      0.0436292   -0.110146     0.0802882   -0.0575851    0.0327021   -0.00630393   0.0107756  -0.23475      -0.0355755    0.318334     0.0758794   -0.148241    -0.0474341   -0.193858  
  0.0154172    7.12751e-5    0.0632059   -0.0676905    0.0384871    0.0556487  -0.0429671   -0.158543    -1.3359       0.0707428   -0.050195     0.0913171     0.0638135    0.111689    -0.0649494    0.189194     0.0542336    0.192262    -0.008384   -0.00844334    0.160132    -0.0146563    0.0314752    0.0706684   -0.0574464    0.0250442 
  0.0652557    0.000886903   0.060629    -0.0644844    0.171907     0.0549319  -0.0157816   -0.1082       1.30837      0.077476    -0.140861     0.0622932     0.063848     0.0957154    0.19267      0.189052     0.050279     0.0829669    0.115982   -0.000950511   0.155651     0.0370451   -0.0211237    0.121041    -0.0181499    0.0336099 
  0.0927221   -0.140455     -0.105823    -0.0760623    0.0355465    0.109079   -0.0970158   -0.0609298    0.0286737    0.0747739   -0.137146     0.145083     -0.0708017    0.0362166   -0.0251141    0.0750387    0.036509     0.0554915   -0.117108    0.00948124    0.202194    -0.0486994   -0.137397    -0.789784    -0.105664     0.0731426 
  0.266079    -0.147987     -0.0906932   -0.0803702    0.0349229    0.111165   -0.0542607    0.0511891    0.0324804    0.051742    -0.159574     0.154997     -0.134114    -0.0136893   -0.047691     0.0153681    0.109842    -0.0243199   -0.142725   -0.0101949     0.249414     0.0130513   -0.23798      0.996057    -0.0753626    0.0703748 
 -0.232251    -0.213138      0.0489946    0.0321741   -0.0840407   -0.21841     0.0356865   -0.930692    -0.150097    -0.154729     0.276826     0.0320518     0.143701     0.0207282    0.294941     0.0354183    0.0579919    0.279172     0.0504867   0.0110712     0.0183557    0.0626005   -0.0396497    0.105542     0.161619     0.258323  
 -0.100805     0.290368      0.0603722    0.0374151   -0.118161    -0.240764    0.0352354    0.82196     -0.0710204   -0.178946     0.225665     0.0922749     0.0658027    0.0486024   -0.282185     0.103934     0.0213861    0.208653    -0.130873   -0.0221699     0.0140391    0.0622434   -0.0740068    0.13315      0.183556     0.257618  
  0.0774689    0.226773     -0.0749366   -0.0458527   -0.16762     -0.0914257   0.0126628   -0.0221412   -0.0356923   -0.0722599   -0.0039488   -0.0497638     0.139549    -0.185221     0.156226    -0.198701    -0.0417703    0.0773129   -0.042689    0.115945      0.123167     0.123688     0.0656539    0.0136806    0.0712976    0.156865  
 -0.0396366   -0.0643995     0.00760803  -0.0588022    0.0856577    0.0604019   0.151229    -0.111557     0.0805066   -0.0792642   -0.0470837    0.132421      0.12839     -0.0101616   -0.0358849    0.109743     0.00451072   0.0681669   -0.0237051  -0.0488235     0.119525     0.0257      -0.0295783   -0.145053    -0.1386      -0.0406632 
 -0.0910866    0.0198119    -0.0746345   -0.163214    -0.101594    -0.0193618   0.12438     -0.0914584    0.137725    -0.131826    -0.0483998   -0.20111       0.0320428    0.0111699    0.1669       0.065576     0.144456    -0.102445    -0.239258   -0.0228785     0.0917546   -0.00792293  -0.100813     0.0145011    0.0480518    0.156226  
  0.254423    -0.19819       0.0571127    0.0146374    0.0262288   -0.0262344  -0.0569082    0.0413065   -0.0188507   -0.121913     0.114499    -0.0166325    -0.0975422    0.0641625   -0.0944003    0.0734672    0.19078      0.00577923   0.0933246   0.103105     -0.00250585   0.142915     0.0343199   -0.0780472    0.0846828   -0.0830781 
 -0.143451     0.00293624   -0.0437771   -0.0780837    0.0168997   -0.0136102   0.0285843   -0.0285856    0.121961     0.0305295    0.065788     0.0913288    -0.116317    -0.00766172  -0.0295844    0.0379596   -0.0468101   -0.0110368   -0.0620302  -0.0536648     0.0344176    0.0605811   -0.110471    -0.0557936   -0.0712505    0.105061  
  0.0312726   -0.000206536  -0.0375719   -0.231587    -0.101321    -0.0217232   0.11154      0.0985235   -0.127667     0.210747    -0.202648    -0.000889274   0.280008     0.103877    -0.145933    -0.0946234   -0.141085    -0.216403     0.135903    0.0302027     0.0816212    0.123598     0.0575157   -0.17791      0.13198      0.00700767
 -0.0413319   -0.067121      0.0297996    0.090605     0.201587    -0.0611419  -0.116335     0.0239917    0.0734427   -0.06528     -0.017701     0.111606     -0.0944386    0.00662665   0.0196146    0.17817     -0.0709806    0.190708    -0.0944545   0.118081      0.0278057    0.00647497   0.0382573    0.14334     -0.016803    -0.0741644 
 -0.124785    -0.0264902    -0.134251     0.0532726   -0.0406763    0.0494768   0.0887632    0.0425258   -0.0567309    0.0157046   -0.148832    -0.124702      0.139563    -0.0213103   -0.0987252    0.124723    -0.0487672   -0.0935155    0.0283433   0.0936562    -0.164348    -0.0289291   -0.048098     0.0746081   -0.102488    -0.0340389 
 -0.100633    -0.00115806   -0.0359062    0.0136826    0.11195     -0.206845    0.0986944   -0.029298     0.0103251   -0.0223449    0.019752    -0.10697       0.0126888    0.0351708    0.130458    -0.115777    -0.0412516    0.0299852   -0.0753827  -0.101295      0.0420828   -0.133056     0.0310727   -0.0826023   -0.00343776   0.00631602
  0.101837    -0.119508      0.0778997    0.00165065  -0.0641515    0.0999216   0.145201     0.0141703   -0.0677124   -0.0243314   -0.03562     -0.021897      0.172017     0.00595812   0.0193384    0.0740442    0.239674    -0.0185353    0.075291   -0.113571      0.0459398    0.170236     0.0231183    0.0218424   -0.167189    -0.0408155 
 -0.0843563    0.145349      0.0409338    0.0488218    0.0659278    0.084588   -0.110985    -0.107927     0.068857    -0.107204    -0.00592048   0.119432     -0.113525    -0.0796767    0.0343343   -0.0362043    0.0815532   -0.0153665   -0.0245892  -0.0676218     0.115711     0.0522235    0.0604495   -0.202467    -0.0257372    0.0191048 
  0.0553262    0.0909704     0.0395677   -0.0349374    0.112093     0.16268    -0.0215046   -0.0702494    0.0180331   -0.0743344    0.0199729   -0.086185      0.0260986   -0.0282587    0.00864397  -0.0275366   -0.0361709   -0.122034     0.0237279  -0.0497842     0.0515886    0.0164776    0.0378871   -0.028017     0.00502248   0.0709972 
  0.0749565    0.0906852    -0.0378765    0.0560388   -0.0506502    0.109577    0.0369341   -0.0400357    0.0283597    0.0192072   -0.16695      0.0879625     0.0205763   -0.00885656  -0.0713789    0.0147206   -0.00444541  -0.0386275    0.0357425   0.0787082     0.0399594    0.0264207    0.0495518   -0.0252432   -0.0559434   -0.0436833 
 -0.0783075    0.0157783     0.242629     0.137155     0.0496476    0.0370393   0.0233622   -0.0472037   -0.00989958   0.159008     0.126289    -0.0303966     0.0399102   -0.130172     0.0291215    0.142409    -0.0410145    0.0661957    0.148345    0.15478      -0.0496734    0.128426     0.00775592  -0.0647391    0.066211     0.137432  
 -0.0283206   -0.188793      0.0603138   -0.0269762    0.00823146   0.0389808  -0.0193747   -0.256082    -0.0632909    0.0597795   -0.125308     0.0556568     0.0024285    0.00233469  -0.0594849   -0.101255    -0.113278     0.132363    -0.0337584  -0.0900833     0.078307     0.0409378    0.0452048    0.137275     0.212387     0.0306712 
 -0.0702904    0.138744     -0.0478663    0.101026    -0.124561    -0.109711    0.0583417   -0.170861    -0.043148    -0.117158     0.0517322   -0.141945     -0.0438652    0.0325395   -0.100665    -0.00941717  -0.0594242   -0.131886     0.0678314  -0.0428569     0.122063    -0.045898     0.0670479   -0.0576051   -0.088581    -0.0739858 
 -0.116326    -0.0924581    -0.0055062    0.0851045   -0.244736    -0.18707    -0.121339    -0.19097     -0.0736253    0.0182655   -0.0782908    0.0553384     0.136686    -0.127259    -0.0718019    0.261202    -0.20915     -0.119141     0.104346   -0.0256735    -1.29546     -0.0664307    0.0821734    0.0220415    0.038182     0.0817133 
 -0.095688    -0.141988      0.0756532    0.066569    -0.241934    -0.125389   -0.0838971   -0.184331    -0.0237284    0.00972594  -0.0629604    0.250366      0.102692    -0.13381     -0.0837254    0.0785596   -0.19928     -0.128074     0.152125   -0.0286415     1.96433     -0.0571222    0.0835799    0.0426772    0.0465121    0.0244497 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 3 4 7 8 16 21 24 28
INFO: iteration 1, average log likelihood -1.110195
WARNING: Variances had to be floored 3 4 7 8 9 10 15 16 21 24 28
INFO: iteration 2, average log likelihood -1.088578
WARNING: Variances had to be floored 3 4 7 8 16 21 22 24 28
INFO: iteration 3, average log likelihood -1.086428
WARNING: Variances had to be floored 3 4 7 8 9 10 15 16 21 24 28 29 30
INFO: iteration 4, average log likelihood -1.082108
WARNING: Variances had to be floored 3 4 7 8 16 21 24 28
INFO: iteration 5, average log likelihood -1.101583
WARNING: Variances had to be floored 3 4 7 8 9 10 15 16 21 22 24 28
INFO: iteration 6, average log likelihood -1.083038
WARNING: Variances had to be floored 3 4 7 8 16 21 24 28
INFO: iteration 7, average log likelihood -1.098058
WARNING: Variances had to be floored 3 4 7 8 9 10 15 16 21 24 28 29 30
INFO: iteration 8, average log likelihood -1.072896
WARNING: Variances had to be floored 3 4 7 8 16 21 22 24 28
INFO: iteration 9, average log likelihood -1.097532
WARNING: Variances had to be floored 3 4 7 8 9 10 15 16 21 24 28
INFO: iteration 10, average log likelihood -1.095634
INFO: EM with 100000 data points 10 iterations avll -1.095634
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.017208e+06
      1       7.833564e+05      -2.338517e+05 |       32
      2       7.469103e+05      -3.644610e+04 |       32
      3       7.282067e+05      -1.870367e+04 |       32
      4       7.194356e+05      -8.771080e+03 |       32
      5       7.138436e+05      -5.591940e+03 |       32
      6       7.098619e+05      -3.981781e+03 |       32
      7       7.071843e+05      -2.677563e+03 |       32
      8       7.054083e+05      -1.775949e+03 |       32
      9       7.042220e+05      -1.186373e+03 |       32
     10       7.034294e+05      -7.925299e+02 |       32
     11       7.028590e+05      -5.703951e+02 |       32
     12       7.022176e+05      -6.414146e+02 |       32
     13       7.012382e+05      -9.794692e+02 |       32
     14       6.999170e+05      -1.321202e+03 |       32
     15       6.990725e+05      -8.444561e+02 |       32
     16       6.987041e+05      -3.683800e+02 |       32
     17       6.985534e+05      -1.507306e+02 |       32
     18       6.984648e+05      -8.863050e+01 |       32
     19       6.984016e+05      -6.318276e+01 |       32
     20       6.983425e+05      -5.909062e+01 |       32
     21       6.982878e+05      -5.471222e+01 |       32
     22       6.982200e+05      -6.782460e+01 |       32
     23       6.981413e+05      -7.862699e+01 |       32
     24       6.980378e+05      -1.035434e+02 |       32
     25       6.979433e+05      -9.452915e+01 |       32
     26       6.978304e+05      -1.128458e+02 |       32
     27       6.977379e+05      -9.253321e+01 |       32
     28       6.976601e+05      -7.772681e+01 |       32
     29       6.975997e+05      -6.046815e+01 |       30
     30       6.975411e+05      -5.858348e+01 |       32
     31       6.974940e+05      -4.707979e+01 |       31
     32       6.974475e+05      -4.656133e+01 |       32
     33       6.974101e+05      -3.734026e+01 |       29
     34       6.973738e+05      -3.633541e+01 |       31
     35       6.973393e+05      -3.451582e+01 |       31
     36       6.973090e+05      -3.030754e+01 |       32
     37       6.972768e+05      -3.220197e+01 |       31
     38       6.972362e+05      -4.054397e+01 |       31
     39       6.971883e+05      -4.794480e+01 |       31
     40       6.971191e+05      -6.921142e+01 |       32
     41       6.970238e+05      -9.522302e+01 |       32
     42       6.968546e+05      -1.692478e+02 |       32
     43       6.965823e+05      -2.722975e+02 |       32
     44       6.963046e+05      -2.776501e+02 |       32
     45       6.960898e+05      -2.148406e+02 |       32
     46       6.960048e+05      -8.502996e+01 |       30
     47       6.959705e+05      -3.428453e+01 |       32
     48       6.959530e+05      -1.750464e+01 |       29
     49       6.959421e+05      -1.085296e+01 |       30
     50       6.959345e+05      -7.632043e+00 |       25
K-means terminated without convergence after 50 iterations (objv = 695934.4918645623)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.397830
INFO: iteration 2, average log likelihood -1.364289
INFO: iteration 3, average log likelihood -1.321893
INFO: iteration 4, average log likelihood -1.276720
INFO: iteration 5, average log likelihood -1.230466
WARNING: Variances had to be floored 12 13 28 31
INFO: iteration 6, average log likelihood -1.166626
WARNING: Variances had to be floored 2 5 7 8 15
INFO: iteration 7, average log likelihood -1.169487
WARNING: Variances had to be floored 1 3 24 25
INFO: iteration 8, average log likelihood -1.173882
WARNING: Variances had to be floored 17 18 19 30
INFO: iteration 9, average log likelihood -1.160419
WARNING: Variances had to be floored 10 12 13 28
INFO: iteration 10, average log likelihood -1.136692
WARNING: Variances had to be floored 2 5 15 31
INFO: iteration 11, average log likelihood -1.147019
WARNING: Variances had to be floored 6 7 8 24
INFO: iteration 12, average log likelihood -1.141175
WARNING: Variances had to be floored 1 18
INFO: iteration 13, average log likelihood -1.140662
WARNING: Variances had to be floored 9 10 12 13 19 25 28 30
INFO: iteration 14, average log likelihood -1.108657
WARNING: Variances had to be floored 2 5 15 22
INFO: iteration 15, average log likelihood -1.171690
WARNING: Variances had to be floored 6
INFO: iteration 16, average log likelihood -1.169000
WARNING: Variances had to be floored 1 7 8 12 18 24
INFO: iteration 17, average log likelihood -1.114770
WARNING: Variances had to be floored 28 30 31
INFO: iteration 18, average log likelihood -1.152513
WARNING: Variances had to be floored 9 10 13 15 22
INFO: iteration 19, average log likelihood -1.120102
WARNING: Variances had to be floored 2 5 6 12 19 25
INFO: iteration 20, average log likelihood -1.127380
WARNING: Variances had to be floored 1 7 8 24
INFO: iteration 21, average log likelihood -1.165986
WARNING: Variances had to be floored 18 28
INFO: iteration 22, average log likelihood -1.166626
WARNING: Variances had to be floored 10 12 22 31
INFO: iteration 23, average log likelihood -1.125608
WARNING: Variances had to be floored 5 9 13 15 19
INFO: iteration 24, average log likelihood -1.129913
WARNING: Variances had to be floored 1 2 8 24 30
INFO: iteration 25, average log likelihood -1.135000
WARNING: Variances had to be floored 6 7 12 18 25
INFO: iteration 26, average log likelihood -1.150016
WARNING: Variances had to be floored 10 22
INFO: iteration 27, average log likelihood -1.165795
WARNING: Variances had to be floored 15 19 28 31
INFO: iteration 28, average log likelihood -1.127314
WARNING: Variances had to be floored 1 5 8 9 12 13 24 25
INFO: iteration 29, average log likelihood -1.102558
WARNING: Variances had to be floored 2 7
INFO: iteration 30, average log likelihood -1.183666
WARNING: Variances had to be floored 6 10 18 22
INFO: iteration 31, average log likelihood -1.144090
WARNING: Variances had to be floored 12 15 19 30
INFO: iteration 32, average log likelihood -1.128633
WARNING: Variances had to be floored 1 5 9 13 24 25 28
INFO: iteration 33, average log likelihood -1.120901
WARNING: Variances had to be floored 2 8
INFO: iteration 34, average log likelihood -1.169627
WARNING: Variances had to be floored 7 10 12 18 22 31
INFO: iteration 35, average log likelihood -1.125566
WARNING: Variances had to be floored 6 15 19
INFO: iteration 36, average log likelihood -1.153043
WARNING: Variances had to be floored 1 5 9 13 24 28
INFO: iteration 37, average log likelihood -1.118368
WARNING: Variances had to be floored 2 8 12 25
INFO: iteration 38, average log likelihood -1.158586
WARNING: Variances had to be floored 10 22
INFO: iteration 39, average log likelihood -1.159206
WARNING: Variances had to be floored 7 15 18 30 31
INFO: iteration 40, average log likelihood -1.117365
WARNING: Variances had to be floored 1 5 6 9 12 13 24 28
INFO: iteration 41, average log likelihood -1.119565
WARNING: Variances had to be floored 2 8
INFO: iteration 42, average log likelihood -1.178651
WARNING: Variances had to be floored 10 19 22 25
INFO: iteration 43, average log likelihood -1.138350
WARNING: Variances had to be floored 7 12 15
INFO: iteration 44, average log likelihood -1.136395
WARNING: Variances had to be floored 1 5 6 9 13 18 24 28
INFO: iteration 45, average log likelihood -1.107949
WARNING: Variances had to be floored 2 8 30 31
INFO: iteration 46, average log likelihood -1.165189
WARNING: Variances had to be floored 12 22
INFO: iteration 47, average log likelihood -1.165491
WARNING: Variances had to be floored 10 15
INFO: iteration 48, average log likelihood -1.131050
WARNING: Variances had to be floored 1 5 7 9 18 19 24 25 28 30
INFO: iteration 49, average log likelihood -1.087123
WARNING: Variances had to be floored 6 8 12
INFO: iteration 50, average log likelihood -1.182002
INFO: EM with 100000 data points 50 iterations avll -1.182002
59.0 data points per parameter
32Ã—26 Array{Float64,2}:
  0.0762071   -0.141143      0.0807162    0.00601857  -0.0351448    0.0896325   0.0764877   0.00466483  -0.0647129   -0.00621185  -0.0374979    -0.00479442   0.161565     0.00357072   0.0352551   0.0807857   0.478102    -0.00153225   0.0412965  -0.11956      0.0482671    0.189178     0.029666     0.00816729  -0.115077   -0.0398993 
  0.0770624    0.222873     -0.0728585   -0.0485254   -0.164582    -0.0919116   0.013182   -0.0205074   -0.0351395   -0.0806507   -0.0056161    -0.0504619    0.138528    -0.184169     0.151498   -0.190842   -0.0430762    0.0752311   -0.0408266   0.114592     0.119146     0.12143      0.0548218    0.0144997    0.0662371   0.15469   
 -0.0786519   -0.007052     -0.0187283    0.0280981    0.0872039   -0.176247    0.0856322  -0.0267399    0.00548901  -0.0443268   -0.0065956    -0.0836096    0.00765994   0.0262514    0.11137    -0.0766229  -0.0345932    0.0202383   -0.0590666  -0.100626     0.0537323   -0.0843072    0.0228809   -0.0897695   -0.0126234   0.00486769
 -0.109285    -0.108827      0.0313036    0.074548    -0.243651    -0.155981   -0.104681   -0.187245    -0.0500406    0.0139493   -0.0718877     0.147097     0.116308    -0.132597    -0.0770177   0.172566   -0.205815    -0.12354      0.129311   -0.0291011    0.246729    -0.0623042    0.0825723    0.0317525    0.0420557   0.051995  
 -0.101278     0.24994      -0.122998     0.133673    -0.11383     -0.134155    0.0836884  -0.16377     -0.0404118   -0.198384     0.0944727    -0.168074    -0.0374948    0.049138    -0.112507   -0.0264812  -0.0904859   -0.186579     0.0591144  -0.0467286    0.123446    -0.0369047    0.0661015   -0.0608599   -0.0815803  -0.0766405 
 -0.0283465   -0.188402      0.0596379   -0.0272219    0.00823617   0.0385512  -0.0197845  -0.255641    -0.0647641    0.0601781   -0.124073      0.0530694    0.00294649   0.00297024  -0.0583619  -0.103248   -0.11717      0.132201    -0.0338024  -0.097469     0.0786717    0.0407047    0.0446995    0.135901     0.210968    0.029247  
 -0.0511779   -0.12306       0.0512152   -0.128756    -0.109487    -0.116961   -0.0108313   0.0348688   -0.0754901    0.105192    -0.0130496    -0.157213    -0.130225    -0.101459    -0.071207   -0.16796    -0.0140405    0.0811283   -0.160421    0.0493477    0.0153225   -0.0495837   -0.00766532   0.214221    -0.105517   -0.0946841 
 -0.0201939    0.171817     -0.0977964   -0.0590294   -0.0363784    0.168315    0.0478129   0.0714552   -0.00731846   0.0695301   -0.0329955    -0.137141    -0.00631722  -0.090473     0.0362547  -0.0572932   0.188729     0.137111    -0.0568303   0.116056    -0.0473375   -0.0224799    0.0424529    0.0475404   -0.0227047   0.12352   
 -0.0338787    0.091859      0.0159083    0.103189     0.0795605   -0.0797993   0.0103483  -0.3103      -0.00681319  -0.184539    -0.114156     -0.0380667   -0.0778766    0.0433181    0.102583   -0.0231951  -0.0692779   -0.145066    -0.0328865  -0.0395628    0.118791    -0.0102667    0.0469732   -0.209339     0.0887166   0.021241  
  0.106464     0.106635     -0.128448     0.00311461  -0.145293     0.0380084   0.172589   -0.00517373  -0.0265695    0.00442627  -0.269966      0.0163357    0.0989724   -0.0924238   -0.0614328  -0.128204    0.00298627  -0.151485     0.143825    0.0734379    0.104724    -0.00425445   0.16312     -0.140567     0.0698532  -0.196654  
  0.251337    -0.195837      0.0582889    0.0157424    0.0255337   -0.0266726  -0.0558094   0.0430721   -0.0184554   -0.12235      0.111236     -0.0220831   -0.0985302    0.0613768   -0.0902935   0.0661666   0.189623     0.00704666   0.0894394   0.103089    -0.00190236   0.140556     0.0343541   -0.0777572    0.0855147  -0.0840016 
  0.0393925   -0.000265162   0.0635384   -0.0665241    0.0999451    0.0553986  -0.0307589  -0.132475    -0.137323     0.0739746   -0.0903672     0.076438     0.0641546    0.101995     0.0508085   0.188352    0.0521052    0.143948     0.0483669  -0.00621621   0.157653     0.00823588   0.00734869   0.0907623   -0.041955    0.0271675 
 -0.0942108    0.0865869     0.0254613    0.13671     -0.0167497    0.0982481   0.0358163  -0.0361439    0.0145897    0.0189714   -0.000675887   0.0581728   -0.0597707   -0.0103253    0.0144392   0.204604   -0.0059065   -0.0438883    0.0199446   0.00657178  -0.0379846    0.0206779   -0.0265688    0.102255    -0.0357207   0.168472  
 -0.0719719    0.13416       0.0487829    0.0455806    0.0798053    0.082718   -0.0801314  -0.0971152    0.0455159   -0.10245     -0.0110566     0.086996    -0.0973874   -0.085324     0.0521193  -0.039881    0.0753699   -0.0186262   -0.0308234  -0.0660263    0.0955303    0.0457639    0.0797293   -0.204902    -0.0306153   0.0253329 
  0.138936     0.149418     -0.0222057   -0.0757868    0.0272435    0.18908    -0.145525   -0.0851585   -0.0099515   -0.101881    -0.00843326   -0.0582154   -0.0343444    0.00151854  -0.165787    0.0316634  -0.0029727   -0.28131      0.0452549  -0.00118914   0.0561602   -0.00371481   0.0389742    0.106073     0.070376    0.192954  
  0.0324947   -0.0102278    -0.0365513   -0.218733    -0.101958    -0.0239719   0.103131    0.0798939   -0.125026     0.219215    -0.190036     -0.00561763   0.258514     0.0990382   -0.135325   -0.0980933  -0.134221    -0.205492     0.127896    0.0292159    0.0816802    0.117007     0.0558031   -0.170205     0.113559   -0.00431043
  0.1751      -0.144781     -0.0982457   -0.0777226    0.0351038    0.110044   -0.0770855  -0.00331393   0.0308569    0.0622212   -0.149295      0.148554    -0.105004     0.0128567   -0.0361554   0.0447951   0.0721826    0.0148262   -0.128467    0.00431866   0.22531     -0.0174201   -0.188387     0.105116    -0.0904413   0.0722752 
 -0.208176    -0.0406404    -0.0976829    0.12123     -0.0387071    0.0452489   0.0455756   0.0240784   -0.0528729   -0.0478184   -0.10297      -0.119608     0.149146    -0.00863647  -0.0680954   0.0742138  -0.0620051   -0.0962563    0.0305665   0.108789    -0.12975     -0.0252352   -0.0233191    0.0750958   -0.088986   -0.0266968 
 -0.0909102    0.0219917     0.29104      0.130306     0.0555116    0.052649    0.0364328  -0.0528093   -0.0154873    0.178009     0.227698     -0.0766987    0.0670043   -0.105115     0.0780109   0.128284   -0.0692764    0.0647894    0.138726    0.430568    -0.0562768    0.138089    -0.0119399   -0.114168     0.045013    0.0954011 
 -0.146879     0.00375525   -0.042431    -0.0776651    0.0124619   -0.0125087   0.0241157  -0.0342374    0.116328     0.0265234    0.0624192     0.0819473   -0.114888    -0.00754284  -0.0288767   0.0351074  -0.0449564   -0.0107409   -0.063327   -0.0568516    0.0306166    0.0642823   -0.102532    -0.0558881   -0.0731107   0.0938345 
 -0.126291    -0.0186997    -0.0304088   -0.130188     0.00181089  -0.0916388  -0.0390706   0.0510512   -0.0395005   -0.0630148   -0.061692     -0.0416093   -0.0397939   -0.109409     0.106267   -0.109608   -0.105649    -0.218021     0.0485801  -0.0368882   -0.211552     0.0402155   -0.170263     0.0209327    0.0690461  -0.161178  
 -0.0516684   -0.0664991     0.0130758   -0.0540529    0.0814988    0.0537121   0.152715   -0.115781     0.0864939   -0.0647501   -0.0472629     0.128072     0.134467    -0.0146873   -0.0408314   0.104171    0.014457     0.0705347   -0.0143448  -0.0462502    0.108236     0.0227933   -0.0278442   -0.143706    -0.131122   -0.0374981 
 -0.00883683  -0.119595     -0.125996     0.0727333    0.12788      0.0574251  -0.036631   -0.0791646    0.0318577   -0.0258543   -0.0333565     0.0919205   -0.0820161    0.129625    -0.0611747  -0.0953117   0.14068      0.127339     0.0446278   0.0314915    0.0446576    0.135774    -0.092924     0.00464346   0.0679494  -0.0417939 
 -0.0386877   -0.0590481     0.0781038    0.109739     0.215428    -0.0214968  -0.0797848   0.00354439   0.0749997   -0.093306    -0.0114449     0.072582    -0.077887    -0.0216798    0.0271444   0.261615   -0.217484     0.176447    -0.0222407   0.125278     0.00944191   0.0187122    0.0233639    0.112319    -0.0268744  -0.036982  
  0.129641    -0.110921      0.00204592  -0.0444756   -0.0694789   -0.0491784  -0.1204     -0.0995874    0.0797971    0.275865    -0.0787114    -0.159716     0.0124891   -0.112836     0.0666374  -0.0776069   0.0172606   -0.00691441   0.0141065  -0.404258    -0.0147024    0.201217     0.0552378   -0.221277    -0.0567593  -0.21319   
  0.062675    -0.0702802    -0.0794287    0.275898    -0.101426     0.0606451   0.0378453  -0.174419     0.150898    -0.10992     -0.141916     -0.0675959    0.0836749    0.0797323    0.240981   -0.0672847  -0.0692028   -0.102115    -0.0548486  -0.130605    -0.0320482    0.0920645    0.117609    -0.0362941   -0.0305914  -0.117807  
 -0.167854     0.0297552     0.0544058    0.0347097   -0.100448    -0.229013    0.0354853  -0.0850032   -0.111888    -0.166247     0.252116      0.0610744    0.106177     0.0339505    0.0172485   0.0682266   0.0409376    0.244916    -0.0363377  -0.0044792    0.0162615    0.0624189   -0.0561259    0.119216     0.172081    0.257949  
 -0.0851162    0.0665687    -0.00408876   0.150819    -0.00777987   0.160733    0.0368962  -0.0250653    0.0211833   -0.035279    -0.0661466     0.101599    -0.0574349    0.0784118    0.0606014   0.234684   -0.0273559   -0.0453129    0.0141755  -0.0336629   -0.0259215    0.0344089   -0.0329519    0.0852565   -0.0237996   0.158145  
  0.186781     0.0290917     0.0949566    0.0449684    0.061403     0.165213   -0.0908288  -0.0816542    0.0888021    0.0588793   -0.120675      0.172689     0.0416727    0.0277843   -0.136845   -0.0406002  -0.0188264    0.12739     -0.0231786   0.174056     0.0465921    0.121123     0.00484871  -0.052577    -0.177516   -0.0782994 
 -0.0425905    0.036326      0.124186    -0.0108821    0.233394     0.167577    0.196861   -0.0853889    0.0922436   -0.0892255    0.069014     -0.187228     0.11047     -0.033865     0.131254   -0.302026   -0.152235     0.0281547    0.0528827  -0.310138     0.065329     0.0392586    0.0173767   -0.173754    -0.0282067  -0.0458922 
 -0.00467493  -0.0519289     0.0495278    0.0249296   -0.0267706   -0.0241696  -0.122362   -0.126483     0.0535106    0.0210152   -0.076819     -0.110737     0.0727597   -0.083369     0.0570432  -0.0227772   0.0414453   -0.0331264   -0.0146863  -0.159975     0.0337537    0.170824     0.0696488   -0.162167    -0.0357658  -0.14361   
 -0.0905456    0.0197031    -0.074535    -0.161792    -0.101985    -0.0198497   0.126167   -0.0914178    0.134687    -0.133964    -0.048985     -0.202387     0.0324487    0.0109351    0.166784    0.0649716   0.142395    -0.100572    -0.233834   -0.0235511    0.0887534   -0.00772565  -0.0993055    0.0145062    0.0449354   0.154761  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 2 22
INFO: iteration 1, average log likelihood -1.164874
WARNING: Variances had to be floored 2 10 15 22 31
INFO: iteration 2, average log likelihood -1.102059
WARNING: Variances had to be floored 1 2 5 7 8 9 12 13 18 22 30
INFO: iteration 3, average log likelihood -1.060104
WARNING: Variances had to be floored 2 6 10 13 15 19 22 24 31
INFO: iteration 4, average log likelihood -1.115527
WARNING: Variances had to be floored 2 22
INFO: iteration 5, average log likelihood -1.113686
WARNING: Variances had to be floored 1 2 5 7 8 9 10 12 15 22 25 28 30 31
INFO: iteration 6, average log likelihood -1.054295
WARNING: Variances had to be floored 2 13 18 19 22 24
INFO: iteration 7, average log likelihood -1.126416
WARNING: Variances had to be floored 2 6 10 15 22 31
INFO: iteration 8, average log likelihood -1.094382
WARNING: Variances had to be floored 1 2 5 7 8 9 12 22 30
INFO: iteration 9, average log likelihood -1.069992
WARNING: Variances had to be floored 2 10 13 15 19 22 24 31
INFO: iteration 10, average log likelihood -1.115712
INFO: EM with 100000 data points 10 iterations avll -1.115712
59.0 data points per parameter
32Ã—26 Array{Float64,2}:
 -0.172011    -0.22631      0.0460417    0.0210892    0.194963     0.154091     0.2953      -0.0326762    -0.0803157    0.0720695   -0.116212     -0.0516186   -0.059369     0.166054    -0.0265617    0.108624    -0.0863134    0.0261741    -0.214307     -0.0978858    0.135349     0.00834336  -0.11674     -0.0135592   0.0613255   -0.147969   
  0.0885487   -0.0803802   -0.0451913   -0.126677    -0.1464       0.0385673   -0.0520768    0.0109019    -0.0995162   -0.161047     0.0928724     0.00649259  -0.038561     0.124118    -0.0477953   -0.116195    -0.0442066   -0.074383     -0.140855      0.0227069   -0.0365686    0.256083    -0.203269     0.0759054  -0.0703114    0.118233   
 -0.0571071   -0.0755235    0.0625987   -0.256098    -0.0568887   -0.081538    -0.0227997   -0.0647863     0.0119395   -0.0296307   -0.0254915    -0.155436     0.11276      0.066553    -0.0288143    0.169933    -0.0588716    0.0110878     0.000780211  -0.075057    -0.0185471   -0.110321    -0.194417    -0.106203   -0.174163     0.0888484  
 -0.0257423   -0.0913806   -0.0295164    0.0723324   -0.216859    -0.04104      0.0931271   -0.093797      0.0650002   -0.0917131    0.0957122    -0.0709734    0.0525788    0.134434    -0.0364913   -0.132055     0.116997    -0.0229158     0.0233678    -0.0159713   -0.171036    -0.0687671    0.057851    -0.067326    0.0364204    0.118485   
  0.0380212   -0.0208574    0.045476     0.183454    -0.118851    -0.210249     0.0847394   -0.0833229    -0.117236     0.067062     0.0573448     0.143138    -0.0209653   -0.0149017   -0.151633     0.0417654    0.0798623   -0.0402081    -0.0483119     0.0625284    0.109833    -0.0408707    0.0330787   -0.162684    0.1524       0.151879   
  0.0797054    0.00152457  -0.0396918   -0.0281184   -0.113452     0.092985    -0.100449     0.0596992     0.0718259    0.161824     0.093287     -0.111186    -0.00236484  -0.0423288    0.110724    -0.0760623   -0.179031    -0.135654     -0.0357584     0.0554475    0.0819166    0.00979032  -0.011834     0.15457    -0.00505386   0.0888524  
 -0.0415051    0.0488949   -0.0908409   -0.0298825   -0.0296662    0.0607595    0.120725    -0.0210232    -0.0406609   -0.197296    -0.0203088     0.179897     0.00454513   0.0715968    0.044588     0.116368    -0.123993     0.188329      0.00324079   -0.23539     -0.172012     0.0917467   -0.0283924    0.239513    0.125128    -0.00990664 
 -0.0050731   -0.0646981   -0.00069142  -0.211829    -0.0208813   -0.0274621   -0.0698649    0.273046     -0.0245887   -0.187169    -0.0283565    -0.0301699    0.0547581   -0.112855     0.00367376   0.169843    -0.224742     0.138198     -0.163288     -0.104707    -0.0273065   -0.076893    -0.00901052   0.142539    0.0975955   -0.00339988 
 -0.0271445   -0.114336    -0.160098     0.177073     0.00865706   0.204581    -0.114631    -0.120061      0.12967      0.146421    -0.0187533    -0.105305     0.180052     0.109394     0.126494     0.0550004    0.00236547  -0.0511383    -0.0407101     0.0197229    0.195488    -0.0266597    0.0410109    0.0646843  -0.143893     0.0345983  
  0.161756     0.161583     0.139369     0.0281063   -0.0229933   -0.0741955    0.113485     0.0101804    -0.0596791   -0.0490645    0.016029      0.0222585   -0.0363914   -0.0351531    0.0582028    0.0779834   -0.0122857   -0.00319865   -0.0640014    -0.104308     0.120564     0.108205     0.0230096   -0.18036     0.0623654    0.01197    
  0.0750209    0.0188442   -0.0276067   -0.136547    -0.105351    -0.0163308   -0.0423465    0.0837417     0.0634229   -0.0765868   -0.000255158   0.15969     -0.113342    -0.0553678    0.138617     0.00723219  -0.0821381   -0.132396      0.0273894     0.00735327   0.0859087    0.0445843   -0.00508979  -0.0736498   0.145601    -0.120259   
  0.0523837   -0.0457967    0.0388918    0.126265    -0.165268     0.123759    -0.045341    -0.159149     -0.170042     0.064295     1.07784e-6    0.0436154    0.0583749    0.00433789   0.0428047    0.098987    -0.0632762   -0.195658      0.0274762     0.0627433    0.182407    -0.00410362  -0.105497     0.103339   -0.0892933   -0.0752487  
 -0.0826089   -0.0229336    0.093573    -0.0434438   -0.032896    -0.0663575   -0.256914    -0.0688942     0.163566    -0.15424      0.159846      0.0164786   -0.107541     0.0833173   -0.0704503   -0.0438676    0.249394    -0.082623      0.235373      0.087195     0.120665     0.0808056    0.0285219    0.143531   -0.0620042    0.193129   
 -4.97678e-5  -0.147569    -0.0401818   -0.0162235    0.0177679   -0.069046     0.051481     0.0332134     0.151149    -0.0100177    0.00323177   -0.056944     0.202833     0.057222    -0.0991833   -0.119393    -0.256401     0.0832911     0.0896942     0.157809     0.0922886   -0.0221424   -0.0282017   -0.0490085  -0.04212     -0.0287526  
 -0.140714     0.0545165    0.157817    -0.109224    -0.151973    -0.0462178    0.0894721   -0.000434916  -0.0545135    0.167284    -0.044101     -0.0816696   -0.0395706    0.055111     0.0388455   -0.0293318   -0.128106     0.0794539    -0.0967649    -0.121214     0.0257691    0.107431     0.17239      0.0564868  -0.0758068    0.0448534  
 -0.07599     -0.0523721   -0.0115545    0.0174735    0.0485082   -0.0239381   -0.114979     0.172232     -0.0211548    0.0696329   -0.209806      0.147824     0.115379    -0.20151     -0.122811     0.128166     0.0512384   -0.1927        0.235312     -0.00711341   0.0297576    0.0185852    0.0284841    0.0404348   0.0577318    0.111468   
  0.095121     0.0245425    0.00815273   0.00749937   0.0277978   -0.00152325  -0.0426803   -0.10722      -0.18419      7.74913e-5   0.0433156    -0.143519    -0.0702283   -0.030878     0.00823993   0.0758238    0.0674182    0.0456871    -0.119295     -0.0113837   -0.0305798    0.0306379    0.0388744   -0.037685    0.0372602    0.000562055
 -0.0545346   -0.0764435   -0.0771547   -0.152326     0.0371522    0.109639     0.0849655    0.0755651     0.127988     0.0254813   -0.0433996    -0.105975    -0.153599     0.0547354   -0.158554     0.0965947    0.211515    -0.161561     -0.0516484     0.1697       0.0496412   -0.0533193   -0.10796      0.174495   -0.10034     -0.0765719  
 -0.0259538    0.220937    -0.00741074   0.0204336   -0.121972     0.0376366    0.103877     0.1532        0.0856918   -0.19951     -0.124755      0.131937    -0.137058    -0.179653     0.034021    -0.0677833    0.025347    -0.0488686    -0.032766     -0.0996313   -0.0915206   -0.0121535    0.062947     0.0956456  -0.116632    -0.143691   
  0.0505161    0.053861    -0.0474544   -0.0544352    0.0298883    0.0158611   -0.00574857  -0.0216928     0.00306065  -0.158082    -0.145768      0.300868     0.231712    -0.0614472    0.079586     0.217676    -0.0363242    0.0113386    -0.0553862    -0.109084     0.0291862   -0.00961917  -0.169341     0.0100071  -0.0555017    0.0771994  
  0.0275107    0.031302    -0.0753763   -0.00318618  -0.0991317    0.0977998   -0.0660717    0.00354743   -0.113544    -0.118699     0.0031513    -0.00496629   0.131852     0.105966     0.0335722    0.100393     0.033643    -0.0970939    -0.0570397     0.0863054   -0.129231    -0.0316432    0.0727497   -0.0039235   0.0919877    0.0415719  
 -0.0418421   -0.195377     0.0126054    0.031667     0.130351    -0.131241    -0.105221     0.0101327     0.226283     0.19012     -0.0991291     0.15259      0.0498461   -0.0256747    0.0645181   -0.0508188   -0.062156     0.0954596    -0.117056     -0.071979     0.033509     0.0652421   -0.162805    -0.0185182  -0.13121     -0.00424685 
 -0.146372    -0.0829186   -0.08824     -0.12444      0.00698098  -0.0792485    0.0784214   -0.178757     -0.0106638    0.0186707    0.00309349   -0.0282242    0.158817     0.0585201   -0.155822     0.128407     0.0785498   -0.123082      0.075092      0.00103852  -0.0921547    0.111044    -0.0545304   -0.0163027  -0.0256396   -0.0378427  
 -0.126525    -0.0759117   -0.0286382    0.188132    -0.161586     0.0630594    0.0535237   -0.0498592    -0.0375485   -0.0231718   -0.0443687     0.132941     0.16572     -0.116831     0.0179726   -0.049808    -0.00717921  -0.0551093     0.0451942    -0.261404     0.0418614    0.0626361   -0.101025     0.095572    0.0735218    0.107172   
 -0.00664728  -0.147564    -0.0289292    0.0507145    0.0519645    0.117932    -0.107824    -0.036349     -0.0946981   -0.0209943   -0.0659878    -0.0933497    0.0207011    0.0696137    0.0741248   -0.139732     0.00991558  -0.0850513     0.0688831     0.047409     0.115383     0.0663421    0.00897198  -0.0614798  -0.127582     0.030345   
 -0.00922709   0.0566241    0.0201786    0.0970548   -0.0196188   -0.0548406   -0.0346365    0.243811      0.0865612   -0.138888    -0.179961      0.0100581    0.0567927    0.0626984    0.045753    -0.0427257   -0.138316     0.000720298   0.0690086     0.0841357    0.0509996    0.152484    -0.00357463   0.0300986   0.12572      0.0191486  
 -0.116502    -0.034512    -0.020549    -0.0722604   -0.0771978    0.026572     0.176955    -0.0251211     0.0932283   -0.224557     0.0717187     0.00163192  -0.00999721   0.151544     0.192514     0.135347     0.0230045    0.0341594    -0.162205     -0.12285     -0.0715799   -0.185973     0.182108    -0.0529402  -0.0112028    0.101425   
  0.084071     0.205009     0.0443495    0.110853    -0.189171    -0.0392845    0.044261    -0.186357      0.177392    -0.052378     0.012753      0.191894     0.0441903    0.0783263   -0.177311    -0.12655     -0.0326285   -0.0599689    -0.070197     -0.0156037   -0.0493023    0.0234454   -0.129171     0.115566   -0.134291     0.0823894  
 -0.0417156    0.03961      0.080248     0.0870144    0.172519    -0.0701888   -0.0572738    0.116713     -0.0617178   -0.135042    -0.11594      -0.0728004   -0.102429    -0.14415     -0.0706899   -0.0513073   -0.0388892    0.0578621     0.0178393    -0.038004     0.0357625   -0.0470608   -0.161972     0.0699619  -0.0813949    0.0989932  
  0.0736956    0.0681469    0.22906      0.00881861   0.0187099    0.0561227    0.0600724   -0.119197      0.202182     0.0193118   -0.0421621     0.0982764   -0.192033    -0.0149269    0.0382431   -0.168353    -0.0768602    0.0548992    -0.225353     -0.017431    -0.126496    -0.0734755   -0.0211516   -0.0237546  -0.117062     0.277993   
 -0.115056     0.0162812   -0.0149529    0.049146     0.114071     0.0144517    0.0655852   -0.0129181    -0.056063     0.00246865   0.0681647     0.0555935   -0.0143504    0.143066     0.0885255   -0.0358654    0.0166736    0.149109      0.0925439    -0.101402     0.147441    -0.045833     0.0815922    0.0382698   0.00293221   0.0547671  
  0.018505     0.0223413   -0.0659377   -0.0270371    0.0308223   -0.021069     0.0763498   -0.135951      0.162068     0.00432565  -0.0105992    -0.0646619   -0.170337     0.0397528   -0.0858029   -0.0125654   -0.0539112   -0.0844289    -0.0534276     0.0434198   -0.00702452  -0.0625138    0.0931893   -0.0609777  -0.0563975    0.152116   kind full, method split
0: avll = -1.427075529199751
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.427096
INFO: iteration 2, average log likelihood -1.427018
INFO: iteration 3, average log likelihood -1.426946
INFO: iteration 4, average log likelihood -1.426845
INFO: iteration 5, average log likelihood -1.426710
INFO: iteration 6, average log likelihood -1.426543
INFO: iteration 7, average log likelihood -1.426362
INFO: iteration 8, average log likelihood -1.426188
INFO: iteration 9, average log likelihood -1.426027
INFO: iteration 10, average log likelihood -1.425852
INFO: iteration 11, average log likelihood -1.425613
INFO: iteration 12, average log likelihood -1.425239
INFO: iteration 13, average log likelihood -1.424672
INFO: iteration 14, average log likelihood -1.423930
INFO: iteration 15, average log likelihood -1.423158
INFO: iteration 16, average log likelihood -1.422539
INFO: iteration 17, average log likelihood -1.422145
INFO: iteration 18, average log likelihood -1.421932
INFO: iteration 19, average log likelihood -1.421824
INFO: iteration 20, average log likelihood -1.421772
INFO: iteration 21, average log likelihood -1.421747
INFO: iteration 22, average log likelihood -1.421735
INFO: iteration 23, average log likelihood -1.421729
INFO: iteration 24, average log likelihood -1.421726
INFO: iteration 25, average log likelihood -1.421724
INFO: iteration 26, average log likelihood -1.421723
INFO: iteration 27, average log likelihood -1.421723
INFO: iteration 28, average log likelihood -1.421722
INFO: iteration 29, average log likelihood -1.421722
INFO: iteration 30, average log likelihood -1.421722
INFO: iteration 31, average log likelihood -1.421722
INFO: iteration 32, average log likelihood -1.421721
INFO: iteration 33, average log likelihood -1.421721
INFO: iteration 34, average log likelihood -1.421721
INFO: iteration 35, average log likelihood -1.421721
INFO: iteration 36, average log likelihood -1.421721
INFO: iteration 37, average log likelihood -1.421721
INFO: iteration 38, average log likelihood -1.421721
INFO: iteration 39, average log likelihood -1.421721
INFO: iteration 40, average log likelihood -1.421720
INFO: iteration 41, average log likelihood -1.421720
INFO: iteration 42, average log likelihood -1.421720
INFO: iteration 43, average log likelihood -1.421720
INFO: iteration 44, average log likelihood -1.421720
INFO: iteration 45, average log likelihood -1.421720
INFO: iteration 46, average log likelihood -1.421720
INFO: iteration 47, average log likelihood -1.421720
INFO: iteration 48, average log likelihood -1.421720
INFO: iteration 49, average log likelihood -1.421720
INFO: iteration 50, average log likelihood -1.421720
INFO: EM with 100000 data points 50 iterations avll -1.421720
952.4 data points per parameter
1: avll = [-1.4271,-1.42702,-1.42695,-1.42685,-1.42671,-1.42654,-1.42636,-1.42619,-1.42603,-1.42585,-1.42561,-1.42524,-1.42467,-1.42393,-1.42316,-1.42254,-1.42215,-1.42193,-1.42182,-1.42177,-1.42175,-1.42173,-1.42173,-1.42173,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.421740
INFO: iteration 2, average log likelihood -1.421660
INFO: iteration 3, average log likelihood -1.421585
INFO: iteration 4, average log likelihood -1.421482
INFO: iteration 5, average log likelihood -1.421344
INFO: iteration 6, average log likelihood -1.421178
INFO: iteration 7, average log likelihood -1.421008
INFO: iteration 8, average log likelihood -1.420858
INFO: iteration 9, average log likelihood -1.420744
INFO: iteration 10, average log likelihood -1.420662
INFO: iteration 11, average log likelihood -1.420603
INFO: iteration 12, average log likelihood -1.420557
INFO: iteration 13, average log likelihood -1.420518
INFO: iteration 14, average log likelihood -1.420484
INFO: iteration 15, average log likelihood -1.420451
INFO: iteration 16, average log likelihood -1.420419
INFO: iteration 17, average log likelihood -1.420387
INFO: iteration 18, average log likelihood -1.420355
INFO: iteration 19, average log likelihood -1.420323
INFO: iteration 20, average log likelihood -1.420292
INFO: iteration 21, average log likelihood -1.420263
INFO: iteration 22, average log likelihood -1.420236
INFO: iteration 23, average log likelihood -1.420213
INFO: iteration 24, average log likelihood -1.420193
INFO: iteration 25, average log likelihood -1.420177
INFO: iteration 26, average log likelihood -1.420164
INFO: iteration 27, average log likelihood -1.420154
INFO: iteration 28, average log likelihood -1.420146
INFO: iteration 29, average log likelihood -1.420140
INFO: iteration 30, average log likelihood -1.420136
INFO: iteration 31, average log likelihood -1.420133
INFO: iteration 32, average log likelihood -1.420130
INFO: iteration 33, average log likelihood -1.420128
INFO: iteration 34, average log likelihood -1.420127
INFO: iteration 35, average log likelihood -1.420125
INFO: iteration 36, average log likelihood -1.420124
INFO: iteration 37, average log likelihood -1.420124
INFO: iteration 38, average log likelihood -1.420123
INFO: iteration 39, average log likelihood -1.420122
INFO: iteration 40, average log likelihood -1.420122
INFO: iteration 41, average log likelihood -1.420122
INFO: iteration 42, average log likelihood -1.420121
INFO: iteration 43, average log likelihood -1.420121
INFO: iteration 44, average log likelihood -1.420121
INFO: iteration 45, average log likelihood -1.420120
INFO: iteration 46, average log likelihood -1.420120
INFO: iteration 47, average log likelihood -1.420120
INFO: iteration 48, average log likelihood -1.420120
INFO: iteration 49, average log likelihood -1.420120
INFO: iteration 50, average log likelihood -1.420119
INFO: EM with 100000 data points 50 iterations avll -1.420119
473.9 data points per parameter
2: avll = [-1.42174,-1.42166,-1.42159,-1.42148,-1.42134,-1.42118,-1.42101,-1.42086,-1.42074,-1.42066,-1.4206,-1.42056,-1.42052,-1.42048,-1.42045,-1.42042,-1.42039,-1.42035,-1.42032,-1.42029,-1.42026,-1.42024,-1.42021,-1.42019,-1.42018,-1.42016,-1.42015,-1.42015,-1.42014,-1.42014,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42012,-1.42012,-1.42012,-1.42012,-1.42012,-1.42012,-1.42012,-1.42012,-1.42012,-1.42012,-1.42012,-1.42012,-1.42012,-1.42012,-1.42012]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.420131
INFO: iteration 2, average log likelihood -1.420078
INFO: iteration 3, average log likelihood -1.420030
INFO: iteration 4, average log likelihood -1.419972
INFO: iteration 5, average log likelihood -1.419898
INFO: iteration 6, average log likelihood -1.419807
INFO: iteration 7, average log likelihood -1.419701
INFO: iteration 8, average log likelihood -1.419588
INFO: iteration 9, average log likelihood -1.419476
INFO: iteration 10, average log likelihood -1.419376
INFO: iteration 11, average log likelihood -1.419291
INFO: iteration 12, average log likelihood -1.419221
INFO: iteration 13, average log likelihood -1.419162
INFO: iteration 14, average log likelihood -1.419114
INFO: iteration 15, average log likelihood -1.419073
INFO: iteration 16, average log likelihood -1.419038
INFO: iteration 17, average log likelihood -1.419007
INFO: iteration 18, average log likelihood -1.418981
INFO: iteration 19, average log likelihood -1.418957
INFO: iteration 20, average log likelihood -1.418936
INFO: iteration 21, average log likelihood -1.418918
INFO: iteration 22, average log likelihood -1.418901
INFO: iteration 23, average log likelihood -1.418885
INFO: iteration 24, average log likelihood -1.418871
INFO: iteration 25, average log likelihood -1.418858
INFO: iteration 26, average log likelihood -1.418846
INFO: iteration 27, average log likelihood -1.418835
INFO: iteration 28, average log likelihood -1.418825
INFO: iteration 29, average log likelihood -1.418815
INFO: iteration 30, average log likelihood -1.418806
INFO: iteration 31, average log likelihood -1.418798
INFO: iteration 32, average log likelihood -1.418790
INFO: iteration 33, average log likelihood -1.418783
INFO: iteration 34, average log likelihood -1.418776
INFO: iteration 35, average log likelihood -1.418769
INFO: iteration 36, average log likelihood -1.418763
INFO: iteration 37, average log likelihood -1.418757
INFO: iteration 38, average log likelihood -1.418751
INFO: iteration 39, average log likelihood -1.418745
INFO: iteration 40, average log likelihood -1.418740
INFO: iteration 41, average log likelihood -1.418734
INFO: iteration 42, average log likelihood -1.418729
INFO: iteration 43, average log likelihood -1.418724
INFO: iteration 44, average log likelihood -1.418719
INFO: iteration 45, average log likelihood -1.418714
INFO: iteration 46, average log likelihood -1.418709
INFO: iteration 47, average log likelihood -1.418704
INFO: iteration 48, average log likelihood -1.418699
INFO: iteration 49, average log likelihood -1.418694
INFO: iteration 50, average log likelihood -1.418690
INFO: EM with 100000 data points 50 iterations avll -1.418690
236.4 data points per parameter
3: avll = [-1.42013,-1.42008,-1.42003,-1.41997,-1.4199,-1.41981,-1.4197,-1.41959,-1.41948,-1.41938,-1.41929,-1.41922,-1.41916,-1.41911,-1.41907,-1.41904,-1.41901,-1.41898,-1.41896,-1.41894,-1.41892,-1.4189,-1.41889,-1.41887,-1.41886,-1.41885,-1.41884,-1.41882,-1.41882,-1.41881,-1.4188,-1.41879,-1.41878,-1.41878,-1.41877,-1.41876,-1.41876,-1.41875,-1.41875,-1.41874,-1.41873,-1.41873,-1.41872,-1.41872,-1.41871,-1.41871,-1.4187,-1.4187,-1.41869,-1.41869]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.418694
INFO: iteration 2, average log likelihood -1.418648
INFO: iteration 3, average log likelihood -1.418607
INFO: iteration 4, average log likelihood -1.418559
INFO: iteration 5, average log likelihood -1.418500
INFO: iteration 6, average log likelihood -1.418426
INFO: iteration 7, average log likelihood -1.418336
INFO: iteration 8, average log likelihood -1.418230
INFO: iteration 9, average log likelihood -1.418113
INFO: iteration 10, average log likelihood -1.417990
INFO: iteration 11, average log likelihood -1.417868
INFO: iteration 12, average log likelihood -1.417751
INFO: iteration 13, average log likelihood -1.417642
INFO: iteration 14, average log likelihood -1.417543
INFO: iteration 15, average log likelihood -1.417455
INFO: iteration 16, average log likelihood -1.417379
INFO: iteration 17, average log likelihood -1.417315
INFO: iteration 18, average log likelihood -1.417259
INFO: iteration 19, average log likelihood -1.417212
INFO: iteration 20, average log likelihood -1.417171
INFO: iteration 21, average log likelihood -1.417135
INFO: iteration 22, average log likelihood -1.417103
INFO: iteration 23, average log likelihood -1.417074
INFO: iteration 24, average log likelihood -1.417047
INFO: iteration 25, average log likelihood -1.417022
INFO: iteration 26, average log likelihood -1.416999
INFO: iteration 27, average log likelihood -1.416977
INFO: iteration 28, average log likelihood -1.416956
INFO: iteration 29, average log likelihood -1.416937
INFO: iteration 30, average log likelihood -1.416919
INFO: iteration 31, average log likelihood -1.416901
INFO: iteration 32, average log likelihood -1.416885
INFO: iteration 33, average log likelihood -1.416869
INFO: iteration 34, average log likelihood -1.416854
INFO: iteration 35, average log likelihood -1.416840
INFO: iteration 36, average log likelihood -1.416826
INFO: iteration 37, average log likelihood -1.416813
INFO: iteration 38, average log likelihood -1.416800
INFO: iteration 39, average log likelihood -1.416788
INFO: iteration 40, average log likelihood -1.416777
INFO: iteration 41, average log likelihood -1.416765
INFO: iteration 42, average log likelihood -1.416755
INFO: iteration 43, average log likelihood -1.416744
INFO: iteration 44, average log likelihood -1.416734
INFO: iteration 45, average log likelihood -1.416724
INFO: iteration 46, average log likelihood -1.416715
INFO: iteration 47, average log likelihood -1.416706
INFO: iteration 48, average log likelihood -1.416697
INFO: iteration 49, average log likelihood -1.416689
INFO: iteration 50, average log likelihood -1.416681
INFO: EM with 100000 data points 50 iterations avll -1.416681
118.1 data points per parameter
4: avll = [-1.41869,-1.41865,-1.41861,-1.41856,-1.4185,-1.41843,-1.41834,-1.41823,-1.41811,-1.41799,-1.41787,-1.41775,-1.41764,-1.41754,-1.41746,-1.41738,-1.41731,-1.41726,-1.41721,-1.41717,-1.41714,-1.4171,-1.41707,-1.41705,-1.41702,-1.417,-1.41698,-1.41696,-1.41694,-1.41692,-1.4169,-1.41688,-1.41687,-1.41685,-1.41684,-1.41683,-1.41681,-1.4168,-1.41679,-1.41678,-1.41677,-1.41675,-1.41674,-1.41673,-1.41672,-1.41672,-1.41671,-1.4167,-1.41669,-1.41668]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.416680
INFO: iteration 2, average log likelihood -1.416621
INFO: iteration 3, average log likelihood -1.416565
INFO: iteration 4, average log likelihood -1.416502
INFO: iteration 5, average log likelihood -1.416425
INFO: iteration 6, average log likelihood -1.416332
INFO: iteration 7, average log likelihood -1.416220
INFO: iteration 8, average log likelihood -1.416094
INFO: iteration 9, average log likelihood -1.415957
INFO: iteration 10, average log likelihood -1.415816
INFO: iteration 11, average log likelihood -1.415679
INFO: iteration 12, average log likelihood -1.415550
INFO: iteration 13, average log likelihood -1.415431
INFO: iteration 14, average log likelihood -1.415323
INFO: iteration 15, average log likelihood -1.415226
INFO: iteration 16, average log likelihood -1.415139
INFO: iteration 17, average log likelihood -1.415061
INFO: iteration 18, average log likelihood -1.414990
INFO: iteration 19, average log likelihood -1.414926
INFO: iteration 20, average log likelihood -1.414868
INFO: iteration 21, average log likelihood -1.414815
INFO: iteration 22, average log likelihood -1.414767
INFO: iteration 23, average log likelihood -1.414722
INFO: iteration 24, average log likelihood -1.414680
INFO: iteration 25, average log likelihood -1.414641
INFO: iteration 26, average log likelihood -1.414605
INFO: iteration 27, average log likelihood -1.414570
INFO: iteration 28, average log likelihood -1.414538
INFO: iteration 29, average log likelihood -1.414506
INFO: iteration 30, average log likelihood -1.414477
INFO: iteration 31, average log likelihood -1.414448
INFO: iteration 32, average log likelihood -1.414421
INFO: iteration 33, average log likelihood -1.414395
INFO: iteration 34, average log likelihood -1.414370
INFO: iteration 35, average log likelihood -1.414345
INFO: iteration 36, average log likelihood -1.414322
INFO: iteration 37, average log likelihood -1.414300
INFO: iteration 38, average log likelihood -1.414278
INFO: iteration 39, average log likelihood -1.414257
INFO: iteration 40, average log likelihood -1.414237
INFO: iteration 41, average log likelihood -1.414217
INFO: iteration 42, average log likelihood -1.414198
INFO: iteration 43, average log likelihood -1.414179
INFO: iteration 44, average log likelihood -1.414161
INFO: iteration 45, average log likelihood -1.414144
INFO: iteration 46, average log likelihood -1.414127
INFO: iteration 47, average log likelihood -1.414110
INFO: iteration 48, average log likelihood -1.414093
INFO: iteration 49, average log likelihood -1.414077
INFO: iteration 50, average log likelihood -1.414062
INFO: EM with 100000 data points 50 iterations avll -1.414062
59.0 data points per parameter
5: avll = [-1.41668,-1.41662,-1.41656,-1.4165,-1.41643,-1.41633,-1.41622,-1.41609,-1.41596,-1.41582,-1.41568,-1.41555,-1.41543,-1.41532,-1.41523,-1.41514,-1.41506,-1.41499,-1.41493,-1.41487,-1.41482,-1.41477,-1.41472,-1.41468,-1.41464,-1.4146,-1.41457,-1.41454,-1.41451,-1.41448,-1.41445,-1.41442,-1.41439,-1.41437,-1.41435,-1.41432,-1.4143,-1.41428,-1.41426,-1.41424,-1.41422,-1.4142,-1.41418,-1.41416,-1.41414,-1.41413,-1.41411,-1.41409,-1.41408,-1.41406]
[-1.42708,-1.4271,-1.42702,-1.42695,-1.42685,-1.42671,-1.42654,-1.42636,-1.42619,-1.42603,-1.42585,-1.42561,-1.42524,-1.42467,-1.42393,-1.42316,-1.42254,-1.42215,-1.42193,-1.42182,-1.42177,-1.42175,-1.42173,-1.42173,-1.42173,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42172,-1.42174,-1.42166,-1.42159,-1.42148,-1.42134,-1.42118,-1.42101,-1.42086,-1.42074,-1.42066,-1.4206,-1.42056,-1.42052,-1.42048,-1.42045,-1.42042,-1.42039,-1.42035,-1.42032,-1.42029,-1.42026,-1.42024,-1.42021,-1.42019,-1.42018,-1.42016,-1.42015,-1.42015,-1.42014,-1.42014,-1.42013,-1.42013,-1.42013,-1.42013,-1.42013,-1.42012,-1.42012,-1.42012,-1.42012,-1.42012,-1.42012,-1.42012,-1.42012,-1.42012,-1.42012,-1.42012,-1.42012,-1.42012,-1.42012,-1.42012,-1.42013,-1.42008,-1.42003,-1.41997,-1.4199,-1.41981,-1.4197,-1.41959,-1.41948,-1.41938,-1.41929,-1.41922,-1.41916,-1.41911,-1.41907,-1.41904,-1.41901,-1.41898,-1.41896,-1.41894,-1.41892,-1.4189,-1.41889,-1.41887,-1.41886,-1.41885,-1.41884,-1.41882,-1.41882,-1.41881,-1.4188,-1.41879,-1.41878,-1.41878,-1.41877,-1.41876,-1.41876,-1.41875,-1.41875,-1.41874,-1.41873,-1.41873,-1.41872,-1.41872,-1.41871,-1.41871,-1.4187,-1.4187,-1.41869,-1.41869,-1.41869,-1.41865,-1.41861,-1.41856,-1.4185,-1.41843,-1.41834,-1.41823,-1.41811,-1.41799,-1.41787,-1.41775,-1.41764,-1.41754,-1.41746,-1.41738,-1.41731,-1.41726,-1.41721,-1.41717,-1.41714,-1.4171,-1.41707,-1.41705,-1.41702,-1.417,-1.41698,-1.41696,-1.41694,-1.41692,-1.4169,-1.41688,-1.41687,-1.41685,-1.41684,-1.41683,-1.41681,-1.4168,-1.41679,-1.41678,-1.41677,-1.41675,-1.41674,-1.41673,-1.41672,-1.41672,-1.41671,-1.4167,-1.41669,-1.41668,-1.41668,-1.41662,-1.41656,-1.4165,-1.41643,-1.41633,-1.41622,-1.41609,-1.41596,-1.41582,-1.41568,-1.41555,-1.41543,-1.41532,-1.41523,-1.41514,-1.41506,-1.41499,-1.41493,-1.41487,-1.41482,-1.41477,-1.41472,-1.41468,-1.41464,-1.4146,-1.41457,-1.41454,-1.41451,-1.41448,-1.41445,-1.41442,-1.41439,-1.41437,-1.41435,-1.41432,-1.4143,-1.41428,-1.41426,-1.41424,-1.41422,-1.4142,-1.41418,-1.41416,-1.41414,-1.41413,-1.41411,-1.41409,-1.41408,-1.41406]
32Ã—26 Array{Float64,2}:
 -0.408197    -0.046678    -0.29327      0.341521    0.0924557   -0.270115    0.111474      0.162017    0.585168     -0.335923    0.0748626  -0.411185    -0.275243      0.127349    -0.571155     0.127791     0.0237295   -0.306437      0.98707      0.55542    -0.357539    0.172698    -0.801645     -0.348975     -0.307846   -0.144524   
  0.4018      -0.209704     0.0794428   -0.289896    0.190115    -0.0609564  -0.123579      0.143104    0.424508      0.347477    0.381822   -0.212538    -0.670681      0.356559    -0.167484    -0.056032    -0.0345319   -0.215653     -0.0856807    0.514744    0.527928   -0.414697    -0.746801     -0.033311     -0.346133    0.488478   
  0.170519    -0.811156    -0.0196043   -0.316708    0.159955     0.297357   -0.381097      0.0630851  -0.458898      0.585947   -0.0338466   0.307998    -0.318364      0.237599     0.149281    -0.343824     0.138872    -0.137175     -0.197483     0.0400632  -0.0629277  -0.137047     0.00560594   -0.634322     -0.0862691   0.0866802  
  0.33579     -0.176308     0.434096    -0.076958    0.251208    -0.0302021   0.0530598     0.0151089  -0.418401      0.485768   -0.567895    0.55724     -0.014886     -0.339428     0.233748     0.186524     0.350338    -0.0657383    -0.463531    -0.327576    0.305331   -0.529679     0.19599       0.682771     -0.0177751   0.082011   
 -0.0852809   -0.227837    -0.00705336  -0.0526093   0.00947057  -0.171584    0.0818956    -0.157751    0.0994365    -0.151856    0.0403553   0.1113       0.00685134    0.0012261   -0.210012    -0.0642456   -0.0170646   -0.230154     -0.0908099    0.130564   -0.0884536  -0.0292871   -0.00474636    0.0330593    -0.0596446  -0.000309037
 -0.191562    -0.247123     0.0488404   -0.181911    0.181248    -0.101966   -0.0752795    -0.0443164  -0.0774989    -0.0559868   0.181324   -0.121097     0.524145      0.186743     0.595965     0.134972    -0.0283159   -0.0288467     0.0697179    0.0434813  -0.222444   -0.173531    -0.0576763     0.064409     -0.0124573   0.481237   
  0.280598    -0.442439     0.553428     0.439499   -0.053114    -0.513304    0.239521     -0.282559    0.372591      0.0406873   0.296901    0.00114397  -0.0193658     0.0777292    0.188781     0.390126    -0.0188047    0.091718     -0.0388805    0.619703   -0.545418   -0.442658    -0.303129     -0.0588272     0.338604   -0.307031   
 -0.193933    -0.174585    -0.581704     0.65822    -0.090926     0.484734    0.260847     -0.0833384   0.524316     -0.297952    0.394275    0.129598     0.157462     -0.328112    -0.483399    -0.160328     0.265328    -0.497017     -0.246599     0.24373    -0.518408   -0.330826    -0.143454     -0.368548      0.0165065  -0.167774   
 -0.100241     0.490881    -0.0124828   -0.266973   -0.409652     0.298024   -0.491848      0.0736407   0.221879     -0.119256    0.114101    0.336128    -0.198876     -0.397164    -0.119477     0.0132408    0.398131    -0.314359     -0.261118    -0.0950739   0.199743    0.175351    -0.243086      0.264277     -0.153175   -0.0231756  
  0.0502599    0.31746     -0.0718656    0.48195    -0.361815     0.367132   -0.334042      0.499345    0.0678023     0.196419   -0.328464    0.237454     0.175234     -0.255551     0.104345     0.379597     0.309913     0.0569646    -0.00480317  -0.0522955  -0.1096      0.100885    -0.208614     -0.0171672    -0.0324387   0.116587   
 -0.048093     0.613379     0.166548     0.0128565  -0.171628     0.243803    0.630373     -0.13804    -0.23441      -0.0240645   0.325217    0.236725    -0.1974        0.196914     0.288248     0.234157     0.00116465   0.229694     -0.386501    -0.0846467  -0.379892    0.492028     0.000668245   0.118827      0.763659   -0.198816   
  0.054017     0.600664     0.381373     0.0916401   0.317945     0.548749    0.2434        0.0514141   0.170528      0.383872    0.480324   -0.238196    -0.127975      0.0316841    0.47274      0.477955    -0.347092    -0.261357      0.706611    -0.115597   -0.267433    0.220374     0.372464      0.16452      -0.111568    0.186558   
 -0.359819     0.138526     0.393642    -0.456722    0.261767    -0.318801   -0.000786236   0.221355    0.263213     -0.0226788  -0.122113   -0.168886     0.160839      0.652599    -0.442554     0.301753    -0.461512     0.0198749     0.1573      -0.0315762   0.0526892   0.610039     0.49249      -0.0712432    -0.318315   -0.0778798  
 -0.401934    -0.189366    -0.427979     0.172275   -0.25796     -0.109283   -0.182504      0.275875   -0.359456     -0.119486   -0.0249057  -0.0151834    0.260177      0.22794     -0.172484     0.167901    -0.361095     0.349543      0.168283    -0.406835    0.1983      0.597376     0.236533     -0.711796      0.180433    0.277996   
  0.245136     0.125715    -0.129449     0.103437    0.0692832    0.0549245   0.218543      0.0573405  -0.102359      0.162282   -0.229301   -0.0945731   -0.207102     -0.0822066   -0.0837674   -0.00858048  -0.104931     0.186523     -0.0693734   -0.251111    0.201801   -0.0391085   -0.116493      0.0739419    -0.062236   -0.017041   
  0.160049    -0.0766707    0.30484      0.0372181  -0.256647     0.0507424  -0.12907      -0.340733   -0.357761     -0.392775   -0.0816071  -0.309087    -0.0374472     0.13473      0.175191    -0.0160473   -0.332253     0.32934      -0.0110383   -0.192477    0.43024     0.242236     0.658119      0.16427       0.2082     -0.238129   
  0.0220072   -0.711651    -0.421284    -0.157847   -0.380552    -0.721811   -0.209036     -0.322538   -0.000530986  -0.54975    -0.465075    0.341328    -0.000472515   0.0599836   -0.582066    -0.467641     0.0176039    0.139593     -0.643203     0.342782    0.338607   -0.186051    -0.611636     -0.0798947     0.052696   -0.0971363  
 -0.136166    -0.0990148   -0.249452     0.254135   -0.363841    -0.379864    0.815407     -0.350512    0.327688      0.567083   -0.796674    0.630428    -0.058043     -0.166298    -0.530898    -0.777977     0.38629      0.0633529    -0.0610481   -0.496441   -0.0107057  -0.614037    -0.463695     -0.371398     -0.0239646   0.13393    
  0.00627643   0.165134    -0.738474    -0.345678   -0.386512     0.0150551  -0.139094      0.218786   -0.219283     -0.362927   -0.447345   -0.0522044   -0.191736     -0.404658    -0.78349     -0.377289    -0.111456    -0.114778     -0.192061    -0.63781     0.768391    0.289501     0.316244      0.185699     -0.438966    0.192622   
  0.0649963    0.651635    -0.161319     0.0377851   0.113469     0.395543   -0.0978332     0.452883   -0.285734     -0.249969   -0.583235    0.21084      0.346366      0.00339803   0.547104    -0.199026    -0.141827     0.336837      0.171331    -0.579799    0.628712    0.562425    -0.0213372     0.778467      0.0320748   0.186291   
  0.39248     -0.653069     0.462135     0.0106391   0.249809     0.0805057   0.0557188    -0.845018    0.541695      0.0331664  -0.206688    0.249448     0.110511     -0.329216    -0.392492     0.279025    -0.244725     0.029677     -0.42481      0.460041   -0.0514079  -0.680377    -0.287995      0.244437     -1.00728    -0.638429   
  0.200335     0.00397889   0.0164874    0.292015   -0.0789706    0.112404   -0.0470037    -0.757473    0.31261       0.201767    0.619187    0.2109      -0.991557     -0.154145     0.122989    -0.205474     0.243471    -0.000874891  -0.0961168    0.145893    0.264834   -0.343099    -0.384596      0.032837      0.222045   -0.49932    
 -0.0190421    0.174634    -0.330001    -0.380071   -0.0555629    0.23017    -0.0166349     0.405233    0.0521611     0.248298    0.140856   -0.00677699  -0.24157      -0.484928     0.0147704   -0.179861     0.485947    -0.426538     -0.365963     0.0313908  -0.281319   -0.122867    -0.532688      0.176217     -0.391206    0.861374   
  0.390486     0.0756121   -0.183753     0.661494    0.104098     0.0332581   0.1482        0.576248    0.722435      0.433754   -0.22679     0.561041     0.565963     -0.454686     0.477939     0.0952287   -0.024732    -0.390691     -0.28884      0.220181   -0.576426   -0.242801    -0.517787      0.168995      0.16657     0.171617   
 -0.947258     0.598777    -0.272705    -0.0610255  -0.814591    -0.398844   -0.0264538    -0.494441   -0.180746     -0.616719   -0.347484   -0.0115885    0.248697     -0.289581     0.0488673    0.343402     0.36512     -0.128452     -0.424106    -0.995886   -0.865105   -0.00598273  -0.11696       0.604125     -0.348098   -0.208816   
 -0.698419    -0.245011    -0.139204    -0.475124   -0.0658367    0.0235983  -0.515992     -0.314089    0.116107     -0.6797      0.385435    0.556288     0.680082     -0.109681     0.31472     -0.0123779    0.442566    -0.253236     -0.384454     0.795132   -0.792413    0.307246     0.249282      0.176743      0.0124875   0.367028   
 -0.48596      0.0725455   -0.732064    -0.447862    0.204481    -0.0757736   0.598572     -0.306761    0.117111     -0.551789   -0.118095   -0.837568     0.408633     -0.484293    -0.133141    -0.62613      0.0427262   -0.236436     -0.184496     0.21523    -0.233806   -0.34047      0.200774      0.0582746     0.198321   -0.135567   
 -0.312204    -0.523207     0.412342     0.0604382  -0.0185942   -0.115283   -0.428264     -0.302292    0.612655     -0.498403    0.164716    0.0977979    0.834362      0.204327    -0.0674933   -0.465977     0.131266    -0.6037        0.441967     0.736336    0.0769234  -0.493584     0.685887     -0.668485      0.321564   -0.126559   
  0.42973      0.472502    -0.0445969    0.638026   -3.00347e-5   0.0565027   0.240905      0.516741    0.147516      0.522569   -0.39032    -0.216423    -0.692046      0.0202392   -0.382173     0.0163733   -0.514467    -0.126754      0.439442    -0.563966    0.525559    0.265133    -0.657185     -0.000712716  -0.116035   -0.409301   
 -0.0220879    0.36523      0.190766    -0.0548417  -0.225732     0.0138027   0.0261213     0.205369   -0.0996045    -0.1828     -0.104081   -0.0289777    0.0694304     0.22857      0.00336458   0.318789    -0.283257     0.112817      0.0133703   -0.199079    0.0558787   0.573824     0.31951       0.00595657    0.163587    0.0755467  
  0.221165    -0.467067     0.303985    -0.0875347   0.262609    -0.673758    0.331581     -0.106397   -0.500685     -0.11376     0.0272302  -0.323029     0.0371655     0.609878     0.287803     0.0411905   -0.427649     0.352228      0.483396    -0.153451    0.224084   -0.0873323    0.203561     -0.0130528    -0.157474    0.291822   
 -0.081396    -0.814274     0.154781    -0.136588    0.193899     0.240956   -0.336219     -0.365385   -0.429916     -0.280863    0.0466605  -0.497876     0.661622      0.302128     0.276298    -0.0670375    0.0394942    0.451908      0.0377778    0.121649    0.220702   -0.611483     0.40445      -0.145765      0.0648792   0.0225319  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.414046
INFO: iteration 2, average log likelihood -1.414031
INFO: iteration 3, average log likelihood -1.414017
INFO: iteration 4, average log likelihood -1.414002
INFO: iteration 5, average log likelihood -1.413988
INFO: iteration 6, average log likelihood -1.413974
INFO: iteration 7, average log likelihood -1.413960
INFO: iteration 8, average log likelihood -1.413947
INFO: iteration 9, average log likelihood -1.413933
INFO: iteration 10, average log likelihood -1.413920
INFO: EM with 100000 data points 10 iterations avll -1.413920
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.535020e+05
      1       7.100217e+05      -2.434803e+05 |       32
      2       6.978509e+05      -1.217070e+04 |       32
      3       6.926386e+05      -5.212347e+03 |       32
      4       6.898732e+05      -2.765416e+03 |       32
      5       6.880342e+05      -1.838995e+03 |       32
      6       6.866834e+05      -1.350801e+03 |       32
      7       6.856476e+05      -1.035828e+03 |       32
      8       6.848703e+05      -7.772470e+02 |       32
      9       6.842072e+05      -6.631323e+02 |       32
     10       6.836152e+05      -5.919520e+02 |       32
     11       6.831068e+05      -5.084039e+02 |       32
     12       6.826811e+05      -4.256841e+02 |       32
     13       6.823542e+05      -3.269806e+02 |       32
     14       6.820675e+05      -2.867078e+02 |       32
     15       6.818107e+05      -2.567653e+02 |       32
     16       6.815597e+05      -2.509924e+02 |       32
     17       6.813218e+05      -2.379021e+02 |       32
     18       6.810924e+05      -2.293719e+02 |       32
     19       6.808577e+05      -2.346891e+02 |       32
     20       6.806394e+05      -2.183541e+02 |       32
     21       6.804561e+05      -1.832473e+02 |       32
     22       6.802972e+05      -1.589529e+02 |       32
     23       6.801555e+05      -1.416672e+02 |       32
     24       6.800244e+05      -1.310794e+02 |       32
     25       6.799007e+05      -1.237081e+02 |       32
     26       6.797781e+05      -1.226498e+02 |       32
     27       6.796598e+05      -1.183107e+02 |       32
     28       6.795572e+05      -1.025697e+02 |       32
     29       6.794613e+05      -9.589727e+01 |       32
     30       6.793786e+05      -8.269242e+01 |       32
     31       6.793028e+05      -7.584295e+01 |       32
     32       6.792363e+05      -6.648143e+01 |       32
     33       6.791676e+05      -6.871514e+01 |       32
     34       6.791062e+05      -6.133929e+01 |       32
     35       6.790426e+05      -6.361376e+01 |       32
     36       6.789834e+05      -5.920140e+01 |       32
     37       6.789183e+05      -6.515814e+01 |       32
     38       6.788555e+05      -6.273148e+01 |       32
     39       6.787977e+05      -5.781543e+01 |       32
     40       6.787415e+05      -5.622835e+01 |       32
     41       6.786865e+05      -5.501188e+01 |       32
     42       6.786292e+05      -5.731379e+01 |       32
     43       6.785795e+05      -4.969677e+01 |       32
     44       6.785316e+05      -4.781549e+01 |       32
     45       6.784808e+05      -5.084444e+01 |       32
     46       6.784267e+05      -5.407169e+01 |       32
     47       6.783871e+05      -3.959670e+01 |       32
     48       6.783472e+05      -3.988490e+01 |       32
     49       6.783108e+05      -3.640228e+01 |       32
     50       6.782777e+05      -3.315327e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 678277.6854760884)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.425437
INFO: iteration 2, average log likelihood -1.420557
INFO: iteration 3, average log likelihood -1.419291
INFO: iteration 4, average log likelihood -1.418379
INFO: iteration 5, average log likelihood -1.417405
INFO: iteration 6, average log likelihood -1.416444
INFO: iteration 7, average log likelihood -1.415721
INFO: iteration 8, average log likelihood -1.415288
INFO: iteration 9, average log likelihood -1.415041
INFO: iteration 10, average log likelihood -1.414885
INFO: iteration 11, average log likelihood -1.414773
INFO: iteration 12, average log likelihood -1.414685
INFO: iteration 13, average log likelihood -1.414610
INFO: iteration 14, average log likelihood -1.414545
INFO: iteration 15, average log likelihood -1.414487
INFO: iteration 16, average log likelihood -1.414435
INFO: iteration 17, average log likelihood -1.414388
INFO: iteration 18, average log likelihood -1.414345
INFO: iteration 19, average log likelihood -1.414305
INFO: iteration 20, average log likelihood -1.414269
INFO: iteration 21, average log likelihood -1.414235
INFO: iteration 22, average log likelihood -1.414203
INFO: iteration 23, average log likelihood -1.414173
INFO: iteration 24, average log likelihood -1.414146
INFO: iteration 25, average log likelihood -1.414119
INFO: iteration 26, average log likelihood -1.414094
INFO: iteration 27, average log likelihood -1.414071
INFO: iteration 28, average log likelihood -1.414048
INFO: iteration 29, average log likelihood -1.414026
INFO: iteration 30, average log likelihood -1.414006
INFO: iteration 31, average log likelihood -1.413986
INFO: iteration 32, average log likelihood -1.413967
INFO: iteration 33, average log likelihood -1.413949
INFO: iteration 34, average log likelihood -1.413931
INFO: iteration 35, average log likelihood -1.413915
INFO: iteration 36, average log likelihood -1.413899
INFO: iteration 37, average log likelihood -1.413883
INFO: iteration 38, average log likelihood -1.413868
INFO: iteration 39, average log likelihood -1.413854
INFO: iteration 40, average log likelihood -1.413841
INFO: iteration 41, average log likelihood -1.413828
INFO: iteration 42, average log likelihood -1.413816
INFO: iteration 43, average log likelihood -1.413804
INFO: iteration 44, average log likelihood -1.413793
INFO: iteration 45, average log likelihood -1.413782
INFO: iteration 46, average log likelihood -1.413772
INFO: iteration 47, average log likelihood -1.413762
INFO: iteration 48, average log likelihood -1.413753
INFO: iteration 49, average log likelihood -1.413744
INFO: iteration 50, average log likelihood -1.413736
INFO: EM with 100000 data points 50 iterations avll -1.413736
59.0 data points per parameter
32Ã—26 Array{Float64,2}:
 -0.068341      0.353916    0.0248243    -0.374514     0.253834    0.0523805   0.462782    0.126043   -0.77234     0.338269   -0.108239    -0.229172    -0.347096     0.814376    0.11214      0.136486    -0.493224    0.576075     0.192674   -0.692474    0.316118     0.411077     0.383828    -0.123026      0.116924     0.233416 
 -0.377777     -0.699844   -0.438492      0.843511    -0.330365    0.126084    0.23235    -0.331477    0.272884   -0.505719   -0.0593976   -0.262949     0.499608     0.0156468  -0.56406     -0.596484     0.112553   -0.478128     0.16164     0.0205925  -0.0880275   -0.703541     0.46519     -0.891636     -0.0348444   -0.240932 
  0.0790514    -0.0567889  -0.243797      0.376129     0.245364   -0.131562   -0.062583    0.192591   -0.578258   -0.34544    -0.34194      0.0270723    0.365242     0.128023    0.329845    -0.243924    -0.240275    0.321973     0.578993   -0.527311    0.563684     0.248934     0.144254     0.302667     -0.0671909    0.105433 
  0.323697      0.83055     0.0203398    -0.141937    -0.217337    0.638366   -0.711879    0.308477    0.226575   -0.0619423   1.01917     -0.403036    -0.0969509    0.14195     0.572284     0.666188    -0.415337   -0.275554    -0.123005    0.38143     0.0322743    0.386756     0.26792      0.487556     -0.00270677   0.453206 
 -0.0505447    -0.652406    0.214573     -0.526852     0.484355   -0.0934235  -0.0541143  -0.0986415  -0.288926   -0.308096    0.242672    -0.706811     0.779486     0.416974    0.540152    -0.053078    -0.0557631   0.300423     0.0272192   0.229166   -0.019967    -0.503457     0.12829      0.0777138    -0.0468806    0.422435 
 -0.0400288     0.237532   -0.760073      0.312304     0.137984    0.45223     0.0506226   0.453602    0.442162    0.0168575  -6.22685e-5  -0.0168602    0.351924    -0.446009   -0.196997    -0.176221     0.434472   -0.680252    -0.119925    0.182595   -0.418658    -0.184923    -0.70894     -0.106578     -0.0500829    0.188356 
 -0.85722      -0.0859288  -0.111252      0.171816     0.0502351  -0.278645    0.0236616   0.0891439   0.342274   -0.177564    0.138258    -0.496812    -0.00385461   0.354795   -0.375553     0.348626    -0.244477    0.0791511    0.837743    0.118606   -0.109619     0.374745    -0.257818    -0.615067     -0.061708     0.0471681
  0.179434     -0.327962    0.212565      0.124208     0.434444    0.0256778   0.0846419  -0.0193578   0.136916    0.17966     0.115229    -0.00459574  -0.0320763    0.113007    0.138594     0.129351    -0.152954   -0.0544317    0.200963    0.308829   -0.115222    -0.265358    -0.0684297    0.0331964    -0.315838     0.0356661
  0.0500125    -1.07141    -0.322979     -0.291929    -0.0715501  -0.585213   -0.127649   -0.376113   -0.123908    0.0372967  -0.477273     0.449389     0.0547993    0.0563947  -0.378961    -0.3827       0.158997    0.0551244   -0.61351     0.294104    0.304407    -0.733053    -0.394065    -0.265126     -0.212892    -0.0675641
  0.340057     -0.44871     0.313705      0.531845    -0.305183   -0.288341    0.0269373  -0.205362    0.534966   -0.0647977   0.275835     0.0903383    0.187401    -0.243444    0.00544757   0.373156     0.0455822  -0.010952    -0.26044     0.557608   -0.684451    -0.37917     -0.372613    -0.0539622     0.0437872   -0.131058 
  0.387143      0.0782904   0.287198      0.159525     0.0822163   0.191983    0.0591098   0.116543   -0.342269    0.345406   -0.683107     0.605876     0.13379     -0.368016    0.34941      0.117473     0.316502    0.0333966   -0.524342   -0.294709    0.149018    -0.31579      0.00164409   0.680895      0.0549245    0.0744553
  0.137584     -0.289078    0.198079     -0.14209      0.664253   -0.21193     0.433069   -0.132261    0.277159    0.0601605  -0.154744    -0.0343204   -2.06599e-5   0.100126   -0.188748     0.0219482   -0.248605   -0.137334     0.0638071   0.158295   -0.0213398   -0.372131    -0.0455121    0.0891654    -0.384426    -0.232011 
  0.000999336   0.192401   -0.821197     -0.284468    -0.191522    0.0483214  -0.251468    0.600136   -0.279149   -0.283224   -0.617694    -0.241549    -0.164902    -0.419525   -0.474377    -0.280709    -0.151601   -0.0390525   -0.135265   -0.651947    0.719352     0.391361     0.0107609    0.303076     -0.731624     0.30677  
 -0.725211     -0.412473   -0.291658     -0.384831     0.0321748   0.114715   -0.61851    -0.323281    0.561407   -0.827246    0.576397     0.723946     0.899444    -0.363933    0.244964    -0.42292      0.661401   -0.455028    -0.379899    0.693182   -0.647613     0.257082     0.30073     -0.000496015   0.0557497    0.232423 
 -1.22214       0.393459   -0.284485     -0.00537357  -0.0896594   0.187004    0.807807   -0.714152   -0.264888    0.233429    0.756644     0.413498    -0.290006     0.309831   -0.136905     0.27751     -0.0462242  -0.139506    -0.355239    0.243267   -0.894704    -0.1251       0.567527    -0.114486      0.283666    -0.288906 
  1.24801       0.0773416   0.320387      0.65019     -0.12662    -0.211655    0.485921    0.0549212   0.0482242   0.468354   -0.124242    -0.60312     -0.142458     0.104695   -0.0195116   -0.164801    -0.423576    0.0433881    0.645096   -0.396619    0.666911    -0.241126    -0.310089    -0.0707027     0.417203    -0.210579 
  0.183462      0.398431    0.000339037   0.171912    -0.524511    0.046596    0.0760589  -0.116405    0.586671    0.217961   -0.193798     0.717591    -0.866851    -0.326373   -0.354493    -0.195501     0.191736   -0.261994    -0.140501   -0.145641    0.181108     0.00707888  -0.54262      0.166635     -0.0574981   -0.227299 
 -0.0156491     0.280869   -0.383002      0.00813705  -0.443301    0.063732    0.129831    0.022779    0.0764124  -0.137607   -0.329622     0.108801    -0.20808     -0.278391   -0.534442    -0.149593    -0.0359505   0.0465212   -0.296134   -0.246431    0.221011     0.264158    -0.00242413  -0.0186408     0.0255468   -0.156639 
 -0.541792     -0.132862   -0.173209     -0.586576    -0.172081   -0.450831    0.0695436  -0.17484     0.0162668  -0.438501    0.118091    -0.175996     0.236171     0.0591095  -0.370903    -0.664051    -0.284929   -0.610061    -0.0339253   0.322885   -0.0541336    0.340839     0.198326    -0.142607     -0.100447     0.598031 
 -0.0412681    -0.0482777   0.0416818    -0.0130602   -0.0537091   0.0318155  -0.0231959  -0.0229833  -0.043811   -0.0116824   0.0417099    0.00299359   0.0937632    0.0551863   0.144603     0.0953289   -0.0262773   0.00857981  -0.0546848  -0.0512503  -0.0365137    0.0518506    0.0526215    0.0418494     0.0518477    0.125853 
 -0.12349       0.14044    -0.301995     -0.20542      0.0422762  -0.012825    0.558682   -0.559189    0.0577262  -0.0286111   0.189936    -0.518491    -0.561147    -0.566334    0.123847    -0.192022     0.277382    0.0441828   -0.425691    0.102744   -0.00451287  -0.901327    -0.720965     0.186131      0.235747     0.0536108
 -0.389312     -0.0435145   0.312944     -0.340487     0.337189    0.0212628   0.130753    0.258666    0.113812    0.949761    0.166786     0.24719      0.0441805   -0.402608    0.529346     0.394802     0.126437   -0.723105     0.433719   -0.432916   -0.176569    -0.101635     0.60135      0.0226567    -0.0943309    0.543105 
  0.0339069     0.0356551   0.424575     -0.135924    -0.0224011  -0.0718034  -0.169155    0.337467   -0.042949   -0.377135   -0.502084     0.00288534   0.507715     0.551574   -0.0351424    0.229812    -0.722368    0.190113    -0.0404972  -0.0530309  -0.0342517    1.0941       0.951696    -0.0859328    -0.152155    -0.2039   
  0.0698546     0.0422337   0.430138      0.257621     0.240159    0.0186074   0.388604   -0.226077   -0.228184    0.28829     0.541036     0.72214     -0.0849177    0.586361    0.782526     0.263955     0.0516502   0.0973483   -0.205517    0.409912   -0.567646     0.254063    -0.158686    -0.175975      1.00514     -0.456392 
  0.298724     -0.12596     0.275119     -0.0864082    0.105208   -0.126617   -0.180489    0.345685    0.224785    0.359066    0.11108      0.156477    -0.681762     0.386692   -0.383791     0.293677     0.106622   -0.227637     0.0677093   0.388566    0.289016    -0.0959517   -0.577519     0.00622751   -0.544607     0.35016  
  0.601207     -0.352957   -0.0710047    -0.0878819    0.141684    0.58169    -0.581167   -0.263176    0.13981     0.365198    0.571907     0.127995    -0.827527     0.0204619   0.405588    -0.588559     0.0432712  -0.198717    -0.153425    0.150365    0.138224    -0.155954    -0.315271    -0.318092     -0.241158    -0.0833919
 -0.920575      0.506085   -0.102802     -0.244518    -0.581627   -0.273376   -0.254414   -0.498917   -0.0161692  -0.487319   -0.368591     0.0328538    0.407971    -0.215567    0.0211158    0.297393     0.434273   -0.169751    -0.0898486  -0.599479   -0.619987     0.00989698  -0.118897     0.712405     -0.604931    -0.239298 
 -0.170275     -0.174032   -0.259623     -0.169263    -0.552753    0.394805   -0.933755    0.271504   -0.389345    0.0518695  -0.209354     0.247175     0.300786    -0.0650925   0.0930605    0.107414     0.128007    0.246595    -0.144462   -0.215884    0.349035     0.209125     0.247239    -0.367288      0.0901406    0.262135 
 -0.159739      0.228093   -0.0943788    -0.225222    -0.646196   -0.116637    0.157237   -0.270017   -0.327063   -0.586472   -0.0213337   -0.104354     0.287482    -0.0807325   0.022985     0.0375717    0.199682    0.359467    -0.595091   -0.328956   -0.0783277    0.255737     0.0422168   -0.08004       0.935025     0.115724 
  0.0902678    -0.116226    0.522904     -0.0479937   -0.155452   -0.200014    0.0112968  -0.692514   -0.194217   -0.604439    0.332559    -0.410282    -0.503605     0.190422   -0.0908984   -0.00888426  -0.153092    0.352979     0.0242153  -0.0842836   0.477101     0.140074     0.44329      0.316086      0.129623    -0.340186 
 -0.208536     -0.196512    0.886353      0.136146     0.225883   -0.235149   -0.504627   -0.293249    0.551342   -0.141346    0.18261      0.181327     0.906843     0.522864    0.11318     -0.0527827    0.0452214  -0.297315     0.91477     1.09066     0.0566663   -0.51624      0.634138    -0.337155      0.677031    -0.478645 
 -0.0482222     1.08402     0.123226      0.315093    -0.0678301   0.390303    0.367042    0.428694    0.20727    -0.0321844   0.0887314    0.0571931   -0.00965061  -0.177803    0.30031      0.351634    -0.0754602   0.130983     0.249788   -0.358047   -0.188751     0.786668    -0.152636     0.375427      0.177763     0.0802959INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.413727
INFO: iteration 2, average log likelihood -1.413720
INFO: iteration 3, average log likelihood -1.413712
INFO: iteration 4, average log likelihood -1.413705
INFO: iteration 5, average log likelihood -1.413698
INFO: iteration 6, average log likelihood -1.413691
INFO: iteration 7, average log likelihood -1.413684
INFO: iteration 8, average log likelihood -1.413678
INFO: iteration 9, average log likelihood -1.413672
INFO: iteration 10, average log likelihood -1.413666
INFO: EM with 100000 data points 10 iterations avll -1.413666
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
