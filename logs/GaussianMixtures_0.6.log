>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.7.0
INFO: Installing JLD v0.6.6
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.1
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.3.0
INFO: Installing ScikitLearnBase v0.2.1
INFO: Installing StaticArrays v0.1.0
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
WARNING: takebuf_string(b) is deprecated, use String(take!(b)) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in takebuf_string(::Base.AbstractIOBuffer{Array{UInt8,1}}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/BinDeps/src/dependencies.jl:887 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::SubString{String}) at ./sysimg.jl:14
 in evalfile(::SubString{String}, ::Array{String,1}) at ./loading.jl:572 (repeats 2 times)
 in cd(::##2#4, ::String) at ./file.jl:69
 in (::##1#3)(::IOStream) at ./none:13
 in open(::##1#3, ::String, ::String) at ./iostream.jl:152
 in eval(::Module, ::Any) at ./boot.jl:236
 in process_options(::Base.JLOptions) at ./client.jl:248
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/Rmath/deps/build.jl, in expression starting on line 39
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1321
Commit a4612cc (2016-11-27 23:20 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-101-generic #148-Ubuntu SMP Thu Oct 20 22:08:32 UTC 2016 x86_64 x86_64
Memory: 2.939281463623047 GB (660.4453125 MB free)
Uptime: 27361.0 sec
Load Avg:  1.0546875  1.001953125  0.98095703125
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3511 MHz    1602370 s       4568 s     155357 s     700464 s         40 s
#2  3511 MHz     845905 s       1817 s      84225 s    1717400 s          1 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.4
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.7.0
 - JLD                           0.6.6
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.1
 - Rmath                         0.1.4
 - SHA                           0.3.0
 - ScikitLearnBase               0.2.1
 - StaticArrays                  0.1.0
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: takebuf_string(b) is deprecated, use String(take!(b)) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in takebuf_string(::Base.AbstractIOBuffer{Array{UInt8,1}}) at ./deprecated.jl:50
 in #_write#17(::Array{Any,1}, ::Function, ::JLD.JldFile, ::String, ::Array{Any,1}, ::JLD.JldWriteSession) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:587
 in #write#14(::Array{Any,1}, ::Function, ::JLD.JldFile, ::String, ::Array{Any,1}, ::JLD.JldWriteSession) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:529
 in #jldopen#9(::Bool, ::Bool, ::Bool, ::Function, ::String, ::Bool, ::Bool, ::Bool, ::Bool, ::Bool) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:198
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::String, ::Bool, ::Bool, ::Bool, ::Bool, ::Bool) at ./<missing>:0
 in #jldopen#10(::Bool, ::Bool, ::Bool, ::Function, ::String, ::String) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:253
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::String, ::String) at ./<missing>:0
 in #jldopen#11(::Array{Any,1}, ::Function, ::JLD.##34#35{String,Array{Float64,2},Tuple{}}, ::String, ::Vararg{String,N}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:263
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::Function, ::String, ::String) at ./<missing>:0
 in #save#33(::Bool, ::Bool, ::Function, ::FileIO.File{FileIO.DataFormat{:JLD}}, ::String, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:1217
 in save(::FileIO.File{FileIO.DataFormat{:JLD}}, ::String, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:1214
 in #save#14(::Array{Any,1}, ::Function, ::String, ::String, ::Vararg{Any,N}) at /home/vagrant/.julia/v0.6/FileIO/src/loadsave.jl:54
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:8 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:66 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:66 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:66 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:66 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:66 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:66 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-5.381606483200318e6,[98697.8,1302.23],
[36.3293 239.929 3622.69; -328.294 -32.9578 -3297.81],

Array{Float64,2}[
[98209.1 76.587 -654.24; 76.587 99317.9 -372.447; -654.24 -372.447 91195.8],

[1317.82 40.9045 721.359; 40.9045 1279.33 35.3136; 721.359 35.3136 8478.01]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.229445e+03
      1       1.069246e+03      -1.601986e+02 |        7
      2       1.010689e+03      -5.855749e+01 |        2
      3       1.004448e+03      -6.240775e+00 |        2
      4       9.928305e+02      -1.161720e+01 |        2
      5       9.889875e+02      -3.843012e+00 |        2
      6       9.663962e+02      -2.259136e+01 |        2
      7       9.555760e+02      -1.082018e+01 |        2
      8       9.541178e+02      -1.458211e+00 |        0
      9       9.541178e+02       0.000000e+00 |        0
K-means converged with 9 iterations (objv = 954.1177740317466)
INFO: K-means with 272 data points using 9 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.069380
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.716910
INFO: iteration 2, lowerbound -3.557368
INFO: iteration 3, lowerbound -3.401670
INFO: iteration 4, lowerbound -3.248756
INFO: iteration 5, lowerbound -3.116293
INFO: iteration 6, lowerbound -3.018863
INFO: dropping number of Gaussions to 6
INFO: iteration 7, lowerbound -2.946544
INFO: dropping number of Gaussions to 4
INFO: iteration 8, lowerbound -2.855298
INFO: iteration 9, lowerbound -2.742810
INFO: iteration 10, lowerbound -2.622404
INFO: iteration 11, lowerbound -2.510424
INFO: iteration 12, lowerbound -2.426234
INFO: iteration 13, lowerbound -2.373916
INFO: dropping number of Gaussions to 3
INFO: iteration 14, lowerbound -2.338514
INFO: iteration 15, lowerbound -2.314364
INFO: iteration 16, lowerbound -2.307396
INFO: dropping number of Gaussions to 2
INFO: iteration 17, lowerbound -2.302942
INFO: iteration 18, lowerbound -2.299262
INFO: iteration 19, lowerbound -2.299257
INFO: iteration 20, lowerbound -2.299255
INFO: iteration 21, lowerbound -2.299254
INFO: iteration 22, lowerbound -2.299253
INFO: iteration 23, lowerbound -2.299253
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Mon 28 Nov 2016 01:05:36 PM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Mon 28 Nov 2016 01:05:37 PM UTC: K-means with 272 data points using 9 iterations
11.3 data points per parameter
,Mon 28 Nov 2016 01:05:39 PM UTC: EM with 272 data points 0 iterations avll -2.069380
5.8 data points per parameter
,Mon 28 Nov 2016 01:05:40 PM UTC: GMM converted to Variational GMM
,Mon 28 Nov 2016 01:05:42 PM UTC: iteration 1, lowerbound -3.716910
,Mon 28 Nov 2016 01:05:42 PM UTC: iteration 2, lowerbound -3.557368
,Mon 28 Nov 2016 01:05:42 PM UTC: iteration 3, lowerbound -3.401670
,Mon 28 Nov 2016 01:05:42 PM UTC: iteration 4, lowerbound -3.248756
,Mon 28 Nov 2016 01:05:42 PM UTC: iteration 5, lowerbound -3.116293
,Mon 28 Nov 2016 01:05:42 PM UTC: iteration 6, lowerbound -3.018863
,Mon 28 Nov 2016 01:05:42 PM UTC: dropping number of Gaussions to 6
,Mon 28 Nov 2016 01:05:42 PM UTC: iteration 7, lowerbound -2.946544
,Mon 28 Nov 2016 01:05:42 PM UTC: dropping number of Gaussions to 4
,Mon 28 Nov 2016 01:05:42 PM UTC: iteration 8, lowerbound -2.855298
,Mon 28 Nov 2016 01:05:43 PM UTC: iteration 9, lowerbound -2.742810
,Mon 28 Nov 2016 01:05:43 PM UTC: iteration 10, lowerbound -2.622404
,Mon 28 Nov 2016 01:05:43 PM UTC: iteration 11, lowerbound -2.510424
,Mon 28 Nov 2016 01:05:43 PM UTC: iteration 12, lowerbound -2.426234
,Mon 28 Nov 2016 01:05:43 PM UTC: iteration 13, lowerbound -2.373916
,Mon 28 Nov 2016 01:05:43 PM UTC: dropping number of Gaussions to 3
,Mon 28 Nov 2016 01:05:43 PM UTC: iteration 14, lowerbound -2.338514
,Mon 28 Nov 2016 01:05:43 PM UTC: iteration 15, lowerbound -2.314364
,Mon 28 Nov 2016 01:05:43 PM UTC: iteration 16, lowerbound -2.307396
,Mon 28 Nov 2016 01:05:43 PM UTC: dropping number of Gaussions to 2
,Mon 28 Nov 2016 01:05:43 PM UTC: iteration 17, lowerbound -2.302942
,Mon 28 Nov 2016 01:05:43 PM UTC: iteration 18, lowerbound -2.299262
,Mon 28 Nov 2016 01:05:43 PM UTC: iteration 19, lowerbound -2.299257
,Mon 28 Nov 2016 01:05:43 PM UTC: iteration 20, lowerbound -2.299255
,Mon 28 Nov 2016 01:05:43 PM UTC: iteration 21, lowerbound -2.299254
,Mon 28 Nov 2016 01:05:43 PM UTC: iteration 22, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:44 PM UTC: iteration 23, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:44 PM UTC: iteration 24, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:44 PM UTC: iteration 25, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:44 PM UTC: iteration 26, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:44 PM UTC: iteration 27, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:44 PM UTC: iteration 28, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:44 PM UTC: iteration 29, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:44 PM UTC: iteration 30, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:44 PM UTC: iteration 31, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:44 PM UTC: iteration 32, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:44 PM UTC: iteration 33, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:44 PM UTC: iteration 34, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:44 PM UTC: iteration 35, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:44 PM UTC: iteration 36, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:45 PM UTC: iteration 37, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:45 PM UTC: iteration 38, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:45 PM UTC: iteration 39, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:45 PM UTC: iteration 40, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:45 PM UTC: iteration 41, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:45 PM UTC: iteration 42, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:45 PM UTC: iteration 43, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:45 PM UTC: iteration 44, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:45 PM UTC: iteration 45, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:45 PM UTC: iteration 46, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:45 PM UTC: iteration 47, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:45 PM UTC: iteration 48, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:45 PM UTC: iteration 49, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:46 PM UTC: iteration 50, lowerbound -2.299253
,Mon 28 Nov 2016 01:05:46 PM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.9549,178.045]
β = [95.9549,178.045]
m = [2.00023 53.852; 4.2503 79.2869]
ν = [97.9549,180.045]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.375876 -0.00895312; 0.0 0.0127487],

[0.184042 -0.00764405; 0.0 0.00858171]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -1.0031324271629618
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -1.0031324271629602
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -1.0031324271629602
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -0.9850684964923242
avll from llpg:  -0.9850684964923242
avll direct:     -0.9850684964923242
sum posterior: 100000.0
32×26 Array{Float64,2}:
 -0.148806     0.106926     0.0830362   -0.037304     0.067333      0.0409804    0.0141716    0.200404     0.184667    0.0634178   -0.138537    -0.0454699    -0.0200456    -0.00374962  -0.005516    -0.0107379   -0.0240577   -0.134918      0.118139    -0.000353868   0.175213    -0.161222      0.0489129   -0.0590128   -0.303233     0.234923  
 -0.0319465   -0.0749915    0.0623656   -0.113424     0.103212     -0.0335381    0.0206514    0.11395     -0.0716538  -0.00754653   0.171385    -0.11936      -0.117703      0.0472411   -0.00520775  -0.00698833   0.00273859   0.0486607    -0.0477348   -0.0387249     0.0153587    0.0401755    -0.0460511   -0.115587    -0.102353    -0.0709249 
  0.107426    -0.0713807    0.0568066   -0.0350011    0.0327802    -0.0773031   -0.0702236    0.0580048    0.0530252   0.0215464    0.0513848   -0.00149696   -0.125504      0.00184323   0.126727     0.0574719    0.0890368    0.0521971     0.0559196    0.00409668    0.196052     0.0850473    -0.163881     0.215009     0.0200098   -0.0413842 
  0.0334365    0.0870093    0.0944113   -0.127574    -0.181766     -0.111303     0.0188082    0.137761     0.0393627   0.0668244   -0.0280105   -0.000823393  -0.131321     -0.0359089    0.155478    -0.0905304    0.00484516  -0.000299602   0.014019     0.0217085     0.0568675   -0.0783151     0.00513101  -0.0372927   -0.153699    -0.248228  
 -0.00971835  -0.0773128   -0.00612067  -0.00467711  -0.049797      0.0452491   -0.0582286   -0.0977888    0.0771167  -0.00784321  -0.108247    -0.0526576    -0.117764      0.00586384  -0.0813538   -0.0251426    0.069389     0.102591     -0.208539    -0.038334     -0.105928    -0.0476074     0.118326    -0.103228    -0.140372     0.159233  
  0.0881443   -0.0827407   -0.048715     0.0190732   -0.0965433     0.130468     0.103837     0.193896    -0.117431    0.0327346   -0.1097      -0.191398      0.125741     -0.139041     0.0485132   -0.166428    -0.00691854   0.017377     -0.0283533    0.0481026     0.102275    -0.0783211    -0.0172845    0.0606806    0.114266     0.158981  
  0.215038    -0.0120461    0.0189999    0.176796     0.12285      -0.00513964   0.0399282   -0.030629     0.198565    0.00638181   0.0228164    0.0700545     0.0647523     0.0957143    0.0889164    0.0566287   -0.00881952   0.215833      0.116179     0.178536     -0.0253662    0.00566115   -0.0305376    0.204718    -0.0776041    0.0975399 
 -0.0606413   -0.0169927   -0.0170411    0.0940897    0.0232729     0.170753     0.0326716   -0.0210486    0.128652    0.175284    -0.0642396   -0.224838      0.117988     -6.69889e-6  -0.175068     0.00555678   0.244112    -0.0806204     0.0303431   -0.107553      0.0643173   -0.0601466     0.00584098  -0.0463114    0.0774264    0.120166  
 -0.0784492    0.0358329    0.226414    -0.0524223   -0.00554158    0.066868    -0.0236508   -0.0834461   -0.0954648  -0.157975     0.0193199    0.144717      0.0366559    -0.110723     0.0973817    0.0434718    0.0197675    0.141601      0.0179576   -0.0750298    -0.0320482   -0.133813      0.0715773    0.114756     0.109356     0.0870174 
  0.0790617    0.131177     0.231141    -0.00423911   0.0825649     0.0958876   -0.249771    -0.0342083    0.139269   -0.0524992   -0.0524949    0.189828     -0.06877      -0.0722376    0.137223    -0.0212265    0.0843971    0.0322852    -0.111156    -0.119306     -0.219623     0.211257      0.194908    -0.0833631    0.247351     0.00557545
  0.00476153   0.108956     0.125062     0.0171551    0.121243     -0.131222    -0.0586152    0.102194     0.0652342  -0.0354386    0.00697855  -0.04808      -0.033392      0.03328      0.0490495   -0.00292913  -0.0669361   -0.110876      0.0540203    0.0378111    -0.0352947   -0.000574829   0.0263599    0.0563101    0.00143208  -0.162214  
 -0.0254172   -0.0667784    0.00281935  -0.0215593    0.0751395     0.0814577    0.0772446    0.0389261    0.152442    0.0185465   -0.0514421   -0.194507      0.117363      0.0727082    0.0115233   -0.20922     -0.0488866    0.123654      0.129632     0.00241416    0.00674215   0.0720117    -0.00907059   0.107136    -0.108134    -0.0319014 
 -0.080141    -0.0903487    0.168627     0.191553    -0.00424545   -0.0704368    0.0479464   -0.0166901   -0.0597895   0.0714837    0.020564     0.0387593    -0.073239     -0.0718292   -0.0921547    0.165481     0.0237373   -0.0143297     0.0880968   -0.267267      0.0277093   -0.100209      0.0407979    0.113707    -0.0547416    0.0654167 
 -0.261793    -0.00866915   0.0515793   -0.043032     0.0984803     0.0907464    0.0505011    0.0404049   -0.221402   -0.149447     0.0011977   -0.243732     -0.118349     -0.0141212   -0.00398477  -0.105739    -0.0882506    0.14234       0.111039    -0.0843121    -0.0164714    0.143411     -0.0648996   -0.0938781   -0.0899091    0.072906  
  0.0412508   -0.0727854    0.0129952    0.0540744    0.0383396     0.0193832    0.0136961   -0.0116736    0.125995    0.0594174   -0.1147       0.0199583    -0.0540618     0.00757225   0.117923    -0.0267294    0.0704874   -0.127507     -0.0877841    0.0376365     0.18058      0.156122      0.035135     0.0786405   -0.0531517   -0.00116912
 -0.022565    -0.188516    -0.166383    -0.164851    -0.198842     -0.0800259   -0.00357112   0.0824599    0.0927544   0.274042     0.00612691   0.0524454    -0.000722809   0.119812    -0.171033    -0.0600873    0.14948      0.081685      0.041889     0.0832268     0.101211    -0.0734076     0.0688968   -0.0327674   -0.0652443   -0.0395517 
 -0.0363221    0.0732432    0.149689    -0.00384166  -0.107677      0.0810275   -0.0469013    0.0186829    0.0933089  -0.0222952    0.0878853   -0.235545     -0.020811      0.0792507    0.103172     0.138982     0.0296664   -0.149303     -0.110917    -0.190463      0.194126    -0.113774      0.036368     0.10498     -0.0769057    0.0447866 
  0.210808    -0.166881     0.107509     0.118853     0.0145673    -0.171157    -0.0273886    0.126363    -0.0677911   0.159292     0.0579522    0.211815      0.0289807     0.0356884    0.112745    -0.0357938   -0.00466519  -0.0612284    -0.13572     -0.00805519   -0.0546932    0.0487306    -0.0873925    0.108739    -0.192609     0.1044    
 -0.0103264   -0.061411     0.247039    -0.00290544   0.0789891    -0.0417168    0.0382201    0.199886    -0.193123   -0.0141765   -0.0660541    0.256088      0.149722     -0.0304574   -0.0764617    0.00849289   0.224094    -0.0772199    -0.0660622   -0.11305       0.131584    -0.0713385    -0.220884    -0.00166533  -0.0588165   -0.0589417 
 -0.0174328   -0.143614     0.108547    -0.0469269   -0.0159044     0.115972    -0.156932    -0.0301554    0.0327207   0.0488561   -0.106258    -0.0754884     0.169082      0.0691132   -0.0826485    0.139232    -0.0443912   -0.00715891    0.203752     0.154246     -0.0143528   -0.0278388    -0.0618078    0.00410602   0.0287056    0.072497  
 -0.0924569    0.0623539   -0.0677497    0.162236     0.06137      -0.0484958   -0.0977918    0.203157     0.0721106  -0.127745    -0.0668965   -0.00732193   -0.0801751     0.131118    -0.0577084    0.0171222   -0.032219     0.00541207    0.0339145   -0.0260792    -0.137477    -0.00278907   -0.0258424   -0.21179     -0.0797867   -0.103858  
 -0.101239    -0.0449609    0.0529545    0.127058    -0.0240468     0.0116563   -0.0253005    0.108796     0.0068086   0.0759392   -0.151528     0.104952      0.195737     -0.171566     0.0723826   -0.0448163   -0.0972173   -0.119494     -0.0580439    0.0497666    -0.00521662  -0.00254212   -0.0120726   -0.087879    -0.100121     0.0187142 
  0.0478962    0.0277818    0.0552195    0.231952    -0.0981218    -0.124351     0.0243268   -0.0147094    0.129994    0.055272    -0.0830036    0.0281457    -0.233592      0.166806     0.00766249  -0.0154909   -0.0709394   -0.0681514    -0.115431     0.165001      0.098265     0.0145348    -0.186871     0.0941937   -0.0561377    0.0733281 
 -0.0430934    0.0122596    0.0290763    0.0531322   -0.0265046    -0.157978    -0.154346     0.0861319    0.0970122  -0.0176385   -0.184902    -0.0994197     0.0884542     0.0249425    0.0275544    0.120702     0.164921     0.135339     -0.0243451    0.00815024    0.0235164    0.0395672     0.0994181   -0.0444102    0.0704935    0.238207  
 -0.147025    -0.096286     0.0420999   -0.0574615    0.0979071     0.0204355    0.0261303   -0.0247093   -0.181729   -0.0806376   -0.122704    -0.0899115     0.158392     -0.107587    -0.0663227    0.179036    -0.0584645    0.0123886    -0.107531    -0.149637     -0.06091     -0.114492      0.1423      -0.166474    -0.0680707   -0.0754871 
  0.0172271   -0.0305192   -0.123852     0.10067     -0.201024     -0.192097    -0.0867886    0.11206      0.085748   -0.0239183    0.201597     0.0733781    -0.0383699     0.0578711   -0.016267     0.294076    -0.00846875   0.0166177    -0.0240746   -0.0511866     0.128303     0.149056     -0.0423251    0.00645118  -0.137557    -0.091529  
  0.0541236   -0.0681511    0.0107387   -0.0188775   -0.0895814    -0.108413    -0.0304576    0.0340515    0.0241547   0.0025653   -0.0698727    0.066077      0.0673661    -0.0501059    0.103336    -0.0865308   -0.106613     0.244562      0.144082    -0.086086     -0.23441     -0.0123773    -0.043405     0.0270801    0.00971837  -0.0214862 
 -0.0592016   -0.1093      -0.0625517   -0.130699    -0.0375478     0.00286812   0.0033706    0.103411    -0.0107613  -0.215635     0.115659    -0.0675221     0.0964852    -0.0783896   -0.0806523   -0.0976154    0.0659116   -0.0522704     0.155649    -0.0742111     0.0130677   -0.0898997     0.0373927    0.0340822    0.155884     0.0568213 
  0.0721281    0.0681652   -0.0335239    0.0493507    0.127097     -0.00573268  -0.130305    -0.00746564   0.205243   -0.0600655    0.061297    -0.231911      0.124229      0.0277753   -0.0393228    0.00720172  -0.110079    -0.104641     -0.016626    -0.0510519     0.0457101    0.046084      0.199868     0.126642     0.0109058   -0.0248422 
 -0.00357793  -0.0838725   -0.161252    -0.198654     0.000793318  -0.00541834  -0.0425119   -0.0905152    0.170397   -0.0407172   -0.0429835   -0.0487832     0.0543226     0.161865    -0.0927395    0.0137219    0.0407829    0.0642307     0.00380711   0.239343      0.0508887    0.0420611    -0.0880871    0.0817502   -0.0617352    0.17111   
  0.0683018    0.136082    -0.00935778   0.00695528  -0.031181      0.149065    -0.0294837   -0.0799091   -0.126769   -0.0248256    0.0378403   -0.00769487   -0.0388455    -0.05        -0.0316649   -0.0814159    0.025219     0.0333696    -0.151756    -0.0104567     0.0651795   -0.0299338     0.0784473   -0.0532872    0.097561     0.0101244 
 -0.231592    -0.076272     0.00774656  -0.106081    -0.0370512     0.0148684    0.0545918   -0.0702583   -0.126091   -0.13396     -0.0523329   -0.13133      -0.190951      0.00731389  -0.00488881   0.0167046    0.190115    -0.0571852    -0.0593447    0.0836172     0.0908303    0.0646601     0.00838161  -0.0334508   -0.0274704    0.0372012 kind diag, method split
0: avll = -1.3997855585975774
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.399880
INFO: iteration 2, average log likelihood -1.399806
INFO: iteration 3, average log likelihood -1.399400
INFO: iteration 4, average log likelihood -1.393771
INFO: iteration 5, average log likelihood -1.374385
INFO: iteration 6, average log likelihood -1.364656
INFO: iteration 7, average log likelihood -1.362391
INFO: iteration 8, average log likelihood -1.361255
INFO: iteration 9, average log likelihood -1.360558
INFO: iteration 10, average log likelihood -1.360186
INFO: iteration 11, average log likelihood -1.360003
INFO: iteration 12, average log likelihood -1.359915
INFO: iteration 13, average log likelihood -1.359869
INFO: iteration 14, average log likelihood -1.359843
INFO: iteration 15, average log likelihood -1.359827
INFO: iteration 16, average log likelihood -1.359815
INFO: iteration 17, average log likelihood -1.359807
INFO: iteration 18, average log likelihood -1.359800
INFO: iteration 19, average log likelihood -1.359794
INFO: iteration 20, average log likelihood -1.359790
INFO: iteration 21, average log likelihood -1.359786
INFO: iteration 22, average log likelihood -1.359783
INFO: iteration 23, average log likelihood -1.359781
INFO: iteration 24, average log likelihood -1.359779
INFO: iteration 25, average log likelihood -1.359778
INFO: iteration 26, average log likelihood -1.359776
INFO: iteration 27, average log likelihood -1.359775
INFO: iteration 28, average log likelihood -1.359775
INFO: iteration 29, average log likelihood -1.359774
INFO: iteration 30, average log likelihood -1.359773
INFO: iteration 31, average log likelihood -1.359773
INFO: iteration 32, average log likelihood -1.359772
INFO: iteration 33, average log likelihood -1.359772
INFO: iteration 34, average log likelihood -1.359772
INFO: iteration 35, average log likelihood -1.359771
INFO: iteration 36, average log likelihood -1.359771
INFO: iteration 37, average log likelihood -1.359771
INFO: iteration 38, average log likelihood -1.359770
INFO: iteration 39, average log likelihood -1.359770
INFO: iteration 40, average log likelihood -1.359770
INFO: iteration 41, average log likelihood -1.359769
INFO: iteration 42, average log likelihood -1.359769
INFO: iteration 43, average log likelihood -1.359768
INFO: iteration 44, average log likelihood -1.359767
INFO: iteration 45, average log likelihood -1.359766
INFO: iteration 46, average log likelihood -1.359765
INFO: iteration 47, average log likelihood -1.359763
INFO: iteration 48, average log likelihood -1.359760
INFO: iteration 49, average log likelihood -1.359755
INFO: iteration 50, average log likelihood -1.359748
INFO: EM with 100000 data points 50 iterations avll -1.359748
952.4 data points per parameter
1: avll = [-1.39988,-1.39981,-1.3994,-1.39377,-1.37439,-1.36466,-1.36239,-1.36125,-1.36056,-1.36019,-1.36,-1.35991,-1.35987,-1.35984,-1.35983,-1.35982,-1.35981,-1.3598,-1.35979,-1.35979,-1.35979,-1.35978,-1.35978,-1.35978,-1.35978,-1.35978,-1.35978,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35976,-1.35976,-1.35976,-1.35976,-1.35975]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.359871
INFO: iteration 2, average log likelihood -1.359699
INFO: iteration 3, average log likelihood -1.358582
INFO: iteration 4, average log likelihood -1.349194
INFO: iteration 5, average log likelihood -1.330586
INFO: iteration 6, average log likelihood -1.320287
INFO: iteration 7, average log likelihood -1.315992
INFO: iteration 8, average log likelihood -1.313490
INFO: iteration 9, average log likelihood -1.311839
INFO: iteration 10, average log likelihood -1.310708
INFO: iteration 11, average log likelihood -1.309958
INFO: iteration 12, average log likelihood -1.309458
INFO: iteration 13, average log likelihood -1.309100
INFO: iteration 14, average log likelihood -1.308818
INFO: iteration 15, average log likelihood -1.308582
INFO: iteration 16, average log likelihood -1.308372
INFO: iteration 17, average log likelihood -1.308169
INFO: iteration 18, average log likelihood -1.307959
INFO: iteration 19, average log likelihood -1.307733
INFO: iteration 20, average log likelihood -1.307501
INFO: iteration 21, average log likelihood -1.307270
INFO: iteration 22, average log likelihood -1.307036
INFO: iteration 23, average log likelihood -1.306806
INFO: iteration 24, average log likelihood -1.306576
INFO: iteration 25, average log likelihood -1.306343
INFO: iteration 26, average log likelihood -1.306114
INFO: iteration 27, average log likelihood -1.305898
INFO: iteration 28, average log likelihood -1.305695
INFO: iteration 29, average log likelihood -1.305502
INFO: iteration 30, average log likelihood -1.305317
INFO: iteration 31, average log likelihood -1.305134
INFO: iteration 32, average log likelihood -1.304948
INFO: iteration 33, average log likelihood -1.304741
INFO: iteration 34, average log likelihood -1.304518
INFO: iteration 35, average log likelihood -1.304325
INFO: iteration 36, average log likelihood -1.304185
INFO: iteration 37, average log likelihood -1.304082
INFO: iteration 38, average log likelihood -1.304009
INFO: iteration 39, average log likelihood -1.303959
INFO: iteration 40, average log likelihood -1.303925
INFO: iteration 41, average log likelihood -1.303901
INFO: iteration 42, average log likelihood -1.303884
INFO: iteration 43, average log likelihood -1.303870
INFO: iteration 44, average log likelihood -1.303860
INFO: iteration 45, average log likelihood -1.303852
INFO: iteration 46, average log likelihood -1.303845
INFO: iteration 47, average log likelihood -1.303840
INFO: iteration 48, average log likelihood -1.303835
INFO: iteration 49, average log likelihood -1.303832
INFO: iteration 50, average log likelihood -1.303829
INFO: EM with 100000 data points 50 iterations avll -1.303829
473.9 data points per parameter
2: avll = [-1.35987,-1.3597,-1.35858,-1.34919,-1.33059,-1.32029,-1.31599,-1.31349,-1.31184,-1.31071,-1.30996,-1.30946,-1.3091,-1.30882,-1.30858,-1.30837,-1.30817,-1.30796,-1.30773,-1.3075,-1.30727,-1.30704,-1.30681,-1.30658,-1.30634,-1.30611,-1.3059,-1.30569,-1.3055,-1.30532,-1.30513,-1.30495,-1.30474,-1.30452,-1.30432,-1.30418,-1.30408,-1.30401,-1.30396,-1.30392,-1.3039,-1.30388,-1.30387,-1.30386,-1.30385,-1.30385,-1.30384,-1.30384,-1.30383,-1.30383]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.303989
INFO: iteration 2, average log likelihood -1.303840
INFO: iteration 3, average log likelihood -1.303463
INFO: iteration 4, average log likelihood -1.299833
INFO: iteration 5, average log likelihood -1.284691
INFO: iteration 6, average log likelihood -1.269798
INFO: iteration 7, average log likelihood -1.264672
INFO: iteration 8, average log likelihood -1.262306
INFO: iteration 9, average log likelihood -1.260713
INFO: iteration 10, average log likelihood -1.259630
INFO: iteration 11, average log likelihood -1.258873
INFO: iteration 12, average log likelihood -1.258158
INFO: iteration 13, average log likelihood -1.257355
INFO: iteration 14, average log likelihood -1.256446
INFO: iteration 15, average log likelihood -1.255610
INFO: iteration 16, average log likelihood -1.255007
INFO: iteration 17, average log likelihood -1.254638
INFO: iteration 18, average log likelihood -1.254424
INFO: iteration 19, average log likelihood -1.254292
INFO: iteration 20, average log likelihood -1.254200
INFO: iteration 21, average log likelihood -1.254129
INFO: iteration 22, average log likelihood -1.254061
INFO: iteration 23, average log likelihood -1.253986
INFO: iteration 24, average log likelihood -1.253896
INFO: iteration 25, average log likelihood -1.253784
INFO: iteration 26, average log likelihood -1.253646
INFO: iteration 27, average log likelihood -1.253486
INFO: iteration 28, average log likelihood -1.253294
INFO: iteration 29, average log likelihood -1.253071
INFO: iteration 30, average log likelihood -1.252838
INFO: iteration 31, average log likelihood -1.252631
INFO: iteration 32, average log likelihood -1.252474
INFO: iteration 33, average log likelihood -1.252378
INFO: iteration 34, average log likelihood -1.252329
INFO: iteration 35, average log likelihood -1.252308
INFO: iteration 36, average log likelihood -1.252299
INFO: iteration 37, average log likelihood -1.252295
INFO: iteration 38, average log likelihood -1.252293
INFO: iteration 39, average log likelihood -1.252292
INFO: iteration 40, average log likelihood -1.252292
INFO: iteration 41, average log likelihood -1.252291
INFO: iteration 42, average log likelihood -1.252291
INFO: iteration 43, average log likelihood -1.252291
INFO: iteration 44, average log likelihood -1.252291
INFO: iteration 45, average log likelihood -1.252291
INFO: iteration 46, average log likelihood -1.252291
INFO: iteration 47, average log likelihood -1.252291
INFO: iteration 48, average log likelihood -1.252291
INFO: iteration 49, average log likelihood -1.252291
INFO: iteration 50, average log likelihood -1.252291
INFO: EM with 100000 data points 50 iterations avll -1.252291
236.4 data points per parameter
3: avll = [-1.30399,-1.30384,-1.30346,-1.29983,-1.28469,-1.2698,-1.26467,-1.26231,-1.26071,-1.25963,-1.25887,-1.25816,-1.25735,-1.25645,-1.25561,-1.25501,-1.25464,-1.25442,-1.25429,-1.2542,-1.25413,-1.25406,-1.25399,-1.2539,-1.25378,-1.25365,-1.25349,-1.25329,-1.25307,-1.25284,-1.25263,-1.25247,-1.25238,-1.25233,-1.25231,-1.2523,-1.2523,-1.25229,-1.25229,-1.25229,-1.25229,-1.25229,-1.25229,-1.25229,-1.25229,-1.25229,-1.25229,-1.25229,-1.25229,-1.25229]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.252500
INFO: iteration 2, average log likelihood -1.252282
INFO: iteration 3, average log likelihood -1.251645
INFO: iteration 4, average log likelihood -1.242963
INFO: iteration 5, average log likelihood -1.207839
WARNING: Variances had to be floored 16
INFO: iteration 6, average log likelihood -1.178164
WARNING: Variances had to be floored 4 14 15
INFO: iteration 7, average log likelihood -1.168766
INFO: iteration 8, average log likelihood -1.185224
WARNING: Variances had to be floored 16
INFO: iteration 9, average log likelihood -1.166265
WARNING: Variances had to be floored 4 15
INFO: iteration 10, average log likelihood -1.163173
WARNING: Variances had to be floored 14
INFO: iteration 11, average log likelihood -1.171269
WARNING: Variances had to be floored 16
INFO: iteration 12, average log likelihood -1.164823
WARNING: Variances had to be floored 15
INFO: iteration 13, average log likelihood -1.160798
WARNING: Variances had to be floored 4
INFO: iteration 14, average log likelihood -1.160253
WARNING: Variances had to be floored 14 16
INFO: iteration 15, average log likelihood -1.159712
WARNING: Variances had to be floored 15
INFO: iteration 16, average log likelihood -1.164555
WARNING: Variances had to be floored 4
INFO: iteration 17, average log likelihood -1.162019
WARNING: Variances had to be floored 16
INFO: iteration 18, average log likelihood -1.159833
WARNING: Variances had to be floored 14 15
INFO: iteration 19, average log likelihood -1.156387
INFO: iteration 20, average log likelihood -1.165871
WARNING: Variances had to be floored 4 16
INFO: iteration 21, average log likelihood -1.152240
WARNING: Variances had to be floored 14 15
INFO: iteration 22, average log likelihood -1.162651
INFO: iteration 23, average log likelihood -1.168022
WARNING: Variances had to be floored 4 16
INFO: iteration 24, average log likelihood -1.154598
WARNING: Variances had to be floored 15
INFO: iteration 25, average log likelihood -1.162660
WARNING: Variances had to be floored 14
INFO: iteration 26, average log likelihood -1.160385
WARNING: Variances had to be floored 4 16
INFO: iteration 27, average log likelihood -1.158500
WARNING: Variances had to be floored 15
INFO: iteration 28, average log likelihood -1.164822
INFO: iteration 29, average log likelihood -1.161966
WARNING: Variances had to be floored 4 14 16
INFO: iteration 30, average log likelihood -1.150940
INFO: iteration 31, average log likelihood -1.168849
WARNING: Variances had to be floored 15
INFO: iteration 32, average log likelihood -1.153535
WARNING: Variances had to be floored 4 16
INFO: iteration 33, average log likelihood -1.158221
WARNING: Variances had to be floored 14
INFO: iteration 34, average log likelihood -1.164196
INFO: iteration 35, average log likelihood -1.159253
WARNING: Variances had to be floored 15 16
INFO: iteration 36, average log likelihood -1.150155
WARNING: Variances had to be floored 4
INFO: iteration 37, average log likelihood -1.160842
WARNING: Variances had to be floored 14
INFO: iteration 38, average log likelihood -1.165000
INFO: iteration 39, average log likelihood -1.160472
WARNING: Variances had to be floored 4 15 16
INFO: iteration 40, average log likelihood -1.150790
INFO: iteration 41, average log likelihood -1.170507
WARNING: Variances had to be floored 14
INFO: iteration 42, average log likelihood -1.160171
INFO: iteration 43, average log likelihood -1.158263
WARNING: Variances had to be floored 4 15 16
INFO: iteration 44, average log likelihood -1.148450
INFO: iteration 45, average log likelihood -1.170483
WARNING: Variances had to be floored 14
INFO: iteration 46, average log likelihood -1.160091
WARNING: Variances had to be floored 4
INFO: iteration 47, average log likelihood -1.158185
WARNING: Variances had to be floored 15 16
INFO: iteration 48, average log likelihood -1.158178
INFO: iteration 49, average log likelihood -1.165601
WARNING: Variances had to be floored 14
INFO: iteration 50, average log likelihood -1.157875
INFO: EM with 100000 data points 50 iterations avll -1.157875
118.1 data points per parameter
4: avll = [-1.2525,-1.25228,-1.25165,-1.24296,-1.20784,-1.17816,-1.16877,-1.18522,-1.16627,-1.16317,-1.17127,-1.16482,-1.1608,-1.16025,-1.15971,-1.16456,-1.16202,-1.15983,-1.15639,-1.16587,-1.15224,-1.16265,-1.16802,-1.1546,-1.16266,-1.16039,-1.1585,-1.16482,-1.16197,-1.15094,-1.16885,-1.15354,-1.15822,-1.1642,-1.15925,-1.15016,-1.16084,-1.165,-1.16047,-1.15079,-1.17051,-1.16017,-1.15826,-1.14845,-1.17048,-1.16009,-1.15819,-1.15818,-1.1656,-1.15788]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 7 8
INFO: iteration 1, average log likelihood -1.156173
WARNING: Variances had to be floored 7 8 29 30 31 32
INFO: iteration 2, average log likelihood -1.148620
WARNING: Variances had to be floored 7 8
INFO: iteration 3, average log likelihood -1.148334
WARNING: Variances had to be floored 7 8 27 28 29 30 31 32
INFO: iteration 4, average log likelihood -1.123296
WARNING: Variances had to be floored 7 8 21
INFO: iteration 5, average log likelihood -1.093873
WARNING: Variances had to be floored 2 7 8 15 19 20 29 30 31 32
INFO: iteration 6, average log likelihood -1.070877
WARNING: Variances had to be floored 1 7 8 21 27 28
INFO: iteration 7, average log likelihood -1.094016
WARNING: Variances had to be floored 7 8 29 30 31 32
INFO: iteration 8, average log likelihood -1.095128
WARNING: Variances had to be floored 7 8 15
INFO: iteration 9, average log likelihood -1.067510
WARNING: Variances had to be floored 2 7 8 19 20 21 27 28 29 30 31 32
INFO: iteration 10, average log likelihood -1.055059
WARNING: Variances had to be floored 1 7 8
INFO: iteration 11, average log likelihood -1.095388
WARNING: Variances had to be floored 7 8 29 30 31 32
INFO: iteration 12, average log likelihood -1.077956
WARNING: Variances had to be floored 7 8 15 21 27 28
INFO: iteration 13, average log likelihood -1.055158
WARNING: Variances had to be floored 2 7 8 19 20 29 30 31 32
INFO: iteration 14, average log likelihood -1.068529
WARNING: Variances had to be floored 1 7 8 21 28 32
INFO: iteration 15, average log likelihood -1.081594
WARNING: Variances had to be floored 7 8 15 27 29 30 31
INFO: iteration 16, average log likelihood -1.078519
WARNING: Variances had to be floored 7 8 21 32
INFO: iteration 17, average log likelihood -1.073166
WARNING: Variances had to be floored 1 2 7 8 19 20 27 29 30 31
INFO: iteration 18, average log likelihood -1.052102
WARNING: Variances had to be floored 7 8 13 15 21 32
INFO: iteration 19, average log likelihood -1.073203
WARNING: Variances had to be floored 7 8 27 28 29 30 31
INFO: iteration 20, average log likelihood -1.075614
WARNING: Variances had to be floored 7 8 21 32
INFO: iteration 21, average log likelihood -1.068600
WARNING: Variances had to be floored 1 2 7 8 15 19 20 27 29 30 31
INFO: iteration 22, average log likelihood -1.050296
WARNING: Variances had to be floored 7 8 13 21 32
INFO: iteration 23, average log likelihood -1.089355
WARNING: Variances had to be floored 7 8 27 29 30 31
INFO: iteration 24, average log likelihood -1.077004
WARNING: Variances had to be floored 1 2 7 8 15 21 28 32
INFO: iteration 25, average log likelihood -1.046893
WARNING: Variances had to be floored 7 8 19 20 27 29 30 31
INFO: iteration 26, average log likelihood -1.070213
WARNING: Variances had to be floored 7 8 13 21 32
INFO: iteration 27, average log likelihood -1.080148
WARNING: Variances had to be floored 7 8 15 27 29 30 31
INFO: iteration 28, average log likelihood -1.071008
WARNING: Variances had to be floored 1 2 7 8 21 28 32
INFO: iteration 29, average log likelihood -1.054975
WARNING: Variances had to be floored 7 8 19 20 27 29 30 31
INFO: iteration 30, average log likelihood -1.062188
WARNING: Variances had to be floored 7 8 13 15 21 32
INFO: iteration 31, average log likelihood -1.074338
WARNING: Variances had to be floored 7 8 27 29 30 31
INFO: iteration 32, average log likelihood -1.079382
WARNING: Variances had to be floored 1 2 7 8 21 28 32
INFO: iteration 33, average log likelihood -1.048352
WARNING: Variances had to be floored 7 8 15 19 20 27 29 30 31
INFO: iteration 34, average log likelihood -1.057715
WARNING: Variances had to be floored 7 8 13 21 31 32
INFO: iteration 35, average log likelihood -1.084329
WARNING: Variances had to be floored 7 8 27 29 30 31 32
INFO: iteration 36, average log likelihood -1.073749
WARNING: Variances had to be floored 1 2 7 8 15 21 28 31 32
INFO: iteration 37, average log likelihood -1.044641
WARNING: Variances had to be floored 7 8 19 20 27 29 30 31 32
INFO: iteration 38, average log likelihood -1.068684
WARNING: Variances had to be floored 7 8 13 21 31 32
INFO: iteration 39, average log likelihood -1.078780
WARNING: Variances had to be floored 7 8 15 27 29 30 31 32
INFO: iteration 40, average log likelihood -1.070508
WARNING: Variances had to be floored 1 2 7 8 21 28 31 32
INFO: iteration 41, average log likelihood -1.054711
WARNING: Variances had to be floored 7 8 19 20 27 29 30 31 32
INFO: iteration 42, average log likelihood -1.062228
WARNING: Variances had to be floored 7 8 13 15 21 31 32
INFO: iteration 43, average log likelihood -1.074348
WARNING: Variances had to be floored 7 8 27 29 30 31 32
INFO: iteration 44, average log likelihood -1.080091
WARNING: Variances had to be floored 1 2 7 8 21 28 31 32
INFO: iteration 45, average log likelihood -1.048647
WARNING: Variances had to be floored 7 8 15 19 20 27 29 30 31 32
INFO: iteration 46, average log likelihood -1.058101
WARNING: Variances had to be floored 7 8 13 21 31 32
INFO: iteration 47, average log likelihood -1.084488
WARNING: Variances had to be floored 7 8 27 29 30 31 32
INFO: iteration 48, average log likelihood -1.073821
WARNING: Variances had to be floored 1 2 7 8 15 21 28 31 32
INFO: iteration 49, average log likelihood -1.044567
WARNING: Variances had to be floored 7 8 19 20 27 29 30 31 32
INFO: iteration 50, average log likelihood -1.068465
INFO: EM with 100000 data points 50 iterations avll -1.068465
59.0 data points per parameter
5: avll = [-1.15617,-1.14862,-1.14833,-1.1233,-1.09387,-1.07088,-1.09402,-1.09513,-1.06751,-1.05506,-1.09539,-1.07796,-1.05516,-1.06853,-1.08159,-1.07852,-1.07317,-1.0521,-1.0732,-1.07561,-1.0686,-1.0503,-1.08936,-1.077,-1.04689,-1.07021,-1.08015,-1.07101,-1.05498,-1.06219,-1.07434,-1.07938,-1.04835,-1.05772,-1.08433,-1.07375,-1.04464,-1.06868,-1.07878,-1.07051,-1.05471,-1.06223,-1.07435,-1.08009,-1.04865,-1.0581,-1.08449,-1.07382,-1.04457,-1.06846]
[-1.39979,-1.39988,-1.39981,-1.3994,-1.39377,-1.37439,-1.36466,-1.36239,-1.36125,-1.36056,-1.36019,-1.36,-1.35991,-1.35987,-1.35984,-1.35983,-1.35982,-1.35981,-1.3598,-1.35979,-1.35979,-1.35979,-1.35978,-1.35978,-1.35978,-1.35978,-1.35978,-1.35978,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35977,-1.35976,-1.35976,-1.35976,-1.35976,-1.35975,-1.35987,-1.3597,-1.35858,-1.34919,-1.33059,-1.32029,-1.31599,-1.31349,-1.31184,-1.31071,-1.30996,-1.30946,-1.3091,-1.30882,-1.30858,-1.30837,-1.30817,-1.30796,-1.30773,-1.3075,-1.30727,-1.30704,-1.30681,-1.30658,-1.30634,-1.30611,-1.3059,-1.30569,-1.3055,-1.30532,-1.30513,-1.30495,-1.30474,-1.30452,-1.30432,-1.30418,-1.30408,-1.30401,-1.30396,-1.30392,-1.3039,-1.30388,-1.30387,-1.30386,-1.30385,-1.30385,-1.30384,-1.30384,-1.30383,-1.30383,-1.30399,-1.30384,-1.30346,-1.29983,-1.28469,-1.2698,-1.26467,-1.26231,-1.26071,-1.25963,-1.25887,-1.25816,-1.25735,-1.25645,-1.25561,-1.25501,-1.25464,-1.25442,-1.25429,-1.2542,-1.25413,-1.25406,-1.25399,-1.2539,-1.25378,-1.25365,-1.25349,-1.25329,-1.25307,-1.25284,-1.25263,-1.25247,-1.25238,-1.25233,-1.25231,-1.2523,-1.2523,-1.25229,-1.25229,-1.25229,-1.25229,-1.25229,-1.25229,-1.25229,-1.25229,-1.25229,-1.25229,-1.25229,-1.25229,-1.25229,-1.2525,-1.25228,-1.25165,-1.24296,-1.20784,-1.17816,-1.16877,-1.18522,-1.16627,-1.16317,-1.17127,-1.16482,-1.1608,-1.16025,-1.15971,-1.16456,-1.16202,-1.15983,-1.15639,-1.16587,-1.15224,-1.16265,-1.16802,-1.1546,-1.16266,-1.16039,-1.1585,-1.16482,-1.16197,-1.15094,-1.16885,-1.15354,-1.15822,-1.1642,-1.15925,-1.15016,-1.16084,-1.165,-1.16047,-1.15079,-1.17051,-1.16017,-1.15826,-1.14845,-1.17048,-1.16009,-1.15819,-1.15818,-1.1656,-1.15788,-1.15617,-1.14862,-1.14833,-1.1233,-1.09387,-1.07088,-1.09402,-1.09513,-1.06751,-1.05506,-1.09539,-1.07796,-1.05516,-1.06853,-1.08159,-1.07852,-1.07317,-1.0521,-1.0732,-1.07561,-1.0686,-1.0503,-1.08936,-1.077,-1.04689,-1.07021,-1.08015,-1.07101,-1.05498,-1.06219,-1.07434,-1.07938,-1.04835,-1.05772,-1.08433,-1.07375,-1.04464,-1.06868,-1.07878,-1.07051,-1.05471,-1.06223,-1.07435,-1.08009,-1.04865,-1.0581,-1.08449,-1.07382,-1.04457,-1.06846]
32×26 Array{Float64,2}:
  0.00195589   0.0690317   0.107226    -0.019693     0.105707    -0.119392     -0.0488156    0.112083     0.0635051   -0.0441779     0.0193171    -0.0336846    -0.0506255    0.00382618    0.0545181   -0.00930632  -0.0446514   -0.106942     0.0577408     0.011769    -0.0463813    -0.011434     0.0237203     0.0472144    0.00859918  -0.160999  
  0.0400113    0.016346    0.0716921    0.198037    -0.0899189   -0.134434      0.0253909   -0.0451697    0.112415     0.0439862    -0.118707      0.0334173    -0.212982     0.113449      0.0143123   -0.041159    -0.0927761   -0.0590533   -0.113603      0.160318     0.117175      0.00920034  -0.163263      0.102427    -0.0478815    0.0695117 
 -0.0261882    0.133585    0.234519     0.0290767    0.0702383    0.0890114    -0.242333    -0.0295105    0.142567    -0.0632507    -0.255801      0.153591     -0.0651622   -0.0651178     0.125335    -0.00577891   0.0569301    0.0550609   -0.125214     -0.065346    -0.220087      0.20354      0.210094      0.0188579   -0.295175    -0.0272789 
  0.117044     0.129334    0.228555    -0.00763772   0.0950499    0.0977049    -0.251618    -0.0807689    0.126775     0.00401639    0.122993      0.218312     -0.0506665   -0.0994539     0.140684    -0.0328438    0.0918176    0.0288241   -0.0782949    -0.156275    -0.205495      0.216035     0.191663     -0.17782      0.785535     0.0427238 
 -0.0831816    0.0963684   0.177903    -0.115298     0.0626385    0.034944      0.0306996    0.1612       0.243718     0.178343     -0.146008     -0.0542171    -0.0997503   -0.108104     -0.00447554  -0.0269125   -0.0119748   -0.113702     0.0839098    -0.0096075    0.140338     -0.851558     0.0438402    -0.105716    -0.315172     0.166113  
 -0.104957     0.113935   -0.00287599   0.0431084    0.0716392    0.0257287    -0.0143609    0.23731      0.102115    -0.000599587  -0.127186     -0.0376011     0.0178064    0.123413     -0.00379161   0.0530104   -0.0409465   -0.173866     0.118178      0.0172227    0.198098      0.453248     0.0537988    -0.0161923   -0.287401     0.244545  
  0.029822    -0.167201   -0.166495    -0.260895    -0.201113     0.0760107     0.0214553    0.0605512    0.0949539    0.258725     -0.309947      0.0376746     0.0185462    0.228354     -0.162626    -0.513829     0.113048     0.0789319   -0.123863      0.0966418    0.111744      0.142164     0.068766     -0.0580228   -0.0303127   -0.107771  
 -0.0625265   -0.192874   -0.166442    -0.0865551   -0.192504    -0.283871     -0.0162983    0.126451     0.0934409    0.269524      0.34158       0.0624975    -0.0610876   -0.0323927    -0.187912     0.352566     0.187502     0.129684     0.15851       0.0527427    0.0916753    -0.308301     0.0688426    -0.0292359   -0.0999381    0.0679933 
  0.123311    -0.153125    0.0372704   -0.056536    -0.585666    -0.0398       -0.0828738    0.0303111    0.0544798    0.0136845     0.0519862     0.00933539   -0.16612     -0.0168844     0.121377     0.0692984    0.0869321    0.0254217    0.0541386     0.0295849    0.278256      0.0972496   -0.166284      0.260733     0.107529    -0.0933798 
  0.0592879    0.0725939   0.0641042   -0.0316348    0.695392    -0.115219     -0.042698     0.0552347    0.050878     0.0555649     0.0506872     0.00284912   -0.157591     0.0196958     0.13267     -0.00460791   0.117995     0.132863     0.0521711    -0.0079783    0.172483      0.0543708   -0.244457      0.207226    -0.064804    -0.00284384
 -0.116897     0.0646981  -0.0692783    0.155696     0.0837037   -0.0497102    -0.0902439    0.168656     0.0741361   -0.115401     -0.0605107     0.0157288    -0.0771451    0.133538     -0.0556881    0.005755    -0.0471086    0.0199465    0.0418329    -0.0252245   -0.142534      0.00252823  -0.0143276    -0.215644    -0.0843128   -0.0991091 
 -0.160344    -0.0925375   0.0443521   -0.0562794    0.053409     0.00802576    0.0298077   -0.0379654   -0.180952    -0.0724748    -0.113852     -0.103841      0.148407    -0.109247     -0.0677093    0.189969    -0.0519481    0.00890271  -0.112352     -0.146458    -0.061448     -0.181445     0.0423865    -0.167926    -0.097514    -0.048881  
  0.0641967    0.129833    0.00250433   0.00487573  -0.0327168    0.104951     -0.0317714   -0.0989988   -0.110782    -0.0261996     0.000698332  -0.00434091   -0.035506    -0.0523329    -0.0198067   -0.0465971    0.0386901    0.0643007   -0.146693     -0.00650129   0.0466235    -0.0123555    0.074976     -0.0460232    0.0959964   -0.012751  
  0.00825029  -0.105723    0.0146581    0.0187045   -0.0154164    0.027854     -0.0238907   -0.048641     0.0855843   -0.0016937    -0.105937      6.82467e-5   -0.0866513    0.0033554     0.0165042   -0.0174856    0.0681985   -0.0213507   -0.145817     -0.0212862    0.052792      0.0570391    0.0744355    -0.0106079   -0.092095     0.104832  
 -0.0718007   -0.0235912  -0.0206667    0.0730754   -0.00142171   0.166699      0.0315995   -0.0206148    0.100596     0.153647     -0.0666271    -0.222461      0.100589    -0.000681936  -0.175981    -0.00252136   0.234275    -0.070904     0.0247998    -0.118265     0.0459075    -0.0295936    0.00181798   -0.0706168    0.0626068    0.0596333 
  0.0544048    0.0835474   0.0955416   -0.12702     -0.177949    -0.109794      0.00200697   0.115368     0.00944109   0.0626795     0.0131059    -0.000455046  -0.133198    -0.0298345     0.154428    -0.0905261    0.00941949   0.00637186   0.0211159     0.0224918    0.0708615    -0.0725049   -0.00511255   -0.0317095   -0.147352    -0.2606    
 -0.130224     0.0139227   0.0603202   -0.0153421   -0.00617873   0.000265867  -0.0563487    0.0543913   -0.0206924   -0.0740089    -0.0337773    -0.166604     -0.00515929   0.0163537     0.0510764    0.0392051    0.0257397    0.0617902    0.0203228    -0.0776564    0.0478882     0.0178437    0.0383217    -0.0111522   -0.0370601    0.119859  
 -0.0169826   -0.0591832   0.15479     -0.0275526    0.0991403   -0.029829      0.0292509    0.154201    -0.157393    -0.0107149     0.0540897     0.112382      0.0334101   -0.00316981   -0.046185    -0.0267804    0.160982    -0.0207974   -0.0613832    -0.087116     0.102549     -0.0271194   -0.148539     -0.0457947   -0.0748125   -0.0554702 
 -0.00795416  -0.142459   -0.157449    -0.195723     0.0136896   -0.00508396   -0.0430167   -0.0743589    0.182913    -0.0375818    -0.0137183    -0.0284461     0.0585046    0.161156     -0.0915173    0.0258722    0.0533602    0.0668232    0.000598176   0.265966     0.0503114     0.0475093   -0.0220837     0.0876927   -0.0396051    0.178583  
 -0.199862    -0.0938686  -0.00373662  -0.1037      -0.0387879    0.000630582   0.0604397   -0.0686595   -0.126391    -0.134344     -0.0586702    -0.153731     -0.193049     0.0420856    -0.0138082    0.0179239    0.192516    -0.0569436   -0.0548382     0.088276     0.0821596     0.0673238    0.0461469    -0.0366831   -0.0406377    0.037259  
  0.0556053    0.0494752  -0.0474005    0.05109      0.11794      0.0189127    -0.0967733    0.00459391   0.196643    -0.0364883     0.0817944    -0.254589      0.143206     0.0282138    -0.0421082    0.00649998  -0.11931     -0.106902    -0.0154431    -0.0728624    0.0442609     0.041708     0.195682      0.124932     0.0229755   -0.0185948 
 -0.0234989   -0.124364    0.106683    -0.0509672   -0.0589378    0.142141     -0.147414    -0.032107     0.0131779    0.0266306    -0.105845     -0.0906364     0.160957     0.0701246    -0.0797079    0.143541    -0.0671237   -0.00138194   0.20264       0.149957    -0.0264329    -0.0308172   -0.0720252     0.00424425   0.0627095    0.0773059 
 -0.0229935   -0.0693024   0.0074943   -0.00514646  -0.0520744   -0.0398778    -0.014116     0.0785531    0.00781046  -0.0515729    -0.0563579     0.0322407     0.116857    -0.0841059     0.0417348   -0.0671048   -0.051809     0.0285175    0.0609875    -0.0440822   -0.064045     -0.0388593    0.000218541   0.0136775    0.0504359    0.0108055 
  0.0998802   -0.0386849   0.00701811   0.0777876    0.0998247    0.0303798     0.0518702   -0.00340787   0.182921    -0.0124796    -0.00535509   -0.0463306     0.0887868    0.0936941     0.0428085   -0.0673588   -0.059676     0.169626     0.101216      0.10983     -0.015885      0.0207105   -0.0197145     0.171004    -0.0638713    0.0421776 
 -0.0844257   -0.221369    0.161841     0.201187    -0.00219329  -0.0627113    -0.0630702   -0.0784047   -0.054898     0.0720973     0.0311962     0.0249647    -0.105644    -0.0715121    -0.0935522    0.149918     0.048937    -0.102884     0.105443     -0.25618      0.000636597  -0.108271     0.0249236     0.076213    -0.0404404    0.0414101 
 -0.0885627    0.190322    0.173049     0.16768     -0.0160943   -0.054335      0.199984     0.082118    -0.114312     0.0249922    -0.0098456     0.0838174    -0.0724441   -0.0766847    -0.0868119    0.165245    -0.0205198    0.112028     0.0597981    -0.311776     0.0712229    -0.116056     0.0481864     0.2289      -0.0693205    0.0714741 
  0.01283     -0.0312807  -0.130574     0.101876    -0.210652    -0.189379     -0.0845846    0.134217     0.0766703   -0.0344964     0.200496      0.0453322    -0.0382917    0.042529     -0.00555499   0.282245    -0.00811781  -0.0052435   -0.0241666    -0.0511911    0.131625      0.145566    -0.0597502    -0.0307993   -0.146476    -0.0983369 
 -0.0468966    0.0257532   0.214225    -0.0702351   -0.00946842   0.0625362     0.0196113    0.020638    -0.0321131   -0.0832446     0.0319923     0.0412341     0.056434    -0.0749981     0.092196    -0.0400139    0.0157467    0.0726609    0.021776     -0.0753876   -0.0274408    -0.0984666    0.0387576     0.0904238    0.0208693    0.0659135 
  0.166296    -0.104693    0.102152     0.127529     0.0170349    0.0905016    -0.0274432   -0.24304     -0.0683374   -0.930891      0.089475      0.205409      0.010133     0.125631      0.105394    -0.0267741    0.0116762   -0.327855    -0.131614      0.0238988   -0.0800478     0.0483006   -0.0803411     0.0818509   -0.189731     0.106967  
  0.2232      -0.202644    0.103602     0.116416     0.0151841   -0.372778     -0.044468     0.342906    -0.0679682    0.74677       0.0379497     0.204204      0.0331257   -0.0204725     0.117841    -0.0349873   -0.016684     0.0147453   -0.134669     -0.0185589   -0.0368797     0.0485001   -0.104923      0.14158     -0.18694      0.106379  
  0.0738321   -0.081069   -0.154205     0.00894669  -0.0854712    0.136457      0.112291    -0.299216    -0.10342      0.00607176    0.122831      0.111363      0.0989493   -0.212773      0.0906667   -0.177305    -0.0792864    0.075327    -0.0287012     0.0425952    0.14781      -0.0536867   -0.00690083    0.0721853    0.109541     0.151478  
  0.087911    -0.0966704   0.0224136    0.00395331  -0.105624     0.133186      0.0430094    0.793215    -0.138407     0.0472219    -0.257853     -0.549484      0.138788    -0.152276      0.00807056  -0.164043     0.0779148   -0.0209983   -0.0283488     0.0482108    0.0213238    -0.115942    -0.0153635     0.0472434    0.115967     0.157717  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 7 8 13 21 31 32
INFO: iteration 1, average log likelihood -1.078616
WARNING: Variances had to be floored 7 8 13 15 21 27 29 30 31 32
INFO: iteration 2, average log likelihood -1.052913
WARNING: Variances had to be floored 1 2 7 8 13 15 21 28 31 32
INFO: iteration 3, average log likelihood -1.041789
WARNING: Variances had to be floored 7 8 13 19 20 21 27 29 30 31 32
INFO: iteration 4, average log likelihood -1.048163
WARNING: Variances had to be floored 7 8 13 15 21 31 32
INFO: iteration 5, average log likelihood -1.064230
WARNING: Variances had to be floored 1 2 7 8 13 21 27 28 29 30 31 32
INFO: iteration 6, average log likelihood -1.045540
WARNING: Variances had to be floored 7 8 13 15 21 31 32
INFO: iteration 7, average log likelihood -1.059632
WARNING: Variances had to be floored 7 8 13 15 19 20 21 27 29 30 31 32
INFO: iteration 8, average log likelihood -1.037898
WARNING: Variances had to be floored 1 2 7 8 13 21 28 31 32
INFO: iteration 9, average log likelihood -1.057525
WARNING: Variances had to be floored 7 8 13 15 21 27 29 30 31 32
INFO: iteration 10, average log likelihood -1.062680
INFO: EM with 100000 data points 10 iterations avll -1.062680
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.729462e+05
      1       6.814503e+05      -1.914958e+05 |       32
      2       6.454676e+05      -3.598273e+04 |       32
      3       6.323570e+05      -1.311063e+04 |       32
      4       6.232378e+05      -9.119138e+03 |       32
      5       6.144582e+05      -8.779670e+03 |       32
      6       6.082720e+05      -6.186170e+03 |       32
      7       6.053553e+05      -2.916664e+03 |       32
      8       6.035549e+05      -1.800398e+03 |       32
      9       6.017403e+05      -1.814646e+03 |       32
     10       6.000852e+05      -1.655095e+03 |       32
     11       5.991246e+05      -9.606316e+02 |       32
     12       5.986103e+05      -5.142450e+02 |       32
     13       5.983094e+05      -3.008939e+02 |       32
     14       5.981438e+05      -1.655983e+02 |       32
     15       5.980580e+05      -8.577784e+01 |       32
     16       5.979993e+05      -5.876087e+01 |       31
     17       5.979577e+05      -4.153546e+01 |       31
     18       5.979122e+05      -4.556818e+01 |       32
     19       5.978513e+05      -6.090422e+01 |       32
     20       5.977659e+05      -8.540593e+01 |       32
     21       5.976410e+05      -1.249107e+02 |       32
     22       5.974079e+05      -2.330332e+02 |       32
     23       5.970131e+05      -3.948489e+02 |       32
     24       5.961751e+05      -8.379756e+02 |       32
     25       5.950433e+05      -1.131820e+03 |       32
     26       5.943943e+05      -6.489459e+02 |       32
     27       5.939361e+05      -4.582301e+02 |       32
     28       5.936949e+05      -2.412443e+02 |       32
     29       5.935804e+05      -1.144333e+02 |       32
     30       5.935022e+05      -7.825788e+01 |       32
     31       5.934322e+05      -6.992775e+01 |       32
     32       5.933641e+05      -6.815043e+01 |       32
     33       5.932903e+05      -7.378246e+01 |       31
     34       5.932104e+05      -7.986319e+01 |       32
     35       5.931194e+05      -9.100012e+01 |       32
     36       5.930004e+05      -1.190565e+02 |       32
     37       5.928586e+05      -1.417438e+02 |       31
     38       5.927620e+05      -9.661839e+01 |       31
     39       5.927231e+05      -3.893243e+01 |       32
     40       5.927072e+05      -1.593011e+01 |       30
     41       5.926986e+05      -8.529002e+00 |       29
     42       5.926933e+05      -5.374260e+00 |       24
     43       5.926901e+05      -3.113596e+00 |       22
     44       5.926890e+05      -1.134059e+00 |       23
     45       5.926878e+05      -1.203026e+00 |       19
     46       5.926866e+05      -1.192961e+00 |       15
     47       5.926861e+05      -5.253988e-01 |        7
     48       5.926859e+05      -1.496827e-01 |        6
     49       5.926858e+05      -1.360066e-01 |        4
     50       5.926857e+05      -6.827086e-02 |        0
K-means terminated without convergence after 50 iterations (objv = 592685.7399828329)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.309100
INFO: iteration 2, average log likelihood -1.275353
INFO: iteration 3, average log likelihood -1.243777
INFO: iteration 4, average log likelihood -1.212182
INFO: iteration 5, average log likelihood -1.178481
WARNING: Variances had to be floored 14
INFO: iteration 6, average log likelihood -1.131218
WARNING: Variances had to be floored 8 21 23
INFO: iteration 7, average log likelihood -1.071179
WARNING: Variances had to be floored 6 10 12 15 25 29 30
INFO: iteration 8, average log likelihood -1.060327
INFO: iteration 9, average log likelihood -1.129268
WARNING: Variances had to be floored 11 19 26 28
INFO: iteration 10, average log likelihood -1.077386
WARNING: Variances had to be floored 5 21
INFO: iteration 11, average log likelihood -1.067481
WARNING: Variances had to be floored 8 10 16 25
INFO: iteration 12, average log likelihood -1.055000
WARNING: Variances had to be floored 12 15 29
INFO: iteration 13, average log likelihood -1.068417
WARNING: Variances had to be floored 6 30
INFO: iteration 14, average log likelihood -1.077219
WARNING: Variances had to be floored 19 21 25 26
INFO: iteration 15, average log likelihood -1.058136
WARNING: Variances had to be floored 11 23
INFO: iteration 16, average log likelihood -1.056287
WARNING: Variances had to be floored 10 12 15 16 29
INFO: iteration 17, average log likelihood -1.038775
WARNING: Variances had to be floored 21 30
INFO: iteration 18, average log likelihood -1.076165
WARNING: Variances had to be floored 6 8 25
INFO: iteration 19, average log likelihood -1.054715
WARNING: Variances had to be floored 14 19 23 26
INFO: iteration 20, average log likelihood -1.044925
WARNING: Variances had to be floored 10 11 12 16 21 29
INFO: iteration 21, average log likelihood -1.034932
WARNING: Variances had to be floored 15 25 30
INFO: iteration 22, average log likelihood -1.077862
WARNING: Variances had to be floored 23
INFO: iteration 23, average log likelihood -1.079932
WARNING: Variances had to be floored 8 10 14 19 21
INFO: iteration 24, average log likelihood -1.020139
WARNING: Variances had to be floored 6 11 12 15 16 17 25 26 29
INFO: iteration 25, average log likelihood -1.025239
WARNING: Variances had to be floored 30
INFO: iteration 26, average log likelihood -1.131587
INFO: iteration 27, average log likelihood -1.090492
WARNING: Variances had to be floored 10 17 21 23 25
INFO: iteration 28, average log likelihood -1.018622
WARNING: Variances had to be floored 12 15 16 19 26 29 30
INFO: iteration 29, average log likelihood -1.040943
WARNING: Variances had to be floored 6 11
INFO: iteration 30, average log likelihood -1.109293
WARNING: Variances had to be floored 14 21
INFO: iteration 31, average log likelihood -1.068869
WARNING: Variances had to be floored 8 10 25
INFO: iteration 32, average log likelihood -1.029948
WARNING: Variances had to be floored 12 15 17 19 23 29 30
INFO: iteration 33, average log likelihood -1.024032
WARNING: Variances had to be floored 16 21
INFO: iteration 34, average log likelihood -1.081888
WARNING: Variances had to be floored 6 11 14 26
INFO: iteration 35, average log likelihood -1.047199
WARNING: Variances had to be floored 8 10 17
INFO: iteration 36, average log likelihood -1.041321
WARNING: Variances had to be floored 12 15 21 25
INFO: iteration 37, average log likelihood -1.030132
WARNING: Variances had to be floored 16 19 29 30
INFO: iteration 38, average log likelihood -1.052239
WARNING: Variances had to be floored 6 8 14 23 26
INFO: iteration 39, average log likelihood -1.051070
WARNING: Variances had to be floored 10 11 21 25
INFO: iteration 40, average log likelihood -1.061777
WARNING: Variances had to be floored 12 15 17
INFO: iteration 41, average log likelihood -1.062942
WARNING: Variances had to be floored 19 29
INFO: iteration 42, average log likelihood -1.066009
WARNING: Variances had to be floored 10 16 21 26 30
INFO: iteration 43, average log likelihood -1.039650
WARNING: Variances had to be floored 8 11 12 14 15 17
INFO: iteration 44, average log likelihood -1.047052
WARNING: Variances had to be floored 6 29
INFO: iteration 45, average log likelihood -1.082373
WARNING: Variances had to be floored 10 19 21 23
INFO: iteration 46, average log likelihood -1.052342
WARNING: Variances had to be floored 16 26
INFO: iteration 47, average log likelihood -1.052292
WARNING: Variances had to be floored 11 17 25 30
INFO: iteration 48, average log likelihood -1.025918
WARNING: Variances had to be floored 8 10 12 14 15 21 29
INFO: iteration 49, average log likelihood -1.025718
INFO: iteration 50, average log likelihood -1.087992
INFO: EM with 100000 data points 50 iterations avll -1.087992
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.210871    -0.00996485   0.00726936   0.171376      0.122371    -0.0114701    0.0343773   -0.0388114   0.207467    -0.0116708    0.0334079    0.0715739     0.0645494    0.0929701    0.0718272    0.0558781   -0.08402      0.221556     0.0916343    0.179924    -0.0369359   -0.00611123   -0.0227908    0.208605    -0.0467505    0.0990067 
 -0.059597    -0.107937    -0.0586433   -0.0875351    -0.00343985  -0.0363847    0.0188985    0.107537   -0.00827258  -0.173767     0.0666078   -0.0208868     0.101808    -0.0799502   -0.052767    -0.0808947    0.0624267   -0.0526928    0.129952    -0.0778571    0.017231    -0.0633298     0.0365767    0.0370619    0.151786     0.0631156 
  0.0556062    0.0833589    0.103167    -0.126718     -0.176395    -0.108469     0.00111078   0.114096    0.00911582   0.0609497    0.0109629    0.000214426  -0.132893    -0.0325934    0.153786    -0.0905377    0.00970012   0.00458058   0.021529     0.0223863    0.069028    -0.0722226    -0.00542356  -0.0319972   -0.14657     -0.259284  
  0.0501516    0.131432     0.231375     0.00937465    0.0836124    0.0936491   -0.24738     -0.0563021   0.133708    -0.0265512   -0.0544636    0.187739     -0.0577799   -0.0838345    0.133359    -0.020179     0.0768076    0.0414515   -0.100455    -0.113448    -0.212247     0.210072      0.200696    -0.0865746    0.278059     0.00994958
  0.112314    -0.101532    -0.00904347   0.110238     -0.0910744   -0.197084    -0.0623408    0.137828   -0.00496326   0.0669118    0.128467     0.132776      0.00114496   0.0342554    0.0590705    0.111806    -0.00680051  -0.0557654   -0.079027    -0.0232238    0.0344909    0.0930072    -0.0788231    0.0482881   -0.161824     0.0157799 
  0.0509731    0.0134969    0.0535392    0.215401     -0.0977291   -0.154135     0.0244859   -0.0297468   0.121882     0.0599278   -0.117666     0.0279146    -0.226305     0.13362      0.0148417   -0.0485147   -0.0885256   -0.0684473   -0.11599      0.1603       0.117596     0.0212395    -0.18245      0.0895636   -0.069421     0.0712287 
 -0.0528288   -0.0639281    0.0222397   -0.111895      0.0946943   -0.0307037    0.0196908    0.12868    -0.09707     -0.00775132   0.174038    -0.0975717    -0.11585      0.0455752   -0.00371001  -0.0354044   -0.00309411   0.0587327   -0.0466644   -0.0332415    0.0404997    0.0378308    -0.0455369   -0.112818    -0.097165    -0.0718885 
  0.0153041   -0.052036    -0.00271967   0.0270155    -0.0925985   -0.120791    -0.0121159    0.0641119   0.0583604   -0.00192312  -0.0985403    0.0929148     0.0994521   -0.0809886    0.0981791   -0.0854824   -0.149938     0.140353     0.146565    -0.150429    -0.146991    -0.0229755    -0.0241933    0.00438018   0.0386883   -0.0672883 
 -0.0872241   -0.0467404    0.165498     0.188098     -0.00911351  -0.0589603    0.0506185   -0.0139102  -0.0793929    0.0534469    0.0140073    0.0485584    -0.0881639   -0.0744454   -0.0910766    0.152008     0.0211602   -0.0133343    0.0860003   -0.275344     0.0306836   -0.1117        0.0349809    0.14209     -0.0524049    0.0536554 
 -0.0110058   -0.176645    -0.162237    -0.2662       -0.182383    -0.159014     0.00711788   0.106171    0.0944305    0.23136      0.07704      0.0782593    -0.0317445    0.278466    -0.150508    -0.106583     0.177989     0.145484     0.0272163    0.15851      0.0599505   -0.107462      0.0657696   -0.0527857   -0.080676    -0.0863768 
  0.0263569   -0.0157109   -0.0493224    0.0778236    -0.00780312   0.166667     0.0281847    0.253977    0.0819059    0.158111    -0.0722749   -0.184052      0.103811    -0.00440068  -0.157659    -0.00304632   0.237162    -0.0710919    0.032853    -0.24745      0.0598276   -0.154753      0.00531987  -0.0492757    0.032583     0.0735518 
  0.0742665    0.0745901   -0.063843     0.0662636     0.0719555    0.0456751   -0.170329     0.0316037   0.187778    -0.00319477   0.0926015   -0.511882      0.262369    -0.0251572   -0.0241494   -0.0187142   -0.144352    -0.0996674   -0.0140061   -0.113681     0.0269849    0.0301517     0.205681     0.0981993    0.0224644   -0.00336273
 -0.11699      0.0624611   -0.0638989    0.154302      0.0724985   -0.0467554   -0.0899535    0.176218    0.0716999   -0.109733    -0.0612944    0.0186528    -0.0718478    0.137271    -0.0571931    0.00202118  -0.0526711    0.018886     0.0408496   -0.0250793   -0.142507    -0.00255315   -0.0135805   -0.211411    -0.0812078   -0.0948605 
 -0.0586053   -0.0535763    0.0279976    0.143374     -0.0530703    0.0127059   -0.0619096    0.0968189   0.0355904    0.0849442   -0.258076     0.209684      0.211283    -0.168737     0.0244659   -0.0479227   -0.153409    -0.0949067   -0.15919      0.217162    -0.0332816    0.00954404    0.0138535   -0.0408361   -0.105084     0.0108316 
 -0.0571402    0.0506615    0.207652    -0.0465298    -0.00807076   0.0638399   -0.0364529   -0.0923203  -0.0961027   -0.1121       0.00834103   0.161506      0.0416119   -0.146518     0.0820384    0.0701492    0.0170948    0.107964     0.00781172  -0.0542756   -0.0267401   -0.120788      0.0853623    0.102347     0.0670353    0.141178  
 -0.0459999    0.00961744   0.00034824   0.0254575    -0.0243344   -0.158799    -0.14798      0.082376    0.134622    -0.0191151   -0.128846    -0.0910274     0.0932812    0.0486382    0.024339     0.132176     0.136832     0.136384    -0.00675935   0.0091495    0.0325013    0.0411108     0.0989511   -0.0496075    0.0554623    0.225592  
  0.0577489    0.111132    -0.0065595   -0.0145328    -0.0483172    0.0876741   -0.0332401   -0.0949063  -0.103921    -0.0132997    0.0286637    0.00139782   -0.0389705   -0.0453138   -0.035653    -0.0334879    0.0425965    0.0687011   -0.136541     0.00164297   0.057382    -0.0123857     0.0737359   -0.0583749    0.0852971   -0.0038549 
  0.0897312   -0.0313325    0.0525078   -0.0426761     0.0959402   -0.0790693   -0.062273     0.0423854   0.0520055    0.0382582    0.0513599    0.00578161   -0.163951     0.00367862   0.127657     0.0320015    0.103349     0.0821893    0.0535468    0.010512     0.231635     0.0759961    -0.209757     0.230773     0.0149634   -0.0435213 
 -0.0792078    0.0471956    0.146226    -0.000567946  -0.120622     0.0502619   -0.056517     0.0379119   0.0727184   -0.0239402    0.0667463   -0.211784     -0.00206053   0.0693501    0.118485     0.122136     0.0191927   -0.125686    -0.121412    -0.174762     0.17874     -0.109863      0.0405166    0.10329     -0.0903657    0.0396589 
 -0.161154    -0.0930077    0.0423626   -0.0563006     0.0532533    0.00687431   0.0296511   -0.0395609  -0.181044    -0.0726779   -0.114747    -0.101695      0.148253    -0.109676    -0.0680586    0.190383    -0.0527197    0.010254    -0.111912    -0.1465      -0.0613544   -0.180888      0.0415517   -0.167905    -0.0974579   -0.0489541 
  0.0869069   -0.099022    -0.0841585   -0.0017307    -0.114887     0.127778     0.0917642    0.419164   -0.0905514    0.06322     -0.104212    -0.199019      0.122394    -0.249415     0.0187002   -0.207294     0.0198229    0.0635512   -0.031084     0.0509152    0.0956767   -0.137016      0.00286286   0.0598143    0.0848665    0.161226  
 -0.00908825  -0.146931    -0.0180282   -0.0163732    -0.0724144    0.0649198   -0.0579706   -0.0827647   0.0578842   -0.021774    -0.111427    -0.0423612    -0.12512      0.00693971  -0.0812753   -0.00908804   0.0822914    0.0716998   -0.201719    -0.0507906   -0.0813309   -0.014256      0.114812    -0.0878089   -0.121828     0.181768  
  0.0100023    0.0609205    0.100583    -0.00489328    0.1094      -0.131604    -0.051692     0.09903     0.0747668   -0.0267292    0.00878861  -0.0491032    -0.0641077    0.0278735    0.0325858   -0.0091222   -0.0476483   -0.111269     0.0531651    0.0327544   -0.036751    -0.000707797   0.0160237    0.0591444    0.00425519  -0.164134  
 -0.0233384   -0.123338     0.10713     -0.0495291    -0.0512886    0.136421    -0.151316    -0.032234    0.0180045    0.0226124   -0.106651    -0.0964947     0.157109     0.0694926   -0.0793426    0.140934    -0.0654516   -0.00441511   0.201665     0.152261    -0.0245732   -0.0289487    -0.0697189    0.00855552   0.0638427    0.0759066 
 -0.101957    -0.0191186   -0.0378493    0.0940438     0.00988198   0.167486     0.0390862   -0.134158    0.148563     0.176352    -0.0788209   -0.227897      0.113302    -0.00171231  -0.185582     0.00504885   0.24293     -0.0739129    0.0242309   -0.073674     0.0602192    0.014139      0.0079548   -0.0849586    0.106739     0.078094  
 -0.00443043  -0.137656    -0.15255     -0.191678      0.0154018   -0.00355724  -0.0428612   -0.0750235   0.185666    -0.0401262   -0.0141856   -0.0286014     0.0560456    0.159842    -0.0917868    0.0250979    0.056517     0.0654821    0.00152983   0.268203     0.0504094    0.0479694    -0.0236608    0.0911862   -0.037353     0.183062  
 -0.028476    -0.0950629   -0.00351649  -0.0359695     0.0674087    0.0806587    0.0911602    0.0432569   0.148059     0.00313716  -0.0636548   -0.200969      0.116346     0.100819    -0.00485743  -0.213742    -0.0531107    0.122132     0.122315     0.00711061   0.00595668   0.0747216    -0.0124337    0.112977    -0.0997788   -0.0310564 
 -0.25723      0.0114705    0.0219408   -0.0510367     0.0737852    0.100484     0.0402232    0.0347605  -0.224984    -0.155652    -0.0179902   -0.248121     -0.116137    -0.00715722  -0.00994926  -0.0938745   -0.0701095    0.101511     0.116717    -0.0916103   -0.016859     0.120016     -0.037646    -0.0728453   -0.0666456    0.0727721 
 -0.143443    -0.122328    -0.00500949  -0.0685138    -0.0153572    0.0267333    0.0816779   -0.0735933  -0.121798    -0.0982814   -0.0692594   -0.0691866    -0.168377    -0.0113371   -0.00110668  -0.00284053   0.138166    -0.0617052   -0.0752279    0.123993     0.100115     0.05365       0.0171143   -0.0204389   -0.0439006    0.0476003 
  0.0100123   -0.0584966    0.221919     0.0286875     0.0615872   -0.0156041    0.0332638    0.174235   -0.171523    -0.0102771   -0.0524638    0.221495      0.136349    -0.0364876   -0.0660914   -0.0323969    0.255363    -0.0264027   -0.0242133   -0.133824     0.0876628   -0.0728309    -0.193441     0.0134203   -0.0415372   -0.0403825 
 -0.0919348    0.104893     0.0855352   -0.0367133     0.0651744    0.0308421    0.00813133   0.198486    0.17096      0.0886719   -0.138128    -0.04611      -0.0429417    0.0120865   -0.00537103   0.0199699   -0.025326    -0.144368     0.103116     0.0103232    0.170732    -0.179558      0.050276    -0.0610478   -0.300259     0.201906  
  0.0432622   -0.0865147    0.00463567   0.0574481     0.030763    -0.00873216   0.0124374   -0.0175123   0.125575     0.0718894   -0.113483     0.0454441    -0.0525733    0.00775452   0.113414    -0.0521093    0.0680432   -0.14756     -0.0927987    0.036529     0.16805      0.138556      0.0288941    0.0742505   -0.0488511   -0.0271787 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 6 16 19 23 25 26
INFO: iteration 1, average log likelihood -1.027940
WARNING: Variances had to be floored 6 10 16 17 19 23 25 26 30
INFO: iteration 2, average log likelihood -0.983852
WARNING: Variances had to be floored 6 8 12 14 15 16 19 21 23 25 26 29
INFO: iteration 3, average log likelihood -0.973582
WARNING: Variances had to be floored 6 10 11 16 17 19 23 25 26 30
INFO: iteration 4, average log likelihood -1.010930
WARNING: Variances had to be floored 6 16 19 21 23 25 26
INFO: iteration 5, average log likelihood -0.996829
WARNING: Variances had to be floored 6 8 10 11 12 14 15 16 17 19 23 25 26 29 30
INFO: iteration 6, average log likelihood -0.963162
WARNING: Variances had to be floored 6 16 19 21 23 25 26
INFO: iteration 7, average log likelihood -1.020893
WARNING: Variances had to be floored 6 10 11 16 17 19 23 25 26 30
INFO: iteration 8, average log likelihood -0.987894
WARNING: Variances had to be floored 6 8 12 14 15 16 19 21 23 25 26 29
INFO: iteration 9, average log likelihood -0.973326
WARNING: Variances had to be floored 6 10 11 16 17 19 23 25 26 30
INFO: iteration 10, average log likelihood -1.010906
INFO: EM with 100000 data points 10 iterations avll -1.010906
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0408369    -0.16927     -0.134332    -0.0920474    0.0508044    0.0560736   -0.159602     0.0142549    0.0823776    -0.118007      0.00956867   -0.0738068    -0.083714    -0.0917095    0.0985445   -0.048806     0.0777388   0.01556       0.0366913   -0.0855985   -0.0450122    -0.0357252   0.102255    -0.220684     0.170363     -0.201213  
 -0.143263      0.0117731   -0.0543825    0.0282267    0.0424975   -0.0382831    0.0948696    0.00433881  -0.0184532     0.100217      0.00961962    0.108266      0.00692305   0.00197706   0.0421213    0.0238555    0.0233012  -0.0178586    -0.113029     0.0360813   -0.0407246    -0.0562585  -0.0967471   -0.0160089   -0.0548485    -0.036712  
 -0.0788666     0.0414698    0.0410934    0.0106937    0.192425     0.0729687   -0.23219      0.118528    -0.0710383    -0.0521235    -0.000344946  -0.0176844     0.0352189   -0.101749     0.0581274   -0.079054    -0.150213    0.0277648    -0.0174993    0.0457281   -0.328448      0.0139821   0.0979937    0.165043    -0.0267572    -0.113016  
 -0.120734     -0.222391    -0.0931008   -0.0262205    0.160361     0.190849     0.00549537   0.121351     0.174189      0.0398558     0.0268545    -0.0629302    -0.0391342   -0.0690259    0.083323    -0.151486     0.141109   -0.109349     -0.173665     0.0539696   -0.0713283    -0.0448501  -0.180911     0.00571128   0.0233184     0.101007  
  0.000889884   0.0739253    0.074792     0.0899862   -0.124886    -0.071831    -0.00944073   0.0653777   -0.0760719    -0.00016816   -0.0143739     0.00477549   -0.0231511   -0.00190287  -0.0229875   -0.185893    -0.0706213   0.0277922     0.0275336   -0.0400104    0.00542312    0.0422747   0.0124855    0.056823    -0.104902     -0.0878025 
 -0.116631      0.0354546   -0.0582156    0.103118     0.0590471    0.156719    -0.133433     0.150199    -0.168684      0.0385123    -0.0529101     0.00775489   -0.0004388    0.0664235    0.0562811   -0.0678751   -0.0347267   0.0723386     0.0926428    0.0645245   -0.195569     -0.0184382  -0.0748138   -0.289403     0.0484198    -0.0614471 
  0.0697893    -0.0393976    0.133984     0.0854713   -0.0767822   -0.0802576   -0.107595    -0.0134267    0.0272161     0.0481872    -0.0380216     0.230963      0.0686411   -0.131615    -0.0053702    0.12878     -0.033077   -0.0647578     0.110401    -0.258715     0.000101881  -0.0312874   0.0706001   -0.059515     0.151392      0.123512  
 -0.0345018     0.0338985   -0.014258    -0.0845364    0.0237252    0.241287     0.0806811   -0.103798     0.146579     -0.0455002    -0.0933623    -0.284546      0.024509     0.0762348    0.0192517    0.0339076   -0.0916959  -0.176453     -0.0942098   -0.0354636   -0.0498101     0.109608    0.0438687   -0.00109571  -0.0734787     0.00877921
 -0.150775     -0.0329223    0.174949    -0.00865548   0.210487     0.185512    -0.201652     0.169757    -0.0456238    -0.000971225   0.1242        0.351958      0.0309555    0.0104575   -0.0100838   -0.125879    -0.0143207   0.0523426    -0.0123135    0.12097     -0.0618252     0.0433888  -0.0451031    0.129821     0.0249639     0.044856  
  0.00735605   -0.035651     0.0958121    0.109978     0.0327236    0.0588706    0.0417448   -0.0915538   -0.130824      0.0205882    -0.0827011    -0.0487583     0.0173111   -0.0403368    0.097076    -0.0394545    0.072953    0.130553     -0.12881     -0.0894301    0.0727915    -0.278525   -0.161972    -0.0163957   -0.0804932    -0.0247351 
 -0.0834632    -0.0687123   -0.0908391    0.0551279   -0.0758872    0.0582661    0.0750195    0.0643285   -0.0276925    -0.134525      0.120386     -0.00844416   -0.00555344   0.101168     0.105698     0.0387884    0.0383091   0.000148665   0.00708074  -0.0114766   -0.0728283    -0.0798566   0.0658317   -0.109065    -0.010584     -0.0705448 
  0.0659606    -0.0060593   -0.0369451   -0.0242122    0.135847    -0.0145984    0.0106654    0.217967    -0.065243     -0.0327453     0.125813      0.00128804   -0.00435713   0.162753     0.213228    -0.0163143    0.0573873   0.167506      0.138511     0.0485214    0.0132054    -0.0557711  -0.044983     0.0623189    0.000121692   0.00909365
 -0.025032     -0.118012    -0.163297    -0.0174543   -0.0716939   -0.0967651    0.0336576    0.231201    -0.0150736    -0.246374      0.00180575   -0.0581992    -0.0216346   -0.21924      0.014426     0.117028     0.0374073  -0.116674     -0.0266831   -0.0198895   -0.0463124    -0.158826    0.103205     0.0566612    0.0496889     0.147317  
  0.142466     -0.0159626    0.0591376    0.165576    -0.00900305  -0.0494592   -0.0245705   -0.160021    -0.0289944    -0.234306     -0.0385634    -0.00926398    0.246023    -0.0449419   -0.0499147   -0.0748494    0.130312   -0.0759246    -0.120072    -0.00409708  -0.0374376    -0.104253    0.113147     0.131221     0.0204567    -0.0332755 
  0.00733015   -0.168261     0.0630692   -0.032526     0.209822    -0.046646    -0.0593503   -0.053773    -0.00333101   -0.059229     -0.0153403    -0.207529     -0.00817757   0.250625    -0.10018      0.0493563   -0.166937    0.0955467    -0.0282065   -0.0272895   -0.0333888     0.0367417   0.111167    -0.0898502    0.0029505    -0.0199317 
 -0.0938476    -0.00058116  -0.03136     -0.0076446    0.00965171  -0.156294    -0.0201001   -0.0263025    0.0311051    -0.0144929    -0.101759     -0.000666157   0.165377    -0.0555355   -0.00459846  -0.0495606    0.0616696  -0.0214792     0.114644     0.0505329    0.168261      0.0724887   0.0508809   -0.0542685   -0.10321       0.142908  
 -0.0343672    -0.0273454    0.184247     0.175558    -0.0484744    0.100087     0.141341     0.162243     0.0156624     0.204991     -0.0934442     0.0373025     0.273514    -0.00636249  -0.0307275   -0.0235889   -0.0340974  -0.22306      -0.0089197    0.0214293   -0.0605126     0.0661986  -0.0613916   -0.0103148    0.146858     -0.0295724 
  0.0129609     0.0837357    0.100694    -0.123179    -0.0228305    0.00860612   0.0618314   -0.0875643    0.0429621    -0.0314102    -0.142477      0.0711124     0.0766913   -0.0963709    0.0462808    0.109654     0.155963    0.00989573   -0.0399529    0.0179995    0.108464     -0.0391477   0.127467     0.0777461   -0.0830237     0.0545217 
 -0.195244     -0.010416     0.0426958    0.010123    -0.112491    -0.145242    -0.0386745   -0.0983758    0.000100682  -0.00658376   -0.0124994     0.00928808    0.0324185   -0.0980392    0.0936246   -0.00929053   0.0385205  -0.0135894    -0.0796021    0.0818675    0.0533633    -0.142147   -0.0579455    0.189911     0.000195901   0.114243  
  0.0383411    -0.11441     -0.036886    -0.12231     -0.120809    -0.0417899   -0.0215481    0.0609241    0.0965074     0.0255574    -0.0869415     0.0548865     0.14613     -0.0469387    0.0404709    0.0487202   -0.0127807   0.0483427     0.0172857    0.0840983    0.139785      0.0114552  -0.034763    -0.224965    -0.0990786    -0.0566788 
 -0.234163     -0.0266657   -0.0181417    0.0963562   -0.188893     0.141036    -0.0287799    0.0242803    0.00406396    0.0350894     0.162421     -0.036258     -0.0721242    0.0758255    0.147458     0.0672573    0.0336378   0.136352     -0.0930984   -0.0603129    0.0523273     0.026044   -0.140322     0.0801793    0.00922675   -0.110934  
  0.0337491     0.00871324  -0.0944038   -0.0931408    0.0321919   -0.0740552   -0.00702511  -0.0675582    0.0537349    -0.00315889   -0.0600293    -0.0602408     0.00449493  -0.00408825   0.0745675    0.154105     0.0616977   0.17058      -0.0799187    0.0117188   -0.0882232     0.0785688  -0.0972092    0.0122451    0.105204     -0.138617  
  0.010543      0.0147502    0.133536     0.0529698   -0.214234     0.0144906    0.0314132    0.0364914   -0.0994355    -0.213671      0.152423      0.0909411     0.025795     0.0548205    0.00396426  -0.16116      0.0455991   0.165555      0.0186687    0.0925447    0.0338094    -0.0186976   0.237187    -0.0786618    0.135755      0.0105304 
  0.0298875    -0.0135774    0.0848035   -0.0624911   -0.204444     0.0716299   -0.0258534   -0.0612467    0.198742     -0.0594958     0.0234184    -0.000737476   0.0899171   -0.0485717   -0.0925257   -0.148627     0.0232253  -0.0774884    -0.0564756   -0.105029    -0.103478     -0.0383908  -0.214872    -0.0297707    0.0460971     0.0750152 
  0.0145686    -0.125923     0.0506526    0.115093     0.201675    -0.0200642   -0.185942     0.106815     0.169184     -0.121556     -0.179677     -0.051683      0.0341512   -0.166987     0.118284    -0.121954     0.0662133   0.137422      0.0840655   -0.0844882   -0.0957844    -0.020845    0.0724952   -0.106828     0.0452772     0.0505102 
  0.0268093     0.00735315   0.042266    -0.14323      0.0718609   -0.0417901   -0.0889036   -0.0227003    0.203151      0.132276     -0.0666397    -0.0845409     0.0970906    0.0122528   -0.0298912   -0.160545     0.0235258   0.0839153     0.105574     0.0976522   -0.128123     -0.0222229   0.0927971   -0.00446278   0.0677478    -0.00608366
 -0.106558     -0.00983176  -0.0141772    0.0936159   -0.0544138   -0.0875465   -0.125868    -0.00753497  -0.0764407     0.169523      0.24821       0.211113      0.0303862   -0.0295438    0.0194743   -0.0620195    0.0641076   0.0990739    -0.13801     -0.0959196    0.0268816    -0.0181002   0.109842     0.0611571   -0.0721458     0.17224   
  0.0865529    -0.0159224   -0.135017     0.0754567   -0.0237212    0.0632611    0.125755    -0.0514381   -0.269326     -0.00432981    0.0224068    -0.136062      0.031977     0.164631     0.0433774   -0.0183471   -0.0434386   0.0792333     0.0118657    0.0211186   -0.147872     -0.0539119  -0.0733844   -0.0809209    0.0625043    -0.077507  
 -0.0716398     0.0592152   -0.0837301   -0.00743202   0.00444195  -0.0700936   -0.0678467   -0.0994068    0.0536021    -0.0760179    -0.0291637    -0.104391     -0.232545    -0.0407718   -0.102401     0.126943    -0.12247     0.00223699    0.0273251    0.0214854    0.0395866    -0.104251    0.0189083   -0.0257036    0.105995      0.0280237 
  0.0982059     0.0186837   -0.00818056   0.130406     0.0242392    0.026915     0.0641486   -0.0949118   -0.0560162     0.169418     -0.1602       -0.0914627    -0.0490861   -0.116296     0.0292029    0.0767994    0.0211862   0.144642     -0.00594356   0.0150774   -0.0796837     0.0144141   0.039474    -0.0389145    0.11345       0.142623  
 -0.00479634    0.0839816    0.0275932    0.0130202    0.108163     0.0414174   -0.0378476   -0.121373     0.0601273    -0.0569258     0.0617386     0.258419      0.0264868    0.0430069    0.191039     0.162129     0.20511    -0.0172695    -0.101575    -0.0407749    0.0624286    -0.104652    0.017363    -0.123547    -0.00984134   -0.0624724 
  0.143808      0.0412457   -0.0886441   -0.0954259   -0.0915918    0.100256     0.0229261    0.0575213    0.103807      0.0048664     0.0139364     0.119977      0.0605572   -0.0368176    0.099       -0.142678     0.0470289  -0.12349       0.00960565   0.0155424    0.100907     -0.0184298  -0.00305615  -0.0738802   -0.0102512    -0.0596316 kind full, method split
0: avll = -1.4184416765190233
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.418459
INFO: iteration 2, average log likelihood -1.418397
INFO: iteration 3, average log likelihood -1.418354
INFO: iteration 4, average log likelihood -1.418307
INFO: iteration 5, average log likelihood -1.418253
INFO: iteration 6, average log likelihood -1.418192
INFO: iteration 7, average log likelihood -1.418126
INFO: iteration 8, average log likelihood -1.418061
INFO: iteration 9, average log likelihood -1.418001
INFO: iteration 10, average log likelihood -1.417948
INFO: iteration 11, average log likelihood -1.417897
INFO: iteration 12, average log likelihood -1.417834
INFO: iteration 13, average log likelihood -1.417728
INFO: iteration 14, average log likelihood -1.417519
INFO: iteration 15, average log likelihood -1.417098
INFO: iteration 16, average log likelihood -1.416347
INFO: iteration 17, average log likelihood -1.415279
INFO: iteration 18, average log likelihood -1.414205
INFO: iteration 19, average log likelihood -1.413476
INFO: iteration 20, average log likelihood -1.413119
INFO: iteration 21, average log likelihood -1.412972
INFO: iteration 22, average log likelihood -1.412915
INFO: iteration 23, average log likelihood -1.412893
INFO: iteration 24, average log likelihood -1.412884
INFO: iteration 25, average log likelihood -1.412880
INFO: iteration 26, average log likelihood -1.412878
INFO: iteration 27, average log likelihood -1.412878
INFO: iteration 28, average log likelihood -1.412877
INFO: iteration 29, average log likelihood -1.412877
INFO: iteration 30, average log likelihood -1.412876
INFO: iteration 31, average log likelihood -1.412876
INFO: iteration 32, average log likelihood -1.412876
INFO: iteration 33, average log likelihood -1.412875
INFO: iteration 34, average log likelihood -1.412875
INFO: iteration 35, average log likelihood -1.412875
INFO: iteration 36, average log likelihood -1.412875
INFO: iteration 37, average log likelihood -1.412875
INFO: iteration 38, average log likelihood -1.412875
INFO: iteration 39, average log likelihood -1.412874
INFO: iteration 40, average log likelihood -1.412874
INFO: iteration 41, average log likelihood -1.412874
INFO: iteration 42, average log likelihood -1.412874
INFO: iteration 43, average log likelihood -1.412874
INFO: iteration 44, average log likelihood -1.412874
INFO: iteration 45, average log likelihood -1.412874
INFO: iteration 46, average log likelihood -1.412874
INFO: iteration 47, average log likelihood -1.412874
INFO: iteration 48, average log likelihood -1.412874
INFO: iteration 49, average log likelihood -1.412874
INFO: iteration 50, average log likelihood -1.412874
INFO: EM with 100000 data points 50 iterations avll -1.412874
952.4 data points per parameter
1: avll = [-1.41846,-1.4184,-1.41835,-1.41831,-1.41825,-1.41819,-1.41813,-1.41806,-1.418,-1.41795,-1.4179,-1.41783,-1.41773,-1.41752,-1.4171,-1.41635,-1.41528,-1.4142,-1.41348,-1.41312,-1.41297,-1.41292,-1.41289,-1.41288,-1.41288,-1.41288,-1.41288,-1.41288,-1.41288,-1.41288,-1.41288,-1.41288,-1.41288,-1.41288,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.412891
INFO: iteration 2, average log likelihood -1.412825
INFO: iteration 3, average log likelihood -1.412780
INFO: iteration 4, average log likelihood -1.412728
INFO: iteration 5, average log likelihood -1.412669
INFO: iteration 6, average log likelihood -1.412602
INFO: iteration 7, average log likelihood -1.412533
INFO: iteration 8, average log likelihood -1.412467
INFO: iteration 9, average log likelihood -1.412409
INFO: iteration 10, average log likelihood -1.412363
INFO: iteration 11, average log likelihood -1.412325
INFO: iteration 12, average log likelihood -1.412294
INFO: iteration 13, average log likelihood -1.412267
INFO: iteration 14, average log likelihood -1.412240
INFO: iteration 15, average log likelihood -1.412213
INFO: iteration 16, average log likelihood -1.412185
INFO: iteration 17, average log likelihood -1.412156
INFO: iteration 18, average log likelihood -1.412125
INFO: iteration 19, average log likelihood -1.412093
INFO: iteration 20, average log likelihood -1.412062
INFO: iteration 21, average log likelihood -1.412032
INFO: iteration 22, average log likelihood -1.412004
INFO: iteration 23, average log likelihood -1.411978
INFO: iteration 24, average log likelihood -1.411955
INFO: iteration 25, average log likelihood -1.411935
INFO: iteration 26, average log likelihood -1.411918
INFO: iteration 27, average log likelihood -1.411903
INFO: iteration 28, average log likelihood -1.411890
INFO: iteration 29, average log likelihood -1.411879
INFO: iteration 30, average log likelihood -1.411868
INFO: iteration 31, average log likelihood -1.411858
INFO: iteration 32, average log likelihood -1.411849
INFO: iteration 33, average log likelihood -1.411840
INFO: iteration 34, average log likelihood -1.411832
INFO: iteration 35, average log likelihood -1.411823
INFO: iteration 36, average log likelihood -1.411815
INFO: iteration 37, average log likelihood -1.411807
INFO: iteration 38, average log likelihood -1.411799
INFO: iteration 39, average log likelihood -1.411791
INFO: iteration 40, average log likelihood -1.411783
INFO: iteration 41, average log likelihood -1.411776
INFO: iteration 42, average log likelihood -1.411768
INFO: iteration 43, average log likelihood -1.411760
INFO: iteration 44, average log likelihood -1.411753
INFO: iteration 45, average log likelihood -1.411745
INFO: iteration 46, average log likelihood -1.411738
INFO: iteration 47, average log likelihood -1.411731
INFO: iteration 48, average log likelihood -1.411724
INFO: iteration 49, average log likelihood -1.411718
INFO: iteration 50, average log likelihood -1.411712
INFO: EM with 100000 data points 50 iterations avll -1.411712
473.9 data points per parameter
2: avll = [-1.41289,-1.41283,-1.41278,-1.41273,-1.41267,-1.4126,-1.41253,-1.41247,-1.41241,-1.41236,-1.41233,-1.41229,-1.41227,-1.41224,-1.41221,-1.41219,-1.41216,-1.41212,-1.41209,-1.41206,-1.41203,-1.412,-1.41198,-1.41196,-1.41194,-1.41192,-1.4119,-1.41189,-1.41188,-1.41187,-1.41186,-1.41185,-1.41184,-1.41183,-1.41182,-1.41182,-1.41181,-1.4118,-1.41179,-1.41178,-1.41178,-1.41177,-1.41176,-1.41175,-1.41175,-1.41174,-1.41173,-1.41172,-1.41172,-1.41171]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.411716
INFO: iteration 2, average log likelihood -1.411660
INFO: iteration 3, average log likelihood -1.411611
INFO: iteration 4, average log likelihood -1.411554
INFO: iteration 5, average log likelihood -1.411485
INFO: iteration 6, average log likelihood -1.411400
INFO: iteration 7, average log likelihood -1.411302
INFO: iteration 8, average log likelihood -1.411198
INFO: iteration 9, average log likelihood -1.411096
INFO: iteration 10, average log likelihood -1.411002
INFO: iteration 11, average log likelihood -1.410918
INFO: iteration 12, average log likelihood -1.410844
INFO: iteration 13, average log likelihood -1.410777
INFO: iteration 14, average log likelihood -1.410715
INFO: iteration 15, average log likelihood -1.410657
INFO: iteration 16, average log likelihood -1.410603
INFO: iteration 17, average log likelihood -1.410554
INFO: iteration 18, average log likelihood -1.410510
INFO: iteration 19, average log likelihood -1.410470
INFO: iteration 20, average log likelihood -1.410435
INFO: iteration 21, average log likelihood -1.410403
INFO: iteration 22, average log likelihood -1.410374
INFO: iteration 23, average log likelihood -1.410348
INFO: iteration 24, average log likelihood -1.410324
INFO: iteration 25, average log likelihood -1.410302
INFO: iteration 26, average log likelihood -1.410281
INFO: iteration 27, average log likelihood -1.410262
INFO: iteration 28, average log likelihood -1.410245
INFO: iteration 29, average log likelihood -1.410228
INFO: iteration 30, average log likelihood -1.410213
INFO: iteration 31, average log likelihood -1.410198
INFO: iteration 32, average log likelihood -1.410185
INFO: iteration 33, average log likelihood -1.410172
INFO: iteration 34, average log likelihood -1.410160
INFO: iteration 35, average log likelihood -1.410149
INFO: iteration 36, average log likelihood -1.410138
INFO: iteration 37, average log likelihood -1.410128
INFO: iteration 38, average log likelihood -1.410119
INFO: iteration 39, average log likelihood -1.410110
INFO: iteration 40, average log likelihood -1.410102
INFO: iteration 41, average log likelihood -1.410094
INFO: iteration 42, average log likelihood -1.410087
INFO: iteration 43, average log likelihood -1.410080
INFO: iteration 44, average log likelihood -1.410073
INFO: iteration 45, average log likelihood -1.410067
INFO: iteration 46, average log likelihood -1.410061
INFO: iteration 47, average log likelihood -1.410055
INFO: iteration 48, average log likelihood -1.410050
INFO: iteration 49, average log likelihood -1.410044
INFO: iteration 50, average log likelihood -1.410039
INFO: EM with 100000 data points 50 iterations avll -1.410039
236.4 data points per parameter
3: avll = [-1.41172,-1.41166,-1.41161,-1.41155,-1.41148,-1.4114,-1.4113,-1.4112,-1.4111,-1.411,-1.41092,-1.41084,-1.41078,-1.41071,-1.41066,-1.4106,-1.41055,-1.41051,-1.41047,-1.41043,-1.4104,-1.41037,-1.41035,-1.41032,-1.4103,-1.41028,-1.41026,-1.41024,-1.41023,-1.41021,-1.4102,-1.41018,-1.41017,-1.41016,-1.41015,-1.41014,-1.41013,-1.41012,-1.41011,-1.4101,-1.41009,-1.41009,-1.41008,-1.41007,-1.41007,-1.41006,-1.41006,-1.41005,-1.41004,-1.41004]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.410042
INFO: iteration 2, average log likelihood -1.409991
INFO: iteration 3, average log likelihood -1.409945
INFO: iteration 4, average log likelihood -1.409892
INFO: iteration 5, average log likelihood -1.409827
INFO: iteration 6, average log likelihood -1.409747
INFO: iteration 7, average log likelihood -1.409648
INFO: iteration 8, average log likelihood -1.409534
INFO: iteration 9, average log likelihood -1.409409
INFO: iteration 10, average log likelihood -1.409279
INFO: iteration 11, average log likelihood -1.409152
INFO: iteration 12, average log likelihood -1.409030
INFO: iteration 13, average log likelihood -1.408917
INFO: iteration 14, average log likelihood -1.408815
INFO: iteration 15, average log likelihood -1.408722
INFO: iteration 16, average log likelihood -1.408641
INFO: iteration 17, average log likelihood -1.408569
INFO: iteration 18, average log likelihood -1.408505
INFO: iteration 19, average log likelihood -1.408449
INFO: iteration 20, average log likelihood -1.408398
INFO: iteration 21, average log likelihood -1.408353
INFO: iteration 22, average log likelihood -1.408311
INFO: iteration 23, average log likelihood -1.408272
INFO: iteration 24, average log likelihood -1.408236
INFO: iteration 25, average log likelihood -1.408203
INFO: iteration 26, average log likelihood -1.408172
INFO: iteration 27, average log likelihood -1.408143
INFO: iteration 28, average log likelihood -1.408117
INFO: iteration 29, average log likelihood -1.408092
INFO: iteration 30, average log likelihood -1.408069
INFO: iteration 31, average log likelihood -1.408048
INFO: iteration 32, average log likelihood -1.408028
INFO: iteration 33, average log likelihood -1.408010
INFO: iteration 34, average log likelihood -1.407992
INFO: iteration 35, average log likelihood -1.407976
INFO: iteration 36, average log likelihood -1.407961
INFO: iteration 37, average log likelihood -1.407947
INFO: iteration 38, average log likelihood -1.407933
INFO: iteration 39, average log likelihood -1.407921
INFO: iteration 40, average log likelihood -1.407909
INFO: iteration 41, average log likelihood -1.407897
INFO: iteration 42, average log likelihood -1.407886
INFO: iteration 43, average log likelihood -1.407875
INFO: iteration 44, average log likelihood -1.407865
INFO: iteration 45, average log likelihood -1.407856
INFO: iteration 46, average log likelihood -1.407846
INFO: iteration 47, average log likelihood -1.407837
INFO: iteration 48, average log likelihood -1.407828
INFO: iteration 49, average log likelihood -1.407820
INFO: iteration 50, average log likelihood -1.407812
INFO: EM with 100000 data points 50 iterations avll -1.407812
118.1 data points per parameter
4: avll = [-1.41004,-1.40999,-1.40994,-1.40989,-1.40983,-1.40975,-1.40965,-1.40953,-1.40941,-1.40928,-1.40915,-1.40903,-1.40892,-1.40881,-1.40872,-1.40864,-1.40857,-1.40851,-1.40845,-1.4084,-1.40835,-1.40831,-1.40827,-1.40824,-1.4082,-1.40817,-1.40814,-1.40812,-1.40809,-1.40807,-1.40805,-1.40803,-1.40801,-1.40799,-1.40798,-1.40796,-1.40795,-1.40793,-1.40792,-1.40791,-1.4079,-1.40789,-1.40788,-1.40787,-1.40786,-1.40785,-1.40784,-1.40783,-1.40782,-1.40781]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.407812
INFO: iteration 2, average log likelihood -1.407744
INFO: iteration 3, average log likelihood -1.407677
INFO: iteration 4, average log likelihood -1.407597
INFO: iteration 5, average log likelihood -1.407494
INFO: iteration 6, average log likelihood -1.407362
INFO: iteration 7, average log likelihood -1.407200
INFO: iteration 8, average log likelihood -1.407013
INFO: iteration 9, average log likelihood -1.406813
INFO: iteration 10, average log likelihood -1.406613
INFO: iteration 11, average log likelihood -1.406425
INFO: iteration 12, average log likelihood -1.406254
INFO: iteration 13, average log likelihood -1.406105
INFO: iteration 14, average log likelihood -1.405978
INFO: iteration 15, average log likelihood -1.405871
INFO: iteration 16, average log likelihood -1.405781
INFO: iteration 17, average log likelihood -1.405705
INFO: iteration 18, average log likelihood -1.405640
INFO: iteration 19, average log likelihood -1.405584
INFO: iteration 20, average log likelihood -1.405536
INFO: iteration 21, average log likelihood -1.405493
INFO: iteration 22, average log likelihood -1.405454
INFO: iteration 23, average log likelihood -1.405420
INFO: iteration 24, average log likelihood -1.405388
INFO: iteration 25, average log likelihood -1.405359
INFO: iteration 26, average log likelihood -1.405332
INFO: iteration 27, average log likelihood -1.405307
INFO: iteration 28, average log likelihood -1.405284
INFO: iteration 29, average log likelihood -1.405262
INFO: iteration 30, average log likelihood -1.405242
INFO: iteration 31, average log likelihood -1.405222
INFO: iteration 32, average log likelihood -1.405204
INFO: iteration 33, average log likelihood -1.405186
INFO: iteration 34, average log likelihood -1.405169
INFO: iteration 35, average log likelihood -1.405153
INFO: iteration 36, average log likelihood -1.405137
INFO: iteration 37, average log likelihood -1.405122
INFO: iteration 38, average log likelihood -1.405108
INFO: iteration 39, average log likelihood -1.405093
INFO: iteration 40, average log likelihood -1.405080
INFO: iteration 41, average log likelihood -1.405067
INFO: iteration 42, average log likelihood -1.405054
INFO: iteration 43, average log likelihood -1.405042
INFO: iteration 44, average log likelihood -1.405030
INFO: iteration 45, average log likelihood -1.405018
INFO: iteration 46, average log likelihood -1.405007
INFO: iteration 47, average log likelihood -1.404996
INFO: iteration 48, average log likelihood -1.404986
INFO: iteration 49, average log likelihood -1.404976
INFO: iteration 50, average log likelihood -1.404966
INFO: EM with 100000 data points 50 iterations avll -1.404966
59.0 data points per parameter
5: avll = [-1.40781,-1.40774,-1.40768,-1.4076,-1.40749,-1.40736,-1.4072,-1.40701,-1.40681,-1.40661,-1.40642,-1.40625,-1.40611,-1.40598,-1.40587,-1.40578,-1.4057,-1.40564,-1.40558,-1.40554,-1.40549,-1.40545,-1.40542,-1.40539,-1.40536,-1.40533,-1.40531,-1.40528,-1.40526,-1.40524,-1.40522,-1.4052,-1.40519,-1.40517,-1.40515,-1.40514,-1.40512,-1.40511,-1.40509,-1.40508,-1.40507,-1.40505,-1.40504,-1.40503,-1.40502,-1.40501,-1.405,-1.40499,-1.40498,-1.40497]
[-1.41844,-1.41846,-1.4184,-1.41835,-1.41831,-1.41825,-1.41819,-1.41813,-1.41806,-1.418,-1.41795,-1.4179,-1.41783,-1.41773,-1.41752,-1.4171,-1.41635,-1.41528,-1.4142,-1.41348,-1.41312,-1.41297,-1.41292,-1.41289,-1.41288,-1.41288,-1.41288,-1.41288,-1.41288,-1.41288,-1.41288,-1.41288,-1.41288,-1.41288,-1.41288,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287,-1.41287,-1.41289,-1.41283,-1.41278,-1.41273,-1.41267,-1.4126,-1.41253,-1.41247,-1.41241,-1.41236,-1.41233,-1.41229,-1.41227,-1.41224,-1.41221,-1.41219,-1.41216,-1.41212,-1.41209,-1.41206,-1.41203,-1.412,-1.41198,-1.41196,-1.41194,-1.41192,-1.4119,-1.41189,-1.41188,-1.41187,-1.41186,-1.41185,-1.41184,-1.41183,-1.41182,-1.41182,-1.41181,-1.4118,-1.41179,-1.41178,-1.41178,-1.41177,-1.41176,-1.41175,-1.41175,-1.41174,-1.41173,-1.41172,-1.41172,-1.41171,-1.41172,-1.41166,-1.41161,-1.41155,-1.41148,-1.4114,-1.4113,-1.4112,-1.4111,-1.411,-1.41092,-1.41084,-1.41078,-1.41071,-1.41066,-1.4106,-1.41055,-1.41051,-1.41047,-1.41043,-1.4104,-1.41037,-1.41035,-1.41032,-1.4103,-1.41028,-1.41026,-1.41024,-1.41023,-1.41021,-1.4102,-1.41018,-1.41017,-1.41016,-1.41015,-1.41014,-1.41013,-1.41012,-1.41011,-1.4101,-1.41009,-1.41009,-1.41008,-1.41007,-1.41007,-1.41006,-1.41006,-1.41005,-1.41004,-1.41004,-1.41004,-1.40999,-1.40994,-1.40989,-1.40983,-1.40975,-1.40965,-1.40953,-1.40941,-1.40928,-1.40915,-1.40903,-1.40892,-1.40881,-1.40872,-1.40864,-1.40857,-1.40851,-1.40845,-1.4084,-1.40835,-1.40831,-1.40827,-1.40824,-1.4082,-1.40817,-1.40814,-1.40812,-1.40809,-1.40807,-1.40805,-1.40803,-1.40801,-1.40799,-1.40798,-1.40796,-1.40795,-1.40793,-1.40792,-1.40791,-1.4079,-1.40789,-1.40788,-1.40787,-1.40786,-1.40785,-1.40784,-1.40783,-1.40782,-1.40781,-1.40781,-1.40774,-1.40768,-1.4076,-1.40749,-1.40736,-1.4072,-1.40701,-1.40681,-1.40661,-1.40642,-1.40625,-1.40611,-1.40598,-1.40587,-1.40578,-1.4057,-1.40564,-1.40558,-1.40554,-1.40549,-1.40545,-1.40542,-1.40539,-1.40536,-1.40533,-1.40531,-1.40528,-1.40526,-1.40524,-1.40522,-1.4052,-1.40519,-1.40517,-1.40515,-1.40514,-1.40512,-1.40511,-1.40509,-1.40508,-1.40507,-1.40505,-1.40504,-1.40503,-1.40502,-1.40501,-1.405,-1.40499,-1.40498,-1.40497]
32×26 Array{Float64,2}:
 -0.497492    0.650274     -0.300162   -0.123548    0.068929     0.248813    -0.198195    0.156062     0.301042    0.013062   -0.485612   -0.131948    -0.0442457    -0.494395    -0.347167   -0.339494   -0.0972405   0.404254   -0.127461   -0.638178     0.454496    0.658968      0.473201    -0.567041      0.152144   -0.402651 
  0.695516    0.685357      0.0349546  -0.184344   -0.0888736    0.0723365   -0.213491   -0.0512546   -0.487278   -0.779764    0.0164077  -0.453598     0.681799     -1.16545     -0.121904    0.328617    0.161012   -0.180754   -0.388053   -0.14096     -0.118312   -0.170115      0.113318    -0.116063     -0.0253842  -0.493713 
  0.130727   -0.564057     -0.408167    0.403588   -0.307202     0.0980597    0.0801728   0.175832     0.0935358   0.0402008   0.238645    0.0994192   -0.117556      0.00705417   0.515349   -0.175177   -0.156038    0.23273    -0.085405   -0.0427405   -0.0327151   0.110028     -0.753126     0.181529     -0.0472695  -0.0770996
 -0.0853299  -0.0228258     0.301612    0.202371   -0.0806419   -0.395428     0.021469   -0.334105     0.111532   -0.0532339   0.160337    0.068166    -0.0808409     0.174843    -0.0275232   0.359058   -0.0906961  -0.179167    0.111091    0.260626    -0.185494   -0.365959      0.636225    -0.382146     -0.083419    0.314837 
  0.514207    0.0111989     0.332143    0.0880322  -0.233999    -0.226587     0.37498     0.168865     0.300484    0.412273   -0.39069    -0.697862    -0.539617      0.308196    -0.561742    0.56558    -0.227795    0.307747    0.570949   -0.143993    -0.162852    0.0323009     0.114849    -0.728303      0.402853   -0.447764 
  0.284156   -0.401129      0.146816   -0.169235    0.61749     -0.0249971   -0.418484   -0.0811334   -0.42343    -0.470987   -0.404526   -0.467557    -0.450925      0.664196    -0.407677   -0.280338   -0.248685    0.521825    0.474373    0.0147101   -0.724589   -0.0512925    -0.638809     0.0963412     0.0567286   0.173083 
  0.679582    0.166384     -0.893566    0.518906    0.320484    -0.441983    -0.0937266  -0.27191      0.383564   -0.0580108   0.354854   -0.101942     0.0534226    -0.38396      0.0896832   0.13623    -0.191844    0.0805346  -0.0909078   0.705067    -1.04525    -0.0972899     0.0813924   -1.11372      -0.0313173  -0.606298 
 -0.168648   -0.227254      0.686874    0.508589    0.0542758   -0.426989    -0.610112   -0.385812     0.367589   -0.276978    0.253244   -0.00756292   0.000512946  -0.239815     0.307531    0.583721   -0.367347    0.484594    0.646025   -0.115949    -0.607591   -0.406978     -0.00425017  -0.267937      0.497014    0.0878991
 -0.24136     0.395046      0.0762808  -0.645639    0.162727     0.571671     0.313943    0.0427835   -0.201263    0.0586872  -0.426645    0.229703    -0.0043424     0.474529     0.0604028  -0.302654    0.464502   -0.210347   -0.155427   -0.0692991    0.641359    0.31362      -0.283799     0.951235      0.22843    -0.102404 
  0.15478    -0.126161     -0.851704   -0.707784    0.612276    -0.301703    -0.338923   -0.35352      0.743325   -0.815359   -0.158158   -0.185175     0.698559      0.153905     0.608308   -0.411925    0.290949    0.1101     -0.114934   -0.0997222    1.03784    -0.0532266    -0.402321    -0.0586125    -0.121155   -0.272666 
 -0.128769   -0.381927      0.445877   -0.764093    0.336136    -0.0638928    0.0183203   0.301209    -0.555892    0.0551743   0.372391    0.0892277   -0.0775304    -0.416833     0.429425   -0.126512    0.53112     0.317499   -0.642529   -0.210231    -0.0566066   0.352146     -0.603925     0.486533      0.182476    0.444069 
  0.131995   -0.0460327     0.395997   -0.305912   -0.135742    -0.47884      0.0950311   0.169107    -0.266042   -0.0337567   0.703921    0.340076     0.267771     -0.0922613    0.427607    0.194981    0.53089     0.239049    0.57537     0.00937112   0.570903    0.139012     -0.0993479    0.581887     -0.323386   -0.24395  
 -0.637446   -0.0398723     0.354341    0.344526    0.00450991   0.932483    -0.568033   -0.0592359    0.256933    0.561717    0.0325777  -0.444845     0.370255      0.0286292   -0.0947262   0.271457    0.0622992  -0.375912   -0.100719   -0.0397175    0.0870312  -0.376288      0.391056     0.0685272    -0.12097     0.54047  
 -0.108998   -0.431971     -0.211428    0.353074   -0.236229     0.301086     0.12609    -0.0860959    0.431232    0.358904   -0.109213   -0.0635915   -0.222444      0.182107     0.405388   -0.0276898  -0.118315   -0.730744    0.0485173   0.601501    -0.0180002  -0.394785     -0.319665     0.333924      0.361241    0.497042 
 -0.532381   -0.42749       0.0736768   0.0337748  -0.203818    -0.346163     0.453257    0.185882     0.370133    1.11743    -0.0128285   0.353642    -0.99414       1.09189     -0.0215338  -0.056414   -0.0281911   0.341232    0.101681   -0.0207167    0.152978    0.119986     -0.294657     0.126046      0.122314    0.167775 
 -0.259817   -0.328917     -0.0903903   0.0305147  -0.140554    -0.0485263   -0.196025    0.589796     0.0246874   0.663963    0.534326    0.527671     0.258713      0.137508     0.323319   -0.301436   -0.31811     0.387923   -0.0688545   0.433555     0.202865    0.171832      0.324771    -0.598753     -0.167293    0.127587 
 -0.382681   -0.0628038     0.0834873  -0.130264    0.0984736    0.116836    -0.159298    0.253571     0.0175025   0.135846    0.0967202   0.0920719    0.103596      0.049736    -0.10622    -0.0265424  -0.110963    0.0661654  -0.0894033   0.0579337    0.12116    -0.00306086   -0.00142955   0.000244483  -0.031513    0.110246 
  0.25392    -0.000191819  -0.0497124   0.0671582  -0.030977     0.00352073   0.0830428  -0.0491779   -0.0718593  -0.102981   -0.0703899  -0.0812814    0.0172085    -0.0857792    0.0905119  -0.0141063   0.0964454   0.0140368   0.0233537  -0.0319594   -0.125255   -0.0250571     0.0357662   -0.0633399     0.0545501  -0.0296684
 -0.0388513  -0.116245      0.0192774   0.130581    0.0996207    0.294896    -0.220837    0.283404    -0.779059    0.27038    -0.248651   -0.307859     0.0616483    -0.474847    -0.407257   -0.538543    0.231587    0.0870347   0.662035    0.303956    -0.0892048   0.287978      0.510116     0.531177      0.387603    0.11798  
 -0.150363   -0.0562723     0.292498   -0.236254    0.21919      0.491089    -0.579251    0.414664     0.350522    0.285655   -0.567778    0.449817    -0.0137841    -0.0655208    0.324039   -0.329333    0.0448117   0.0361964   0.185894   -0.226094    -0.214122    0.483128      0.277861    -0.0136685     0.290941    0.109713 
 -0.506698   -0.211914     -0.583721    0.209523   -0.0766711    0.0732499    0.0664293  -0.455981     0.511398   -0.0626681  -0.387079   -0.0795788   -0.126889      0.36728     -0.137141   -0.496805   -0.0793656   0.0690697   0.200698   -0.0190902    0.458882   -0.0396524     0.181508    -0.178821     -0.163599    0.0470209
  0.669395   -0.0922607    -0.535711    0.1065     -0.118904    -0.354554     0.463933    0.0100549   -0.209966   -0.421454   -0.332274    0.217835    -0.211273     -0.234871     0.129698   -0.684527   -0.0436559   0.247611   -0.023359   -0.211549    -0.258624    0.380229      0.0973785   -0.18793      -0.0174775   0.0345208
  0.0642375   0.359174     -0.481883    0.197958   -0.156706     0.230432     0.588726    0.536439    -0.329541   -0.128996   -0.118676   -0.00895523   0.0118744     0.140015    -0.742936   -0.0744061   0.157189    0.0432344  -0.0315353   0.0373045    0.477934    0.14805      -0.146799    -0.177448     -0.461179   -0.748993 
  0.332445    0.522692     -0.416928   -0.0151238  -0.0483143    0.0979427    0.0607634  -0.00369701  -0.0196326  -0.199175   -0.14059    -0.0351383    0.210731     -0.0295593    0.145686   -0.0881664  -0.123936   -0.0172734  -0.206206   -0.159846     0.155161    0.363451     -0.567972     0.18108       0.351226   -1.00026  
  0.165827    0.428182      0.0435283   0.130058   -0.210653    -0.298156     0.428089   -0.764111    -0.216866   -0.589977    0.0719872  -0.415052     0.149566      0.273442    -0.393356    0.096954    0.176437    0.0842761  -0.527247   -0.383967    -0.235621   -0.529717     -0.0670697   -0.142284      0.212554   -0.0737898
  0.0667349   0.164865      0.570466   -0.276651   -0.102585    -0.292771    -0.240547   -0.482466     0.209741    0.191137   -0.12736    -0.205341     0.0253765    -0.284281    -0.027611    0.735809    0.193005   -0.348879   -0.0882045   0.110789    -0.199118   -0.13928       0.290733     0.345067      0.533308   -0.120959 
  0.0206129   0.256218      0.179854    0.0473511   0.431328     0.0339478   -0.602098   -0.431304    -0.228857   -0.396991   -0.288982   -0.373431     0.79495      -0.107566     0.338353   -0.28654     0.495463    0.336771    0.0858501  -0.263243    -0.0871642  -0.310434      0.386579     0.00891985   -0.142921    0.0955159
 -0.418855    0.292923     -0.426544    0.121137   -0.0186575    0.47432     -0.26213    -0.378472    -0.513509   -0.695722    0.242814    0.472675     0.808448     -0.64578      0.155681   -0.478136    0.282857   -0.734613   -0.648145    0.163164    -0.2736     -0.000957325   0.401149     0.485669     -0.290609    0.359314 
 -0.180564   -0.129422      0.264046   -0.12791    -0.0688917   -0.34792      0.115598   -0.178979     0.404319   -0.180571    0.409164    0.37717     -0.052598      0.226602     0.262501    0.484918   -0.176755   -0.13962    -0.554256   -0.0354614   -0.0874871  -0.459077     -0.0219074   -0.673629     -0.663766    0.180527 
 -0.387731   -0.16686       0.0235061   0.0782091   0.867484     0.171554     0.186608    0.00604902   0.122812   -0.547023    0.136225    0.520244     0.387668      0.656323     0.632882   -0.164038   -0.145078   -0.0226069  -0.352631    0.012947    -0.394291   -0.394427     -0.703981     0.334687     -0.0863838   0.254934 
  0.446023   -0.677882     -0.356618    0.331127   -0.710632    -0.3719       0.19331     0.431088    -0.131977   -0.25297     0.322499    0.153183    -0.418605     -0.461055    -0.437052    0.0551633  -0.180373   -0.438251    0.377822   -0.190192    -0.302311   -0.442573      0.328772    -0.465913     -0.404121    0.340291 
  0.0320353   0.0106479     0.510349    0.141016   -0.172423     0.286628     0.0712729   0.883641    -0.270784    0.327812    0.206577    0.0887627   -0.430165     -0.625312    -0.126824    0.64596    -0.248495    0.063811   -0.229751    0.0518601   -0.506601   -0.246006      0.0986869   -0.138947      0.0417524   0.317441 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.404956
INFO: iteration 2, average log likelihood -1.404947
INFO: iteration 3, average log likelihood -1.404938
INFO: iteration 4, average log likelihood -1.404929
INFO: iteration 5, average log likelihood -1.404921
INFO: iteration 6, average log likelihood -1.404912
INFO: iteration 7, average log likelihood -1.404904
INFO: iteration 8, average log likelihood -1.404896
INFO: iteration 9, average log likelihood -1.404889
INFO: iteration 10, average log likelihood -1.404881
INFO: EM with 100000 data points 10 iterations avll -1.404881
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.767339e+05
      1       7.021242e+05      -1.746097e+05 |       32
      2       6.843154e+05      -1.780881e+04 |       32
      3       6.783234e+05      -5.992001e+03 |       32
      4       6.754710e+05      -2.852476e+03 |       32
      5       6.737001e+05      -1.770849e+03 |       32
      6       6.724962e+05      -1.203929e+03 |       32
      7       6.716011e+05      -8.950944e+02 |       32
      8       6.708855e+05      -7.156318e+02 |       32
      9       6.702163e+05      -6.691279e+02 |       32
     10       6.696176e+05      -5.986940e+02 |       32
     11       6.690771e+05      -5.405605e+02 |       32
     12       6.685736e+05      -5.034576e+02 |       32
     13       6.681345e+05      -4.391380e+02 |       32
     14       6.677842e+05      -3.503032e+02 |       32
     15       6.674713e+05      -3.128534e+02 |       32
     16       6.672168e+05      -2.545506e+02 |       32
     17       6.670123e+05      -2.044223e+02 |       32
     18       6.668280e+05      -1.843024e+02 |       32
     19       6.666720e+05      -1.560596e+02 |       32
     20       6.665331e+05      -1.388498e+02 |       32
     21       6.664161e+05      -1.170147e+02 |       32
     22       6.663241e+05      -9.205046e+01 |       32
     23       6.662416e+05      -8.244591e+01 |       32
     24       6.661661e+05      -7.551780e+01 |       32
     25       6.660914e+05      -7.469362e+01 |       32
     26       6.660220e+05      -6.942798e+01 |       32
     27       6.659554e+05      -6.658704e+01 |       32
     28       6.658883e+05      -6.706639e+01 |       32
     29       6.658326e+05      -5.576706e+01 |       32
     30       6.657797e+05      -5.290733e+01 |       32
     31       6.657245e+05      -5.518967e+01 |       32
     32       6.656766e+05      -4.785136e+01 |       32
     33       6.656315e+05      -4.506443e+01 |       32
     34       6.655934e+05      -3.817021e+01 |       32
     35       6.655538e+05      -3.953929e+01 |       32
     36       6.655154e+05      -3.844234e+01 |       32
     37       6.654779e+05      -3.750243e+01 |       32
     38       6.654377e+05      -4.018274e+01 |       32
     39       6.654068e+05      -3.092285e+01 |       32
     40       6.653844e+05      -2.242553e+01 |       32
     41       6.653589e+05      -2.542237e+01 |       32
     42       6.653340e+05      -2.492830e+01 |       32
     43       6.653068e+05      -2.724294e+01 |       32
     44       6.652793e+05      -2.751573e+01 |       32
     45       6.652576e+05      -2.167401e+01 |       32
     46       6.652377e+05      -1.987488e+01 |       32
     47       6.652170e+05      -2.066274e+01 |       32
     48       6.651937e+05      -2.334184e+01 |       32
     49       6.651715e+05      -2.218819e+01 |       32
     50       6.651459e+05      -2.563675e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 665145.876745892)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.416964
INFO: iteration 2, average log likelihood -1.411902
INFO: iteration 3, average log likelihood -1.410485
INFO: iteration 4, average log likelihood -1.409388
INFO: iteration 5, average log likelihood -1.408274
INFO: iteration 6, average log likelihood -1.407325
INFO: iteration 7, average log likelihood -1.406702
INFO: iteration 8, average log likelihood -1.406343
INFO: iteration 9, average log likelihood -1.406126
INFO: iteration 10, average log likelihood -1.405977
INFO: iteration 11, average log likelihood -1.405864
INFO: iteration 12, average log likelihood -1.405772
INFO: iteration 13, average log likelihood -1.405696
INFO: iteration 14, average log likelihood -1.405630
INFO: iteration 15, average log likelihood -1.405572
INFO: iteration 16, average log likelihood -1.405522
INFO: iteration 17, average log likelihood -1.405477
INFO: iteration 18, average log likelihood -1.405437
INFO: iteration 19, average log likelihood -1.405401
INFO: iteration 20, average log likelihood -1.405369
INFO: iteration 21, average log likelihood -1.405339
INFO: iteration 22, average log likelihood -1.405313
INFO: iteration 23, average log likelihood -1.405288
INFO: iteration 24, average log likelihood -1.405265
INFO: iteration 25, average log likelihood -1.405244
INFO: iteration 26, average log likelihood -1.405225
INFO: iteration 27, average log likelihood -1.405206
INFO: iteration 28, average log likelihood -1.405189
INFO: iteration 29, average log likelihood -1.405173
INFO: iteration 30, average log likelihood -1.405157
INFO: iteration 31, average log likelihood -1.405142
INFO: iteration 32, average log likelihood -1.405128
INFO: iteration 33, average log likelihood -1.405115
INFO: iteration 34, average log likelihood -1.405102
INFO: iteration 35, average log likelihood -1.405089
INFO: iteration 36, average log likelihood -1.405077
INFO: iteration 37, average log likelihood -1.405065
INFO: iteration 38, average log likelihood -1.405053
INFO: iteration 39, average log likelihood -1.405042
INFO: iteration 40, average log likelihood -1.405031
INFO: iteration 41, average log likelihood -1.405021
INFO: iteration 42, average log likelihood -1.405010
INFO: iteration 43, average log likelihood -1.405000
INFO: iteration 44, average log likelihood -1.404990
INFO: iteration 45, average log likelihood -1.404980
INFO: iteration 46, average log likelihood -1.404971
INFO: iteration 47, average log likelihood -1.404961
INFO: iteration 48, average log likelihood -1.404952
INFO: iteration 49, average log likelihood -1.404943
INFO: iteration 50, average log likelihood -1.404934
INFO: EM with 100000 data points 50 iterations avll -1.404934
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.149457    -0.233479   -0.258733     -0.295388     0.0504771  -0.0774769   0.00791158   0.575428   -0.0090831    0.639266     0.0622696   0.241891    -0.285488     0.202168   -0.145687   -0.310137    -0.390592    0.158303    0.06864     0.158585     0.314955     0.438447    -0.0545147   -0.0110475   0.0285289  -0.087604  
 -0.375547     0.0314181   0.241573      0.512188     0.078302    0.808294   -0.377729    -0.1138      0.093118     0.439364    -0.155055   -0.690623     0.0652034   -0.107185   -0.386854    0.218247    -0.0721854  -0.454037   -0.4935      0.124375    -0.333352    -0.472207     0.431287    -0.276334   -0.220481    0.747414  
 -0.119744    -0.823876   -0.206375      0.24655     -0.0930358   0.237888   -0.0339466    0.162988    0.651699     0.389665    -0.126813    0.176751    -0.376342     0.274424    0.687434   -0.161028    -0.468878   -0.441982    0.278248    0.509156    -0.112882    -0.2051      -0.371421     0.245424    0.545979    0.730705  
 -0.515121     0.138676   -0.0319018    -0.396899     0.49796    -0.377581   -0.248007    -0.665475    0.632237    -0.0901053   -0.256896   -0.847413     0.328019     0.249433   -0.633929    0.0410948    0.103143    0.516583    0.0556467   0.110826     0.602562     0.109053     0.209771    -0.291161    0.340009   -0.250208  
 -0.0976276    0.179514   -0.165523     -0.265101    -0.0981532   0.790244   -0.313737     0.607736   -0.58744      0.411071    -0.146906    0.0373993    0.508031    -0.512033   -0.432114   -0.258505    -0.133127   -0.139212    0.120338    0.298956     0.0711694    0.544362     0.165292     0.169766    0.480427   -0.321946  
 -0.0457745    0.252525   -0.194914      0.261786    -0.340994    0.0323942   0.293638    -0.417885    0.491197     0.21161     -0.157517    0.0459617    0.058158     0.412113    0.167808    0.0762026    0.0180007  -0.45821    -0.258174    0.106199     0.0655199   -0.102061    -0.10709     -0.0556664   0.272076   -0.288069  
  0.444396     0.109095    0.0279629     0.239233     0.0321648  -0.331193   -0.221238    -0.192451   -0.0721287   -0.452571     0.300244   -0.125241     0.233992    -0.433505    0.0470807   0.331203    -0.034019   -0.017059   -0.0703752   0.129218    -0.520064    -0.247423     0.251881    -0.371622   -0.0369597  -0.00183177
  0.218788     0.719003    0.0224594     0.00668899  -0.0874338  -0.0438632   0.125892    -0.199576   -0.485039    -0.453271    -0.0783927  -0.436705     0.395163    -0.309529   -0.401263    0.232334     0.220817    0.223807   -0.611814   -0.600038     0.106876    -0.205887    -0.109682    -0.0915872  -0.0674351  -0.652377  
 -0.272508    -0.118996    0.0487209    -0.132908     0.207367   -0.0930307  -0.0837936   -0.0557616   0.383724    -0.497146    -0.11363     0.36798     -0.0876077    0.384713    0.0354395  -0.182916    -0.119215    0.228003   -0.279077   -0.154417    -0.172152    -0.145796    -0.114397    -0.544188   -0.554636    0.436567  
  0.0802085   -0.105546    0.63367       0.346581    -0.341909   -0.310714    0.396638     0.464918    0.254081     0.438031    -0.0872487  -0.171002    -0.792895     0.254446   -0.449721    0.627775    -0.50225     0.475727    0.338999   -0.00152617  -0.353027    -0.134574     0.04879     -0.799468    0.174814   -0.141221  
 -0.345202     0.255362   -0.403858      0.0740777    0.0114058   0.428635   -0.0378865   -0.28059    -0.42693     -0.834087     0.137352    0.485365     0.53068     -0.466163    0.128799   -0.486936     0.282217   -0.737479   -0.645988   -0.00021139  -0.318587     0.00452127   0.10034      0.635684   -0.326997    0.434358  
  0.189026    -0.0550063   0.502316     -0.375078     0.221106   -0.191161   -0.0561684    0.490128   -0.0521055    0.38007      0.458389    0.29411      0.0440099   -0.354951    0.443847    0.580513     0.175287   -0.146786   -0.817077    0.126901    -0.280279    -0.284895    -0.290966    -0.0645427  -0.0422707   0.163539  
  0.569498    -0.202316   -0.0136882     0.215301    -0.773543   -0.22515     0.152984     0.39141    -0.28573     -0.0417287    0.130458    0.195808    -0.762439    -0.708111   -0.338247    0.0817839   -0.154379   -0.493205    0.294561   -0.376444    -0.414926    -0.216503     0.804107    -0.41916    -0.328422    0.404288  
  0.475161    -0.168692   -0.826962     -0.675249     0.428853   -0.170484   -0.477221    -0.226962    0.745035    -0.885796    -0.0111149  -0.104769     0.688394    -0.0308529   0.82809    -0.343326     0.230772   -0.085162   -0.190866   -0.215848     0.938264    -0.130607    -0.494761    -0.01878    -0.256083   -0.335468  
 -0.287279     0.109044    0.486194     -0.964359     0.182405    0.0856382   0.196063     0.0891186  -0.416586    -0.0498071   -0.139271    0.326908    -0.150404     0.132819    0.346757   -0.268192     0.590663    0.45618    -0.171476   -0.465439     0.484977     0.518499    -0.343514     0.882077    0.213161    0.158149  
 -0.641664    -0.159668   -0.000486073  -0.0774346    1.01934     0.197387    0.245241    -0.0300406   0.111462    -0.645117    -0.0192592   0.203768     0.65473      0.70037     0.518965   -0.133844    -0.166849    0.264917   -0.318435    0.251182    -0.271509    -0.279166    -0.977112     0.657071    0.238431   -0.051054  
 -0.635942     0.14736     0.362606     -0.018963     0.022807    0.591762   -0.792468    -0.20246     0.403967     0.0620533   -0.0797282   0.00895335   1.07027     -0.216162    0.600728    0.120426    -0.0436273  -0.097376    0.0166297  -0.204696     0.303634    -0.0184964    0.540922     0.0868177   0.294987    0.387325  
 -0.181878    -0.297131   -0.5831        0.80047     -0.461675    0.0218106  -0.183157     0.286937   -0.137114     0.295008     0.508923    0.261199     0.542935    -0.0576988   0.0688911   0.0222431   -0.501674    0.135429   -0.144091    0.172404    -0.139324    -0.391683     0.0910347   -0.784502   -0.37254    -0.181992  
 -0.187042     0.176599    0.0970142    -0.160503     0.052418   -0.147588    0.591991    -0.629869   -0.0609829    0.23318      0.715611    0.4331       0.138328     0.954114    0.29417     0.795989    -0.238742   -0.442246   -0.10053     0.303059     0.696234    -0.334482    -0.00638125  -0.0246955  -0.616129   -0.14677   
  0.0779565    0.111569    0.184866     -0.046315     0.333045   -0.122912   -0.534863    -0.201129   -0.452768    -0.161855    -0.0410987  -0.0700617    0.542345    -0.0877747   0.290267   -0.426291     0.806419    0.133341    0.420426    0.0239583    0.143898    -0.226934     0.374893     0.403932   -0.226626   -0.0460778 
 -0.604787     0.0723841  -0.747215      0.385735    -0.17591    -0.0363887   0.103593    -0.53989     1.13815     -0.0129092   -0.615183    0.0374105   -0.359131     0.0453495  -0.188063   -0.488021    -0.138626   -0.167193   -0.0225939  -0.178307     0.505605    -0.0991024    0.471803    -0.587813   -0.0781191  -0.199072  
 -0.00399731   0.253749   -0.470625     -0.0449331   -0.0242002   0.339905    0.278866     0.433357   -0.00881988  -0.00660843  -0.210995    0.162777    -0.0596964    0.061188   -0.017053   -0.518733     0.189045    0.160822   -0.0720372  -0.126599     0.45766      0.457229    -0.2863       0.199719   -0.0783686  -0.476427  
  0.850463     0.30059    -0.70917      -0.0245445    0.0524135  -0.415198    0.647181    -0.247014   -0.142335    -0.434002    -0.268404   -0.0272788    0.0549789   -0.0247483   0.175404   -0.340769    -0.255588    0.327256   -0.191503    0.117637    -0.258003     0.382936     0.011178    -0.488654    0.138625   -0.385132  
  0.105521    -0.0287164   0.250891      0.312839    -0.0432606  -0.651394   -0.231221    -0.659662    0.253853    -0.388904     0.398277    0.0039229    0.11448     -0.387405    0.333549    0.49337     -0.0844696   0.341891    0.218712   -0.0520321   -0.659776    -0.325077     0.292881    -0.35183     0.470838   -0.131762  
 -0.797133    -0.524941   -0.0454534     0.271737    -0.43793     0.10534     0.20225      0.0367113   0.293504     1.12884      0.136325    0.140886    -0.683492     0.72627     0.0201752  -0.108557     0.394527    0.267374    0.298809   -0.117674     0.277131    -0.203045    -0.0679389    0.48695    -0.183159    0.315382  
  0.169306    -0.0637024  -0.331044      0.260894    -0.0794759   0.392271    0.0268308    0.120408   -0.252092    -0.201806    -0.542428   -0.268305     0.00435751  -0.214241   -0.166567   -0.480796     0.17271     0.203046    0.475785   -0.0993178    0.00124346   0.398754     0.0986567    0.0966165   0.137578   -0.291503  
 -0.143632     0.152384    0.725271     -0.0674757    0.201617    0.160435   -0.568279     0.264958    0.288838     0.206887    -0.179917    0.009164    -0.143551    -0.179372    0.119606    0.33727     -0.279584    0.335641    0.396715   -0.093038    -0.383818     0.132726     0.110289    -0.0251064   0.521749   -0.020922  
  0.553519    -0.602276   -0.19607       0.138762     0.468483   -0.10846    -0.325439    -0.113296   -0.37186     -0.375391    -0.112905   -0.448966    -0.562836     0.433569   -0.0457068  -0.326439    -0.20953     0.473714    0.393414   -0.241017    -0.905566    -0.0796546   -0.64821     -0.0022681   0.045709    0.12401   
 -0.0347891   -0.0352626   0.0596871    -0.0372256    0.0483991   0.0657023  -0.0544486    0.0564165  -0.0519191   -0.00162336   0.0276882  -0.0046236    0.0786965   -0.0731579   0.0518101   0.00488652   0.049414   -0.0136981  -0.0526068   0.00268982  -0.0366525   -0.0738679    0.0348333    0.0255303   0.033184    0.0748742 
  0.110498    -0.0580196   0.618267     -0.249493    -0.0986789  -0.0578496  -0.103168    -0.506069   -0.119972    -0.147513    -0.217196   -0.402425    -0.191698     0.0516075  -0.29357     0.394225     0.342996   -0.610915    0.203176    0.0568173   -0.216216    -0.374466    -0.0289277    0.698136    0.437695    0.186652  
 -0.440048    -0.566223    0.614477      0.118874     0.281734   -0.101518   -0.262727     0.379032   -0.392908     0.486289     0.453312    0.61896     -0.115875    -0.088837    0.455025   -0.209586     0.297257    0.150299    0.0687191   0.456175    -0.0740954    0.049844     0.692954    -0.208897   -0.18603     0.966023  
  0.313526    -0.339914   -0.117631      0.0510618   -0.411494   -0.382116    0.640789     0.38779    -0.102921    -0.406394     0.389519   -0.00627281  -0.163376    -0.229295   -0.275427    0.267619     0.150331   -0.119299    0.190885    0.172122     0.215649    -0.19359     -0.381159    -0.013121   -0.387493   -0.132756  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.404925
INFO: iteration 2, average log likelihood -1.404916
INFO: iteration 3, average log likelihood -1.404907
INFO: iteration 4, average log likelihood -1.404899
INFO: iteration 5, average log likelihood -1.404891
INFO: iteration 6, average log likelihood -1.404883
INFO: iteration 7, average log likelihood -1.404875
INFO: iteration 8, average log likelihood -1.404867
INFO: iteration 9, average log likelihood -1.404859
INFO: iteration 10, average log likelihood -1.404852
INFO: EM with 100000 data points 10 iterations avll -1.404852
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
