>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.6.6
INFO: Installing JLD v0.6.4
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.0
INFO: Installing Rmath v0.1.3
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.1.1
INFO: Installing StaticArrays v0.0.8
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.787
Commit c71f205 (2016-09-26 16:28 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-96-generic #143-Ubuntu SMP Mon Aug 29 20:15:20 UTC 2016 x86_64 x86_64
Memory: 2.9392929077148438 GB (687.26171875 MB free)
Uptime: 25134.0 sec
Load Avg:  0.98974609375  1.0146484375  1.04248046875
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3500 MHz    1537186 s        101 s     198463 s     511168 s         61 s
#2  3500 MHz     622385 s       6407 s      96604 s    1677602 s          1 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.2
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.6.6
 - JLD                           0.6.4
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.0
 - Rmath                         0.1.3
 - SHA                           0.2.1
 - ScikitLearnBase               0.1.1
 - StaticArrays                  0.0.8
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:345
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:378
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:346
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1739
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-576103.8638850873,[94753.9,5246.08],
[-1992.85 8526.52 -4536.79; 2204.3 -8513.75 4821.93],

Array{Float64,2}[
[92890.7 3374.0 3.74824; 3374.0 84231.0 6497.95; 3.74824 6497.95 90229.0],

[7646.21 -2831.49 42.0858; -2831.49 16194.0 -6418.61; 42.0858 -6418.61 9757.61]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       2.285324e+03
      1       1.312620e+03      -9.727037e+02 |        8
      2       1.138826e+03      -1.737940e+02 |        5
      3       1.122861e+03      -1.596533e+01 |        0
      4       1.122861e+03       0.000000e+00 |        0
K-means converged with 4 iterations (objv = 1122.860863707458)
INFO: K-means with 272 data points using 4 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.090276
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.734711
INFO: iteration 2, lowerbound -3.574064
INFO: iteration 3, lowerbound -3.396895
INFO: iteration 4, lowerbound -3.195379
INFO: iteration 5, lowerbound -2.990412
INFO: dropping number of Gaussions to 7
INFO: iteration 6, lowerbound -2.793806
INFO: dropping number of Gaussions to 6
INFO: iteration 7, lowerbound -2.621115
INFO: dropping number of Gaussions to 5
INFO: iteration 8, lowerbound -2.489683
INFO: iteration 9, lowerbound -2.406726
INFO: dropping number of Gaussions to 4
INFO: iteration 10, lowerbound -2.360569
INFO: dropping number of Gaussions to 3
INFO: iteration 11, lowerbound -2.326926
INFO: iteration 12, lowerbound -2.309174
INFO: iteration 13, lowerbound -2.308831
INFO: dropping number of Gaussions to 2
INFO: iteration 14, lowerbound -2.302916
INFO: iteration 15, lowerbound -2.299259
INFO: iteration 16, lowerbound -2.299256
INFO: iteration 17, lowerbound -2.299254
INFO: iteration 18, lowerbound -2.299254
INFO: iteration 19, lowerbound -2.299253
INFO: iteration 20, lowerbound -2.299253
INFO: iteration 21, lowerbound -2.299253
INFO: iteration 22, lowerbound -2.299253
INFO: iteration 23, lowerbound -2.299253
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Mon 10 Oct 2016 11:28:29 AM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Mon 10 Oct 2016 11:28:31 AM UTC: K-means with 272 data points using 4 iterations
11.3 data points per parameter
,Mon 10 Oct 2016 11:28:32 AM UTC: EM with 272 data points 0 iterations avll -2.090276
5.8 data points per parameter
,Mon 10 Oct 2016 11:28:33 AM UTC: GMM converted to Variational GMM
,Mon 10 Oct 2016 11:28:35 AM UTC: iteration 1, lowerbound -3.734711
,Mon 10 Oct 2016 11:28:35 AM UTC: iteration 2, lowerbound -3.574064
,Mon 10 Oct 2016 11:28:35 AM UTC: iteration 3, lowerbound -3.396895
,Mon 10 Oct 2016 11:28:35 AM UTC: iteration 4, lowerbound -3.195379
,Mon 10 Oct 2016 11:28:35 AM UTC: iteration 5, lowerbound -2.990412
,Mon 10 Oct 2016 11:28:35 AM UTC: dropping number of Gaussions to 7
,Mon 10 Oct 2016 11:28:35 AM UTC: iteration 6, lowerbound -2.793806
,Mon 10 Oct 2016 11:28:35 AM UTC: dropping number of Gaussions to 6
,Mon 10 Oct 2016 11:28:35 AM UTC: iteration 7, lowerbound -2.621115
,Mon 10 Oct 2016 11:28:36 AM UTC: dropping number of Gaussions to 5
,Mon 10 Oct 2016 11:28:36 AM UTC: iteration 8, lowerbound -2.489683
,Mon 10 Oct 2016 11:28:36 AM UTC: iteration 9, lowerbound -2.406726
,Mon 10 Oct 2016 11:28:36 AM UTC: dropping number of Gaussions to 4
,Mon 10 Oct 2016 11:28:36 AM UTC: iteration 10, lowerbound -2.360569
,Mon 10 Oct 2016 11:28:36 AM UTC: dropping number of Gaussions to 3
,Mon 10 Oct 2016 11:28:36 AM UTC: iteration 11, lowerbound -2.326926
,Mon 10 Oct 2016 11:28:36 AM UTC: iteration 12, lowerbound -2.309174
,Mon 10 Oct 2016 11:28:36 AM UTC: iteration 13, lowerbound -2.308831
,Mon 10 Oct 2016 11:28:36 AM UTC: dropping number of Gaussions to 2
,Mon 10 Oct 2016 11:28:36 AM UTC: iteration 14, lowerbound -2.302916
,Mon 10 Oct 2016 11:28:36 AM UTC: iteration 15, lowerbound -2.299259
,Mon 10 Oct 2016 11:28:36 AM UTC: iteration 16, lowerbound -2.299256
,Mon 10 Oct 2016 11:28:36 AM UTC: iteration 17, lowerbound -2.299254
,Mon 10 Oct 2016 11:28:36 AM UTC: iteration 18, lowerbound -2.299254
,Mon 10 Oct 2016 11:28:36 AM UTC: iteration 19, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:36 AM UTC: iteration 20, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:37 AM UTC: iteration 21, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:37 AM UTC: iteration 22, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:37 AM UTC: iteration 23, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:37 AM UTC: iteration 24, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:37 AM UTC: iteration 25, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:37 AM UTC: iteration 26, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:37 AM UTC: iteration 27, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:37 AM UTC: iteration 28, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:37 AM UTC: iteration 29, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:37 AM UTC: iteration 30, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:37 AM UTC: iteration 31, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:37 AM UTC: iteration 32, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:37 AM UTC: iteration 33, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:37 AM UTC: iteration 34, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:38 AM UTC: iteration 35, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:38 AM UTC: iteration 36, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:38 AM UTC: iteration 37, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:38 AM UTC: iteration 38, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:38 AM UTC: iteration 39, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:38 AM UTC: iteration 40, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:38 AM UTC: iteration 41, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:38 AM UTC: iteration 42, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:38 AM UTC: iteration 43, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:38 AM UTC: iteration 44, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:38 AM UTC: iteration 45, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:38 AM UTC: iteration 46, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:39 AM UTC: iteration 47, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:39 AM UTC: iteration 48, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:39 AM UTC: iteration 49, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:39 AM UTC: iteration 50, lowerbound -2.299253
,Mon 10 Oct 2016 11:28:39 AM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.9549,178.045]
β = [95.9549,178.045]
m = [2.00023 53.852; 4.2503 79.2869]
ν = [97.9549,180.045]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.375876 -0.00895312; 0.0 0.0127487],

[0.184042 -0.00764405; 0.0 0.00858171]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 99999.99999999997
avll from stats: -1.0027033431293972
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -1.002703343129397
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -1.002703343129397
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -0.9861862094314459
avll from llpg:  -0.9861862094314457
avll direct:     -0.9861862094314457
sum posterior: 100000.0
32×26 Array{Float64,2}:
 -0.0344386    -0.0177069   0.00655485   0.110131    -0.11995     -0.00949705    0.0294704    0.11755     -0.0187258  -0.058652     0.0622466    0.0369348    -0.00211027    0.000126023   0.0248986    0.155982      0.130963      0.045992   -0.0503937    0.00952378   0.126346    -0.0198468     0.11701      -0.0542841   -0.0997287    0.110607  
 -0.0522246     0.0390626  -0.0641745    0.0380194    0.156082    -0.0522639    -0.154747     0.0569488   -0.0145643  -0.0390319    0.00191679   0.057598     -0.0106991     0.00716133    0.0255477   -0.0353736     0.0634758     0.0595084   0.0705085   -0.0845171   -0.0452514    0.0373018    -0.0540896    -0.0176399   -0.0971123    0.0240194 
 -0.0148069     0.0222627   0.0040191    0.109809    -0.0061586    0.0451462    -0.00607568  -0.0794163   -0.0295036  -0.112667     0.0452892    0.000915174   0.150598      0.145211     -0.128091     0.122504     -0.0487222     0.0230995  -0.0551978    0.0169337    0.183377    -0.00295819   -0.083981      0.0513007   -0.0543705    0.0843482 
 -0.198416     -0.105511    0.0629378    0.0703662    0.209359     0.0240904     0.150959    -0.059263     0.0178311   0.0845654    0.225771     0.0165998     0.0204359     0.0246477     0.0855266    0.13043      -0.0923934    -0.0860291  -0.101591     0.0470627    0.0623707    0.0270672     0.0365056     0.235624    -0.13035      0.147908  
  0.000222218  -0.200785    0.0439773    0.137369     0.0725861    0.0722334    -0.0402072    0.00702749  -0.117804   -0.0475602    0.0139797   -0.250255      0.0306675     0.160584     -0.203584     0.055539     -0.0184895     0.140793    0.0344535    0.0845221   -0.026857     0.1272       -0.10393       0.0355565   -0.047705     0.200665  
  0.0593803     0.0254103   0.0178477    0.0603362    0.0315298   -0.164561      0.086641     0.0770425   -0.130929    0.0432779    0.0114459    0.0428215     0.075831      0.0357495    -0.0080154    0.0333904    -0.0875287    -0.0286958  -0.041424    -0.0770944   -0.0416884    0.0274541    -0.0309843     0.0943771   -0.0568724    0.203533  
  0.00104712   -0.175676   -0.105785    -0.157285    -0.0994965    0.127159     -0.118316     0.101719     0.152619    0.192479    -0.040031     0.0703455     0.0740147    -0.0359439    -0.0301574   -0.0251545    -0.121591      0.0581615   0.043703    -0.0371097    0.0466502    0.102927      0.0162477    -0.113912     0.0467885    0.059783  
 -0.0146798     0.0421275  -0.0126115    0.175625    -0.0181487    0.070076      0.00641008  -0.033612     0.0572857   0.130263     0.0358306    0.0614856    -0.000357529   0.149656      0.00284292   0.0628127    -0.00856424   -0.182279    0.118436     0.0942878   -0.0558704   -0.063616      0.000107868  -0.0950814    0.132769     0.0148235 
 -0.0398627     0.107916    0.104755     0.0666675    0.127276     0.198086     -0.242357     0.126949     0.0670681   0.154436     0.0432427   -0.141164     -0.0148106     0.0131801     0.0365863   -0.000195668  -0.146302     -0.188865    0.031245     0.0331098    0.115783    -0.124381     -0.0964547    -0.0182861    0.119874     0.0891553 
  0.150924     -0.0199419  -0.00488955   0.0589636   -0.0178561    0.141413      0.118583    -0.049799    -0.0936406   0.143103     0.066131     0.0165314    -0.0515614     0.103122      0.112009    -0.0812254     0.0420348     0.0383206   0.0676809   -0.0748902    0.177805     0.0655514    -0.0975685    -0.0348345   -0.00862909  -0.197041  
  0.0472237     0.0823805   0.187575    -0.0677408    0.0790798    0.0650903     0.0476418    0.0380697   -0.0561892   0.05167     -0.0255433   -0.192277      0.0778044     0.0302303     0.0141867   -0.126112     -0.130527     -0.0542652   0.130221    -0.123921     0.0444881   -0.0367094    -0.121036     -0.0717004   -0.128811    -0.0329925 
  0.0260891     0.0683308  -0.060406    -0.0888484    0.165034    -0.0633781     0.0582544   -0.065253     0.07213     0.188118    -0.262419     0.141243     -0.00849144    0.075236     -0.17936     -0.117917     -0.0727083     0.0246515  -0.00362198  -0.0464603   -0.0341045    0.189691      0.0577906     0.0135941    0.176056    -0.00453544
 -0.233763     -0.0350152   0.11286      0.0813405    0.155114     0.0154122     0.060492    -0.0549204    0.0129173  -0.143326    -0.192856     0.067589     -0.00890043   -0.0782829     0.0208208   -0.0818761     0.117577     -0.0308236  -0.0244571   -0.0593991    0.0433927    0.0149971    -0.0228116    -0.0154325   -0.148232     0.0951183 
  0.0701789     0.0440634  -0.182639     0.0293155    0.165934    -0.17925       0.0713965    0.0724878    0.124096   -0.091496     0.0421816    0.00956561    0.150331     -0.0132882     0.0515751   -0.0475773    -0.00534856   -0.0175122   0.208278     0.0925256    0.0398479    0.0560537    -0.00226206    0.0473229    0.0174229    0.0952664 
  0.0476926     0.190379   -0.0308864    0.196362     0.0979021    0.0499526     0.0138699    0.145981     0.187393   -0.0367245   -0.0371929    0.00238781    0.254814     -0.0198167    -0.129071     0.0178171     0.137069      0.0479632  -0.0316935    0.175009    -0.0406683    0.0360226     0.0196674    -0.0074783   -0.0984696   -0.088041  
 -0.0299122     0.153836    0.0968074   -0.115977     0.120757    -0.120759     -0.0631038    0.120213    -0.0591927   0.00501835   0.16231     -0.0465917    -0.103616      0.0412425     0.194123     0.132434     -0.180724      0.139787    0.00910805  -0.0172826    0.120943    -0.0583129    -0.143767      0.158096     0.163511    -0.0772623 
  0.050394     -0.0637272   0.0519142   -0.0442086    0.0902501   -0.0416786    -0.138159    -0.00395667   0.130078    0.0316773   -0.009674     0.085052     -0.0216679     0.113102      0.0322167    0.254825      0.000661593  -0.0442605   0.138989     0.17157     -0.025236    -0.0191698    -0.0797718     0.0679723   -0.134625    -0.0774981 
  0.133234      0.0803576  -0.0302299   -0.137885    -0.0575747   -0.179579      0.00753807   0.0199189    0.0614254  -0.0900624   -0.153198     0.0450045    -0.000496089  -0.0292523    -0.0571191    0.178974     -0.122043     -0.0130705  -0.0360889   -0.0947663   -0.0165338   -0.0511375    -0.120563     -0.143372    -0.0138213    0.0648674 
 -0.0260005    -0.0305471  -0.00859307  -0.101531     0.207726     0.0410007    -0.0706685   -0.00906086   0.0647143   0.196338     0.0390442    0.0453307     0.0077789    -0.132413      0.110167    -0.203832     -0.0575496    -0.0067738  -0.00615779  -0.146967     0.110846    -0.126312      0.0250891     0.0479051    0.191003    -0.00869004
 -0.0466906    -0.241861    0.18131     -0.0790494   -0.121162    -0.0300315    -0.052113    -0.189978    -0.0288321   0.0145371   -0.105889     0.0532684    -0.155339      0.15416       0.0518598    0.0330188    -0.10474       0.0440192  -0.049909    -0.0542853    0.186983    -0.000912092  -0.027139      0.112105    -0.0248995   -0.0764058 
  0.0197628     0.0798271  -0.0673735    0.00926057  -0.0686755    0.0447411     0.0220511    0.0196064   -0.0326201   0.0306143   -0.0192487   -0.122642      0.106324      0.176505     -0.0667634    0.250008     -0.0901315     0.152175   -0.121467    -0.0963532   -0.0572913    0.182458     -0.0621839     0.00105735   0.0601958   -0.208211  
 -0.0450799    -0.125625    0.0622144   -0.129635     0.00334381   0.0504988     0.0776732   -0.052529     0.0758101  -0.0826748    0.0566643    0.0803269    -0.00447419   -0.0784558    -0.0832939    0.00246093   -0.110287      0.10576     0.166528     0.0648777   -0.129927    -0.0516555    -0.140769     -0.123951     0.128655     0.0624372 
  0.229612     -0.101039    0.00179488   0.0431517   -0.0114824   -0.0785378    -0.0798318   -0.094293    -0.0639502   0.15598      0.116479     0.0421238     0.0223043    -0.0652494     0.0392162    0.0259236    -0.129433      0.017381   -0.217462     0.0501302    0.168956    -0.184796      0.0121451     0.044813    -0.196246     0.0313644 
 -0.039683     -0.0839673  -0.100502     0.0959095   -0.17314      0.0565843     0.0803562   -0.173258    -0.0692173  -0.00700473   0.0216466   -0.0344757     0.059716     -0.0525751    -0.0998794   -0.0852778     0.0674652    -0.0367034   0.091354    -0.0377424    0.00602314   0.018478     -0.0505006     0.00231702  -0.0662637   -0.00615726
  0.100588     -0.148809    0.027234     0.0689101   -0.0703345    0.204063     -0.126538    -0.168181    -0.110873    0.0275703    0.0317126   -0.0143536    -0.0138524    -0.0190139    -0.102433     0.0585802     0.117183      0.0735012  -0.0911901    0.0820678   -0.107618    -0.108112     -0.127262      0.187063     0.113519    -0.134935  
 -0.0280826    -0.0343666  -0.00597568   0.00765891  -0.0359829    0.0421363     0.00550183  -0.025065     0.0525136   0.00477602  -0.0998993    0.0777566    -0.301213     -0.125529     -0.0474643    0.0173976     0.0561939     0.0959031   0.0624118   -0.0139481   -0.023614    -0.0560858     0.0130475     0.0792581    0.0358453   -0.0264005 
  0.0369835    -0.114799    0.056767    -0.038619     0.155101    -0.000539994  -0.0292722    0.0919974   -0.111928    0.116328    -0.0999923   -0.0393202    -0.00354185    0.0552318     0.0996261   -0.0452024     0.0195068     0.102421    0.0357533    0.254042    -0.0143849   -0.102291      0.0233966     0.197374     0.0655624   -0.0433677 
 -0.0509434     0.029455   -0.0782862   -0.0564662   -0.207893     0.0585569    -0.115936     0.115388    -0.0517062  -0.0904058   -0.140195    -0.0574801    -0.133391     -0.0261769    -0.217502    -0.0844253    -0.127755      0.0302202  -0.0727242   -0.20971      0.135025    -0.0169057     0.0466849    -0.110746    -0.027578     0.0346847 
  0.0618274    -0.0308657   0.0101429   -0.196957     0.3094      -0.0446701    -0.033792    -0.153443     0.030076   -0.138578    -0.033217    -0.209763      0.0404009     0.174431     -0.101996     0.0708261    -0.0252126    -0.0629397  -0.0841471    0.244347     0.0274379    0.0525097    -0.0464882    -0.106291     0.0136659   -0.108832  
  0.0746456     0.0648075  -0.045098    -0.0201321    0.0653959    0.0181399     0.0203878   -0.00364851  -0.19813    -0.0662453   -0.0245922    0.139692      0.0222932    -0.0891213     0.0436558    0.0222219     0.13069      -0.0436647  -0.123015     0.00164074   0.0783903    0.0458224     0.0456818    -0.137131     0.0338948    0.0852597 
  0.185491      0.0883639  -0.088295     0.152591     0.160181    -0.0301678     0.0627234   -0.00766608  -0.0532263   0.166898    -0.0361756   -0.0246844    -0.0313733     0.137657      0.126539     0.108662     -0.0331248     0.0125632  -0.116348     0.114242    -0.0461922    0.08127       0.0872211     0.0373137   -0.00650266  -0.0372936 
 -0.0216634    -0.0572897   0.00118627   0.190118    -0.105277     0.000830053   0.0622013    0.0554368    0.0445609  -0.0548656    0.0508463    0.132092      0.0783617     0.118626      0.0398806    0.0276143    -0.115364      0.106049    0.0404336   -0.109418    -0.0187483    0.281246     -0.13         -0.0314867    0.00577946  -0.0241811 kind diag, method split
0: avll = -1.4038485429882672
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.403893
INFO: iteration 2, average log likelihood -1.403833
INFO: iteration 3, average log likelihood -1.403393
INFO: iteration 4, average log likelihood -1.398041
INFO: iteration 5, average log likelihood -1.380088
INFO: iteration 6, average log likelihood -1.371161
INFO: iteration 7, average log likelihood -1.369746
INFO: iteration 8, average log likelihood -1.369157
INFO: iteration 9, average log likelihood -1.368818
INFO: iteration 10, average log likelihood -1.368611
INFO: iteration 11, average log likelihood -1.368481
INFO: iteration 12, average log likelihood -1.368398
INFO: iteration 13, average log likelihood -1.368343
INFO: iteration 14, average log likelihood -1.368306
INFO: iteration 15, average log likelihood -1.368281
INFO: iteration 16, average log likelihood -1.368263
INFO: iteration 17, average log likelihood -1.368251
INFO: iteration 18, average log likelihood -1.368242
INFO: iteration 19, average log likelihood -1.368235
INFO: iteration 20, average log likelihood -1.368230
INFO: iteration 21, average log likelihood -1.368226
INFO: iteration 22, average log likelihood -1.368223
INFO: iteration 23, average log likelihood -1.368221
INFO: iteration 24, average log likelihood -1.368219
INFO: iteration 25, average log likelihood -1.368217
INFO: iteration 26, average log likelihood -1.368216
INFO: iteration 27, average log likelihood -1.368214
INFO: iteration 28, average log likelihood -1.368213
INFO: iteration 29, average log likelihood -1.368212
INFO: iteration 30, average log likelihood -1.368210
INFO: iteration 31, average log likelihood -1.368209
INFO: iteration 32, average log likelihood -1.368207
INFO: iteration 33, average log likelihood -1.368206
INFO: iteration 34, average log likelihood -1.368204
INFO: iteration 35, average log likelihood -1.368201
INFO: iteration 36, average log likelihood -1.368198
INFO: iteration 37, average log likelihood -1.368194
INFO: iteration 38, average log likelihood -1.368189
INFO: iteration 39, average log likelihood -1.368182
INFO: iteration 40, average log likelihood -1.368173
INFO: iteration 41, average log likelihood -1.368159
INFO: iteration 42, average log likelihood -1.368138
INFO: iteration 43, average log likelihood -1.368106
INFO: iteration 44, average log likelihood -1.368049
INFO: iteration 45, average log likelihood -1.367949
INFO: iteration 46, average log likelihood -1.367829
INFO: iteration 47, average log likelihood -1.367750
INFO: iteration 48, average log likelihood -1.367705
INFO: iteration 49, average log likelihood -1.367679
INFO: iteration 50, average log likelihood -1.367664
INFO: EM with 100000 data points 50 iterations avll -1.367664
952.4 data points per parameter
1: avll = [-1.40389,-1.40383,-1.40339,-1.39804,-1.38009,-1.37116,-1.36975,-1.36916,-1.36882,-1.36861,-1.36848,-1.3684,-1.36834,-1.36831,-1.36828,-1.36826,-1.36825,-1.36824,-1.36824,-1.36823,-1.36823,-1.36822,-1.36822,-1.36822,-1.36822,-1.36822,-1.36821,-1.36821,-1.36821,-1.36821,-1.36821,-1.36821,-1.36821,-1.3682,-1.3682,-1.3682,-1.36819,-1.36819,-1.36818,-1.36817,-1.36816,-1.36814,-1.36811,-1.36805,-1.36795,-1.36783,-1.36775,-1.3677,-1.36768,-1.36766]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.367751
INFO: iteration 2, average log likelihood -1.367656
INFO: iteration 3, average log likelihood -1.367342
INFO: iteration 4, average log likelihood -1.364372
INFO: iteration 5, average log likelihood -1.353013
INFO: iteration 6, average log likelihood -1.340854
INFO: iteration 7, average log likelihood -1.335677
INFO: iteration 8, average log likelihood -1.332630
INFO: iteration 9, average log likelihood -1.330260
INFO: iteration 10, average log likelihood -1.328457
INFO: iteration 11, average log likelihood -1.327109
INFO: iteration 12, average log likelihood -1.326116
INFO: iteration 13, average log likelihood -1.325410
INFO: iteration 14, average log likelihood -1.324845
INFO: iteration 15, average log likelihood -1.324273
INFO: iteration 16, average log likelihood -1.323610
INFO: iteration 17, average log likelihood -1.322788
INFO: iteration 18, average log likelihood -1.321753
INFO: iteration 19, average log likelihood -1.320558
INFO: iteration 20, average log likelihood -1.319437
INFO: iteration 21, average log likelihood -1.318649
INFO: iteration 22, average log likelihood -1.318185
INFO: iteration 23, average log likelihood -1.317926
INFO: iteration 24, average log likelihood -1.317777
INFO: iteration 25, average log likelihood -1.317681
INFO: iteration 26, average log likelihood -1.317612
INFO: iteration 27, average log likelihood -1.317560
INFO: iteration 28, average log likelihood -1.317519
INFO: iteration 29, average log likelihood -1.317486
INFO: iteration 30, average log likelihood -1.317459
INFO: iteration 31, average log likelihood -1.317435
INFO: iteration 32, average log likelihood -1.317416
INFO: iteration 33, average log likelihood -1.317399
INFO: iteration 34, average log likelihood -1.317386
INFO: iteration 35, average log likelihood -1.317375
INFO: iteration 36, average log likelihood -1.317365
INFO: iteration 37, average log likelihood -1.317357
INFO: iteration 38, average log likelihood -1.317351
INFO: iteration 39, average log likelihood -1.317346
INFO: iteration 40, average log likelihood -1.317341
INFO: iteration 41, average log likelihood -1.317337
INFO: iteration 42, average log likelihood -1.317334
INFO: iteration 43, average log likelihood -1.317331
INFO: iteration 44, average log likelihood -1.317329
INFO: iteration 45, average log likelihood -1.317327
INFO: iteration 46, average log likelihood -1.317325
INFO: iteration 47, average log likelihood -1.317324
INFO: iteration 48, average log likelihood -1.317322
INFO: iteration 49, average log likelihood -1.317321
INFO: iteration 50, average log likelihood -1.317320
INFO: EM with 100000 data points 50 iterations avll -1.317320
473.9 data points per parameter
2: avll = [-1.36775,-1.36766,-1.36734,-1.36437,-1.35301,-1.34085,-1.33568,-1.33263,-1.33026,-1.32846,-1.32711,-1.32612,-1.32541,-1.32484,-1.32427,-1.32361,-1.32279,-1.32175,-1.32056,-1.31944,-1.31865,-1.31818,-1.31793,-1.31778,-1.31768,-1.31761,-1.31756,-1.31752,-1.31749,-1.31746,-1.31744,-1.31742,-1.3174,-1.31739,-1.31737,-1.31737,-1.31736,-1.31735,-1.31735,-1.31734,-1.31734,-1.31733,-1.31733,-1.31733,-1.31733,-1.31733,-1.31732,-1.31732,-1.31732,-1.31732]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.317455
INFO: iteration 2, average log likelihood -1.317294
INFO: iteration 3, average log likelihood -1.316473
INFO: iteration 4, average log likelihood -1.310554
INFO: iteration 5, average log likelihood -1.296143
INFO: iteration 6, average log likelihood -1.283484
INFO: iteration 7, average log likelihood -1.276661
INFO: iteration 8, average log likelihood -1.273275
INFO: iteration 9, average log likelihood -1.270752
INFO: iteration 10, average log likelihood -1.268326
INFO: iteration 11, average log likelihood -1.266184
INFO: iteration 12, average log likelihood -1.264454
INFO: iteration 13, average log likelihood -1.263080
INFO: iteration 14, average log likelihood -1.262103
INFO: iteration 15, average log likelihood -1.261512
INFO: iteration 16, average log likelihood -1.261117
INFO: iteration 17, average log likelihood -1.260764
INFO: iteration 18, average log likelihood -1.260390
INFO: iteration 19, average log likelihood -1.259964
INFO: iteration 20, average log likelihood -1.259538
INFO: iteration 21, average log likelihood -1.259231
INFO: iteration 22, average log likelihood -1.259053
INFO: iteration 23, average log likelihood -1.258956
INFO: iteration 24, average log likelihood -1.258903
INFO: iteration 25, average log likelihood -1.258871
INFO: iteration 26, average log likelihood -1.258850
INFO: iteration 27, average log likelihood -1.258834
INFO: iteration 28, average log likelihood -1.258822
INFO: iteration 29, average log likelihood -1.258812
INFO: iteration 30, average log likelihood -1.258804
INFO: iteration 31, average log likelihood -1.258797
INFO: iteration 32, average log likelihood -1.258791
INFO: iteration 33, average log likelihood -1.258785
INFO: iteration 34, average log likelihood -1.258780
INFO: iteration 35, average log likelihood -1.258774
INFO: iteration 36, average log likelihood -1.258768
INFO: iteration 37, average log likelihood -1.258762
INFO: iteration 38, average log likelihood -1.258755
INFO: iteration 39, average log likelihood -1.258746
INFO: iteration 40, average log likelihood -1.258735
INFO: iteration 41, average log likelihood -1.258720
INFO: iteration 42, average log likelihood -1.258698
INFO: iteration 43, average log likelihood -1.258662
INFO: iteration 44, average log likelihood -1.258603
INFO: iteration 45, average log likelihood -1.258501
INFO: iteration 46, average log likelihood -1.258314
INFO: iteration 47, average log likelihood -1.258005
INFO: iteration 48, average log likelihood -1.257551
INFO: iteration 49, average log likelihood -1.257064
INFO: iteration 50, average log likelihood -1.256813
INFO: EM with 100000 data points 50 iterations avll -1.256813
236.4 data points per parameter
3: avll = [-1.31745,-1.31729,-1.31647,-1.31055,-1.29614,-1.28348,-1.27666,-1.27328,-1.27075,-1.26833,-1.26618,-1.26445,-1.26308,-1.2621,-1.26151,-1.26112,-1.26076,-1.26039,-1.25996,-1.25954,-1.25923,-1.25905,-1.25896,-1.2589,-1.25887,-1.25885,-1.25883,-1.25882,-1.25881,-1.2588,-1.2588,-1.25879,-1.25879,-1.25878,-1.25877,-1.25877,-1.25876,-1.25876,-1.25875,-1.25874,-1.25872,-1.2587,-1.25866,-1.2586,-1.2585,-1.25831,-1.258,-1.25755,-1.25706,-1.25681]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.256934
INFO: iteration 2, average log likelihood -1.256724
INFO: iteration 3, average log likelihood -1.256058
INFO: iteration 4, average log likelihood -1.249177
INFO: iteration 5, average log likelihood -1.226822
INFO: iteration 6, average log likelihood -1.207427
INFO: iteration 7, average log likelihood -1.198830
INFO: iteration 8, average log likelihood -1.194372
INFO: iteration 9, average log likelihood -1.191977
INFO: iteration 10, average log likelihood -1.190955
INFO: iteration 11, average log likelihood -1.190345
INFO: iteration 12, average log likelihood -1.189699
INFO: iteration 13, average log likelihood -1.188798
INFO: iteration 14, average log likelihood -1.187445
INFO: iteration 15, average log likelihood -1.185699
INFO: iteration 16, average log likelihood -1.184415
INFO: iteration 17, average log likelihood -1.183779
INFO: iteration 18, average log likelihood -1.182828
WARNING: Variances had to be floored 12
INFO: iteration 19, average log likelihood -1.178399
INFO: iteration 20, average log likelihood -1.183619
INFO: iteration 21, average log likelihood -1.182343
WARNING: Variances had to be floored 12
INFO: iteration 22, average log likelihood -1.181252
INFO: iteration 23, average log likelihood -1.182523
INFO: iteration 24, average log likelihood -1.182141
WARNING: Variances had to be floored 12
INFO: iteration 25, average log likelihood -1.181323
INFO: iteration 26, average log likelihood -1.182268
INFO: iteration 27, average log likelihood -1.181894
WARNING: Variances had to be floored 12
INFO: iteration 28, average log likelihood -1.180913
INFO: iteration 29, average log likelihood -1.181512
INFO: iteration 30, average log likelihood -1.180363
WARNING: Variances had to be floored 12
INFO: iteration 31, average log likelihood -1.178208
INFO: iteration 32, average log likelihood -1.178094
INFO: iteration 33, average log likelihood -1.177490
WARNING: Variances had to be floored 12
INFO: iteration 34, average log likelihood -1.176744
INFO: iteration 35, average log likelihood -1.177618
INFO: iteration 36, average log likelihood -1.177392
WARNING: Variances had to be floored 12
INFO: iteration 37, average log likelihood -1.176725
INFO: iteration 38, average log likelihood -1.177609
INFO: iteration 39, average log likelihood -1.177388
WARNING: Variances had to be floored 12
INFO: iteration 40, average log likelihood -1.176714
INFO: iteration 41, average log likelihood -1.177610
INFO: iteration 42, average log likelihood -1.177387
WARNING: Variances had to be floored 12
INFO: iteration 43, average log likelihood -1.176706
INFO: iteration 44, average log likelihood -1.177612
INFO: iteration 45, average log likelihood -1.177387
WARNING: Variances had to be floored 12
INFO: iteration 46, average log likelihood -1.176702
INFO: iteration 47, average log likelihood -1.177613
INFO: iteration 48, average log likelihood -1.177386
WARNING: Variances had to be floored 12
INFO: iteration 49, average log likelihood -1.176699
INFO: iteration 50, average log likelihood -1.177613
INFO: EM with 100000 data points 50 iterations avll -1.177613
118.1 data points per parameter
4: avll = [-1.25693,-1.25672,-1.25606,-1.24918,-1.22682,-1.20743,-1.19883,-1.19437,-1.19198,-1.19095,-1.19034,-1.1897,-1.1888,-1.18744,-1.1857,-1.18441,-1.18378,-1.18283,-1.1784,-1.18362,-1.18234,-1.18125,-1.18252,-1.18214,-1.18132,-1.18227,-1.18189,-1.18091,-1.18151,-1.18036,-1.17821,-1.17809,-1.17749,-1.17674,-1.17762,-1.17739,-1.17673,-1.17761,-1.17739,-1.17671,-1.17761,-1.17739,-1.17671,-1.17761,-1.17739,-1.1767,-1.17761,-1.17739,-1.1767,-1.17761]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.177614
WARNING: Variances had to be floored 23 24
INFO: iteration 2, average log likelihood -1.176594
INFO: iteration 3, average log likelihood -1.174443
WARNING: Variances had to be floored 23 24
INFO: iteration 4, average log likelihood -1.147229
WARNING: Variances had to be floored 13
INFO: iteration 5, average log likelihood -1.096007
WARNING: Variances had to be floored 14 19 23 24 32
INFO: iteration 6, average log likelihood -1.069524
WARNING: Variances had to be floored 10 13 24
INFO: iteration 7, average log likelihood -1.086595
WARNING: Variances had to be floored 23
INFO: iteration 8, average log likelihood -1.082040
WARNING: Variances had to be floored 13 14 19 24 32
INFO: iteration 9, average log likelihood -1.057817
WARNING: Variances had to be floored 23 24
INFO: iteration 10, average log likelihood -1.087248
WARNING: Variances had to be floored 3 10 24
INFO: iteration 11, average log likelihood -1.068655
WARNING: Variances had to be floored 13 19 23 24 32
INFO: iteration 12, average log likelihood -1.071224
WARNING: Variances had to be floored 4 14 24
INFO: iteration 13, average log likelihood -1.082961
WARNING: Variances had to be floored 10 13 23 24
INFO: iteration 14, average log likelihood -1.072925
WARNING: Variances had to be floored 14 19 24 32
INFO: iteration 15, average log likelihood -1.070076
WARNING: Variances had to be floored 4 13 23 24
INFO: iteration 16, average log likelihood -1.084040
WARNING: Variances had to be floored 10 14 24
INFO: iteration 17, average log likelihood -1.071887
WARNING: Variances had to be floored 13 19 23 24 32
INFO: iteration 18, average log likelihood -1.069588
WARNING: Variances had to be floored 4 14 24
INFO: iteration 19, average log likelihood -1.083568
WARNING: Variances had to be floored 10 13 23 24
INFO: iteration 20, average log likelihood -1.071988
WARNING: Variances had to be floored 14 19 24 32
INFO: iteration 21, average log likelihood -1.069443
WARNING: Variances had to be floored 4 13 23 24
INFO: iteration 22, average log likelihood -1.083566
WARNING: Variances had to be floored 10 14 24 32
INFO: iteration 23, average log likelihood -1.071800
WARNING: Variances had to be floored 13 19 23 24
INFO: iteration 24, average log likelihood -1.083026
WARNING: Variances had to be floored 4 14 24
INFO: iteration 25, average log likelihood -1.075843
WARNING: Variances had to be floored 10 13 23 24 25 32
INFO: iteration 26, average log likelihood -1.066275
WARNING: Variances had to be floored 14 19 24
INFO: iteration 27, average log likelihood -1.082888
WARNING: Variances had to be floored 4 13 20 23 24
INFO: iteration 28, average log likelihood -1.076062
WARNING: Variances had to be floored 10 14 24 32
INFO: iteration 29, average log likelihood -1.071550
WARNING: Variances had to be floored 13 19 23 24
INFO: iteration 30, average log likelihood -1.083442
WARNING: Variances had to be floored 4 14 24
INFO: iteration 31, average log likelihood -1.075823
WARNING: Variances had to be floored 10 13 23 24 32
INFO: iteration 32, average log likelihood -1.066149
WARNING: Variances had to be floored 14 19 24
INFO: iteration 33, average log likelihood -1.082844
WARNING: Variances had to be floored 4 13 20 23 24
INFO: iteration 34, average log likelihood -1.076072
WARNING: Variances had to be floored 10 14 24 32
INFO: iteration 35, average log likelihood -1.071509
WARNING: Variances had to be floored 13 19 23 24 25
INFO: iteration 36, average log likelihood -1.083438
WARNING: Variances had to be floored 4 14 24
INFO: iteration 37, average log likelihood -1.075845
WARNING: Variances had to be floored 10 13 23 24 32
INFO: iteration 38, average log likelihood -1.066121
WARNING: Variances had to be floored 14 19 24
INFO: iteration 39, average log likelihood -1.082849
WARNING: Variances had to be floored 4 13 20 23 24
INFO: iteration 40, average log likelihood -1.076101
WARNING: Variances had to be floored 10 14 24 32
INFO: iteration 41, average log likelihood -1.071475
WARNING: Variances had to be floored 13 19 23 24
INFO: iteration 42, average log likelihood -1.083448
WARNING: Variances had to be floored 4 14 24
INFO: iteration 43, average log likelihood -1.075863
WARNING: Variances had to be floored 10 13 23 24 25 32
INFO: iteration 44, average log likelihood -1.066100
WARNING: Variances had to be floored 14 19 24
INFO: iteration 45, average log likelihood -1.082866
WARNING: Variances had to be floored 4 13 20 23 24
INFO: iteration 46, average log likelihood -1.076134
WARNING: Variances had to be floored 10 14 24 32
INFO: iteration 47, average log likelihood -1.071453
WARNING: Variances had to be floored 13 19 23 24
INFO: iteration 48, average log likelihood -1.083461
WARNING: Variances had to be floored 4 14 24
INFO: iteration 49, average log likelihood -1.075890
WARNING: Variances had to be floored 10 13 23 24 32
INFO: iteration 50, average log likelihood -1.066088
INFO: EM with 100000 data points 50 iterations avll -1.066088
59.0 data points per parameter
5: avll = [-1.17761,-1.17659,-1.17444,-1.14723,-1.09601,-1.06952,-1.0866,-1.08204,-1.05782,-1.08725,-1.06866,-1.07122,-1.08296,-1.07292,-1.07008,-1.08404,-1.07189,-1.06959,-1.08357,-1.07199,-1.06944,-1.08357,-1.0718,-1.08303,-1.07584,-1.06628,-1.08289,-1.07606,-1.07155,-1.08344,-1.07582,-1.06615,-1.08284,-1.07607,-1.07151,-1.08344,-1.07585,-1.06612,-1.08285,-1.0761,-1.07147,-1.08345,-1.07586,-1.0661,-1.08287,-1.07613,-1.07145,-1.08346,-1.07589,-1.06609]
[-1.40385,-1.40389,-1.40383,-1.40339,-1.39804,-1.38009,-1.37116,-1.36975,-1.36916,-1.36882,-1.36861,-1.36848,-1.3684,-1.36834,-1.36831,-1.36828,-1.36826,-1.36825,-1.36824,-1.36824,-1.36823,-1.36823,-1.36822,-1.36822,-1.36822,-1.36822,-1.36822,-1.36821,-1.36821,-1.36821,-1.36821,-1.36821,-1.36821,-1.36821,-1.3682,-1.3682,-1.3682,-1.36819,-1.36819,-1.36818,-1.36817,-1.36816,-1.36814,-1.36811,-1.36805,-1.36795,-1.36783,-1.36775,-1.3677,-1.36768,-1.36766,-1.36775,-1.36766,-1.36734,-1.36437,-1.35301,-1.34085,-1.33568,-1.33263,-1.33026,-1.32846,-1.32711,-1.32612,-1.32541,-1.32484,-1.32427,-1.32361,-1.32279,-1.32175,-1.32056,-1.31944,-1.31865,-1.31818,-1.31793,-1.31778,-1.31768,-1.31761,-1.31756,-1.31752,-1.31749,-1.31746,-1.31744,-1.31742,-1.3174,-1.31739,-1.31737,-1.31737,-1.31736,-1.31735,-1.31735,-1.31734,-1.31734,-1.31733,-1.31733,-1.31733,-1.31733,-1.31733,-1.31732,-1.31732,-1.31732,-1.31732,-1.31745,-1.31729,-1.31647,-1.31055,-1.29614,-1.28348,-1.27666,-1.27328,-1.27075,-1.26833,-1.26618,-1.26445,-1.26308,-1.2621,-1.26151,-1.26112,-1.26076,-1.26039,-1.25996,-1.25954,-1.25923,-1.25905,-1.25896,-1.2589,-1.25887,-1.25885,-1.25883,-1.25882,-1.25881,-1.2588,-1.2588,-1.25879,-1.25879,-1.25878,-1.25877,-1.25877,-1.25876,-1.25876,-1.25875,-1.25874,-1.25872,-1.2587,-1.25866,-1.2586,-1.2585,-1.25831,-1.258,-1.25755,-1.25706,-1.25681,-1.25693,-1.25672,-1.25606,-1.24918,-1.22682,-1.20743,-1.19883,-1.19437,-1.19198,-1.19095,-1.19034,-1.1897,-1.1888,-1.18744,-1.1857,-1.18441,-1.18378,-1.18283,-1.1784,-1.18362,-1.18234,-1.18125,-1.18252,-1.18214,-1.18132,-1.18227,-1.18189,-1.18091,-1.18151,-1.18036,-1.17821,-1.17809,-1.17749,-1.17674,-1.17762,-1.17739,-1.17673,-1.17761,-1.17739,-1.17671,-1.17761,-1.17739,-1.17671,-1.17761,-1.17739,-1.1767,-1.17761,-1.17739,-1.1767,-1.17761,-1.17761,-1.17659,-1.17444,-1.14723,-1.09601,-1.06952,-1.0866,-1.08204,-1.05782,-1.08725,-1.06866,-1.07122,-1.08296,-1.07292,-1.07008,-1.08404,-1.07189,-1.06959,-1.08357,-1.07199,-1.06944,-1.08357,-1.0718,-1.08303,-1.07584,-1.06628,-1.08289,-1.07606,-1.07155,-1.08344,-1.07582,-1.06615,-1.08284,-1.07607,-1.07151,-1.08344,-1.07585,-1.06612,-1.08285,-1.0761,-1.07147,-1.08345,-1.07586,-1.0661,-1.08287,-1.07613,-1.07145,-1.08346,-1.07589,-1.06609]
32×26 Array{Float64,2}:
 -0.0460951   -0.101268     0.0525469    -0.110348     0.00454645   0.0562929     0.0766337   -0.0445907    0.0623604  -0.0724952    0.0594594    0.0673813   -0.00591252  -0.0625405   -0.0880771    0.011734     -0.0991233     0.0909156    0.156985      0.0181316   -0.160736    -0.0107432   -0.166578     -0.109744     0.126006     0.0322592 
 -0.0358882   -0.0368404   -0.0954858     0.0318363   -0.196399     0.0658209    -0.0221785   -0.0388373   -0.0637016  -0.0473406   -0.0561188   -0.0432809   -0.034668    -0.0326437   -0.149215    -0.0811034    -0.0282884    -0.00381316   0.0138149    -0.11046      0.048062    -0.0108501   -0.0348588    -0.0469128   -0.0406206    0.0204165 
 -0.143003    -0.0891043    0.0786109     0.115748     0.104712     0.0468323     0.013557    -0.0242311   -0.0588948  -0.110127    -0.088415    -0.0919844    0.0183527    0.045818    -0.075574    -0.00761298    0.0599508     0.0407092    0.00733089    0.00729154   0.00248744   0.080243    -0.0615941    -0.00174516  -0.120575     0.148459  
  0.0220619    0.0571096   -0.0773394    -0.0787677    0.172092    -0.025926      0.0664484   -0.0665908    0.0644311   0.171251    -0.266922     0.127288    -0.00659281   0.0526906   -0.165194    -0.127638     -0.0608289     0.0484952   -0.00762059   -0.0622829   -0.0284048    0.21234      0.0599901     0.00391772   0.162696     0.01265   
  0.0465228    0.0254632   -0.0429411    -0.117011     0.0717192   -0.0716641    -0.0416792    0.00164135   0.0594174   0.0638175   -0.0652972    0.0502971    0.00817311  -0.0758755    0.0216883   -0.000956607  -0.0922231    -0.00733616  -0.00626971   -0.135247     0.0545886   -0.0784786   -0.0632211    -0.050658     0.0841784    0.0323759 
 -0.042282    -0.00593435  -0.0282146     0.0923382   -0.128161    -0.0122234     0.039222     0.113494    -0.0222577  -0.0875898    0.0496582    0.0939795   -0.0287406   -0.0114571    0.0201099    0.17646       0.122849      0.0699328   -0.0467638     0.00680124   0.122274    -0.0174402    0.103355     -0.0916784   -0.103827     0.0940795 
  0.0790782    0.060086     0.183155     -0.141395     0.116776     0.0277361     0.0420441    0.0385375   -0.0608476   0.00103731  -0.183607    -0.315476     0.0760368    0.00122402   0.0376205   -0.0667148    -0.0700391    -0.107716    -0.471565     -0.365596     0.138193    -0.0294736   -0.144684     -0.043752    -0.166543    -0.0371702 
  0.0182707    0.104515     0.214495      0.0224939    0.0302282    0.0672567     0.0514887    0.0337885   -0.0568618   0.0777       0.141624    -0.100666     0.0873865    0.0294165   -0.0203271   -0.200107     -0.195689     -0.0723802    0.534964      0.0958323   -0.0339082   -0.0491086   -0.0984226    -0.104387    -0.0861972   -0.0240737 
  0.230197    -0.10222     -0.000737883   0.0289792   -0.0157992   -0.0280396    -0.0972102   -0.101082    -0.0142983   0.111528     0.136461     0.0369497    0.00155109  -0.0697255    0.041273     0.0299351    -0.131365      0.0091052   -0.201049      0.0416815    0.157184    -0.170329    -0.0246066     0.0473407   -0.191588     0.0153875 
  0.150675    -0.014562    -0.0098136     0.0585636   -0.00222491   0.144448      0.259215    -0.0621961   -0.0677379   0.140259     0.04839      0.017785    -0.0809973    0.0958096    0.104855    -0.105471      0.0407763     0.032836     0.066186     -0.0387462    0.197615     0.0817105   -0.11391       0.0242295    0.00507514  -0.202258  
 -0.0625976    0.0403317   -0.061899      0.051934     0.14361     -0.0707276    -0.156751     0.0436797   -0.0170403  -0.022788    -0.00227554   0.0795072    0.00701554   0.0080024    0.0260785   -0.0350936     0.0639403     0.0584157    0.0715426    -0.0578586   -0.0503434    0.0569332   -0.0599993    -0.102212    -0.0919218    0.0357745 
  0.0219221    0.188724    -0.0191134     0.176169     0.0828453    0.0202408     0.0207894    0.154781     0.165221   -0.0368466   -0.0335981    0.00283392   0.234615    -0.0126464   -0.127026     0.0166523     0.133997      0.0660583   -0.0373425     0.180865    -0.0415603    0.0330279    0.0490803     0.00161711  -0.100312    -0.127958  
  0.0712779   -0.033312     0.0178461    -0.162381     0.309724    -0.0350033    -0.0298604   -0.136642     0.0259012  -0.203865    -0.0331557   -0.167044     0.0566336    0.175248    -0.116733     0.0449211    -0.0128329    -0.0750121   -0.0753514     0.240679     0.0240418    0.0647366   -0.0460964    -0.113838    -0.00599813  -0.0810231 
  0.0392405   -0.130128     0.0113267    -0.00188434   0.082443    -0.0458725    -0.15583      0.0046298    0.116184    0.0448589   -0.0134332    0.0766095   -0.021174     0.115567     0.0289998    0.217555     -0.0011509    -0.0169181    0.142989      0.15831      0.0253401   -0.0144816   -0.0972463     0.0669761   -0.134792    -0.0876284 
  0.0380795    0.057002    -0.00592051    0.0508557   -0.0139386   -0.0533594     0.0571163    0.0635527   -0.107094    0.0396907    0.00420714  -0.107447     0.0837983    0.115023    -0.0323142    0.147053     -0.0789415     0.0567831   -0.0770853    -0.0786989   -0.0309814    0.0994306   -0.0485558     0.0515938    0.00523728   0.00490711
 -0.0218756    0.0371955   -0.00739496    0.169446    -0.0224951    0.0627903    -0.013502    -0.0273069    0.0717397   0.138787     0.0306516    0.0466874   -0.0159913    0.135605     0.00295398   0.0595847    -0.000954601  -0.17268      0.118386      0.0964127   -0.0810339   -0.0996745   -0.000110385  -0.0965751    0.129715    -0.00388329
 -0.0294571   -0.234216     0.184339     -0.125901    -0.118013    -0.0428732    -0.0529657   -0.188544    -0.0633608   0.00959719  -0.107259     0.0459614   -0.142334     0.164125     0.0552704   -0.00840642   -0.107689      0.0451019   -0.0606484    -0.0568107    0.186801     0.00985862  -0.0346175     0.111397    -0.044686    -0.0770864 
  0.00749915  -0.160185    -0.105885     -0.134662    -0.102016     0.100596     -0.0913513    0.108702     0.157547    0.192714    -0.0392986    0.0701398    0.0819668   -0.0466843   -0.0305315   -0.0137514    -0.109208      0.0486886    0.0475883    -0.0628343    0.0495924    0.0936922    0.0143527    -0.112783     0.053193     0.062744  
  0.0457256   -0.125642     0.043558     -0.0486458    0.12632     -0.000799501  -0.0709418    0.0894656   -0.0956736   0.114735    -0.0769288   -0.0448481   -0.0106987    0.0584901    0.116567    -0.0542097    -0.00766419    0.108169     0.000609938   0.224834    -0.0130403   -0.104509     0.027973      0.177364     0.0577458   -0.041252  
  0.0413304    0.137702     0.0746957    -0.0974985    0.0991902   -0.139047     -0.090639     0.131299    -0.0650773   0.00508475   0.174305    -0.0369761   -0.0740128    0.0437999    0.216878     0.0959655    -0.224552      0.148782     0.0105109    -0.0429402    0.120998    -0.0904529   -0.157722      0.156117     0.156493    -0.0699613 
  0.23207     -0.0392019   -0.126061      0.129177     0.174393     0.0384451    -0.196397    -0.00707796  -0.0446069   0.159246    -0.489851     0.0430873   -0.0515115    0.136674     0.0497256    0.106788     -0.14868       0.0106279   -0.175189      0.119774    -0.0344148    0.0308677    0.106678      0.0349634   -0.0668187   -0.026451  
  0.166999     0.177335    -0.0356709     0.175863     0.149822    -0.146407      0.235304    -0.00690909  -0.0447307   0.171091     0.309571    -0.125662    -0.024562     0.143648     0.263733     0.11328       0.123583      0.0173007   -0.170281      0.136282    -0.0479879    0.142747     0.0628271     0.0357717    0.0627259   -0.147292  
  0.00662623   0.105096    -0.144563      0.074918     0.0816378    0.0729853     0.167725    -0.00239269  -0.167163    0.14168     -0.0598665    0.144086    -0.0455978    0.100405     0.0608765    0.0501734     0.043972     -0.0348322   -0.178533      0.00127737   0.0589374    0.11185      0.136253      0.0822559    0.00996055   0.0805802 
  0.0393355   -0.146046    -0.0015182    -0.157216     0.0703728   -0.142316      0.128093     0.460221    -0.191677   -0.53909      0.0450721    0.151094    -0.383946    -0.40177     -0.0806604    0.170471     -0.37907      -0.0473157   -0.0988011     0.00075848   0.0748177    0.0995242   -0.0465454    -0.399273     0.324853     0.0518566 
  0.00441748  -0.0191638    0.0424899    -0.00981203  -0.0834895    0.0271012    -0.00777776  -0.0203226    0.0954539   0.0368977   -0.109402     0.0345526   -0.460698    -0.113864    -0.0462584   -0.0545451     0.0524367     0.029726     0.0577144    -0.10146     -0.0897398   -0.0595673    0.0133207     0.02919      0.0490233   -0.94586   
 -0.111673    -0.0740417   -0.0650676    -0.0180838   -0.00407367   0.0614965     0.00921132  -0.0291484    0.0017763   0.0281657   -0.0914525    0.122997    -0.156285    -0.14743     -0.0475987    0.21792       0.148086      0.106926     0.065401      0.0632987    0.0384719   -0.0517508    0.0107895     0.160648     0.0582587    0.84543   
 -0.0992355   -0.190378     0.0483934     0.108564     0.0852314    0.0257844     0.158316    -0.053132     0.0509123   0.175713     0.242459     0.019504     0.0231046    0.0232067    0.082659     0.211086     -0.126319     -0.0365889   -0.0996494     0.0777618   -0.472648     0.0254751    0.0186868     0.163038    -0.109774     0.0865046 
 -0.286508    -0.0104615    0.0922592     0.0510091    0.286118     0.0252428     0.132501    -0.0631528    0.0131856  -0.0377745    0.21029      0.0123561    0.0201075    0.0242613    0.0861285    0.163039      0.0537611    -0.101281    -0.0995435    -0.026498     0.642762     0.0365188    0.0397528     0.351578    -0.135332     0.210307  
  0.024208    -0.0180968    0.075956      0.0686689    0.0213105    0.244776     -0.171464    -0.0277657   -0.0283254   0.0765816    0.0260047   -0.0745125   -0.0287272    0.00122125  -0.0245399    0.0525675    -0.026955     -0.0564919   -0.0272816     0.0604275    0.013763    -0.114226    -0.109541      0.0761809    0.0908347   -0.043191  
  0.0235561    0.0367571   -0.0085063     0.0358117    0.0372149    0.0417043    -0.00572634  -0.0560717   -0.0950253  -0.0884229    0.0229103    0.0559282    0.111099     0.0244449   -0.0858881    0.0751918    -9.19921e-5    0.0045238   -0.0834266     0.00364749   0.137402     0.0288477   -0.0376257    -0.07861      0.010406     0.119563  
  0.0650394    0.0508059   -0.167801      0.0308658    0.16523     -0.212384      0.0737995    0.0884596    0.151291   -0.0907609    0.0393647    0.0151052    0.191404    -0.0154974    0.00573586  -0.0877728    -0.0243751    -0.0228592    0.197169      0.0788921    0.0211662    0.0771316   -0.00151135    0.0467138    0.0284471    0.103565  
 -0.0208806   -0.0566806    0.000982631   0.191441    -0.0871926    0.00402147    0.0597523    0.049515     0.048158   -0.034285     0.0476325    0.132833     0.102803     0.128863     0.0851857    0.0276936    -0.125468      0.143175     0.0377426    -0.115846    -0.0144521    0.281045    -0.115691     -0.0360898   -0.0145723   -0.0201325 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 14 19 24
INFO: iteration 1, average log likelihood -1.082872
WARNING: Variances had to be floored 4 13 14 19 23 24
INFO: iteration 2, average log likelihood -1.059188
WARNING: Variances had to be floored 10 14 19 24 25 32
INFO: iteration 3, average log likelihood -1.057871
WARNING: Variances had to be floored 4 13 14 19 23 24 25
INFO: iteration 4, average log likelihood -1.073451
WARNING: Variances had to be floored 14 19 24 32
INFO: iteration 5, average log likelihood -1.068040
WARNING: Variances had to be floored 4 10 13 14 19 23 24 25
INFO: iteration 6, average log likelihood -1.061786
WARNING: Variances had to be floored 14 19 24
INFO: iteration 7, average log likelihood -1.074996
WARNING: Variances had to be floored 4 13 14 19 23 24 25 32
INFO: iteration 8, average log likelihood -1.052992
WARNING: Variances had to be floored 10 14 19 24
INFO: iteration 9, average log likelihood -1.071241
WARNING: Variances had to be floored 4 13 14 19 23 24 25
INFO: iteration 10, average log likelihood -1.065756
INFO: EM with 100000 data points 10 iterations avll -1.065756
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.957420e+05
      1       6.773666e+05      -2.183754e+05 |       32
      2       6.465615e+05      -3.080513e+04 |       32
      3       6.294087e+05      -1.715282e+04 |       32
      4       6.197505e+05      -9.658206e+03 |       32
      5       6.133456e+05      -6.404829e+03 |       32
      6       6.086006e+05      -4.745052e+03 |       32
      7       6.051815e+05      -3.419134e+03 |       32
      8       6.028899e+05      -2.291561e+03 |       32
      9       6.018684e+05      -1.021543e+03 |       32
     10       6.014476e+05      -4.207570e+02 |       32
     11       6.012353e+05      -2.123013e+02 |       32
     12       6.011036e+05      -1.316663e+02 |       32
     13       6.009947e+05      -1.088931e+02 |       32
     14       6.009029e+05      -9.181780e+01 |       32
     15       6.008067e+05      -9.621695e+01 |       32
     16       6.006749e+05      -1.317696e+02 |       31
     17       6.005162e+05      -1.587656e+02 |       32
     18       6.003240e+05      -1.921745e+02 |       32
     19       6.001576e+05      -1.663756e+02 |       32
     20       6.000436e+05      -1.140255e+02 |       32
     21       5.999799e+05      -6.367576e+01 |       32
     22       5.999177e+05      -6.225829e+01 |       32
     23       5.998486e+05      -6.906706e+01 |       30
     24       5.997758e+05      -7.279226e+01 |       30
     25       5.996976e+05      -7.823631e+01 |       32
     26       5.996303e+05      -6.725777e+01 |       32
     27       5.995812e+05      -4.905480e+01 |       31
     28       5.995486e+05      -3.269208e+01 |       31
     29       5.995135e+05      -3.507761e+01 |       32
     30       5.994867e+05      -2.680312e+01 |       31
     31       5.994672e+05      -1.952274e+01 |       31
     32       5.994502e+05      -1.691996e+01 |       28
     33       5.994378e+05      -1.244774e+01 |       28
     34       5.994292e+05      -8.617780e+00 |       25
     35       5.994232e+05      -5.921751e+00 |       22
     36       5.994196e+05      -3.648421e+00 |       25
     37       5.994167e+05      -2.882021e+00 |       20
     38       5.994152e+05      -1.468589e+00 |       14
     39       5.994147e+05      -5.872721e-01 |       13
     40       5.994143e+05      -3.588463e-01 |        7
     41       5.994141e+05      -1.615764e-01 |        6
     42       5.994140e+05      -1.259154e-01 |        2
     43       5.994140e+05      -1.717931e-02 |        2
     44       5.994140e+05      -1.390395e-02 |        0
     45       5.994140e+05       0.000000e+00 |        0
K-means converged with 45 iterations (objv = 599413.9805057262)
INFO: K-means with 32000 data points using 45 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.315859
INFO: iteration 2, average log likelihood -1.283477
INFO: iteration 3, average log likelihood -1.249511
INFO: iteration 4, average log likelihood -1.210287
INFO: iteration 5, average log likelihood -1.174999
WARNING: Variances had to be floored 7
INFO: iteration 6, average log likelihood -1.129364
WARNING: Variances had to be floored 1 8 18 19 32
INFO: iteration 7, average log likelihood -1.089144
WARNING: Variances had to be floored 16 21
INFO: iteration 8, average log likelihood -1.109207
WARNING: Variances had to be floored 9
INFO: iteration 9, average log likelihood -1.079205
WARNING: Variances had to be floored 10 32
INFO: iteration 10, average log likelihood -1.040052
WARNING: Variances had to be floored 1 8 18
INFO: iteration 11, average log likelihood -1.035402
WARNING: Variances had to be floored 7 16 19 21
INFO: iteration 12, average log likelihood -1.038945
WARNING: Variances had to be floored 9 10 32
INFO: iteration 13, average log likelihood -1.049453
WARNING: Variances had to be floored 6 8
INFO: iteration 14, average log likelihood -1.050413
WARNING: Variances had to be floored 1 7 18
INFO: iteration 15, average log likelihood -1.038921
WARNING: Variances had to be floored 10 16 19 21
INFO: iteration 16, average log likelihood -1.040356
WARNING: Variances had to be floored 9 32
INFO: iteration 17, average log likelihood -1.050449
WARNING: Variances had to be floored 7 8 18
INFO: iteration 18, average log likelihood -1.038035
WARNING: Variances had to be floored 1 10
INFO: iteration 19, average log likelihood -1.045337
WARNING: Variances had to be floored 16 19 21
INFO: iteration 20, average log likelihood -1.034204
WARNING: Variances had to be floored 6 7 8 9 32
INFO: iteration 21, average log likelihood -1.035630
WARNING: Variances had to be floored 10 18
INFO: iteration 22, average log likelihood -1.064599
WARNING: Variances had to be floored 1
INFO: iteration 23, average log likelihood -1.050150
WARNING: Variances had to be floored 7 16 19 21
INFO: iteration 24, average log likelihood -1.022663
WARNING: Variances had to be floored 8 18 32
INFO: iteration 25, average log likelihood -1.038916
WARNING: Variances had to be floored 9 10
INFO: iteration 26, average log likelihood -1.038599
WARNING: Variances had to be floored 1 7
INFO: iteration 27, average log likelihood -1.037531
WARNING: Variances had to be floored 16 21 32
INFO: iteration 28, average log likelihood -1.036006
WARNING: Variances had to be floored 6 8 18 19
INFO: iteration 29, average log likelihood -1.039207
WARNING: Variances had to be floored 7 10
INFO: iteration 30, average log likelihood -1.041046
WARNING: Variances had to be floored 1 9
INFO: iteration 31, average log likelihood -1.036464
WARNING: Variances had to be floored 16 21 32
INFO: iteration 32, average log likelihood -1.028662
WARNING: Variances had to be floored 7 8 10 18
INFO: iteration 33, average log likelihood -1.043157
WARNING: Variances had to be floored 19
INFO: iteration 34, average log likelihood -1.060818
WARNING: Variances had to be floored 1
INFO: iteration 35, average log likelihood -1.025064
WARNING: Variances had to be floored 7 8 9 16 21 32
INFO: iteration 36, average log likelihood -1.004406
WARNING: Variances had to be floored 10 18
INFO: iteration 37, average log likelihood -1.069487
WARNING: Variances had to be floored 6
INFO: iteration 38, average log likelihood -1.054471
WARNING: Variances had to be floored 1 7 19
INFO: iteration 39, average log likelihood -1.019183
WARNING: Variances had to be floored 8 16 18 21 32
INFO: iteration 40, average log likelihood -1.022760
WARNING: Variances had to be floored 10
INFO: iteration 41, average log likelihood -1.062105
WARNING: Variances had to be floored 7
INFO: iteration 42, average log likelihood -1.036498
WARNING: Variances had to be floored 1 19 32
INFO: iteration 43, average log likelihood -1.016367
WARNING: Variances had to be floored 8 16 18 21
INFO: iteration 44, average log likelihood -1.026909
WARNING: Variances had to be floored 7 10
INFO: iteration 45, average log likelihood -1.047931
WARNING: Variances had to be floored 32
INFO: iteration 46, average log likelihood -1.041874
WARNING: Variances had to be floored 1 19
INFO: iteration 47, average log likelihood -1.020754
WARNING: Variances had to be floored 7 8 16 18
INFO: iteration 48, average log likelihood -1.015856
WARNING: Variances had to be floored 10 21 32
INFO: iteration 49, average log likelihood -1.046939
WARNING: Variances had to be floored 6
INFO: iteration 50, average log likelihood -1.050205
INFO: EM with 100000 data points 50 iterations avll -1.050205
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0325084    0.108429     0.100892      0.0894827    0.0898762     0.227455    -0.227629      0.127568      0.0612726   0.109521     0.0292781   -0.118083   -0.0513123    0.0102738    0.103118    -0.000147033  -0.139725     -0.175438    0.0301808     0.0239754    0.138023    -0.104988     -0.0865478    -0.0221769    0.0885447    0.09707   
 -0.0841015    0.0557441    0.00246272   -0.0644468   -0.0210347     0.100875     0.0223384     0.202789      0.163825   -0.0595512   -0.0251683   -0.176389    0.158695     0.0426953   -0.0527001    0.0128214     0.142458      0.154409   -0.0841228     0.23158     -0.0303721   -0.634234      0.00883593    0.234709    -0.116894    -0.0282214 
  0.041007     0.07673      0.187688     -0.0417099    0.0623793     0.0450475    0.0476191     0.0369437    -0.0576324   0.0479463   -0.00902161  -0.188053    0.0849673    0.0143263    0.00952863  -0.126282     -0.128799     -0.0716525   0.0534602    -0.128368     0.0494869   -0.0238925    -0.115606     -0.0758913   -0.124527    -0.0299166 
  0.0644343    0.0558903   -0.170346      0.0299052    0.165652     -0.216398     0.0747625     0.0892133     0.150895   -0.0906113    0.0389399    0.0147634   0.194657    -0.0160705    0.00465382  -0.0897692    -0.0244109    -0.0238591   0.199047      0.0789949    0.0214182    0.0777491    -0.000632295   0.0467993    0.0299208    0.102682  
 -0.0474218   -0.101731     0.0510529    -0.112907     0.00328988    0.0544135    0.0771344    -0.0451463     0.0708214  -0.0742928    0.0575269    0.0779856  -0.00576642  -0.065652    -0.0894826    0.00933193   -0.101575      0.0940349   0.160711      0.0173888   -0.161859    -0.0155029    -0.170926     -0.11312      0.124306     0.0284606 
  0.033156     0.14089      0.0749736    -0.1018       0.101769     -0.137177    -0.092263      0.133749     -0.0681178   0.00519571   0.173196    -0.0346892  -0.0763203    0.0400681    0.218156     0.0931717    -0.229446      0.14854     0.009826     -0.0445276    0.119305    -0.0975465    -0.157979      0.156397     0.162692    -0.0742097 
  0.04483     -0.123152     0.0477405    -0.0440676    0.134627     -0.00111871  -0.0687405     0.0910101    -0.0967773   0.113494    -0.080562    -0.0423078  -0.011493     0.0616123    0.112942    -0.0538626    -0.00803675    0.108934    0.00587372    0.233152    -0.0147144   -0.101748      0.0273988     0.170975     0.0522928   -0.0449307 
  0.0311394    0.0616006   -0.111685     -0.0686774    0.17216      -0.0578422    0.0742691    -0.0626423     0.0755889   0.20403     -0.273518     0.131236   -0.00356882   0.0776809   -0.187108    -0.154429     -0.0661479     0.0933661  -0.000603914  -0.0719921   -0.032872     0.282729      0.105315      0.0493661    0.161877    -0.00846987
  0.0621506    0.0594752   -0.0330653    -0.01937      0.060044      0.0364927    0.0480352     0.000634627  -0.186592   -0.0690214    0.00270954   0.128646    0.0216405   -0.098769     0.0298199    0.0404231     0.109653     -0.044042   -0.107781      0.00223334   0.0810764    0.0539856     0.0553692    -0.138908     0.0465573    0.102095  
 -0.0350678   -0.0489891   -0.00263366    0.202799    -0.117943     -0.00656721   0.0534692     0.0860418     0.0395892  -0.0413775    0.0506232    0.161176    0.129117     0.109461     0.150144     0.0488037    -0.0691261     0.221333    0.0167261    -0.113563     0.0136034    0.214627     -0.095641     -0.0445758   -0.0249904    0.00973316
  0.0156444    0.0716361   -0.0686506     0.00559527  -0.0646131     0.0453482    0.0171474     0.0390626    -0.0409277   0.0306868   -0.0115025   -0.179316    0.0931997    0.185337    -0.0598355    0.249357     -0.0735191     0.146827   -0.120827     -0.0992663   -0.0210577    0.1712       -0.063315      0.00193999   0.0645951   -0.19406   
 -0.0616725    0.0405461   -0.0634275     0.0523533    0.141352     -0.070863    -0.157875      0.045733     -0.0175313  -0.0266397   -0.00152283   0.0795064   0.00810141   0.00891648   0.0258532   -0.0353479     0.0640602     0.0580226   0.0722168    -0.0600122   -0.0503198    0.0581835    -0.0598454    -0.103598    -0.092543     0.0358043 
  0.00488392  -0.163845    -0.106699     -0.134383    -0.103092      0.0996217   -0.0911816     0.108233      0.159424    0.194094    -0.039085     0.0709809   0.0859654   -0.0460435   -0.0305146   -0.00914331   -0.109101      0.0473148   0.0487021    -0.0624971    0.0472694    0.0963361     0.0109846    -0.113222     0.0525809    0.0619275 
 -0.0501183   -0.0460258   -0.0100523    -0.014394    -0.0449326     0.0449181   -0.000245611  -0.0249091     0.0491473   0.0329578   -0.100321     0.0778539  -0.311384    -0.130859    -0.0468997    0.0807315     0.0983982     0.0665765   0.0617725    -0.0206772   -0.0271341   -0.0558351     0.0121849     0.0928716    0.053963    -0.0723775 
 -0.027928    -0.0728817   -0.103736      0.0899213   -0.146535      0.077352     0.0729068    -0.184979     -0.0588635   0.00775468   0.0111513   -0.0181792   0.0575187   -0.0328171   -0.0858044   -0.0660956     0.0656866    -0.0375639   0.0839509    -0.0213659    0.0108857   -0.000719735  -0.103233      0.00585665  -0.0491717   -0.0253737 
  0.0414227   -0.0914896    0.00859772   -0.0323691    0.0961627    -0.0427616   -0.121234     -0.0238784     0.0899337  -0.0166283   -0.0160693    0.0269725  -0.00795493   0.128609    -0.00889129   0.185218      0.0124928    -0.0197482   0.0762331     0.154536     0.0212103    0.011254     -0.0715902     0.0132994   -0.125159    -0.0834982 
 -0.00383203  -0.191236     0.0405054     0.136458     0.0731125     0.0722813   -0.0427187     0.0134685    -0.109787   -0.0457929    0.0192666   -0.250676    0.0287036    0.173832    -0.184104     0.0511328    -0.0207928     0.134423    0.0344957     0.0748239   -0.0276853    0.120703     -0.106829      0.0360029   -0.0495197    0.199433  
 -0.150836     0.0155694    0.0791826     0.0535718    0.126442      0.0241652    0.0690369    -0.0499692     0.0198424  -0.0919329   -0.17281      0.0911559   0.00186914  -0.045455     0.0172471   -0.0549304     0.0827962    -0.0418517  -0.014269     -0.0585757    0.0195262    0.0549121    -0.0234341    -0.0597141   -0.0882919    0.0769594 
 -0.0205661    0.00950869   0.0111296     0.114607     0.000641069   0.0431777   -0.00871606   -0.0769545    -0.0154535  -0.0906429    0.0388768    0.014953    0.136837     0.137792    -0.170812     0.111168     -0.100636      0.0542697  -0.0564198    -0.0101567    0.168171     0.0242307    -0.0853952     0.00957765  -0.0367238    0.134052  
 -0.0504058    0.0264879   -0.0818336    -0.052489    -0.230008      0.0553051   -0.108723      0.118911     -0.0498738  -0.0857312   -0.141749    -0.0556165  -0.133175    -0.0301189   -0.202978    -0.0913477    -0.127462      0.0305296  -0.0666135    -0.205908     0.121524    -0.0215369     0.049922     -0.0961417   -0.0254418    0.0346226 
  0.0302548   -0.0338326   -0.000821358  -0.118693     0.271604     -0.028899    -0.0240971    -0.114503      0.0203361  -0.297647     0.0069166   -0.156192    0.0661374    0.15197     -0.142508     0.050878     -0.00879045   -0.105877   -0.0676958     0.235193     0.039584     0.0863344    -0.0256935    -0.116996     0.0488068   -0.0473224 
  0.176829    -0.0800514   -0.00400889    0.0458784   -0.0357454    -0.0230799   -0.059187     -0.0683622    -0.0282141   0.0704377    0.108487     0.0460766  -0.00433236  -0.0468731    0.0333601    0.0575338    -0.0859016     0.0303206  -0.15653       0.0312314    0.140213    -0.12021      -0.00942516    0.0155389   -0.178183     0.0212459 
 -0.0284597   -0.235923     0.184304     -0.128491    -0.118515     -0.0436042   -0.0523708    -0.188619     -0.0622959   0.0110134   -0.108234     0.0451421  -0.141021     0.165502     0.0551451   -0.00735986   -0.107404      0.0450287  -0.0628647    -0.0569178    0.186992     0.0111015    -0.035746      0.112077    -0.0458306   -0.0772464 
  0.0349843    0.20709     -0.02124       0.206457     0.0962494     0.0127132    0.0191974     0.150438      0.167294   -0.0342525   -0.0348816    0.0234972   0.24632     -0.0193962   -0.138009     0.0165176     0.132901      0.0564192  -0.032714      0.17366     -0.0419968    0.123341      0.0528657    -0.0295819   -0.0988147   -0.141727  
  0.114322     0.0800566   -0.0669663    -0.139614    -0.0587087    -0.178876    -0.0119474     0.0186008     0.0595485  -0.0501847   -0.170806     0.0511435   0.0210283   -0.0174237   -0.070109     0.182705     -0.121354     -0.0120468  -0.0285977    -0.101172    -0.00405622  -0.0455964    -0.125316     -0.150752    -0.0126081    0.0707604 
  0.18699      0.0701935   -0.0882803     0.15204      0.159691     -0.0427479    0.0197447    -0.00716003   -0.0492931   0.164681    -0.10592     -0.0258725  -0.0370241    0.139732     0.146993     0.108576     -0.0199977     0.0116029  -0.17763       0.120506    -0.0368844    0.0868976     0.0901474     0.0372229   -0.00676504  -0.0751149 
 -0.188647    -0.103054     0.0693545     0.0799417    0.186988      0.0280538    0.144448     -0.0613448     0.0302148   0.0661407    0.227337     0.0166265   0.0234117    0.0262144    0.0849428    0.179011     -0.0343687    -0.0683327  -0.0983355     0.028484     0.0782266    0.0324156     0.0301244     0.256172    -0.122423     0.140181  
  0.0599802    0.0386667    0.0599188     0.088972     0.0324361    -0.151189     0.0910503     0.0974534    -0.169106    0.0475463    0.0120073   -0.0172499   0.0744282    0.0422614   -0.00566674   0.0458681    -0.0789604    -0.0350859  -0.0367261    -0.0534404   -0.0400172    0.0237921    -0.0305756     0.0976916   -0.0546297    0.202653  
 -0.0202837   -0.0320704   -0.0198587    -0.0927918    0.20728       0.0414265   -0.0694113    -0.0190541     0.060738    0.181561     0.0504127    0.0480205  -0.00770866  -0.133012     0.118306    -0.199351     -0.0584913    -0.0015747   0.0140606    -0.166113     0.107749    -0.113185     -0.00250663    0.0552119    0.184867    -0.0081902 
  0.0767227   -0.161437     0.0408474     0.0498564   -0.062878      0.230027    -0.108633     -0.174296     -0.115566    0.0233386    0.0221605   -0.0144652  -0.00560462  -0.015489    -0.110436     0.112568      0.112213      0.0737911  -0.0798321     0.0943523   -0.0965861   -0.109643     -0.115138      0.172933     0.0718582   -0.171267  
 -0.0195231    0.0357838   -0.00678431    0.169399    -0.0201239     0.0613451   -0.0111042    -0.0287668     0.0719169   0.1399       0.0346783    0.0443259  -0.0151224    0.137464     0.00300165   0.0584094    -0.000706252  -0.175941    0.120091      0.0979298   -0.0800043   -0.100463     -0.000332321  -0.0969092    0.130783    -0.00551725
  0.140205    -0.0208932   -0.0103698     0.0595561   -0.0225093     0.135769     0.377477     -0.0382727    -0.0744729   0.182155     0.0737765    0.0355803  -0.129901     0.0846117    0.106928    -0.0888188     0.0430823     0.0389989   0.0528013    -0.0496547    0.246509     0.0886607    -0.101805     -0.0262069    0.0063661   -0.19412   INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 7 9 19
INFO: iteration 1, average log likelihood -1.016894
WARNING: Variances had to be floored 1 7 8 9 18 32
INFO: iteration 2, average log likelihood -0.991771
WARNING: Variances had to be floored 1 7 9 10 16 19
INFO: iteration 3, average log likelihood -0.992655
WARNING: Variances had to be floored 1 7 8 9 18 19 21 32
INFO: iteration 4, average log likelihood -0.990340
WARNING: Variances had to be floored 1 7 9 16 19
INFO: iteration 5, average log likelihood -0.998558
WARNING: Variances had to be floored 1 7 8 9 10 18 19 32
INFO: iteration 6, average log likelihood -0.980295
WARNING: Variances had to be floored 1 7 9 16 19 21
INFO: iteration 7, average log likelihood -0.998815
WARNING: Variances had to be floored 1 7 8 9 18 19 32
INFO: iteration 8, average log likelihood -0.989883
WARNING: Variances had to be floored 1 7 9 10 16 19
INFO: iteration 9, average log likelihood -0.989938
WARNING: Variances had to be floored 1 7 8 9 18 19 21 32
INFO: iteration 10, average log likelihood -0.990448
INFO: EM with 100000 data points 10 iterations avll -0.990448
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0442261    0.17029       0.131837     0.03454     -0.370176     -0.107817    -0.117675   -0.00826888   0.0970206    0.173565     0.188309    -0.128695    -0.214765   -0.0784596   -0.0947444   -0.0342258    -0.07798     -0.0288381    0.0394593    0.15705      -0.27698       0.0532035   -0.0483726    0.0900741   -0.0487945    0.0204228 
  0.0877859   -0.0632001    -0.138614     0.0958101   -0.111213     -0.0536164   -0.14502     0.0189228   -0.0878969    0.0219842   -0.191515    -0.0398984   -0.10511     0.0201191   -0.144492    -0.0275204    -0.0771455    0.125196    -0.023194    -0.0505401     0.141082      0.0950187    0.107914    -0.082014    -0.044464     0.0571454 
  0.0579726   -0.162514      0.0536135   -0.106839    -0.112779     -0.0516544   -0.17537     0.0982975   -0.194273     0.00297244   0.0352277   -0.00945536  -0.0544247  -0.0507933   -0.0122353   -0.14828      -0.0100645   -0.109565    -0.133249     0.0753691    -0.0145362     0.00542255  -0.0928851    0.00782168  -0.0106431    0.0501502 
  0.00146369   0.0600689     0.0806079   -0.125674    -0.0229966    -0.162012     0.0560103  -0.104216    -0.0663677   -0.0399541    0.144004     0.0941553   -0.0928885   0.116978    -0.00668165  -0.14588       0.0961248    0.0768129    0.00271531   0.133029     -0.11337      -0.072711    -0.124196    -0.0279247   -0.0575557    0.0658667 
  0.0186199   -0.118623     -0.276269    -0.0616198    0.0435947     0.0421968    0.061688    0.101246    -0.269836     0.124936    -0.0191735   -0.0122284    0.058663   -0.138765    -0.0445577    0.187093     -0.130573    -0.160392    -0.0784704    0.00549569    0.0126062     0.0335086   -0.0982025   -0.0401226   -0.12817     -0.0645536 
 -0.0657761   -0.0696359     0.0653859   -0.172314     0.0228225     0.0400088    0.118327   -0.0876669    0.209299     0.101216     0.0100775   -0.0131813   -0.0142024  -0.0867025   -0.17697     -0.000148962   0.0216102   -0.0664079    0.037278     0.00530265    0.0293217    -0.0525378    0.0702797   -0.143583    -0.0242721    0.128436  
  0.0214623    0.0121642     0.0960675   -0.00257045   0.0399337     0.106525    -0.0692059  -0.0158349    0.207028    -0.0412449    0.0741204    0.0948271    0.0194922  -0.0200914   -0.0239       0.0452741     0.0399261    0.135142     0.12477     -0.0875545    -0.0408951     0.0325894    0.010941     0.0554181    0.157832    -0.141418  
  0.0583994    0.0720604    -0.0690302    0.0569946   -0.163551      0.0465898    0.0548453  -0.1212       0.040868     0.0699748    0.116176    -0.142416     0.0512204  -0.00113715   0.011114     0.00988102    0.086039    -0.00761869   0.0112804   -0.101634     -0.0749676    -0.0222582    0.0844494    0.166676    -0.0359695   -0.151559  
 -0.00408867  -0.178767      0.0203839    0.00826866   0.06282       0.00339221  -0.26908    -0.109579    -0.0313223   -0.167141    -0.0358518    0.165005    -0.0308699  -0.00604588   0.0786168    0.0482782    -0.0615199   -0.102488    -0.103976     0.0288877    -0.15782      -0.0970229   -0.159575     0.0962615   -0.0720666   -0.106621  
  0.0102478   -0.0121289    -0.224871    -0.00460051  -0.136225     -0.0482694   -0.0463529  -0.0131909   -0.172364     0.300829    -0.0583956   -0.0478274    0.0563711   0.0486048    0.0181444   -0.0555393    -0.106048     0.0112651    0.192634    -0.0702488    -0.0723147     0.0676836    0.115206     0.161476    -0.0201193   -0.0390641 
  0.122115    -0.103011      0.0173754   -0.0463615    0.190889     -0.0431327   -0.0420708  -0.141466    -0.0214373    0.0388805   -0.207514     0.0570118    0.210365    0.0170831    0.0811067   -0.0106413     0.125875    -0.0406622   -0.0172959   -0.229746     -0.0229769    -0.091665     0.0682993    0.109238    -0.172152    -0.0939526 
  0.154876     0.223108      0.0849466   -0.0941064    0.00819131    0.0390314   -0.111958   -0.0950974    0.0292683    0.275353    -0.0792094    0.173039     0.109108   -0.191109     0.00299639  -0.149558      0.0747164    0.0575879    0.110623    -0.0172357    -0.0209694    -0.0131165   -0.0432042    0.124056     0.116167     0.110789  
 -0.188315    -0.0400386     0.101433    -0.104275    -0.0383282     0.122046     0.0502646   0.0700994    0.179189     0.0202328    0.234112     0.149077     0.0689406  -0.1329       0.0958025    0.0318135    -0.00277068  -0.0328737    0.0517784   -0.00962211   -0.0519594    -0.0858683    0.0216845    0.0276086    0.0687491   -0.0554637 
  0.013067     0.0877534     0.0325772   -0.162137     0.000668166   0.0768316   -0.0848996   0.00308998  -0.0337159   -0.103886     0.0119348    0.137984     0.156751    0.0558949   -0.161809    -0.0363096     0.0145848    0.028741    -0.221764     0.00271775   -0.0705539     0.0557514    0.0319499    0.0958783   -0.233742     0.104565  
 -0.023988    -0.116072     -0.0650683    0.0638411   -0.0199083    -0.0388472   -0.0921191   0.2217       0.130869     0.0103479   -0.0646928   -0.0712077    0.0111623   0.0105473   -0.0669782   -0.147317     -0.063032    -0.0166458   -0.0884731   -0.129897     -0.00486561   -0.0983214    0.103167     0.0725632   -0.0678831   -0.0797096 
  0.127843    -0.0372019    -0.0196795   -0.100078     0.0393452    -0.00948779   0.0432518  -0.0289473    0.107361    -0.0193375    0.0225546    0.0971222    0.172768    0.0772516    0.146438    -0.0826963    -0.0662589    0.114596     0.104507    -0.0940071    -0.0686398    -0.0492078   -0.00461705  -0.172748     0.162771     0.0262119 
 -0.0942239   -0.0128042     0.00289659   0.186774     0.00023366   -0.0379109   -0.0493258   0.0111856    0.0191664    0.0358236    0.0779111   -0.00750595   0.0685746  -0.272511     0.151682    -0.143339      0.0472187   -0.0104816    0.0672891    0.0213997     0.000337325  -0.0451813   -0.0602921   -0.0488663    0.181306     0.165432  
 -0.215343     0.0691859     0.0425667   -0.0858469    0.0704565     0.0811245    0.139686   -0.0507952    0.0701013   -0.132497     0.107765    -0.0857401    0.0135683   0.0528563    0.284545    -0.0837923     0.091091    -0.0683197   -0.0352598   -0.0541502     0.156569      0.0176765   -0.203284    -0.0742176   -0.0365945   -0.105924  
 -0.15336     -0.0453004    -0.0450867    0.0146213    0.00843771    0.0534815    0.0831396   0.167306    -0.0836553   -0.0786802   -0.00403456   0.121963     0.0257425  -0.0581691    0.0879794   -0.0094806    -0.139064     0.105679    -0.0826392    0.0402543     0.0620753    -0.0941651   -0.0729357    0.0793135   -0.043152     0.0883824 
  0.0945991    0.000878481  -0.153838     0.0171022   -0.078997     -0.11487      0.17875    -0.0722017   -0.00764885   0.0691521   -0.100532    -0.135464    -0.129621   -0.183725    -0.0853491    0.00332285   -0.00479829  -0.105351    -0.196678     0.0518419    -0.0619899     0.0717511    0.109952     0.00534981  -0.061899    -0.154474  
 -0.0713394    0.0751727     0.127861    -0.0300638   -0.200455     -0.0596673   -0.0158842  -0.0367727    0.064121    -0.0357965   -0.0899327    0.00558634   0.134888   -0.354909    -0.0405895   -0.0096226    -0.0682471    0.0173935   -0.0404659    0.181404      0.0120753    -0.0214674   -0.290997     0.0484797    0.0599253   -0.0413094 
  0.0521229    0.105061      0.13072      0.106536    -0.0490298     0.158582     0.198787    0.00776295   0.0751218   -0.0792268    0.185885     0.147507    -0.0624397  -0.0113334   -0.0268028    0.137867     -0.0971396    0.0439413   -0.0677218    0.0465       -0.0255499     0.171266     0.0384255    0.128486     0.00397342   0.00691964
  0.217958     0.0587501     0.155547    -0.132564     0.000897762   0.0334791   -0.153637    0.0623461    0.0863222   -0.0849909    0.0630342   -0.0447945   -0.123343   -0.183765    -0.0475417    0.0504831    -0.0749695    0.00451049   0.0793635    0.190412     -0.0618543     0.186079     0.015555     0.178118     0.0957011   -0.0713379 
  0.087156    -0.0973092     0.229961    -0.0221026   -0.0512831     0.0779492    0.165469   -0.0169134   -0.0292047   -0.0104137    0.12215     -0.0285441    0.0108825  -0.0270635   -0.0325103   -0.0724275     0.0832575    0.165272     0.0524142   -0.0401556     0.133381     -0.0174728    0.0747928   -0.0546931    0.317971    -0.123322  
  0.121866     0.0946735    -0.00190389  -0.0282194    0.0517212    -0.12667     -0.133387    0.00314346  -0.083867     0.0954186    0.0664144   -0.0133423   -0.0691008  -0.00838934  -0.00419124  -0.0789074     0.140449    -0.237823     0.140682     0.138329      0.037574      0.150983    -0.197373    -0.101035    -0.0655958   -0.0934627 
 -0.0908674   -0.0466175     0.190232     0.0617806    0.0540811     0.031837    -0.0250548  -0.00501051  -0.129467     0.0248461   -0.135301    -0.222034     0.0933843  -0.0247562   -0.0956916   -0.135548      0.0324691    0.0862865    0.167707     0.0506856    -0.0383708     0.0287752    0.125972    -0.0816615    0.0872804   -0.0418072 
  0.11775     -0.0832405     0.0359806   -0.0366636    0.0630838     0.0481824    0.24111    -0.0839146    0.209389     0.046421    -0.118373     0.0329466    0.0537048   0.0732068   -0.0658608    0.179693      0.030177    -0.0188623   -0.0489341    0.0440889     0.101286      0.0424311    0.192089     0.0479582   -0.0571682   -0.0172751 
 -0.288861    -0.041861     -0.102202     0.0466685    0.146622     -0.0105285    0.182097   -9.93727e-5   0.00748272   0.107919     0.0449824   -0.0216507    0.139425   -0.0449321   -0.170117    -0.0757021     0.00631895   0.122096    -0.177352    -0.145599      0.178229     -0.0700324    0.0886881   -0.0484918   -0.0337992    0.016764  
 -0.10503     -0.0216892    -0.032995    -0.0517654    0.156322      0.0211828   -0.110865   -0.075279     0.16163     -0.248059    -0.0759585    0.124471     0.0230466   0.153762     0.0978333   -0.0843972    -0.0260135   -0.109802     0.164102     0.120253      0.0622487    -0.0613691   -0.219595    -0.110786     0.111513     0.151919  
 -0.0644663    0.0552058    -0.112217     0.0738624   -0.0292055     0.0496369    0.0722415   0.280846    -0.0494449   -0.0733493    0.0516632    0.0530859   -0.110732   -0.0255152    0.067689     0.0361872    -0.00546259  -0.070842     0.0631313    0.0245526    -0.0336634     0.127367    -0.0034639   -0.0237803   -0.00587139  -0.0867075 
 -0.150169    -0.141811      0.0916863   -0.0570065    0.0758367     0.0450989   -0.147079    0.0124877   -0.0440788   -0.125004     0.160922     0.144864     0.0569509  -0.0187981    0.00093176   0.194874     -0.0199736    0.0222964   -0.0833935    0.273706      0.00446476   -0.0234972    0.0444348   -0.116095     0.1355      -0.0530836 
 -0.0889304   -0.0119833    -0.0361411   -0.218784     0.0450175     0.0762171   -0.199461    0.216944     0.0136161    0.119033     0.230494    -0.0967703    0.0280308  -0.0482602   -0.102805    -0.0393049    -0.116376     0.0817184    0.0451318    0.000860029   0.227667     -0.0722556    0.10099     -0.0164807   -0.0325806   -0.0686833 kind full, method split
0: avll = -1.4275710871787999
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.427590
INFO: iteration 2, average log likelihood -1.427542
INFO: iteration 3, average log likelihood -1.427512
INFO: iteration 4, average log likelihood -1.427480
INFO: iteration 5, average log likelihood -1.427441
INFO: iteration 6, average log likelihood -1.427387
INFO: iteration 7, average log likelihood -1.427296
INFO: iteration 8, average log likelihood -1.427112
INFO: iteration 9, average log likelihood -1.426709
INFO: iteration 10, average log likelihood -1.425912
INFO: iteration 11, average log likelihood -1.424728
INFO: iteration 12, average log likelihood -1.423603
INFO: iteration 13, average log likelihood -1.422941
INFO: iteration 14, average log likelihood -1.422662
INFO: iteration 15, average log likelihood -1.422557
INFO: iteration 16, average log likelihood -1.422517
INFO: iteration 17, average log likelihood -1.422500
INFO: iteration 18, average log likelihood -1.422493
INFO: iteration 19, average log likelihood -1.422490
INFO: iteration 20, average log likelihood -1.422489
INFO: iteration 21, average log likelihood -1.422488
INFO: iteration 22, average log likelihood -1.422487
INFO: iteration 23, average log likelihood -1.422486
INFO: iteration 24, average log likelihood -1.422486
INFO: iteration 25, average log likelihood -1.422486
INFO: iteration 26, average log likelihood -1.422485
INFO: iteration 27, average log likelihood -1.422485
INFO: iteration 28, average log likelihood -1.422485
INFO: iteration 29, average log likelihood -1.422485
INFO: iteration 30, average log likelihood -1.422484
INFO: iteration 31, average log likelihood -1.422484
INFO: iteration 32, average log likelihood -1.422484
INFO: iteration 33, average log likelihood -1.422484
INFO: iteration 34, average log likelihood -1.422484
INFO: iteration 35, average log likelihood -1.422484
INFO: iteration 36, average log likelihood -1.422483
INFO: iteration 37, average log likelihood -1.422483
INFO: iteration 38, average log likelihood -1.422483
INFO: iteration 39, average log likelihood -1.422483
INFO: iteration 40, average log likelihood -1.422483
INFO: iteration 41, average log likelihood -1.422483
INFO: iteration 42, average log likelihood -1.422483
INFO: iteration 43, average log likelihood -1.422483
INFO: iteration 44, average log likelihood -1.422483
INFO: iteration 45, average log likelihood -1.422483
INFO: iteration 46, average log likelihood -1.422483
INFO: iteration 47, average log likelihood -1.422483
INFO: iteration 48, average log likelihood -1.422483
INFO: iteration 49, average log likelihood -1.422483
INFO: iteration 50, average log likelihood -1.422483
INFO: EM with 100000 data points 50 iterations avll -1.422483
952.4 data points per parameter
1: avll = [-1.42759,-1.42754,-1.42751,-1.42748,-1.42744,-1.42739,-1.4273,-1.42711,-1.42671,-1.42591,-1.42473,-1.4236,-1.42294,-1.42266,-1.42256,-1.42252,-1.4225,-1.42249,-1.42249,-1.42249,-1.42249,-1.42249,-1.42249,-1.42249,-1.42249,-1.42249,-1.42249,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.422498
INFO: iteration 2, average log likelihood -1.422436
INFO: iteration 3, average log likelihood -1.422384
INFO: iteration 4, average log likelihood -1.422320
INFO: iteration 5, average log likelihood -1.422240
INFO: iteration 6, average log likelihood -1.422144
INFO: iteration 7, average log likelihood -1.422040
INFO: iteration 8, average log likelihood -1.421937
INFO: iteration 9, average log likelihood -1.421843
INFO: iteration 10, average log likelihood -1.421763
INFO: iteration 11, average log likelihood -1.421694
INFO: iteration 12, average log likelihood -1.421632
INFO: iteration 13, average log likelihood -1.421575
INFO: iteration 14, average log likelihood -1.421523
INFO: iteration 15, average log likelihood -1.421476
INFO: iteration 16, average log likelihood -1.421434
INFO: iteration 17, average log likelihood -1.421395
INFO: iteration 18, average log likelihood -1.421361
INFO: iteration 19, average log likelihood -1.421331
INFO: iteration 20, average log likelihood -1.421303
INFO: iteration 21, average log likelihood -1.421279
INFO: iteration 22, average log likelihood -1.421259
INFO: iteration 23, average log likelihood -1.421241
INFO: iteration 24, average log likelihood -1.421226
INFO: iteration 25, average log likelihood -1.421213
INFO: iteration 26, average log likelihood -1.421203
INFO: iteration 27, average log likelihood -1.421194
INFO: iteration 28, average log likelihood -1.421188
INFO: iteration 29, average log likelihood -1.421182
INFO: iteration 30, average log likelihood -1.421178
INFO: iteration 31, average log likelihood -1.421174
INFO: iteration 32, average log likelihood -1.421171
INFO: iteration 33, average log likelihood -1.421169
INFO: iteration 34, average log likelihood -1.421167
INFO: iteration 35, average log likelihood -1.421166
INFO: iteration 36, average log likelihood -1.421165
INFO: iteration 37, average log likelihood -1.421164
INFO: iteration 38, average log likelihood -1.421163
INFO: iteration 39, average log likelihood -1.421162
INFO: iteration 40, average log likelihood -1.421161
INFO: iteration 41, average log likelihood -1.421161
INFO: iteration 42, average log likelihood -1.421160
INFO: iteration 43, average log likelihood -1.421160
INFO: iteration 44, average log likelihood -1.421160
INFO: iteration 45, average log likelihood -1.421159
INFO: iteration 46, average log likelihood -1.421159
INFO: iteration 47, average log likelihood -1.421159
INFO: iteration 48, average log likelihood -1.421158
INFO: iteration 49, average log likelihood -1.421158
INFO: iteration 50, average log likelihood -1.421158
INFO: EM with 100000 data points 50 iterations avll -1.421158
473.9 data points per parameter
2: avll = [-1.4225,-1.42244,-1.42238,-1.42232,-1.42224,-1.42214,-1.42204,-1.42194,-1.42184,-1.42176,-1.42169,-1.42163,-1.42158,-1.42152,-1.42148,-1.42143,-1.4214,-1.42136,-1.42133,-1.4213,-1.42128,-1.42126,-1.42124,-1.42123,-1.42121,-1.4212,-1.42119,-1.42119,-1.42118,-1.42118,-1.42117,-1.42117,-1.42117,-1.42117,-1.42117,-1.42116,-1.42116,-1.42116,-1.42116,-1.42116,-1.42116,-1.42116,-1.42116,-1.42116,-1.42116,-1.42116,-1.42116,-1.42116,-1.42116,-1.42116]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.421169
INFO: iteration 2, average log likelihood -1.421120
INFO: iteration 3, average log likelihood -1.421082
INFO: iteration 4, average log likelihood -1.421039
INFO: iteration 5, average log likelihood -1.420989
INFO: iteration 6, average log likelihood -1.420931
INFO: iteration 7, average log likelihood -1.420865
INFO: iteration 8, average log likelihood -1.420793
INFO: iteration 9, average log likelihood -1.420720
INFO: iteration 10, average log likelihood -1.420648
INFO: iteration 11, average log likelihood -1.420582
INFO: iteration 12, average log likelihood -1.420521
INFO: iteration 13, average log likelihood -1.420466
INFO: iteration 14, average log likelihood -1.420416
INFO: iteration 15, average log likelihood -1.420372
INFO: iteration 16, average log likelihood -1.420333
INFO: iteration 17, average log likelihood -1.420297
INFO: iteration 18, average log likelihood -1.420266
INFO: iteration 19, average log likelihood -1.420238
INFO: iteration 20, average log likelihood -1.420213
INFO: iteration 21, average log likelihood -1.420190
INFO: iteration 22, average log likelihood -1.420169
INFO: iteration 23, average log likelihood -1.420149
INFO: iteration 24, average log likelihood -1.420131
INFO: iteration 25, average log likelihood -1.420114
INFO: iteration 26, average log likelihood -1.420097
INFO: iteration 27, average log likelihood -1.420082
INFO: iteration 28, average log likelihood -1.420067
INFO: iteration 29, average log likelihood -1.420053
INFO: iteration 30, average log likelihood -1.420040
INFO: iteration 31, average log likelihood -1.420027
INFO: iteration 32, average log likelihood -1.420015
INFO: iteration 33, average log likelihood -1.420003
INFO: iteration 34, average log likelihood -1.419992
INFO: iteration 35, average log likelihood -1.419981
INFO: iteration 36, average log likelihood -1.419970
INFO: iteration 37, average log likelihood -1.419960
INFO: iteration 38, average log likelihood -1.419951
INFO: iteration 39, average log likelihood -1.419941
INFO: iteration 40, average log likelihood -1.419932
INFO: iteration 41, average log likelihood -1.419924
INFO: iteration 42, average log likelihood -1.419915
INFO: iteration 43, average log likelihood -1.419907
INFO: iteration 44, average log likelihood -1.419900
INFO: iteration 45, average log likelihood -1.419892
INFO: iteration 46, average log likelihood -1.419885
INFO: iteration 47, average log likelihood -1.419878
INFO: iteration 48, average log likelihood -1.419871
INFO: iteration 49, average log likelihood -1.419864
INFO: iteration 50, average log likelihood -1.419857
INFO: EM with 100000 data points 50 iterations avll -1.419857
236.4 data points per parameter
3: avll = [-1.42117,-1.42112,-1.42108,-1.42104,-1.42099,-1.42093,-1.42086,-1.42079,-1.42072,-1.42065,-1.42058,-1.42052,-1.42047,-1.42042,-1.42037,-1.42033,-1.4203,-1.42027,-1.42024,-1.42021,-1.42019,-1.42017,-1.42015,-1.42013,-1.42011,-1.4201,-1.42008,-1.42007,-1.42005,-1.42004,-1.42003,-1.42001,-1.42,-1.41999,-1.41998,-1.41997,-1.41996,-1.41995,-1.41994,-1.41993,-1.41992,-1.41992,-1.41991,-1.4199,-1.41989,-1.41988,-1.41988,-1.41987,-1.41986,-1.41986]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.419860
INFO: iteration 2, average log likelihood -1.419809
INFO: iteration 3, average log likelihood -1.419765
INFO: iteration 4, average log likelihood -1.419715
INFO: iteration 5, average log likelihood -1.419656
INFO: iteration 6, average log likelihood -1.419585
INFO: iteration 7, average log likelihood -1.419502
INFO: iteration 8, average log likelihood -1.419407
INFO: iteration 9, average log likelihood -1.419305
INFO: iteration 10, average log likelihood -1.419198
INFO: iteration 11, average log likelihood -1.419091
INFO: iteration 12, average log likelihood -1.418986
INFO: iteration 13, average log likelihood -1.418887
INFO: iteration 14, average log likelihood -1.418794
INFO: iteration 15, average log likelihood -1.418709
INFO: iteration 16, average log likelihood -1.418632
INFO: iteration 17, average log likelihood -1.418562
INFO: iteration 18, average log likelihood -1.418501
INFO: iteration 19, average log likelihood -1.418445
INFO: iteration 20, average log likelihood -1.418396
INFO: iteration 21, average log likelihood -1.418351
INFO: iteration 22, average log likelihood -1.418311
INFO: iteration 23, average log likelihood -1.418275
INFO: iteration 24, average log likelihood -1.418241
INFO: iteration 25, average log likelihood -1.418211
INFO: iteration 26, average log likelihood -1.418183
INFO: iteration 27, average log likelihood -1.418157
INFO: iteration 28, average log likelihood -1.418132
INFO: iteration 29, average log likelihood -1.418110
INFO: iteration 30, average log likelihood -1.418088
INFO: iteration 31, average log likelihood -1.418068
INFO: iteration 32, average log likelihood -1.418049
INFO: iteration 33, average log likelihood -1.418032
INFO: iteration 34, average log likelihood -1.418015
INFO: iteration 35, average log likelihood -1.417998
INFO: iteration 36, average log likelihood -1.417983
INFO: iteration 37, average log likelihood -1.417968
INFO: iteration 38, average log likelihood -1.417954
INFO: iteration 39, average log likelihood -1.417940
INFO: iteration 40, average log likelihood -1.417927
INFO: iteration 41, average log likelihood -1.417914
INFO: iteration 42, average log likelihood -1.417902
INFO: iteration 43, average log likelihood -1.417890
INFO: iteration 44, average log likelihood -1.417879
INFO: iteration 45, average log likelihood -1.417867
INFO: iteration 46, average log likelihood -1.417856
INFO: iteration 47, average log likelihood -1.417846
INFO: iteration 48, average log likelihood -1.417836
INFO: iteration 49, average log likelihood -1.417826
INFO: iteration 50, average log likelihood -1.417816
INFO: EM with 100000 data points 50 iterations avll -1.417816
118.1 data points per parameter
4: avll = [-1.41986,-1.41981,-1.41976,-1.41971,-1.41966,-1.41958,-1.4195,-1.41941,-1.4193,-1.4192,-1.41909,-1.41899,-1.41889,-1.41879,-1.41871,-1.41863,-1.41856,-1.4185,-1.41845,-1.4184,-1.41835,-1.41831,-1.41827,-1.41824,-1.41821,-1.41818,-1.41816,-1.41813,-1.41811,-1.41809,-1.41807,-1.41805,-1.41803,-1.41801,-1.418,-1.41798,-1.41797,-1.41795,-1.41794,-1.41793,-1.41791,-1.4179,-1.41789,-1.41788,-1.41787,-1.41786,-1.41785,-1.41784,-1.41783,-1.41782]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.417815
INFO: iteration 2, average log likelihood -1.417746
INFO: iteration 3, average log likelihood -1.417680
INFO: iteration 4, average log likelihood -1.417602
INFO: iteration 5, average log likelihood -1.417505
INFO: iteration 6, average log likelihood -1.417385
INFO: iteration 7, average log likelihood -1.417244
INFO: iteration 8, average log likelihood -1.417085
INFO: iteration 9, average log likelihood -1.416918
INFO: iteration 10, average log likelihood -1.416753
INFO: iteration 11, average log likelihood -1.416596
INFO: iteration 12, average log likelihood -1.416452
INFO: iteration 13, average log likelihood -1.416323
INFO: iteration 14, average log likelihood -1.416209
INFO: iteration 15, average log likelihood -1.416108
INFO: iteration 16, average log likelihood -1.416020
INFO: iteration 17, average log likelihood -1.415941
INFO: iteration 18, average log likelihood -1.415871
INFO: iteration 19, average log likelihood -1.415808
INFO: iteration 20, average log likelihood -1.415751
INFO: iteration 21, average log likelihood -1.415700
INFO: iteration 22, average log likelihood -1.415654
INFO: iteration 23, average log likelihood -1.415612
INFO: iteration 24, average log likelihood -1.415574
INFO: iteration 25, average log likelihood -1.415539
INFO: iteration 26, average log likelihood -1.415508
INFO: iteration 27, average log likelihood -1.415479
INFO: iteration 28, average log likelihood -1.415452
INFO: iteration 29, average log likelihood -1.415428
INFO: iteration 30, average log likelihood -1.415405
INFO: iteration 31, average log likelihood -1.415383
INFO: iteration 32, average log likelihood -1.415363
INFO: iteration 33, average log likelihood -1.415344
INFO: iteration 34, average log likelihood -1.415325
INFO: iteration 35, average log likelihood -1.415308
INFO: iteration 36, average log likelihood -1.415291
INFO: iteration 37, average log likelihood -1.415274
INFO: iteration 38, average log likelihood -1.415258
INFO: iteration 39, average log likelihood -1.415243
INFO: iteration 40, average log likelihood -1.415228
INFO: iteration 41, average log likelihood -1.415213
INFO: iteration 42, average log likelihood -1.415199
INFO: iteration 43, average log likelihood -1.415185
INFO: iteration 44, average log likelihood -1.415171
INFO: iteration 45, average log likelihood -1.415158
INFO: iteration 46, average log likelihood -1.415144
INFO: iteration 47, average log likelihood -1.415131
INFO: iteration 48, average log likelihood -1.415119
INFO: iteration 49, average log likelihood -1.415106
INFO: iteration 50, average log likelihood -1.415093
INFO: EM with 100000 data points 50 iterations avll -1.415093
59.0 data points per parameter
5: avll = [-1.41782,-1.41775,-1.41768,-1.4176,-1.4175,-1.41739,-1.41724,-1.41709,-1.41692,-1.41675,-1.4166,-1.41645,-1.41632,-1.41621,-1.41611,-1.41602,-1.41594,-1.41587,-1.41581,-1.41575,-1.4157,-1.41565,-1.41561,-1.41557,-1.41554,-1.41551,-1.41548,-1.41545,-1.41543,-1.4154,-1.41538,-1.41536,-1.41534,-1.41533,-1.41531,-1.41529,-1.41527,-1.41526,-1.41524,-1.41523,-1.41521,-1.4152,-1.41518,-1.41517,-1.41516,-1.41514,-1.41513,-1.41512,-1.41511,-1.41509]
[-1.42757,-1.42759,-1.42754,-1.42751,-1.42748,-1.42744,-1.42739,-1.4273,-1.42711,-1.42671,-1.42591,-1.42473,-1.4236,-1.42294,-1.42266,-1.42256,-1.42252,-1.4225,-1.42249,-1.42249,-1.42249,-1.42249,-1.42249,-1.42249,-1.42249,-1.42249,-1.42249,-1.42249,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.42248,-1.4225,-1.42244,-1.42238,-1.42232,-1.42224,-1.42214,-1.42204,-1.42194,-1.42184,-1.42176,-1.42169,-1.42163,-1.42158,-1.42152,-1.42148,-1.42143,-1.4214,-1.42136,-1.42133,-1.4213,-1.42128,-1.42126,-1.42124,-1.42123,-1.42121,-1.4212,-1.42119,-1.42119,-1.42118,-1.42118,-1.42117,-1.42117,-1.42117,-1.42117,-1.42117,-1.42116,-1.42116,-1.42116,-1.42116,-1.42116,-1.42116,-1.42116,-1.42116,-1.42116,-1.42116,-1.42116,-1.42116,-1.42116,-1.42116,-1.42116,-1.42117,-1.42112,-1.42108,-1.42104,-1.42099,-1.42093,-1.42086,-1.42079,-1.42072,-1.42065,-1.42058,-1.42052,-1.42047,-1.42042,-1.42037,-1.42033,-1.4203,-1.42027,-1.42024,-1.42021,-1.42019,-1.42017,-1.42015,-1.42013,-1.42011,-1.4201,-1.42008,-1.42007,-1.42005,-1.42004,-1.42003,-1.42001,-1.42,-1.41999,-1.41998,-1.41997,-1.41996,-1.41995,-1.41994,-1.41993,-1.41992,-1.41992,-1.41991,-1.4199,-1.41989,-1.41988,-1.41988,-1.41987,-1.41986,-1.41986,-1.41986,-1.41981,-1.41976,-1.41971,-1.41966,-1.41958,-1.4195,-1.41941,-1.4193,-1.4192,-1.41909,-1.41899,-1.41889,-1.41879,-1.41871,-1.41863,-1.41856,-1.4185,-1.41845,-1.4184,-1.41835,-1.41831,-1.41827,-1.41824,-1.41821,-1.41818,-1.41816,-1.41813,-1.41811,-1.41809,-1.41807,-1.41805,-1.41803,-1.41801,-1.418,-1.41798,-1.41797,-1.41795,-1.41794,-1.41793,-1.41791,-1.4179,-1.41789,-1.41788,-1.41787,-1.41786,-1.41785,-1.41784,-1.41783,-1.41782,-1.41782,-1.41775,-1.41768,-1.4176,-1.4175,-1.41739,-1.41724,-1.41709,-1.41692,-1.41675,-1.4166,-1.41645,-1.41632,-1.41621,-1.41611,-1.41602,-1.41594,-1.41587,-1.41581,-1.41575,-1.4157,-1.41565,-1.41561,-1.41557,-1.41554,-1.41551,-1.41548,-1.41545,-1.41543,-1.4154,-1.41538,-1.41536,-1.41534,-1.41533,-1.41531,-1.41529,-1.41527,-1.41526,-1.41524,-1.41523,-1.41521,-1.4152,-1.41518,-1.41517,-1.41516,-1.41514,-1.41513,-1.41512,-1.41511,-1.41509]
32×26 Array{Float64,2}:
 -0.679403   -0.334318    0.341897    -0.836117    -0.30082    -0.0523989  -0.122986   -0.172835   -0.552912   -0.173359    -0.0883134    0.722193   -0.372746   -0.448513     0.219417   -0.556529      0.427034   -0.22876     -0.515234    -0.00685352  -0.40153     -0.105443    -0.495343     0.478187     0.0739807    0.0519738 
 -0.405157   -0.126867   -0.0210066    0.0219835   -0.219318   -0.481693    0.0420598  -0.218107    0.151626   -0.143525     0.0944446    0.249261   -0.312502    0.0151894    0.169061   -0.301013      0.268654    0.0133613   -0.00204578  -0.64167     -0.162409     0.0454688   -0.0471703    0.0835573    0.391495     0.00836888
 -0.30488     0.496704   -0.19974     -0.265978     0.239069    0.16386    -0.128734   -0.0217011  -0.0212553  -0.259153     0.0132735    0.312067   -0.532064    0.178553     0.1763     -0.331161     -0.377412   -0.36097     -0.115582    -0.211991    -0.110398     0.0740986   -0.795074    -0.417605     0.708098    -0.30892   
  0.193356    0.531787   -0.153372    -0.211413     0.0439699  -0.165646   -1.00384    -0.478798    0.225856    0.437417     0.0248352   -0.214378   -0.14575     0.147652    -0.279789   -0.258042      0.0737132   0.370773    -0.0829863   -0.144067     0.225164     0.222088    -0.494519     0.316149     1.13482     -0.0874999 
 -0.0937554  -0.124798   -0.25972     -0.0733346   -0.771946    0.769748   -0.417797   -0.204666   -0.0367984  -0.00180484   0.666976    -0.447773    0.448265   -0.614332    -0.0253282   0.368818     -0.315932    0.106847    -0.126928     0.162977     0.395485     0.251278     0.0111428   -0.541043     0.119119     0.436634  
  0.732317   -0.232617    0.486905     0.0761791   -0.336738   -0.0404059  -0.50471    -0.271307    0.345506    0.270427     0.484963    -0.354769    0.455756   -0.491246     0.0841564   0.291142     -0.256127    0.934926     0.46539     -0.182989     0.271785     0.0229344    0.383092    -0.334706     0.53378      0.0393525 
  0.257543    0.209416    0.0185701   -0.0597646    0.023034    0.357708   -0.212722    0.18535    -0.111562    0.151569    -0.0264848   -0.0228037   0.159262   -0.0184819    0.0878335   0.187905     -0.238867   -0.0296277   -0.0532836    0.487866     0.242401     0.077138     0.0817748   -0.0449002   -0.25213     -0.161422  
  0.0350606  -0.482543   -0.0100881   -0.00316879  -0.098568   -0.116462    0.893539    0.444801   -0.249109   -0.0500605    0.12871      0.0378425   0.45718    -0.264738     0.204817    0.536973     -0.0838433   0.238362    -0.0993407    0.256047    -0.0105176   -0.0254497    0.676575     0.380721    -0.571469    -0.112785  
 -0.665647   -0.406603   -0.653951    -0.426889     0.168723   -0.130147    0.498113    0.390791    0.568538   -0.378636     0.227986    -0.0521147   0.27313     0.283237    -0.180656   -0.212455      0.61146    -0.530516     0.239947     0.266846    -0.256981    -0.525555    -0.267604    -0.472324     0.187368     0.0907465 
 -0.0830456  -0.226906    0.523518     0.206688     0.204241   -0.0560046   0.688934    0.122926    0.189655   -0.318737     0.0521126    0.475378    0.205423   -0.144916     0.447351    0.079776     -0.155191   -0.0764465    0.276119     0.195378    -0.257835    -0.697413    -0.0887269   -0.802712    -0.469485     0.42017   
  0.330947   -0.345401    0.704865     0.566085     0.199826   -0.224924    0.511641   -0.507282    0.305208    0.493957    -0.0192151   -0.191537   -0.0787944  -0.49915     -0.691643   -0.00280544    0.445132   -0.00778446  -0.474491    -0.229438    -0.610553     0.00845119   0.101937    -0.314506    -0.427162     0.379447  
 -0.112759    0.269193    0.691749     0.979005     0.271015    0.254293   -0.323358   -0.491065    0.0717942  -0.0708975    0.396213    -0.41896     0.0694124   0.943499    -0.341255    0.000488317  -0.0143874  -0.0296971   -0.0245096    0.16572      0.00469302  -0.377618    -0.522176    -0.494272    -0.503017     0.0251856 
  0.0475119   0.284793   -0.105897     0.267119     0.215077    0.27691     0.0942625   0.663355   -0.0280382   0.26342     -0.219927    -0.422426    0.106403    0.351368    -0.143406   -0.08859      -0.486961   -0.546149     0.107434     0.912845    -0.0381127   -0.200925    -0.217724     0.206745    -0.541318    -0.0700641 
  0.237754    0.32141    -0.065318    -0.490913     0.433251    0.898075    0.122609    0.697257   -0.227933   -0.0873854   -0.396676     0.351473   -0.12057    -0.200091    -0.718113   -0.174963      0.180053   -0.0598968    0.156702     0.421605    -0.333138     0.644919    -0.188027    -0.206502    -0.596524     0.240188  
  0.494316    0.144756   -0.150601     0.0481798    0.5194      0.0987855   0.58436    -0.255435    0.955196    0.155567    -0.57353     -0.329793    0.276545    0.176724    -0.446594   -0.248451     -0.0275089  -0.0410861    0.427031    -0.679424    -0.320338    -0.198818    -0.0611468   -0.358657     0.116562     0.361601  
  0.298768    0.431233   -0.21982      0.339036    -0.275779    0.0246349  -0.0249348   0.637969    1.12676    -0.299458     0.704853    -0.0887641  -0.150368    0.258066    -0.30685     0.601694      0.0943589   0.044122     1.00934     -0.500717    -0.0207483    0.748783     0.132194    -0.403906     0.153976    -0.500835  
 -0.0285973  -0.42648    -0.134036     0.422257    -0.0237218  -0.714384   -0.0843463  -0.475205   -0.203345    0.266297    -0.0307532   -0.75542     0.265418   -0.0757018    0.263434   -0.13002      -0.150658   -0.0598743   -0.279327    -0.10521      0.29457     -0.552328     0.321295     0.294911     0.532091     0.0562937 
  0.296555   -0.60389    -0.334898    -0.165263     0.22645     0.17475    -0.164914   -0.352344   -0.671435   -0.224081    -0.302847    -0.283422    0.0889723   0.0744822   -0.564807    0.723209      0.149921    0.329015    -0.363968    -0.196128    -0.522044    -0.130948    -0.121493    -0.130444     0.00074535  -0.701357  
  0.505594   -0.367005    0.224572    -0.740251     0.36003     0.277722   -0.438092   -0.396924    0.198331    0.20091      0.196924    -0.665167    0.331066   -0.123544     0.0092166  -0.0977749    -0.109614   -0.109884     0.503853     0.100336     0.424403    -0.0722983   -0.169519     0.209636     0.04976     -0.356682  
 -0.0240658  -0.0122545   0.0685277   -0.218709     0.0672061   0.123869   -0.344903    0.22834     0.104471    0.324515    -0.00221069  -0.285533    0.392886   -0.656775    -0.0361661  -0.652928      0.391561    0.0252517    0.959987     0.250447     0.508351     0.0148188    0.00106957   0.276173    -0.136976     0.897478  
 -0.0923972  -0.0440783   0.153268    -0.205046    -0.24566    -0.179838   -0.20295    -0.032054   -0.150051    0.178013     0.318573     0.0449978   0.599716    0.284274    -0.0801498   0.412708      0.110568   -0.0671091    0.207634     0.14639     -0.0378362    0.439249     0.114135     0.00227053  -0.50978     -0.0931112 
 -0.476697    0.0558529  -0.272257    -0.204527     0.0274871  -0.0767591   0.360825   -0.233606   -0.249058    0.100752     0.673886    -0.125848    0.42629     0.341361    -0.323795   -0.0511915    -0.0903894  -0.194034     0.111226     0.038188    -0.177213     0.448421    -0.167413    -0.501642     0.228531     0.864263  
  0.1158     -0.249966   -0.00246485   0.0292644    0.133687    0.0554991   0.214411    0.133311    0.269583   -0.0226266   -0.0940229   -0.140988    0.0583099  -0.0813586   -0.186764   -0.0834096     0.0392719  -0.0672099    0.233806    -0.0143239   -0.0848793   -0.164781    -0.104594    -0.0725771   -0.15372      0.201941  
  0.265308    0.350205   -0.126499     0.0271203    0.161155    0.277037   -0.056259   -0.0464204   0.158434    0.0764091   -0.0584434   -0.0112244   0.132877   -0.0859837   -0.110592    0.167343     -0.204144    0.0807459   -0.238699     0.274264     0.0620825   -0.147646     0.0255862   -0.464728     0.274672    -0.0738969 
  0.263497   -0.421187    0.252367     0.495088    -0.0745207   0.0347491  -0.723963    0.424228   -0.255065   -0.292456    -0.540191    -0.0780362  -0.235936   -0.553332     0.048093   -0.147919      0.100385    0.403331    -0.0177796   -0.156976     0.288712    -0.185196     0.246579     0.593864    -0.418258    -0.331634  
  0.135194    0.428863    0.738843     0.221576    -0.199433    0.0329533  -0.294733   -0.365563   -0.205635    0.107466    -0.0927706    0.702612   -0.426525   -0.332427     0.471787   -0.150899     -0.119267    0.504004    -0.340073     0.0241113    0.459129    -0.0045254    0.383684     0.207796     0.0843602   -0.24833   
 -0.303889    0.383487    0.0498852    0.448814     0.0421926  -0.49553     0.165682    0.313558   -0.305928   -0.30974     -0.25444      0.473635   -0.231134    0.432653    -0.31725    -0.0195614     0.185736    0.0990045   -0.383347    -0.00494499  -0.512483     0.170971     0.237607     0.0221758   -0.41517     -0.309784  
 -0.0241246   0.17689     0.128172     0.00516532   0.131868   -0.724774    0.454206    0.484072   -0.190914    0.0668143   -0.238458     0.0110741  -0.63826     0.162532     0.153079   -0.373662      0.144929   -0.531819    -0.0164157    0.0407772   -0.310626     0.0818657   -0.356135     1.23397     -0.27071     -0.139366  
 -0.208693   -0.0168383   0.142914    -0.0542685   -0.204786   -0.0465568  -0.145155   -0.02981    -0.297261    0.0123899    0.105915     0.148131   -0.115144   -0.179328     0.106621   -0.168682     -0.0230916   0.155574    -0.0414269   -0.0222427   -0.0107817    0.225384     0.00952668   0.2904       0.0791363   -0.00399024
 -0.279673   -0.0341914   0.0449496    0.153371    -0.265017   -0.12773     0.0204035  -0.261008    0.190286    0.160695     0.179186     0.136614   -0.205718    0.215304     0.509559   -0.215946      0.246809   -0.280684    -0.273269    -0.426948     0.114445    -0.315549    -0.21406      0.119388     0.132942    -0.363004  
 -0.309178    0.147272   -0.813358    -0.333454    -0.190179    0.114709   -0.274441    0.408      -0.130607   -0.133037    -0.22302      0.47441    -0.162326    0.00543294   0.259536    0.133215      0.0139955   0.0407618   -0.271868     0.190281     0.306228     0.384674     0.314678     0.6077       0.284989    -0.563743  
  0.0784584  -0.0353331  -0.452398    -0.742858    -0.224118    0.0618496   0.494798    0.0299711   0.348099    0.0773206   -0.0496012    0.319995   -0.274804   -0.417873     0.314089    0.0929357    -0.551388   -0.254233     0.20321     -0.154811     0.0364406    0.476157     0.468048     0.248672     0.568537     0.31944   INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.415081
INFO: iteration 2, average log likelihood -1.415069
INFO: iteration 3, average log likelihood -1.415057
INFO: iteration 4, average log likelihood -1.415046
INFO: iteration 5, average log likelihood -1.415035
INFO: iteration 6, average log likelihood -1.415023
INFO: iteration 7, average log likelihood -1.415013
INFO: iteration 8, average log likelihood -1.415002
INFO: iteration 9, average log likelihood -1.414992
INFO: iteration 10, average log likelihood -1.414982
INFO: EM with 100000 data points 10 iterations avll -1.414982
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.702670e+05
      1       7.111343e+05      -2.591327e+05 |       32
      2       6.990750e+05      -1.205926e+04 |       32
      3       6.942412e+05      -4.833846e+03 |       32
      4       6.916358e+05      -2.605401e+03 |       32
      5       6.899188e+05      -1.717027e+03 |       32
      6       6.886923e+05      -1.226453e+03 |       32
      7       6.877425e+05      -9.497621e+02 |       32
      8       6.869762e+05      -7.663788e+02 |       32
      9       6.862982e+05      -6.779202e+02 |       32
     10       6.857227e+05      -5.755693e+02 |       32
     11       6.852677e+05      -4.549869e+02 |       32
     12       6.848833e+05      -3.844233e+02 |       32
     13       6.845632e+05      -3.200091e+02 |       32
     14       6.843047e+05      -2.585616e+02 |       32
     15       6.840771e+05      -2.276284e+02 |       32
     16       6.838902e+05      -1.868804e+02 |       32
     17       6.837152e+05      -1.750226e+02 |       32
     18       6.835694e+05      -1.457492e+02 |       32
     19       6.834417e+05      -1.276660e+02 |       32
     20       6.833202e+05      -1.215714e+02 |       32
     21       6.832054e+05      -1.147476e+02 |       32
     22       6.830973e+05      -1.081062e+02 |       32
     23       6.829919e+05      -1.053804e+02 |       32
     24       6.828985e+05      -9.348128e+01 |       32
     25       6.828144e+05      -8.407737e+01 |       32
     26       6.827431e+05      -7.129224e+01 |       32
     27       6.826870e+05      -5.603305e+01 |       32
     28       6.826350e+05      -5.205253e+01 |       32
     29       6.825869e+05      -4.813652e+01 |       32
     30       6.825426e+05      -4.422664e+01 |       32
     31       6.825033e+05      -3.936116e+01 |       32
     32       6.824618e+05      -4.146588e+01 |       32
     33       6.824253e+05      -3.646929e+01 |       32
     34       6.823910e+05      -3.434333e+01 |       32
     35       6.823580e+05      -3.296813e+01 |       32
     36       6.823223e+05      -3.573293e+01 |       32
     37       6.822890e+05      -3.328100e+01 |       32
     38       6.822552e+05      -3.379621e+01 |       32
     39       6.822198e+05      -3.540002e+01 |       32
     40       6.821804e+05      -3.946150e+01 |       32
     41       6.821393e+05      -4.100952e+01 |       32
     42       6.820911e+05      -4.827057e+01 |       32
     43       6.820490e+05      -4.207576e+01 |       32
     44       6.820090e+05      -4.003448e+01 |       32
     45       6.819766e+05      -3.232822e+01 |       32
     46       6.819476e+05      -2.900532e+01 |       32
     47       6.819153e+05      -3.234914e+01 |       32
     48       6.818802e+05      -3.508171e+01 |       32
     49       6.818486e+05      -3.155774e+01 |       32
     50       6.818196e+05      -2.903322e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 681819.6084940641)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.426785
INFO: iteration 2, average log likelihood -1.421834
INFO: iteration 3, average log likelihood -1.420515
INFO: iteration 4, average log likelihood -1.419544
INFO: iteration 5, average log likelihood -1.418503
INFO: iteration 6, average log likelihood -1.417512
INFO: iteration 7, average log likelihood -1.416815
INFO: iteration 8, average log likelihood -1.416426
INFO: iteration 9, average log likelihood -1.416211
INFO: iteration 10, average log likelihood -1.416075
INFO: iteration 11, average log likelihood -1.415975
INFO: iteration 12, average log likelihood -1.415895
INFO: iteration 13, average log likelihood -1.415827
INFO: iteration 14, average log likelihood -1.415768
INFO: iteration 15, average log likelihood -1.415715
INFO: iteration 16, average log likelihood -1.415668
INFO: iteration 17, average log likelihood -1.415625
INFO: iteration 18, average log likelihood -1.415585
INFO: iteration 19, average log likelihood -1.415548
INFO: iteration 20, average log likelihood -1.415514
INFO: iteration 21, average log likelihood -1.415481
INFO: iteration 22, average log likelihood -1.415450
INFO: iteration 23, average log likelihood -1.415421
INFO: iteration 24, average log likelihood -1.415393
INFO: iteration 25, average log likelihood -1.415367
INFO: iteration 26, average log likelihood -1.415341
INFO: iteration 27, average log likelihood -1.415317
INFO: iteration 28, average log likelihood -1.415293
INFO: iteration 29, average log likelihood -1.415271
INFO: iteration 30, average log likelihood -1.415249
INFO: iteration 31, average log likelihood -1.415228
INFO: iteration 32, average log likelihood -1.415208
INFO: iteration 33, average log likelihood -1.415189
INFO: iteration 34, average log likelihood -1.415170
INFO: iteration 35, average log likelihood -1.415152
INFO: iteration 36, average log likelihood -1.415134
INFO: iteration 37, average log likelihood -1.415117
INFO: iteration 38, average log likelihood -1.415101
INFO: iteration 39, average log likelihood -1.415085
INFO: iteration 40, average log likelihood -1.415069
INFO: iteration 41, average log likelihood -1.415054
INFO: iteration 42, average log likelihood -1.415039
INFO: iteration 43, average log likelihood -1.415024
INFO: iteration 44, average log likelihood -1.415009
INFO: iteration 45, average log likelihood -1.414995
INFO: iteration 46, average log likelihood -1.414981
INFO: iteration 47, average log likelihood -1.414968
INFO: iteration 48, average log likelihood -1.414955
INFO: iteration 49, average log likelihood -1.414942
INFO: iteration 50, average log likelihood -1.414929
INFO: EM with 100000 data points 50 iterations avll -1.414929
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.126097    -0.258671    0.242442     0.155821    -0.121884      0.158138     0.123134   -0.0413328  -0.0019132   0.238235    0.0518669    -0.0119481   0.125815   -0.190959    0.0440534   0.138371   -3.1245e-5    0.177641   -0.0963712     0.0434301    0.00414055   0.0368529    0.319612    -0.00880634  -0.264232    -0.0346425
  0.00318357   0.57371     0.429189     0.746303     0.449991      0.150504    -0.0950788  -0.197899   -0.0746522   0.0426876   0.139316     -0.213474    0.0815655   0.873333   -0.516544    0.0564139   0.0477203   -0.239008   -0.20591       0.406522    -0.0854721   -0.00815551  -0.31353     -0.361076    -0.699709    -0.126201 
 -0.142261    -0.307222    0.191427    -0.264442    -0.567152     -0.376066     0.0240609  -0.113054   -0.106454   -0.105283    0.659028      0.360042   -0.172667   -0.506064    0.362177    0.118487    0.137312     0.327535    0.000207471  -0.373545     0.00724767   0.382863     0.206332     0.27516      0.187146     0.0187563
 -0.22172     -0.398074    0.256679     0.0720835    0.265382     -0.0745071    0.74227     0.352942    0.292296   -0.296838    0.115052      0.381923    0.328161   -0.0906068   0.375222    0.191799   -0.107318    -0.321154    0.281979      0.379594    -0.346363    -0.766223    -0.134849    -0.603022    -0.413062     0.367456 
 -0.665368    -0.197751    0.0178627    0.00478346   0.243448     -0.526099     0.24011    -0.0904099  -0.22299    -0.553649    0.59013       0.118665    0.195333   -0.458917   -0.790736   -0.43392     0.700806    -0.111603    0.205653      0.00318073  -0.0161247   -0.050709    -0.319783    -0.601362     0.0949509    1.48641  
  0.934124    -0.37213     0.162689    -0.103712     0.00493219    0.2282      -0.118684   -0.119543   -0.0187642   0.495513    0.185752     -0.611554    0.760639   -0.373696    0.164291    0.238174   -0.297184     0.462179    0.242104      0.440561     0.400018    -0.331888     0.278507    -0.102469     0.0717914   -0.148918 
 -0.174746    -0.264106    0.510637    -0.545318     0.0173771    -0.165011    -0.372511   -0.474529   -1.15816     0.133955   -0.135466     -0.0453413  -0.0978363  -0.469526    0.327385   -0.514406    0.134477    -0.0263864  -0.676907      0.555354    -0.236503    -0.485732    -0.606223     0.56935      0.00574101   0.0566876
  0.0395626    0.112588    0.719108     0.39826     -0.0548203     0.211118     0.0846215  -0.225616    0.254782    0.227378    0.097648      0.14492    -0.115419    0.163696   -0.150977   -0.178229    0.00666893   0.329814   -0.0243707    -0.31106     -0.346381    -0.391233    -0.280597    -0.544014    -0.0850374   -0.0327066
  0.448178    -0.355435    0.0960581    0.176895     0.417597     -0.144868     0.449197   -0.214487    0.529263    0.179954   -0.329363     -0.806086    0.195002   -0.0892629  -0.563364   -0.37526     0.0247716   -0.214146    0.336402     -0.357571    -0.478057    -0.343591    -0.173263    -0.221886    -0.0292444    0.447946 
 -0.171218     0.161908    0.0999597    0.35778     -0.209015     -0.353728    -0.060711    0.351539   -0.468526   -0.21049    -0.122279      0.17819    -0.0145988   0.266499   -0.225563   -0.0254316   0.203298     0.0690464  -0.229033      0.0648797   -0.417496     0.134189     0.0990471    0.266157    -0.467292    -0.305095 
 -0.00711156   0.133995   -0.367406     0.0860814    0.170291      0.0380777    0.0258355  -0.0153789   0.197531    0.0541712  -0.198039     -0.139605   -0.0944289   0.0681228   0.110051   -0.125211   -0.01813     -0.341139   -0.353615      0.132716     0.136005    -0.428006    -0.220893    -0.220431     0.488466    -0.14945  
  0.138804     0.090124   -0.10941      0.241093     0.1756       -0.126103     0.649405    0.0483049   0.393661   -0.0924439  -0.624981      0.731294    0.218937    0.0472945  -0.0274229   0.269229    0.168176     0.213776   -0.0883268    -0.171157     0.04987      0.167372     0.835656    -0.143913    -0.618943     0.283587 
 -0.11727      0.0939003   0.126073    -0.0930488    0.137985     -0.594897     0.415404    0.425391   -0.102219    0.0757197  -0.272539      0.0977172  -0.614066    0.19155     0.141187   -0.340723    0.134279    -0.514066    0.00507968    0.0694673   -0.372473     0.112323    -0.303615     1.0379      -0.228787    -0.164419 
 -0.0780511    0.400025   -0.470293    -0.155838    -0.244685      0.375341    -0.612871    0.228376   -0.236659    0.133616   -0.19715       0.292998    0.0329597  -0.0341943   0.0953908   0.578499   -0.0889283    0.137871   -0.610564      0.0986177    0.164635     0.41066      0.26113      0.534008     0.181812    -0.817559 
 -0.0376297   -0.0436267   0.084704    -0.042363    -0.0235634    -0.0143622   -0.0529591  -0.0113138   0.0393097   0.0518315   0.0287863     0.0258825   0.032506   -0.134686    0.0915392  -0.078657   -0.0180583    0.0219072   0.118371     -0.00590135   0.0885975    0.052681     0.0240366    0.117133    -0.00819012   0.0501507
  0.396762    -0.0514977   0.285089     0.0553902   -0.0845759     0.490048    -0.477058   -0.120648    0.12282    -0.0330496  -0.297777      0.301794   -0.560837   -0.457444    0.557332   -0.281147    0.0776971    0.119695    0.0171721    -0.33126      0.442306    -0.119207     0.0516411    0.309488    -0.00678008  -0.593062 
 -0.185333     0.110799    0.776019     0.00874788  -0.255528      0.333785    -0.32559    -0.287997   -0.581801    0.151606   -0.191327      0.765451   -0.313739   -0.472705    0.373434   -0.204091   -0.40956      0.619591   -0.301066      0.552336     0.581177     0.173971     0.273833     0.0600248    0.082802     0.118739 
  0.261356    -0.0760228  -0.28521     -0.362812     0.10001       0.462193     0.0481961   0.0953527  -0.010834    0.011477    0.257049     -0.1244      0.289309   -0.0828108  -0.574823    0.300748   -0.088497     0.035321    0.140386      0.262699    -0.234959     0.335191    -0.202202    -0.434638    -0.0575328    0.176412 
 -0.218194     0.230899    0.589213     0.850378    -0.19857      -0.93555     -0.0502746  -0.564516   -0.0735817  -0.0077363   0.136218      0.125122   -0.215849    0.262447    0.502288   -0.146213   -0.0407742    0.234219   -0.521559     -0.358583     0.157663    -0.349471     0.264295     0.500608     0.184501    -0.378743 
 -0.311692    -0.0523426  -0.872515    -0.554125    -0.000526298  -0.405444     0.312234    0.122024   -0.0244309  -0.135506   -0.605031      0.519118   -0.141715   -0.301989    0.200703   -0.560919    0.0345925    0.221958   -0.436156     -0.0547299   -0.0360509    0.220959     0.334009     0.474511     0.878148    -0.0231883
 -0.104574     0.349092    0.415677     0.301528    -0.724157      0.412631    -0.324476    0.0902436   0.422161   -0.259728    0.7062       -0.635472    0.456047   -0.428255    0.0216454   0.371706   -0.255086    -0.186326   -0.192494      0.568178     0.391719     0.215183    -0.179616    -0.25661     -0.303989     0.670878 
 -0.907425     0.0118544  -0.584486    -0.21403     -0.150872      0.20951     -0.142218    0.43594    -0.0246127  -0.131376    0.359437     -0.0173368  -0.0617816   0.484748    0.187424   -0.575262   -0.0474658   -0.126914    0.318371      0.52786      0.47463      0.0695123    0.168044     0.0261835    0.0941631    0.0697644
  0.0864246   -0.44561    -0.30726     -0.22826      0.501173     -0.0871142   -0.284839   -0.0948486  -0.763288   -0.643734   -0.491289      0.0582365  -0.202682    0.316853   -0.549627    0.753942    0.106195     0.683304   -0.173499     -0.143206    -0.750944    -0.182772     0.0202869   -0.220033    -0.153784    -0.931129 
  0.215518     0.280559   -0.0148122   -0.273531     0.44386       0.657961     0.0397549   0.838192   -0.220987    0.0131262  -0.568293      0.165494   -0.0149162  -0.32619    -0.497237   -0.256111    0.0156313   -0.0141908   0.214427      0.595457    -0.138552     0.371846    -0.0206181    0.199385    -0.750041     0.135216 
 -0.121382     0.394202   -0.415751    -0.48079     -0.0918446     0.216445     0.707192   -0.435841    0.188579    0.492753    0.136941      0.248893   -0.0429458   0.0820984   0.136951    0.299923   -0.820878    -0.463272    0.0254146    -0.124607    -0.169146     0.443893     0.252008    -0.311994     0.646524     0.505086 
  0.464581     0.285159   -0.866299    -0.336847    -0.47476      -0.0120274    0.237627    0.931289   -0.588616   -0.162968    0.255777      0.0186516  -0.18425    -0.182817    0.713417    0.422324   -0.442682    -0.624004    0.124431      0.272314     0.397602     0.141462    -0.384185     0.29873      0.235005    -0.121583 
  0.337117     0.405745    0.115013    -0.95543      0.456368      0.070305    -0.1848     -0.303609    0.391795   -0.150133    0.289696     -0.168554    0.241986   -0.0319135   0.0430119  -0.573587    0.11037     -0.0633901   0.86223      -0.179105     0.565841     0.267284    -0.196627     0.100405     0.283849    -0.0598583
  0.286851     0.272079    0.00800181  -0.12746     -0.0971548    -0.0201916   -1.054      -0.57828     0.338641    0.407518    0.126182     -0.325433   -0.0025535  -0.0130328  -0.166625   -0.0859966  -0.149036     0.490512    0.061146     -0.234616     0.184586     0.00786402  -0.279061    -0.107304     1.19752      0.0753025
 -0.517841    -0.405683   -0.341242    -0.284112    -0.250528     -0.13882      0.01938    -0.573588   -0.111517    0.357102    0.493295     -0.405957    0.8447      0.346491    0.0159034   0.147151    0.274284    -0.193784   -0.0210299    -0.389918    -0.00106324   0.0252239   -0.00862872  -0.11631     -0.0165018    0.0135735
  0.381554     0.286383   -0.214962     0.283049    -0.130569     -0.00642645  -0.158129    0.526294    1.06577    -0.246145    0.593902     -0.225176    0.0333515   0.298698   -0.289908    0.724991   -0.122053     0.17406     1.05038      -0.302615     0.0611024    0.604698     0.27117     -0.393762     0.224071    -0.355156 
  0.27253     -0.764757   -0.0845273    0.463712    -0.026251     -0.480636    -0.566387    0.134852   -0.301988   -0.195564   -0.489517     -0.617803   -0.153233   -0.757016   -0.299215    0.286857   -0.10503      0.0912264  -0.00401695   -0.348433     0.622179    -0.0888774    0.551848     0.76526     -0.045853     0.061145 
 -0.790276     0.10543    -0.0136894   -0.370274    -0.0373066     0.0351603   -0.0300266   0.0970773   0.163838   -0.37641    -0.000388769   0.582267   -0.593436    0.107916   -0.0642613  -0.315976    0.298827    -0.65989    -0.203372     -0.390893    -0.586508     0.114457    -0.738759    -0.191489     0.294751    -0.276048 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.414917
INFO: iteration 2, average log likelihood -1.414905
INFO: iteration 3, average log likelihood -1.414893
INFO: iteration 4, average log likelihood -1.414882
INFO: iteration 5, average log likelihood -1.414871
INFO: iteration 6, average log likelihood -1.414861
INFO: iteration 7, average log likelihood -1.414851
INFO: iteration 8, average log likelihood -1.414841
INFO: iteration 9, average log likelihood -1.414832
INFO: iteration 10, average log likelihood -1.414823
INFO: EM with 100000 data points 10 iterations avll -1.414823
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
