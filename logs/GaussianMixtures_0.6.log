>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.6.6
INFO: Installing JLD v0.6.4
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.0
INFO: Installing Rmath v0.1.3
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.1.1
INFO: Installing StaticArrays v0.0.8
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.787
Commit c71f205 (2016-09-26 16:28 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-96-generic #143-Ubuntu SMP Mon Aug 29 20:15:20 UTC 2016 x86_64 x86_64
Memory: 2.9392929077148438 GB (678.3046875 MB free)
Uptime: 23865.0 sec
Load Avg:  1.0029296875  1.0146484375  1.013671875
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3500 MHz    1450891 s         16 s     130955 s     539966 s         74 s
#2  3500 MHz     641232 s       5460 s      79926 s    1560704 s          5 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.2
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.6.6
 - JLD                           0.6.4
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.0
 - Rmath                         0.1.3
 - SHA                           0.2.1
 - ScikitLearnBase               0.1.1
 - StaticArrays                  0.0.8
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:345
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:378
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:346
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1739
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-1.1649617364664745e7,[22647.8,77352.2],
[5160.36 -23031.6 18399.1; -5145.84 23186.8 -18402.4],

Array{Float64,2}[
[22858.3 -3294.1 2246.39; -3294.1 35288.6 -10670.3; 2246.39 -10670.3 30746.0],

[76785.4 3573.65 -2576.99; 3573.65 64988.5 10440.6; -2576.99 10440.6 69300.1]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.315569e+03
      1       9.396037e+02      -3.759651e+02 |        7
      2       8.994144e+02      -4.018934e+01 |        0
      3       8.994144e+02       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 899.4143518731676)
INFO: K-means with 272 data points using 3 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.086097
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.832744
INFO: iteration 2, lowerbound -3.682429
INFO: iteration 3, lowerbound -3.512293
INFO: iteration 4, lowerbound -3.320000
INFO: iteration 5, lowerbound -3.130804
INFO: iteration 6, lowerbound -2.966544
INFO: dropping number of Gaussions to 7
INFO: iteration 7, lowerbound -2.838125
INFO: dropping number of Gaussions to 6
INFO: iteration 8, lowerbound -2.742061
INFO: dropping number of Gaussions to 4
INFO: iteration 9, lowerbound -2.656669
INFO: iteration 10, lowerbound -2.581401
INFO: dropping number of Gaussions to 3
INFO: iteration 11, lowerbound -2.508578
INFO: iteration 12, lowerbound -2.441475
INFO: iteration 13, lowerbound -2.390557
INFO: iteration 14, lowerbound -2.352421
INFO: iteration 15, lowerbound -2.325140
INFO: iteration 16, lowerbound -2.309839
INFO: iteration 17, lowerbound -2.308386
INFO: dropping number of Gaussions to 2
INFO: iteration 18, lowerbound -2.302916
INFO: iteration 19, lowerbound -2.299259
INFO: iteration 20, lowerbound -2.299256
INFO: iteration 21, lowerbound -2.299254
INFO: iteration 22, lowerbound -2.299254
INFO: iteration 23, lowerbound -2.299253
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Sat 08 Oct 2016 11:07:53 AM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Sat 08 Oct 2016 11:07:54 AM UTC: K-means with 272 data points using 3 iterations
11.3 data points per parameter
,Sat 08 Oct 2016 11:07:56 AM UTC: EM with 272 data points 0 iterations avll -2.086097
5.8 data points per parameter
,Sat 08 Oct 2016 11:07:56 AM UTC: GMM converted to Variational GMM
,Sat 08 Oct 2016 11:07:59 AM UTC: iteration 1, lowerbound -3.832744
,Sat 08 Oct 2016 11:07:59 AM UTC: iteration 2, lowerbound -3.682429
,Sat 08 Oct 2016 11:07:59 AM UTC: iteration 3, lowerbound -3.512293
,Sat 08 Oct 2016 11:07:59 AM UTC: iteration 4, lowerbound -3.320000
,Sat 08 Oct 2016 11:07:59 AM UTC: iteration 5, lowerbound -3.130804
,Sat 08 Oct 2016 11:07:59 AM UTC: iteration 6, lowerbound -2.966544
,Sat 08 Oct 2016 11:07:59 AM UTC: dropping number of Gaussions to 7
,Sat 08 Oct 2016 11:07:59 AM UTC: iteration 7, lowerbound -2.838125
,Sat 08 Oct 2016 11:08:00 AM UTC: dropping number of Gaussions to 6
,Sat 08 Oct 2016 11:08:00 AM UTC: iteration 8, lowerbound -2.742061
,Sat 08 Oct 2016 11:08:00 AM UTC: dropping number of Gaussions to 4
,Sat 08 Oct 2016 11:08:00 AM UTC: iteration 9, lowerbound -2.656669
,Sat 08 Oct 2016 11:08:00 AM UTC: iteration 10, lowerbound -2.581401
,Sat 08 Oct 2016 11:08:00 AM UTC: dropping number of Gaussions to 3
,Sat 08 Oct 2016 11:08:00 AM UTC: iteration 11, lowerbound -2.508578
,Sat 08 Oct 2016 11:08:00 AM UTC: iteration 12, lowerbound -2.441475
,Sat 08 Oct 2016 11:08:00 AM UTC: iteration 13, lowerbound -2.390557
,Sat 08 Oct 2016 11:08:00 AM UTC: iteration 14, lowerbound -2.352421
,Sat 08 Oct 2016 11:08:00 AM UTC: iteration 15, lowerbound -2.325140
,Sat 08 Oct 2016 11:08:00 AM UTC: iteration 16, lowerbound -2.309839
,Sat 08 Oct 2016 11:08:00 AM UTC: iteration 17, lowerbound -2.308386
,Sat 08 Oct 2016 11:08:00 AM UTC: dropping number of Gaussions to 2
,Sat 08 Oct 2016 11:08:00 AM UTC: iteration 18, lowerbound -2.302916
,Sat 08 Oct 2016 11:08:00 AM UTC: iteration 19, lowerbound -2.299259
,Sat 08 Oct 2016 11:08:00 AM UTC: iteration 20, lowerbound -2.299256
,Sat 08 Oct 2016 11:08:00 AM UTC: iteration 21, lowerbound -2.299254
,Sat 08 Oct 2016 11:08:00 AM UTC: iteration 22, lowerbound -2.299254
,Sat 08 Oct 2016 11:08:01 AM UTC: iteration 23, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:01 AM UTC: iteration 24, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:01 AM UTC: iteration 25, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:01 AM UTC: iteration 26, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:01 AM UTC: iteration 27, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:01 AM UTC: iteration 28, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:01 AM UTC: iteration 29, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:01 AM UTC: iteration 30, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:01 AM UTC: iteration 31, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:01 AM UTC: iteration 32, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:01 AM UTC: iteration 33, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:01 AM UTC: iteration 34, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:01 AM UTC: iteration 35, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:01 AM UTC: iteration 36, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:02 AM UTC: iteration 37, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:02 AM UTC: iteration 38, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:02 AM UTC: iteration 39, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:02 AM UTC: iteration 40, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:02 AM UTC: iteration 41, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:02 AM UTC: iteration 42, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:02 AM UTC: iteration 43, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:02 AM UTC: iteration 44, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:02 AM UTC: iteration 45, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:02 AM UTC: iteration 46, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:02 AM UTC: iteration 47, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:02 AM UTC: iteration 48, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:02 AM UTC: iteration 49, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:02 AM UTC: iteration 50, lowerbound -2.299253
,Sat 08 Oct 2016 11:08:02 AM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.9549,178.045]
β = [95.9549,178.045]
m = [2.00023 53.852; 4.2503 79.2869]
ν = [97.9549,180.045]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.375876 -0.00895312; 0.0 0.0127487],

[0.184042 -0.00764405; 0.0 0.00858171]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -0.9687794505453032
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9687794505453023
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9687794505453025
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -0.9900066226236287
avll from llpg:  -0.9900066226236289
avll direct:     -0.9900066226236289
sum posterior: 100000.0
32×26 Array{Float64,2}:
 -0.0246703    0.0700605    0.0354851    0.0910691    0.114541     0.186929     0.0396615   -0.22708     -0.016181     0.0493318   -0.0391655   -0.0201318     0.0807978   -0.255746     -0.183566     -0.0510292    0.00371799   0.0427359    0.072945     0.0113735   -0.0713616    0.099639    -0.111322    -0.179746    -0.0487809    0.127061  
  0.105115     0.140008     0.169156    -0.061354    -0.0520779   -0.0261477    0.041017    -0.00524913   0.0734645   -0.0192061   -0.0159855    0.0298073     0.334454    -0.178532     -0.0156481     0.0491009    0.0138671   -0.0332894   -0.00417075  -0.126323     0.0761972    0.100101    -0.020476     0.0227337    0.084391    -0.00135514
 -0.0888857   -0.106094    -0.0940936    0.0161522   -0.0871615   -0.141296    -0.042627     0.058533     0.133315    -0.0347062    0.214289    -0.0755047     0.0618701   -0.152323      0.0527399     0.0086958    0.0887199    0.0210284    0.0463737    0.0493029    0.0603827    0.116738     0.0302224    0.0925975    0.161804    -0.0910124 
  0.0389844    0.0193049   -0.110706     0.0107301    0.0911076    0.226734    -0.00620454  -0.140889     0.0231214    0.096477    -0.103616     0.162519     -0.0372662   -0.059975     -0.0420385     0.0667132    0.00131292  -0.0752567    0.165526     0.0627836    0.0237069    0.0736144    0.0752069    0.227814     0.00434338   0.100288  
  0.0124849   -0.150691    -0.0743034   -0.0258748   -0.019847    -0.0403914    0.0408383    0.0239718    0.145563     0.0488987    0.0870306    0.0118007     0.198376    -0.189735      0.06309      -0.0455981    0.077732     0.102736     0.0571479    0.23837     -0.0352734    0.0312705    0.0280807    0.248238    -0.0211912    0.0914476 
  0.024523     0.00863961   0.0292048    0.0417462   -0.00343551   0.0958611    0.0375269   -0.0711084    0.0810559   -0.0777275    0.0268145   -0.0344336     0.164394    -0.0821617     0.0725511     0.134167     0.0476689   -0.0874984    0.0254433    0.0592775    0.0656952    0.064857    -0.233562     0.0509362    0.00600732  -0.0974506 
 -0.109081     0.276472     0.0597925   -0.0175598   -0.010239     0.02309      0.0898276   -0.0404789   -0.0581017   -0.0860024    0.134354     0.0158739    -0.150375     0.0988211     0.181464     -0.143342     0.0707352   -0.0145965   -0.0867657   -0.136791     0.0888614   -0.0686728    0.0120985    0.139381     0.0293306   -0.0518461 
 -0.0052852   -0.00318076   0.0335408   -0.0498157   -0.0672446    0.0913217    0.0937782   -0.0602091    0.137759    -0.0724127   -0.0388491   -0.0590511    -0.00214354   0.160916      0.0811209     0.234809     0.136907     0.0882759   -0.188039    -0.0190641   -0.032749     0.0981152   -0.00490916   0.111201     0.0078472   -0.158     
  0.0343071    0.065772     0.015874     0.0975017   -0.0932818    0.0658595    0.0259948   -0.110016     0.0847803    0.0600064    0.0573263   -0.090018      0.0497472   -0.171327      0.000911193  -0.151924     0.0910433   -0.0623978    0.0222551    0.0142564   -0.0805662    0.0127105   -0.222286     0.0828319   -0.10221      0.303085  
  0.0332849    0.0281415   -0.0307906    0.0886357    0.0987801   -0.00443891   0.0467971    0.172485     0.0394154    0.012104     0.0987239   -0.0686665    -0.170686     0.00682995   -0.191462      0.190827    -0.00817524   0.0165618    0.047133     0.0434065    0.197385    -0.0699369   -0.122303    -0.0259974   -0.0969183   -0.00715866
 -0.00945632   0.0296073    0.064728    -0.0718195    0.117794     0.144585    -0.165167     0.0619062   -0.0201573   -0.0697687    0.0131112    0.12844      -0.112553    -0.106681     -0.0355371     0.0412622    0.0510328    0.253712     0.0196903   -0.188721    -0.00383373   0.137638     0.0156606    0.0441968    0.0231706   -0.0832867 
  0.163667    -0.183484     0.0591872    0.103245     0.103878    -0.0526994   -0.0378423    0.0821778   -0.0048995   -0.00693433   0.234236     0.000843663   0.0382144    0.00466499    0.00889875    0.015698     0.087679     0.0102028    0.136579    -0.00465676  -0.0403943    0.0386048    0.0477367    0.00747201  -0.127423    -0.153888  
  0.0527998    0.0446332    0.00103649  -0.164755     0.0407683    0.0707209   -0.0782365   -0.106237    -0.00804108  -0.0371039   -0.0719173   -0.0312538    -0.0193669    0.0663816    -0.039736     -0.101467    -0.0697147    0.0816517    0.0471612    0.0181875   -0.078264    -0.0134111    0.00802004  -0.0874206   -0.099459    -0.0826218 
  0.0873923   -0.0448995   -0.0168893    0.00581864  -0.0799726    0.118991     0.134122     0.0874425    0.00814075   0.0817642    0.262593    -0.00153492    0.0533775   -0.0903499    -0.0117752     0.0630828   -0.0413913   -0.138924     0.129525     0.0958529   -0.0714473   -0.0113149   -0.078001    -0.0809748   -0.0341904    0.00916922
  0.0562227    0.118775     0.126371    -0.0474369   -0.0356102   -0.109085     0.0992862    0.0958693    0.12457      0.092012     0.174627     0.0196993    -0.0623862    0.101579     -0.0872022    -0.100217     0.059073    -0.118915    -0.177386     0.0342855    0.0565883    0.103224    -0.0672863    0.0367275    0.195212     0.00522133
  0.18165      0.170181     0.139051    -0.0411283   -0.0254192    0.0415714    0.0740728   -0.096009     0.0153359    0.0893073    0.105794    -0.153778      0.0425807   -0.0858641     0.0499178    -0.0153355    0.113428     0.0302023   -0.063199     0.0151828    0.0147716   -0.235931    -0.278044    -0.108668     0.0968315    0.0682739 
 -0.0399052   -0.151861    -0.0873056    0.0839186    0.175633    -0.0956126    0.0999213   -0.121679     0.0255652   -0.124671     0.0150631   -0.0563835    -0.0617914    0.113356     -0.134142      0.0378838   -0.108278    -0.00502788   0.0524224    0.0476275   -0.0191666    0.011828     0.0420385    0.115279     0.132616     0.0905191 
 -0.00599596   0.209636     0.272758     0.0216985   -0.135278     0.0277886    0.13668     -0.0725328   -0.138524    -0.0522679    0.194071     0.153048      0.0439933   -0.000270075  -0.0351003     0.112788     0.0251241    0.213237     0.0204431   -0.0388502   -0.121909    -0.141515    -0.0876589    0.00479566  -0.221381    -0.0511777 
  0.0756618    0.089939    -0.133834     0.0357108   -0.00860084   0.0344356    0.185439    -0.0392796   -0.124963    -0.289064    -0.0351215    0.132063      0.161506    -0.0956671     0.0351019    -0.0360908    0.0218887    0.0792961    0.139028     0.0968524    0.0459595   -0.130871    -0.0787826   -0.215646     0.0742793    0.132869  
  0.00290394  -0.0316942   -0.0145337    0.0877954    0.0930722   -0.0282235   -0.0169226    0.0257505   -0.0902779    0.20065     -0.038564    -0.164383      0.0612001   -0.0743717    -0.0918728    -0.0818567    0.102866     0.117251     0.0229857   -0.0183825    0.0786717   -0.126409    -0.0738236    0.108175    -0.0860622   -0.00920407
  0.0306964   -0.0791338   -0.0268977   -0.0635709    0.00455006   0.326607    -0.0223285   -0.0405088    0.109269    -0.117984     0.0399673    0.0302055     0.121989     0.0796944    -0.0724977    -0.0924019    0.0741016   -0.00158912  -0.014353    -0.038482    -0.253075     0.00719549  -0.0588263    0.0229889   -0.00533124   0.0229631 
 -0.160931    -0.0580175    0.0847358   -0.0193289    0.0145543   -0.265661     0.0988242    0.0179056   -0.148093     0.00388686  -0.0490359    0.0287651     0.0169461   -0.0858909    -0.0088042    -0.0538887   -0.0501224    0.0337276   -0.221894     0.00416824  -0.0972512   -0.087525     0.0686942   -0.0157749   -0.0105026   -0.203449  
  0.108778    -0.0180496   -0.17503      0.119872     0.129254     0.169388    -0.0895688   -0.0142145    0.0290856   -0.118052     0.126195    -0.112545      0.131098    -0.158802      0.0842375    -0.0170009    0.197348     0.111023    -0.0644798    0.0103561   -0.101718     0.134172    -0.123807    -0.0533568   -0.0718574   -0.0833132 
 -0.0162575    0.172856     0.162004     0.0383254   -0.123588    -0.163946     0.0118089    0.0594527   -0.0870711    0.114617     0.0794982    0.00774007   -0.0293491    0.174648     -0.0364058    -0.204951     0.0974576    0.100364     0.00779901   0.033062    -0.112532     0.0908651   -0.194543    -0.102338     0.0325427   -0.0920623 
  0.21449     -0.0405131   -0.0469061   -0.0810258    0.0160429   -0.173688    -0.021549     0.015009     0.00683304   0.207117     0.0230278   -0.0581303    -0.0370036   -0.134091     -0.0789198    -0.00194799   0.163006     0.0394896   -0.00676944   0.0915351   -0.1716      -0.0977124    0.0531536   -0.0137464   -0.117709     0.0714173 
 -0.0724309    0.0145612    0.0245099    0.0279877    0.0698564    0.00168308  -0.0492174   -0.0880013   -0.0738078   -0.0867646   -0.0623502   -0.178526      0.119132    -0.220263      0.10519       0.0732686    0.0506888    0.068651     0.115479    -0.0451473   -0.117365    -2.67742e-5  -0.0587878   -0.0531375   -0.0558003    0.0853807 
 -0.0957102   -0.0511511    0.0981703    0.0498739    0.1078       0.0408808   -0.0430738   -0.0999058    0.0609776    0.149482     0.187128    -0.00600166   -0.161213     0.0360349     0.162179      0.0734124   -0.076791     0.0291407   -0.177185     0.12399      0.0192328    0.104046    -0.0452502   -0.0269068    0.241816    -0.0575729 
  0.137293    -0.00860929  -0.0556116    0.183118     0.175524     0.0132487   -0.0790237   -0.0211989   -0.0687347   -0.00973608  -0.058661     0.0474546    -0.0495308    0.0272771    -0.157344     -0.0294266   -0.107627    -0.137056    -0.0597921    0.0169416    0.0305922    0.15584      0.147848    -0.0493661    0.221496     0.0936511 
 -0.127345     0.207514     0.0396701   -0.0227565   -0.0208572   -0.0232261   -0.206088    -0.261765    -0.0489609   -0.080555     0.127653     0.0200106    -0.112111    -0.091811      0.00571452    0.0813286    0.0991189   -0.00716563  -0.0378711    0.0236451   -0.0459595    0.0898497    0.132029     0.0131138    0.0796406   -0.00293423
  0.0712275   -0.153706    -0.118532    -0.15849      0.0542432    0.0904603   -0.0292143    0.1369      -0.140892     0.114765    -0.0612571   -0.00426084    0.0324932    0.0599198     0.063767     -0.137867     0.0447664   -0.114441     0.20883      0.0177348    0.0418996    0.0438145    0.119425    -0.0888587   -0.0161043   -0.099569  
 -0.106989     0.25019      0.232258    -0.0644315    0.0385067    0.0614674   -0.0838545   -0.00451586   0.0364553    0.0650458   -0.00144344   0.0753525     0.0205395    0.210558      0.0997205     0.0824759   -0.0800494   -0.248184     0.0273203    0.054422    -0.00759097  -0.0686097    0.068649     0.0162276    0.104554    -0.149122  
  0.0385052    0.00124211   0.167002    -0.0136392   -0.0234698   -0.136122     0.112397    -0.0107166    0.0333847   -0.0477772   -0.140994     0.19472       0.0440048    0.0457733    -0.0144259    -0.0921582    0.0849239   -0.00333595   0.141237     0.0411425   -0.0858416    0.0390463    0.00890802   0.200383     0.124527     0.0971018 kind diag, method split
0: avll = -1.4528831633753778
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.452958
INFO: iteration 2, average log likelihood -1.452887
INFO: iteration 3, average log likelihood -1.452387
INFO: iteration 4, average log likelihood -1.446451
INFO: iteration 5, average log likelihood -1.431662
INFO: iteration 6, average log likelihood -1.424835
INFO: iteration 7, average log likelihood -1.422816
INFO: iteration 8, average log likelihood -1.421321
INFO: iteration 9, average log likelihood -1.419778
INFO: iteration 10, average log likelihood -1.418309
INFO: iteration 11, average log likelihood -1.417019
INFO: iteration 12, average log likelihood -1.416047
INFO: iteration 13, average log likelihood -1.415369
INFO: iteration 14, average log likelihood -1.414948
INFO: iteration 15, average log likelihood -1.414692
INFO: iteration 16, average log likelihood -1.414543
INFO: iteration 17, average log likelihood -1.414439
INFO: iteration 18, average log likelihood -1.414336
INFO: iteration 19, average log likelihood -1.414213
INFO: iteration 20, average log likelihood -1.414050
INFO: iteration 21, average log likelihood -1.413847
INFO: iteration 22, average log likelihood -1.413656
INFO: iteration 23, average log likelihood -1.413524
INFO: iteration 24, average log likelihood -1.413442
INFO: iteration 25, average log likelihood -1.413386
INFO: iteration 26, average log likelihood -1.413347
INFO: iteration 27, average log likelihood -1.413316
INFO: iteration 28, average log likelihood -1.413289
INFO: iteration 29, average log likelihood -1.413262
INFO: iteration 30, average log likelihood -1.413233
INFO: iteration 31, average log likelihood -1.413201
INFO: iteration 32, average log likelihood -1.413165
INFO: iteration 33, average log likelihood -1.413122
INFO: iteration 34, average log likelihood -1.413074
INFO: iteration 35, average log likelihood -1.413024
INFO: iteration 36, average log likelihood -1.412976
INFO: iteration 37, average log likelihood -1.412932
INFO: iteration 38, average log likelihood -1.412894
INFO: iteration 39, average log likelihood -1.412863
INFO: iteration 40, average log likelihood -1.412837
INFO: iteration 41, average log likelihood -1.412815
INFO: iteration 42, average log likelihood -1.412798
INFO: iteration 43, average log likelihood -1.412783
INFO: iteration 44, average log likelihood -1.412770
INFO: iteration 45, average log likelihood -1.412759
INFO: iteration 46, average log likelihood -1.412749
INFO: iteration 47, average log likelihood -1.412740
INFO: iteration 48, average log likelihood -1.412732
INFO: iteration 49, average log likelihood -1.412725
INFO: iteration 50, average log likelihood -1.412718
INFO: EM with 100000 data points 50 iterations avll -1.412718
952.4 data points per parameter
1: avll = [-1.45296,-1.45289,-1.45239,-1.44645,-1.43166,-1.42483,-1.42282,-1.42132,-1.41978,-1.41831,-1.41702,-1.41605,-1.41537,-1.41495,-1.41469,-1.41454,-1.41444,-1.41434,-1.41421,-1.41405,-1.41385,-1.41366,-1.41352,-1.41344,-1.41339,-1.41335,-1.41332,-1.41329,-1.41326,-1.41323,-1.4132,-1.41316,-1.41312,-1.41307,-1.41302,-1.41298,-1.41293,-1.41289,-1.41286,-1.41284,-1.41282,-1.4128,-1.41278,-1.41277,-1.41276,-1.41275,-1.41274,-1.41273,-1.41272,-1.41272]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.412891
INFO: iteration 2, average log likelihood -1.412692
INFO: iteration 3, average log likelihood -1.411725
INFO: iteration 4, average log likelihood -1.403498
INFO: iteration 5, average log likelihood -1.386178
INFO: iteration 6, average log likelihood -1.376729
INFO: iteration 7, average log likelihood -1.373118
INFO: iteration 8, average log likelihood -1.371208
INFO: iteration 9, average log likelihood -1.369835
INFO: iteration 10, average log likelihood -1.368759
INFO: iteration 11, average log likelihood -1.367928
INFO: iteration 12, average log likelihood -1.367297
INFO: iteration 13, average log likelihood -1.366782
INFO: iteration 14, average log likelihood -1.366322
INFO: iteration 15, average log likelihood -1.365894
INFO: iteration 16, average log likelihood -1.365482
INFO: iteration 17, average log likelihood -1.365081
INFO: iteration 18, average log likelihood -1.364669
INFO: iteration 19, average log likelihood -1.364204
INFO: iteration 20, average log likelihood -1.363667
INFO: iteration 21, average log likelihood -1.363089
INFO: iteration 22, average log likelihood -1.362464
INFO: iteration 23, average log likelihood -1.361853
INFO: iteration 24, average log likelihood -1.361377
INFO: iteration 25, average log likelihood -1.361070
INFO: iteration 26, average log likelihood -1.360895
INFO: iteration 27, average log likelihood -1.360795
INFO: iteration 28, average log likelihood -1.360734
INFO: iteration 29, average log likelihood -1.360695
INFO: iteration 30, average log likelihood -1.360668
INFO: iteration 31, average log likelihood -1.360650
INFO: iteration 32, average log likelihood -1.360636
INFO: iteration 33, average log likelihood -1.360626
INFO: iteration 34, average log likelihood -1.360619
INFO: iteration 35, average log likelihood -1.360613
INFO: iteration 36, average log likelihood -1.360608
INFO: iteration 37, average log likelihood -1.360605
INFO: iteration 38, average log likelihood -1.360602
INFO: iteration 39, average log likelihood -1.360600
INFO: iteration 40, average log likelihood -1.360598
INFO: iteration 41, average log likelihood -1.360597
INFO: iteration 42, average log likelihood -1.360596
INFO: iteration 43, average log likelihood -1.360595
INFO: iteration 44, average log likelihood -1.360594
INFO: iteration 45, average log likelihood -1.360594
INFO: iteration 46, average log likelihood -1.360593
INFO: iteration 47, average log likelihood -1.360592
INFO: iteration 48, average log likelihood -1.360592
INFO: iteration 49, average log likelihood -1.360592
INFO: iteration 50, average log likelihood -1.360591
INFO: EM with 100000 data points 50 iterations avll -1.360591
473.9 data points per parameter
2: avll = [-1.41289,-1.41269,-1.41173,-1.4035,-1.38618,-1.37673,-1.37312,-1.37121,-1.36983,-1.36876,-1.36793,-1.3673,-1.36678,-1.36632,-1.36589,-1.36548,-1.36508,-1.36467,-1.3642,-1.36367,-1.36309,-1.36246,-1.36185,-1.36138,-1.36107,-1.3609,-1.36079,-1.36073,-1.36069,-1.36067,-1.36065,-1.36064,-1.36063,-1.36062,-1.36061,-1.36061,-1.36061,-1.3606,-1.3606,-1.3606,-1.3606,-1.3606,-1.36059,-1.36059,-1.36059,-1.36059,-1.36059,-1.36059,-1.36059,-1.36059]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.360807
INFO: iteration 2, average log likelihood -1.360596
INFO: iteration 3, average log likelihood -1.359889
INFO: iteration 4, average log likelihood -1.353887
INFO: iteration 5, average log likelihood -1.337967
INFO: iteration 6, average log likelihood -1.321708
INFO: iteration 7, average log likelihood -1.311312
INFO: iteration 8, average log likelihood -1.306280
INFO: iteration 9, average log likelihood -1.303953
INFO: iteration 10, average log likelihood -1.302652
INFO: iteration 11, average log likelihood -1.301516
INFO: iteration 12, average log likelihood -1.300034
INFO: iteration 13, average log likelihood -1.298068
INFO: iteration 14, average log likelihood -1.296123
INFO: iteration 15, average log likelihood -1.294789
INFO: iteration 16, average log likelihood -1.294253
INFO: iteration 17, average log likelihood -1.294073
INFO: iteration 18, average log likelihood -1.293970
INFO: iteration 19, average log likelihood -1.293898
INFO: iteration 20, average log likelihood -1.293843
INFO: iteration 21, average log likelihood -1.293799
INFO: iteration 22, average log likelihood -1.293759
INFO: iteration 23, average log likelihood -1.293719
INFO: iteration 24, average log likelihood -1.293677
INFO: iteration 25, average log likelihood -1.293634
INFO: iteration 26, average log likelihood -1.293592
INFO: iteration 27, average log likelihood -1.293553
INFO: iteration 28, average log likelihood -1.293518
INFO: iteration 29, average log likelihood -1.293488
INFO: iteration 30, average log likelihood -1.293461
INFO: iteration 31, average log likelihood -1.293438
INFO: iteration 32, average log likelihood -1.293417
INFO: iteration 33, average log likelihood -1.293397
INFO: iteration 34, average log likelihood -1.293379
INFO: iteration 35, average log likelihood -1.293362
INFO: iteration 36, average log likelihood -1.293345
INFO: iteration 37, average log likelihood -1.293329
INFO: iteration 38, average log likelihood -1.293314
INFO: iteration 39, average log likelihood -1.293299
INFO: iteration 40, average log likelihood -1.293284
INFO: iteration 41, average log likelihood -1.293270
INFO: iteration 42, average log likelihood -1.293256
INFO: iteration 43, average log likelihood -1.293243
INFO: iteration 44, average log likelihood -1.293230
INFO: iteration 45, average log likelihood -1.293217
INFO: iteration 46, average log likelihood -1.293204
INFO: iteration 47, average log likelihood -1.293191
INFO: iteration 48, average log likelihood -1.293179
INFO: iteration 49, average log likelihood -1.293167
INFO: iteration 50, average log likelihood -1.293155
INFO: EM with 100000 data points 50 iterations avll -1.293155
236.4 data points per parameter
3: avll = [-1.36081,-1.3606,-1.35989,-1.35389,-1.33797,-1.32171,-1.31131,-1.30628,-1.30395,-1.30265,-1.30152,-1.30003,-1.29807,-1.29612,-1.29479,-1.29425,-1.29407,-1.29397,-1.2939,-1.29384,-1.2938,-1.29376,-1.29372,-1.29368,-1.29363,-1.29359,-1.29355,-1.29352,-1.29349,-1.29346,-1.29344,-1.29342,-1.2934,-1.29338,-1.29336,-1.29335,-1.29333,-1.29331,-1.2933,-1.29328,-1.29327,-1.29326,-1.29324,-1.29323,-1.29322,-1.2932,-1.29319,-1.29318,-1.29317,-1.29315]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.293437
INFO: iteration 2, average log likelihood -1.293074
INFO: iteration 3, average log likelihood -1.290927
WARNING: Variances had to be floored 3 4
INFO: iteration 4, average log likelihood -1.270875
WARNING: Variances had to be floored 3
INFO: iteration 5, average log likelihood -1.251292
WARNING: Variances had to be floored 3 4
INFO: iteration 6, average log likelihood -1.229326
WARNING: Variances had to be floored 3
INFO: iteration 7, average log likelihood -1.220546
WARNING: Variances had to be floored 3 4
INFO: iteration 8, average log likelihood -1.209532
WARNING: Variances had to be floored 3 4
INFO: iteration 9, average log likelihood -1.210523
WARNING: Variances had to be floored 3
INFO: iteration 10, average log likelihood -1.209115
WARNING: Variances had to be floored 3 4
INFO: iteration 11, average log likelihood -1.204509
WARNING: Variances had to be floored 3 4
INFO: iteration 12, average log likelihood -1.208087
WARNING: Variances had to be floored 3
INFO: iteration 13, average log likelihood -1.207860
WARNING: Variances had to be floored 3 4
INFO: iteration 14, average log likelihood -1.203676
WARNING: Variances had to be floored 3
INFO: iteration 15, average log likelihood -1.206858
WARNING: Variances had to be floored 3 4
INFO: iteration 16, average log likelihood -1.201194
WARNING: Variances had to be floored 3
INFO: iteration 17, average log likelihood -1.201986
WARNING: Variances had to be floored 3 4
INFO: iteration 18, average log likelihood -1.194639
WARNING: Variances had to be floored 3
INFO: iteration 19, average log likelihood -1.197773
WARNING: Variances had to be floored 3 4
INFO: iteration 20, average log likelihood -1.193185
WARNING: Variances had to be floored 3
INFO: iteration 21, average log likelihood -1.196890
WARNING: Variances had to be floored 3 4
INFO: iteration 22, average log likelihood -1.192625
WARNING: Variances had to be floored 3
INFO: iteration 23, average log likelihood -1.196628
WARNING: Variances had to be floored 3 4
INFO: iteration 24, average log likelihood -1.192538
WARNING: Variances had to be floored 3
INFO: iteration 25, average log likelihood -1.196601
WARNING: Variances had to be floored 3 4
INFO: iteration 26, average log likelihood -1.192530
WARNING: Variances had to be floored 3
INFO: iteration 27, average log likelihood -1.196598
WARNING: Variances had to be floored 3 4
INFO: iteration 28, average log likelihood -1.192529
WARNING: Variances had to be floored 3
INFO: iteration 29, average log likelihood -1.196598
WARNING: Variances had to be floored 3 4
INFO: iteration 30, average log likelihood -1.192529
WARNING: Variances had to be floored 3
INFO: iteration 31, average log likelihood -1.196598
WARNING: Variances had to be floored 3 4
INFO: iteration 32, average log likelihood -1.192529
WARNING: Variances had to be floored 3
INFO: iteration 33, average log likelihood -1.196598
WARNING: Variances had to be floored 3 4
INFO: iteration 34, average log likelihood -1.192529
WARNING: Variances had to be floored 3
INFO: iteration 35, average log likelihood -1.196598
WARNING: Variances had to be floored 3 4
INFO: iteration 36, average log likelihood -1.192529
WARNING: Variances had to be floored 3
INFO: iteration 37, average log likelihood -1.196598
WARNING: Variances had to be floored 3 4
INFO: iteration 38, average log likelihood -1.192529
WARNING: Variances had to be floored 3
INFO: iteration 39, average log likelihood -1.196598
WARNING: Variances had to be floored 3 4
INFO: iteration 40, average log likelihood -1.192529
WARNING: Variances had to be floored 3
INFO: iteration 41, average log likelihood -1.196598
WARNING: Variances had to be floored 3 4
INFO: iteration 42, average log likelihood -1.192529
WARNING: Variances had to be floored 3
INFO: iteration 43, average log likelihood -1.196598
WARNING: Variances had to be floored 3 4
INFO: iteration 44, average log likelihood -1.192529
WARNING: Variances had to be floored 3
INFO: iteration 45, average log likelihood -1.196598
WARNING: Variances had to be floored 3 4
INFO: iteration 46, average log likelihood -1.192529
WARNING: Variances had to be floored 3
INFO: iteration 47, average log likelihood -1.196598
WARNING: Variances had to be floored 3 4
INFO: iteration 48, average log likelihood -1.192529
WARNING: Variances had to be floored 3
INFO: iteration 49, average log likelihood -1.196598
WARNING: Variances had to be floored 3 4
INFO: iteration 50, average log likelihood -1.192529
INFO: EM with 100000 data points 50 iterations avll -1.192529
118.1 data points per parameter
4: avll = [-1.29344,-1.29307,-1.29093,-1.27088,-1.25129,-1.22933,-1.22055,-1.20953,-1.21052,-1.20911,-1.20451,-1.20809,-1.20786,-1.20368,-1.20686,-1.20119,-1.20199,-1.19464,-1.19777,-1.19318,-1.19689,-1.19263,-1.19663,-1.19254,-1.1966,-1.19253,-1.1966,-1.19253,-1.1966,-1.19253,-1.1966,-1.19253,-1.1966,-1.19253,-1.1966,-1.19253,-1.1966,-1.19253,-1.1966,-1.19253,-1.1966,-1.19253,-1.1966,-1.19253,-1.1966,-1.19253,-1.1966,-1.19253,-1.1966,-1.19253]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 5 6 8
INFO: iteration 1, average log likelihood -1.196964
WARNING: Variances had to be floored 5 6 7
INFO: iteration 2, average log likelihood -1.193255
WARNING: Variances had to be floored 5 6 7
INFO: iteration 3, average log likelihood -1.194714
WARNING: Variances had to be floored 5 6 7 8 25 26
INFO: iteration 4, average log likelihood -1.170632
WARNING: Variances had to be floored 1 3 5 6 7 8
INFO: iteration 5, average log likelihood -1.131227
WARNING: Variances had to be floored 5 6 7 8 24 25 26 30 32
INFO: iteration 6, average log likelihood -1.111990
WARNING: Variances had to be floored 1 3 5 6 7 8
INFO: iteration 7, average log likelihood -1.124907
WARNING: Variances had to be floored 5 6 7 8 24 25 26
INFO: iteration 8, average log likelihood -1.109513
WARNING: Variances had to be floored 1 3 5 6 7 8 30 32
INFO: iteration 9, average log likelihood -1.108222
WARNING: Variances had to be floored 5 6 7 8 24 25 26
INFO: iteration 10, average log likelihood -1.117383
WARNING: Variances had to be floored 1 3 5 6 7 8
INFO: iteration 11, average log likelihood -1.111932
WARNING: Variances had to be floored 5 6 7 8 24 25 26 30
INFO: iteration 12, average log likelihood -1.104498
WARNING: Variances had to be floored 1 3 5 6 7 8
INFO: iteration 13, average log likelihood -1.113387
WARNING: Variances had to be floored 5 6 8 24 25 26
INFO: iteration 14, average log likelihood -1.103163
WARNING: Variances had to be floored 1 3 5 6 7 8 30 31
INFO: iteration 15, average log likelihood -1.097480
WARNING: Variances had to be floored 5 6 7 8 24 25 26
INFO: iteration 16, average log likelihood -1.112821
WARNING: Variances had to be floored 1 3 5 6 7 8
INFO: iteration 17, average log likelihood -1.105940
WARNING: Variances had to be floored 5 6 8 24 25 26 30 31
INFO: iteration 18, average log likelihood -1.095290
WARNING: Variances had to be floored 1 3 5 6 7 8
INFO: iteration 19, average log likelihood -1.112750
WARNING: Variances had to be floored 5 6 7 8 24 25 26
INFO: iteration 20, average log likelihood -1.101489
WARNING: Variances had to be floored 1 3 5 6 7 8 30 31
INFO: iteration 21, average log likelihood -1.099292
WARNING: Variances had to be floored 5 6 8 24 25 26
INFO: iteration 22, average log likelihood -1.112816
WARNING: Variances had to be floored 1 3 5 6 7 8
INFO: iteration 23, average log likelihood -1.101754
WARNING: Variances had to be floored 5 6 7 8 24 25 26 30 31
INFO: iteration 24, average log likelihood -1.095268
WARNING: Variances had to be floored 1 3 5 6 7 8
INFO: iteration 25, average log likelihood -1.116875
WARNING: Variances had to be floored 5 6 8 24 25 26
INFO: iteration 26, average log likelihood -1.101463
WARNING: Variances had to be floored 1 3 5 6 7 8 30 31
INFO: iteration 27, average log likelihood -1.095163
WARNING: Variances had to be floored 5 6 7 8 24 25 26
INFO: iteration 28, average log likelihood -1.112820
WARNING: Variances had to be floored 1 3 5 6 7 8
INFO: iteration 29, average log likelihood -1.105869
WARNING: Variances had to be floored 5 6 8 24 25 26 30 31
INFO: iteration 30, average log likelihood -1.095237
WARNING: Variances had to be floored 1 3 5 6 7 8
INFO: iteration 31, average log likelihood -1.112759
WARNING: Variances had to be floored 5 6 7 8 24 25 26
INFO: iteration 32, average log likelihood -1.101457
WARNING: Variances had to be floored 1 3 5 6 7 8 30 31
INFO: iteration 33, average log likelihood -1.099259
WARNING: Variances had to be floored 5 6 8 24 25 26
INFO: iteration 34, average log likelihood -1.112784
WARNING: Variances had to be floored 1 3 5 6 7 8
INFO: iteration 35, average log likelihood -1.101766
WARNING: Variances had to be floored 5 6 7 8 24 25 26 30 31
INFO: iteration 36, average log likelihood -1.095241
WARNING: Variances had to be floored 1 3 5 6 7 8
INFO: iteration 37, average log likelihood -1.116847
WARNING: Variances had to be floored 5 6 8 24 25 26
INFO: iteration 38, average log likelihood -1.101436
WARNING: Variances had to be floored 1 3 5 6 7 8 30 31
INFO: iteration 39, average log likelihood -1.095178
WARNING: Variances had to be floored 5 6 7 8 24 25 26
INFO: iteration 40, average log likelihood -1.112799
WARNING: Variances had to be floored 1 3 5 6 7 8
INFO: iteration 41, average log likelihood -1.105847
WARNING: Variances had to be floored 5 6 8 24 25 26 30 31
INFO: iteration 42, average log likelihood -1.095217
WARNING: Variances had to be floored 1 3 5 6 7 8
INFO: iteration 43, average log likelihood -1.112772
WARNING: Variances had to be floored 5 6 7 8 24 25 26
INFO: iteration 44, average log likelihood -1.101442
WARNING: Variances had to be floored 1 3 5 6 7 8 30 31
INFO: iteration 45, average log likelihood -1.099242
WARNING: Variances had to be floored 5 6 8 24 25 26
INFO: iteration 46, average log likelihood -1.112769
WARNING: Variances had to be floored 1 3 5 6 7 8
INFO: iteration 47, average log likelihood -1.101775
WARNING: Variances had to be floored 5 6 7 8 24 25 26 30 31
INFO: iteration 48, average log likelihood -1.095230
WARNING: Variances had to be floored 1 3 5 6 7 8
INFO: iteration 49, average log likelihood -1.116834
WARNING: Variances had to be floored 5 6 8 24 25 26
INFO: iteration 50, average log likelihood -1.101425
INFO: EM with 100000 data points 50 iterations avll -1.101425
59.0 data points per parameter
5: avll = [-1.19696,-1.19326,-1.19471,-1.17063,-1.13123,-1.11199,-1.12491,-1.10951,-1.10822,-1.11738,-1.11193,-1.1045,-1.11339,-1.10316,-1.09748,-1.11282,-1.10594,-1.09529,-1.11275,-1.10149,-1.09929,-1.11282,-1.10175,-1.09527,-1.11688,-1.10146,-1.09516,-1.11282,-1.10587,-1.09524,-1.11276,-1.10146,-1.09926,-1.11278,-1.10177,-1.09524,-1.11685,-1.10144,-1.09518,-1.1128,-1.10585,-1.09522,-1.11277,-1.10144,-1.09924,-1.11277,-1.10177,-1.09523,-1.11683,-1.10142]
[-1.45288,-1.45296,-1.45289,-1.45239,-1.44645,-1.43166,-1.42483,-1.42282,-1.42132,-1.41978,-1.41831,-1.41702,-1.41605,-1.41537,-1.41495,-1.41469,-1.41454,-1.41444,-1.41434,-1.41421,-1.41405,-1.41385,-1.41366,-1.41352,-1.41344,-1.41339,-1.41335,-1.41332,-1.41329,-1.41326,-1.41323,-1.4132,-1.41316,-1.41312,-1.41307,-1.41302,-1.41298,-1.41293,-1.41289,-1.41286,-1.41284,-1.41282,-1.4128,-1.41278,-1.41277,-1.41276,-1.41275,-1.41274,-1.41273,-1.41272,-1.41272,-1.41289,-1.41269,-1.41173,-1.4035,-1.38618,-1.37673,-1.37312,-1.37121,-1.36983,-1.36876,-1.36793,-1.3673,-1.36678,-1.36632,-1.36589,-1.36548,-1.36508,-1.36467,-1.3642,-1.36367,-1.36309,-1.36246,-1.36185,-1.36138,-1.36107,-1.3609,-1.36079,-1.36073,-1.36069,-1.36067,-1.36065,-1.36064,-1.36063,-1.36062,-1.36061,-1.36061,-1.36061,-1.3606,-1.3606,-1.3606,-1.3606,-1.3606,-1.36059,-1.36059,-1.36059,-1.36059,-1.36059,-1.36059,-1.36059,-1.36059,-1.36081,-1.3606,-1.35989,-1.35389,-1.33797,-1.32171,-1.31131,-1.30628,-1.30395,-1.30265,-1.30152,-1.30003,-1.29807,-1.29612,-1.29479,-1.29425,-1.29407,-1.29397,-1.2939,-1.29384,-1.2938,-1.29376,-1.29372,-1.29368,-1.29363,-1.29359,-1.29355,-1.29352,-1.29349,-1.29346,-1.29344,-1.29342,-1.2934,-1.29338,-1.29336,-1.29335,-1.29333,-1.29331,-1.2933,-1.29328,-1.29327,-1.29326,-1.29324,-1.29323,-1.29322,-1.2932,-1.29319,-1.29318,-1.29317,-1.29315,-1.29344,-1.29307,-1.29093,-1.27088,-1.25129,-1.22933,-1.22055,-1.20953,-1.21052,-1.20911,-1.20451,-1.20809,-1.20786,-1.20368,-1.20686,-1.20119,-1.20199,-1.19464,-1.19777,-1.19318,-1.19689,-1.19263,-1.19663,-1.19254,-1.1966,-1.19253,-1.1966,-1.19253,-1.1966,-1.19253,-1.1966,-1.19253,-1.1966,-1.19253,-1.1966,-1.19253,-1.1966,-1.19253,-1.1966,-1.19253,-1.1966,-1.19253,-1.1966,-1.19253,-1.1966,-1.19253,-1.1966,-1.19253,-1.1966,-1.19253,-1.19696,-1.19326,-1.19471,-1.17063,-1.13123,-1.11199,-1.12491,-1.10951,-1.10822,-1.11738,-1.11193,-1.1045,-1.11339,-1.10316,-1.09748,-1.11282,-1.10594,-1.09529,-1.11275,-1.10149,-1.09929,-1.11282,-1.10175,-1.09527,-1.11688,-1.10146,-1.09516,-1.11282,-1.10587,-1.09524,-1.11276,-1.10146,-1.09926,-1.11278,-1.10177,-1.09524,-1.11685,-1.10144,-1.09518,-1.1128,-1.10585,-1.09522,-1.11277,-1.10144,-1.09924,-1.11277,-1.10177,-1.09523,-1.11683,-1.10142]
32×26 Array{Float64,2}:
 -0.015369    -0.0703163    0.0928282    0.0538394    0.097781    0.0521641    0.0241343   -0.0952327    0.0610422    0.145338     0.181523     0.00624419  -0.131452      0.0297171   0.147633     0.0756556   -0.0685872    0.0428138   -0.151086     0.12181     -0.00020479   0.101987    -0.0454385    -0.00909058   0.236202     -0.0638655 
  0.211652    -0.0156315   -0.0295623   -0.0763022    0.0156949  -0.186978    -0.0102825    0.00569594  -0.00498424   0.218       -0.0220131   -0.0987703   -0.0366881    -0.132735   -0.0957363   -0.0289253    0.158574     0.095336     0.0273618    0.0893856   -0.187615    -0.0944467   -0.000229255   0.00189151  -0.120187      0.0575116 
  0.0415859    0.00496978   0.198168    -0.0106262   -0.0474319  -0.132505     0.0970381   -0.00613575   0.0334149   -0.0378942   -0.140693     0.194579     0.0437972     0.0448893  -0.00807795  -0.114768     0.0742522   -0.0328215    0.129153     0.0386191   -0.0665311    0.0503317    0.0534428     0.246797     0.138445      0.0838043 
  0.00602782   0.0265378   -0.093103     0.00979134   0.0626534   0.221281     0.0159059   -0.145875     0.0134081    0.0951974   -0.123868     0.15988     -0.0344173    -0.0638372  -0.00210477   0.0300383    0.0449164   -0.0732565    0.179122     0.0705378    0.024795     0.0598861    0.051186      0.223264     0.000117276   0.102764  
  0.0367936    0.222166     0.183696     0.307425    -0.0988689   0.556749     0.113675     0.0809785   -0.0843683    0.113705     0.979604    -0.0600383   -0.0311493     0.269816   -0.0338096   -0.207374    -0.146264     0.106198     0.0719722    0.061907    -0.149938    -0.223068    -0.201932     -0.0972276    0.0349956    -0.112741  
  0.0334079    0.190236     0.172944    -0.507782    -0.0922958  -1.05239     -0.113281     0.0437423   -0.0849112    0.114574     0.0603009    0.379413    -0.0346086     0.206093   -0.0453892   -0.204459     0.169669     0.101857    -0.00820405   0.0820605   -0.0662393    0.06942     -0.186465     -0.0907988    0.0370118    -0.0932395 
 -0.0342932    0.0765532    0.0261932    0.0880568    0.0599678   0.22054      0.0479017   -0.234828    -0.0237781    0.0509168   -0.0379742   -0.0185474    0.0773186    -0.259281   -0.180003    -0.063468     0.00967289   0.079539     0.0712227    0.0368334   -0.0704443    0.0945744   -0.104371     -0.17653     -0.0451486     0.128699  
  0.00888357   0.111591     0.20518      0.376484    -0.106559   -0.0374422    0.00615843   0.0864933   -0.0835217    0.119411    -1.08262     -0.258009    -0.0338545     0.112106   -0.0428711   -0.216031     0.239859     0.114578    -0.0404944   -0.0231908   -0.12502      0.513294    -0.192854     -0.0775388    0.0345638    -0.121585  
 -3.27353e-5  -0.123606    -0.0425747    0.0227274    0.0894094   0.113607     0.0419022   -0.0993881    0.071632    -0.133876     0.011243    -0.00371179   0.0210413     0.0879419  -0.142465    -0.0317463   -0.00147932  -0.00226064   0.0237763    0.00703147  -0.144421     0.0065391   -0.0438274     0.0687518    0.069627      0.0579271 
  0.00705462   0.114004     0.00030641   0.0245953    0.0847128   0.117827    -0.0836014   -0.00283658   0.0341723   -0.0324279    0.0585949   -0.0080582    0.0766819     0.0201402   0.0616881    0.0482297    0.0543792   -0.0510145   -0.0277441    0.0296079   -0.0565575    0.037195    -0.0665612    -0.0181413    0.02095      -0.120796  
 -0.0107614   -0.0332654   -5.58147e-5   0.0834501    0.13608    -0.132552    -0.00196915  -0.00180114  -0.111082     0.00339483  -0.0506104    0.0412116   -0.00621117   -0.0345536  -0.0849521   -0.0410049   -0.0666767   -0.0476094   -0.0995518    0.0102771   -0.032093     0.0429103    0.112075     -0.0361132    0.103802     -0.0493532 
  0.00673175  -0.00765428   0.0792254    0.0569066    0.0116779   0.0877791    0.0633603   -0.115439     0.0770075    0.00330549   0.0220513   -0.0329261    0.150737     -0.0829809   0.0117992    0.132996     0.0185416   -0.0457346    0.0436386    0.0625094    0.0979192    0.0656953   -0.22954       0.0507084    0.0461504    -0.0979496 
 -0.00907753   0.0407977    0.0288      -0.0338453    0.118413    0.137925    -0.166962     0.057966    -0.946211    -0.0648692    0.00579494   0.12999     -0.162962     -0.10967    -0.0339022    0.0207914    0.0411963    0.185127     0.0335314   -0.204041    -0.00581823   0.154142     0.109846      0.0393576    0.0121419    -0.0809348 
 -0.0156306   -0.0328453    0.158794    -0.197982     0.118929    0.153739    -0.0894702    0.0795681    1.0469      -0.0348819    0.0140558    0.124251    -0.0652149    -0.160136   -0.0346379    0.0884317    0.0452349    0.292545    -0.0104423   -0.171494    -0.00303956   0.138093    -0.0988287     0.0514976    0.0359605    -0.0800659 
  0.132796    -0.390199    -0.0166524    0.086944     0.141985   -0.0580068   -0.0284106    0.0784398   -0.222805     0.0767317    0.146779    -0.00331119   0.0321147     0.0917074   0.00813891   0.0428318    0.0742646    0.015751     0.0810476   -0.031736    -0.00931902  -0.00423951   0.0522968    -0.0108719   -0.180274     -0.0423006 
  0.173859    -0.0884355    0.103136     0.126556     0.0822513  -0.0479262   -0.0512383    0.0833619    0.339762    -0.12989      0.29733     -0.0447956    0.0617307     0.0148551   0.00331644   0.0932547    0.096443    -0.0015506    0.16557     -0.00648167  -0.0723116    0.0962778    0.0492736     0.0107507   -0.0860215    -0.172591  
 -0.0172395    0.180623     0.131029    -0.0947889   -0.0208461  -0.0264468    0.109628    -0.184628     0.0139043    0.0297795    0.0995214   -0.161673    -0.000194676  -0.102519    0.20325     -0.00807831   0.100567     0.0054442   -0.0634675   -0.0115227    0.0418908   -0.879846    -0.269672     -0.0934572   -0.0375538     0.319753  
  0.284554     0.163292     0.206059    -0.00236673  -0.0280934  -0.0163284   -0.0104804    0.0636524    0.0151466    0.14777      0.103243    -0.159055     0.075293     -0.0544391  -0.0766762   -0.0124359    0.139174     0.0798492   -0.0624611   -0.0633765    0.0172911    0.345048    -0.292325     -0.118294     0.271204     -0.186069  
  0.0544788    0.0581213    0.0139925    0.0138441    0.013741   -0.0183202    0.084834    -0.00649584  -0.0410821   -0.0411524   -0.0254735    0.00561251   0.179261     -0.126176   -0.0215907   -0.0230209    0.0441759    0.0541103    0.0640537   -0.00916185   0.0576939   -0.0293197   -0.0520251    -0.029887     0.0202367     0.038462  
  0.0302206    0.101606     0.126737    -0.00512286  -0.110213    0.083596     0.0966665    0.0112598   -0.0434346    0.0439124    0.218686     0.075865     0.0361422    -0.0454902  -0.0796427    0.100835    -0.00173611   0.0447106    0.0622799    0.00652767  -0.112786    -0.0836572   -0.0850277    -0.0270332   -0.128255     -0.0230537 
  0.131636    -0.169454    -0.0988458   -0.704334     0.0535276   0.113329    -0.0556643    0.139757    -0.137568     0.113344     0.0439237    0.00582715   0.0787303    -0.0608822   0.0786275   -0.143872     0.0601032   -0.187943     0.149092     0.0864621    0.1163       0.0497324    0.0859921    -0.0826069   -0.00479711   -0.0493889 
  0.00226204  -0.176399    -0.148552     0.223155     0.0573965   0.0571401   -0.0248013    0.0704385   -0.144994     0.116301    -0.164453    -0.0119439    0.00155559    0.121197    0.0497181   -0.150408     0.053757    -0.0583025    0.307788    -0.040724    -0.0137289    0.0249179    0.112408     -0.0937472   -0.0279083    -0.195519  
 -0.0884777    0.0124421    0.011709     0.0266753    0.0670246   0.0117736   -0.0488939   -0.107575    -0.0779051   -0.0881725   -0.0461614   -0.184505     0.110992     -0.242713    0.130291     0.0611509    0.0728278    0.0963349    0.128251    -0.052555    -0.147555    -0.00937507  -0.0578048    -0.0520625   -0.136825      0.0437032 
 -0.143614     0.281799     0.0507403   -0.0417272   -0.0110598   0.0377548    0.0905819   -0.0447074   -0.0589322   -0.0821512    0.102158     0.0117414   -0.127075      0.0919674   0.179251    -0.147215     0.0473541   -0.0146248   -0.0866499   -0.116298     0.0844219   -0.0662881   -0.0188522     0.140173     0.0243608    -0.0460262 
  0.0564521    0.128008     0.147314    -0.0439626   -0.0448541  -0.0952362    0.0834311    0.0957151    0.144826     0.0980197    0.171141    -0.00527347  -0.0592162     0.104988   -0.0825853   -0.100767     0.0941102   -0.0914321   -0.17543      0.0350489    0.0557626    0.0788579   -0.0683616     0.0384267    0.193098      0.0500902 
 -0.125042     0.210862     0.0153892   -0.0130481   -0.0321217   0.00197567  -0.204748    -0.258369    -0.0631064   -0.0786902    0.151509     0.0632282   -0.11335      -0.123435    0.0113958    0.062449     0.103007    -0.0311093   -0.0289949    0.0255088   -0.045425     0.0887858    0.131489      0.0147754    0.0838771     0.00203568
  0.043093     0.109252     0.0362568    0.0763863   -0.0801797   0.0598589    0.00532223  -0.0940401    0.100902     0.063463     0.0438479   -0.0720527    0.0661104    -0.279762    0.00558323  -0.15907      0.0993021   -0.0784857    0.00721885  -0.0123809   -0.0808478    0.0532555   -0.22524       0.0745329   -0.124379     -0.108784  
  0.0338561    0.0395471   -0.0238234    0.103672    -0.0952273   0.069974    -0.00341269  -0.170252     0.0594202    0.056542     0.0730141   -0.12159      0.0378252    -0.0987444  -0.00464194  -0.156942     0.095947     0.00919217   0.0293131    0.0610144   -0.0800157   -0.0321181   -0.220985      0.0617086   -0.10497       0.757645  
  0.00883669  -0.0491348    0.0421478   -0.0743711   -0.0755999   0.0906633    0.121232    -0.0441563    0.0467047   -0.0675053   -0.0460941   -0.0566096   -0.0359449     0.155262    0.0336655    0.27017      0.142181     0.10516     -0.195387    -0.0189382   -0.0414306    0.0979361   -0.00878787    0.105771     0.00811733   -0.157661  
  0.0789204    0.0417563    0.00403401  -0.203054     0.0609857   0.0595616   -0.0659591   -0.104965    -0.0229579   -0.0305747   -0.0737978   -0.0251429    0.0195265     0.0835004  -0.0189918   -0.094062    -0.0568336    0.094011     0.0471094    0.00694196  -0.0723512   -0.00958871   0.00193404   -0.0822928   -0.088601     -0.0880745 
  0.0203108   -0.167309    -0.0811007   -0.0313753   -0.0405649  -0.0420194    0.0513924   -0.00946982   0.128586     0.0481788    0.0863071   -0.00400864   0.199168     -0.188959    0.0611338   -0.0415494    0.0756918    0.114888     0.0586491    0.237512    -0.0227552    0.0276922    0.0219189     0.251261    -0.0154408     0.0808074 
 -0.0259474   -0.0306032   -0.0610859    0.0522038   -0.0104133  -0.0855155    0.00426225   0.130773     0.0800593   -0.017756     0.154712    -0.0713027   -0.0557778    -0.0707113  -0.0782       0.133138     0.058896     0.00912242   0.053557     0.0460926    0.124173     0.023329    -0.050068      0.0587443    0.0511213    -0.0372992 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 3 5 6 7 8 30 31
INFO: iteration 1, average log likelihood -1.095183
WARNING: Variances had to be floored 1 3 5 6 7 8 24 25 26 30 31
INFO: iteration 2, average log likelihood -1.075185
WARNING: Variances had to be floored 1 3 5 6 7 8 30 31
INFO: iteration 3, average log likelihood -1.095113
WARNING: Variances had to be floored 1 3 5 6 7 8 24 25 26 30 31
INFO: iteration 4, average log likelihood -1.075172
WARNING: Variances had to be floored 1 3 5 6 7 8 30 31
INFO: iteration 5, average log likelihood -1.095112
WARNING: Variances had to be floored 1 3 5 6 7 8 24 25 26 30 31
INFO: iteration 6, average log likelihood -1.075171
WARNING: Variances had to be floored 1 3 5 6 7 8 30 31
INFO: iteration 7, average log likelihood -1.095111
WARNING: Variances had to be floored 1 3 5 6 7 8 24 25 26 30 31
INFO: iteration 8, average log likelihood -1.075170
WARNING: Variances had to be floored 1 3 5 6 7 8 30 31
INFO: iteration 9, average log likelihood -1.095110
WARNING: Variances had to be floored 1 3 5 6 7 8 24 25 26 30 31
INFO: iteration 10, average log likelihood -1.075169
INFO: EM with 100000 data points 10 iterations avll -1.075169
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.418380e+05
      1       7.379496e+05      -2.038885e+05 |       32
      2       7.077345e+05      -3.021503e+04 |       32
      3       6.908514e+05      -1.688314e+04 |       32
      4       6.780921e+05      -1.275934e+04 |       32
      5       6.696132e+05      -8.478824e+03 |       32
      6       6.644876e+05      -5.125678e+03 |       32
      7       6.613433e+05      -3.144249e+03 |       32
      8       6.594836e+05      -1.859745e+03 |       32
      9       6.582131e+05      -1.270460e+03 |       32
     10       6.572574e+05      -9.556905e+02 |       32
     11       6.566619e+05      -5.955133e+02 |       32
     12       6.563594e+05      -3.024777e+02 |       32
     13       6.561738e+05      -1.856546e+02 |       32
     14       6.560532e+05      -1.205824e+02 |       32
     15       6.559674e+05      -8.582492e+01 |       32
     16       6.558837e+05      -8.365919e+01 |       32
     17       6.558066e+05      -7.711957e+01 |       32
     18       6.557110e+05      -9.555827e+01 |       32
     19       6.556064e+05      -1.046690e+02 |       32
     20       6.555178e+05      -8.852446e+01 |       31
     21       6.554436e+05      -7.422473e+01 |       32
     22       6.553797e+05      -6.387171e+01 |       32
     23       6.553182e+05      -6.152006e+01 |       32
     24       6.552664e+05      -5.176898e+01 |       32
     25       6.552146e+05      -5.179391e+01 |       32
     26       6.551725e+05      -4.212767e+01 |       32
     27       6.551314e+05      -4.114916e+01 |       31
     28       6.550853e+05      -4.609292e+01 |       31
     29       6.550385e+05      -4.677977e+01 |       32
     30       6.550014e+05      -3.713720e+01 |       31
     31       6.549777e+05      -2.365490e+01 |       29
     32       6.549635e+05      -1.421315e+01 |       30
     33       6.549531e+05      -1.040776e+01 |       26
     34       6.549457e+05      -7.373822e+00 |       24
     35       6.549401e+05      -5.599540e+00 |       24
     36       6.549362e+05      -3.955663e+00 |       24
     37       6.549333e+05      -2.814844e+00 |       22
     38       6.549313e+05      -2.086646e+00 |       16
     39       6.549302e+05      -1.051986e+00 |       12
     40       6.549298e+05      -4.410521e-01 |        9
     41       6.549292e+05      -5.952933e-01 |       10
     42       6.549286e+05      -5.792265e-01 |       13
     43       6.549278e+05      -7.453793e-01 |       11
     44       6.549270e+05      -8.134376e-01 |       12
     45       6.549265e+05      -5.455624e-01 |       10
     46       6.549262e+05      -2.431516e-01 |        5
     47       6.549261e+05      -1.543154e-01 |        8
     48       6.549259e+05      -2.226494e-01 |        6
     49       6.549258e+05      -1.030092e-01 |        2
     50       6.549257e+05      -5.860961e-02 |        4
K-means terminated without convergence after 50 iterations (objv = 654925.7016869133)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.352311
INFO: iteration 2, average log likelihood -1.319080
INFO: iteration 3, average log likelihood -1.285682
INFO: iteration 4, average log likelihood -1.248898
INFO: iteration 5, average log likelihood -1.208884
WARNING: Variances had to be floored 5 7 18
INFO: iteration 6, average log likelihood -1.157724
WARNING: Variances had to be floored 12 17
INFO: iteration 7, average log likelihood -1.137428
WARNING: Variances had to be floored 1 15 20 29
INFO: iteration 8, average log likelihood -1.093904
WARNING: Variances had to be floored 3 16 27
INFO: iteration 9, average log likelihood -1.099537
WARNING: Variances had to be floored 7 17 18 23 24
INFO: iteration 10, average log likelihood -1.097758
WARNING: Variances had to be floored 5
INFO: iteration 11, average log likelihood -1.120383
WARNING: Variances had to be floored 3 12 27 29
INFO: iteration 12, average log likelihood -1.070310
WARNING: Variances had to be floored 1 7 15 16 18 20
INFO: iteration 13, average log likelihood -1.082037
WARNING: Variances had to be floored 5 17 23
INFO: iteration 14, average log likelihood -1.120239
WARNING: Variances had to be floored 24
INFO: iteration 15, average log likelihood -1.106436
WARNING: Variances had to be floored 3 7 12 18 27 28 29
INFO: iteration 16, average log likelihood -1.056221
WARNING: Variances had to be floored 1 5 16 17 20
INFO: iteration 17, average log likelihood -1.103708
WARNING: Variances had to be floored 23
INFO: iteration 18, average log likelihood -1.135479
WARNING: Variances had to be floored 7 15 24
INFO: iteration 19, average log likelihood -1.083788
WARNING: Variances had to be floored 3 12 18 27 29
INFO: iteration 20, average log likelihood -1.058061
WARNING: Variances had to be floored 1 5 16 17 20 23
INFO: iteration 21, average log likelihood -1.083583
WARNING: Variances had to be floored 7 28
INFO: iteration 22, average log likelihood -1.132825
WARNING: Variances had to be floored 15 18 24
INFO: iteration 23, average log likelihood -1.089797
WARNING: Variances had to be floored 3 17 27
INFO: iteration 24, average log likelihood -1.075083
WARNING: Variances had to be floored 1 5 12 16 20
INFO: iteration 25, average log likelihood -1.074979
WARNING: Variances had to be floored 7 18 23 28
INFO: iteration 26, average log likelihood -1.107073
WARNING: Variances had to be floored 3 17 24 27 29
INFO: iteration 27, average log likelihood -1.100011
WARNING: Variances had to be floored 5 15 16
INFO: iteration 28, average log likelihood -1.111251
WARNING: Variances had to be floored 1 12 18 20
INFO: iteration 29, average log likelihood -1.090704
WARNING: Variances had to be floored 3 7 28
INFO: iteration 30, average log likelihood -1.094694
WARNING: Variances had to be floored 5 16 17 23 24 27
INFO: iteration 31, average log likelihood -1.082316
WARNING: Variances had to be floored 15 18
INFO: iteration 32, average log likelihood -1.128294
WARNING: Variances had to be floored 3 7 12 29
INFO: iteration 33, average log likelihood -1.084877
WARNING: Variances had to be floored 1 20 28
INFO: iteration 34, average log likelihood -1.081909
WARNING: Variances had to be floored 5 18 23 27
INFO: iteration 35, average log likelihood -1.079529
WARNING: Variances had to be floored 15 16 17
INFO: iteration 36, average log likelihood -1.095117
WARNING: Variances had to be floored 3 7 28
INFO: iteration 37, average log likelihood -1.088780
WARNING: Variances had to be floored 12 18 20 24 27
INFO: iteration 38, average log likelihood -1.075733
WARNING: Variances had to be floored 1 5 23 29
INFO: iteration 39, average log likelihood -1.091992
WARNING: Variances had to be floored 3 7 15 17
INFO: iteration 40, average log likelihood -1.099757
WARNING: Variances had to be floored 16 18 28
INFO: iteration 41, average log likelihood -1.107940
WARNING: Variances had to be floored 5 27
INFO: iteration 42, average log likelihood -1.093373
WARNING: Variances had to be floored 1 20 24
INFO: iteration 43, average log likelihood -1.068060
WARNING: Variances had to be floored 3 7 15 16 17 18 23 28
INFO: iteration 44, average log likelihood -1.061603
WARNING: Variances had to be floored 5 27 29
INFO: iteration 45, average log likelihood -1.145571
WARNING: Variances had to be floored 12
INFO: iteration 46, average log likelihood -1.112074
WARNING: Variances had to be floored 1 3 18 20 24
INFO: iteration 47, average log likelihood -1.061865
WARNING: Variances had to be floored 5 7 15 17
INFO: iteration 48, average log likelihood -1.093260
WARNING: Variances had to be floored 16 27 28 29
INFO: iteration 49, average log likelihood -1.109913
WARNING: Variances had to be floored 3 12 18 23
INFO: iteration 50, average log likelihood -1.079801
INFO: EM with 100000 data points 50 iterations avll -1.079801
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.022929    -0.1655      -0.0789156   -0.0314919   -0.0454621   -0.0388271     0.0544593    -0.0047749    0.130198     0.0472472    0.0871538   -0.00480878   0.197574   -0.18294      0.0643925    -0.0380989   0.0785721   0.114214     0.0603379    0.231859    -0.025955     0.0306169    0.0217785     0.25571     -0.0138442    0.080816  
 -0.0481765    0.1475       0.0109519    0.0386809   -0.0625601    0.0323996    -0.111301     -0.203496     0.00884843  -0.0119342    0.106969    -0.0156776   -0.0343929  -0.155104     0.00540753   -0.0359117   0.0994196  -0.0322713   -0.00729159   0.0261297   -0.0619685    0.0528039   -0.0393452     0.0398796   -0.0120941    0.171523  
 -0.143453     0.280303     0.0520962   -0.0453702   -0.0118832    0.0385408     0.091735     -0.0424769   -0.0565289   -0.0830396    0.103643     0.0149202   -0.130656    0.0905409    0.184149     -0.14745     0.0395133  -0.0147127   -0.0857245   -0.118351     0.0854665   -0.0674954   -0.0193778     0.139853     0.022966    -0.0442159 
  0.0294259   -0.0826033    0.0163158   -0.0299672    0.00955142   0.319966     -0.0207116    -0.0634637    0.111648    -0.133962     0.0094134    0.0415296    0.111433    0.0577369   -0.0806857    -0.0946198   0.0894007  -0.0011819   -0.00667347  -0.0300471   -0.27225      0.00282034  -0.13048       0.0653467    0.00549739   0.0323306 
 -0.0180162   -0.0631035    0.0927671    0.0478272    0.0736971    0.0358475     0.030234     -0.0936193    0.059873     0.124697     0.179599     0.0222098   -0.109431    0.0308125    0.137873      0.0906887  -0.056062    0.0372489   -0.111489     0.108862     0.00111177   0.0986717   -0.0266113     0.032773     0.240032    -0.0485564 
  0.050711    -0.00246448   0.0213661   -0.153864    -0.00993236   0.0789462     0.0216251    -0.07846      0.0118932   -0.0475533   -0.0572716   -0.0407112   -0.0116224   0.113594     0.0156906     0.0832814   0.038871    0.0987798   -0.0578992   -0.00410347  -0.0585709    0.0412994   -0.0031151     0.0134661   -0.0401463   -0.115753  
 -0.00947753   0.213031     0.242769     0.0295456   -0.128308     0.0456724     0.0850504    -0.0589283   -0.111551     0.0157313    0.185711     0.145625     0.0252677  -0.0124062   -0.0363998     0.105462    0.0277998   0.191405     0.00442736  -0.0793924   -0.138384    -0.146761    -0.0990456     0.00261513  -0.220315    -0.0434099 
  0.154242    -0.242359     0.0475179    0.108929     0.111971    -0.052495     -0.0404559     0.0812391    0.0624523   -0.0261825    0.226249    -0.0251862    0.0478973   0.0506883    0.00555033    0.0666946   0.0881285   0.00759314   0.12682     -0.0161358   -0.0419926    0.0457791    0.0518118    -0.00104927  -0.133252    -0.110252  
 -0.105277     0.244106     0.215485    -0.0672014    0.0376636    0.0624389    -0.0779982     0.00722047   0.0349796    0.0488332   -0.00119431   0.0863912    0.0347924   0.198593     0.0954889     0.128279   -0.0886572  -0.217506     0.00608973   0.0499195   -0.00846416  -0.0702148    0.0602853     0.0115135    0.120214    -0.140573  
  0.0320239    0.0490386   -0.0328522    0.0865395    0.109747    -0.0235826     0.0430782     0.194467     0.0306603   -0.0068842    0.103568    -0.0682742   -0.160578    0.00827781  -0.196704      0.214754    0.0117911   0.0124726    0.0557285    0.0436013    0.183035    -0.0367768   -0.126329     -0.0262392   -0.0668983    0.0147155 
  0.117766    -0.0209913   -0.195972     0.12035      0.131816     0.147227     -0.083216     -0.00347503   0.0308932   -0.11183      0.109587    -0.103374     0.127286   -0.155869     0.0168433    -0.0203978   0.209578    0.113946    -0.0688696    0.00889121  -0.101676     0.149145    -0.174841     -0.0496817   -0.0708993   -0.0975809 
  0.0265296    0.176339     0.171935     0.0275748   -0.101372    -0.16254       0.000719982   0.0374738   -0.0867111    0.115451     0.065103     0.0493738   -0.0291247   0.161236    -0.0462014    -0.204861    0.0735252   0.100868     0.0155912    0.0380811   -0.109427     0.0828176   -0.192167     -0.0979688    0.0330493   -0.0926149 
 -0.0122808    0.0062028    0.0903908   -0.111786     0.1188       0.145505     -0.130321      0.06762     -0.00276876  -0.0504185    0.0101793    0.127331    -0.117663   -0.133278    -0.0342213     0.0527641   0.0435013   0.236283     0.0123268   -0.188844    -0.00455265   0.145739     0.0109736     0.0449834    0.0235103   -0.0805476 
  0.13085      0.17222      0.167366    -0.048915    -0.0247434   -0.0214621     0.0522688    -0.0617575    0.0143819    0.0866361    0.10138     -0.161679     0.0366378  -0.0789174    0.0673093    -0.0103348   0.120022    0.0407289   -0.0628879   -0.0384832    0.0294757   -0.284593    -0.280995     -0.105989     0.114125     0.0751311 
 -0.0380799    0.0714641    0.0346453    0.0859574    0.0922283    0.21358       0.0503853    -0.227128    -0.0130342    0.0360958   -0.0455092   -0.0164316    0.0820869  -0.234774    -0.184281     -0.072365    0.017696    0.0862122    0.0877646    0.0542611   -0.074921     0.104925    -0.107366     -0.190537    -0.0472794    0.129292  
  0.0609917   -0.00969576   0.187738    -0.00921743  -0.0378819   -0.12201       0.0931338     0.0177876    0.0377281   -0.0437159   -0.17773      0.216211     0.0415916   0.0338063    0.000282883  -0.16136     0.0740085  -0.046755     0.136024     0.0368595   -0.064575     0.0435282    0.0631327     0.287182     0.12717      0.0664398 
  0.0954797    0.135362     0.205412    -0.057804    -0.0540093   -0.0160111     0.0382693    -0.00253594   0.0942999   -0.0390654   -0.00896005   0.0314172    0.347963   -0.237178    -0.0176574     0.0739523   0.0261409  -0.03455      0.00984112  -0.125435     0.0783868    0.123209    -0.0231871    -0.00655106   0.0697477    0.00395297
  0.0542475    0.130256     0.159033    -0.0472732   -0.0446277   -0.0999988     0.0874124     0.0920163    0.150329     0.104177     0.171561    -0.00635028  -0.0591935   0.109223    -0.0776663    -0.100777    0.0970222  -0.0994824   -0.174907     0.0346704    0.0556134    0.0773451   -0.0706179     0.0389295    0.193586     0.0473047 
  0.00441393  -0.00696774   0.0753817    0.0549743    0.00664575   0.0865254     0.0628975    -0.110976     0.076866     0.00676507   0.0191242   -0.0301382    0.156229   -0.0814825   -0.0112655     0.135453    0.0231609  -0.0457083    0.0399484    0.0616176    0.0875458    0.0651105   -0.228249      0.0478382    0.0465885   -0.0967119 
 -0.0852379   -0.113936    -0.0910204    0.0113823   -0.134858    -0.144498     -0.0324977     0.0560604    0.137361    -0.032581     0.206927    -0.0737976    0.0554562  -0.155745     0.0500326     0.0318016   0.105502    0.00231914   0.0465572    0.0489003    0.0559727    0.0853878    0.0248857     0.161843     0.169436    -0.100522  
  0.212043    -0.0148732   -0.030846    -0.0734463    0.015181    -0.188269     -0.00990649    0.00809421  -0.00456008   0.214464    -0.0220239   -0.09682     -0.0361167  -0.130893    -0.095214     -0.0258123   0.157835    0.0970167    0.0312583    0.0900281   -0.187864    -0.0897464    0.000945144   0.00491786  -0.118208     0.0566277 
 -0.163634    -0.0660869    0.0769195   -0.0118203    0.0485866   -0.273422      0.0961685     0.0203855   -0.153116     0.0110145   -0.0462224    0.0369901    0.0295232  -0.101933    -0.0106283    -0.0508833  -0.0471191   0.0370878   -0.138515     0.00130626  -0.0996303   -0.0625692    0.0553534    -0.00904877  -0.00458779  -0.200075  
 -0.0343116   -0.160947    -0.0900332    0.0843921    0.169445    -0.0943516     0.115222     -0.14387      0.0327492   -0.133175     0.00835037  -0.0550934   -0.0800104   0.115065    -0.201848      0.0305485  -0.0959864  -0.00118845   0.038995     0.0438008   -0.00971748   0.0112126    0.0396932     0.0802137    0.132307     0.0812711 
  0.076787     0.08502     -0.121842     0.0315408   -0.0112677    0.000109405   0.190163     -0.039327    -0.103259    -0.277562    -0.0365912    0.135746     0.131678   -0.078627     0.0470045    -0.067847    0.0151088   0.0813283    0.138306     0.0623522    0.045733    -0.107512    -0.0754026    -0.224032     0.0809166    0.11933   
  0.00474831  -0.035453    -0.0241512    0.055333     0.0819481   -0.0189462     0.00529302    0.0277629   -0.0877688    0.176529    -0.0232789   -0.133682     0.0834809  -0.0918955   -0.110713     -0.046902    0.0848753   0.0915377    0.0342093    0.0313991    0.0460018   -0.0737331   -0.0682322     0.107475    -0.0818681   -0.00652356
 -0.0893259    0.0125143    0.00902434   0.0278027    0.0672342    0.010008     -0.0485904    -0.105991    -0.0773039   -0.0872972   -0.0439854   -0.185115     0.1119     -0.242707     0.131349      0.0614966   0.0730876   0.0971655    0.127579    -0.054467    -0.147235    -0.0096899   -0.0584288    -0.0520947   -0.141289     0.0454412 
  0.0519686    0.256706     0.179411     0.110491    -0.156986    -0.0974695    -0.0395068     0.0979418   -0.0682553    0.0995975   -0.0031407   -0.0679555    0.0185941  -0.0280477   -0.0199869    -0.177725    0.0313322   0.0105007    0.0466134   -0.137706    -0.142083     0.0679516   -0.148604     -0.0954533    0.00703574  -0.00874914
  0.0711025   -0.135454    -0.0863257   -0.0541071   -0.0932293    0.142907      0.143546      0.084528     0.0678102    0.0770767    0.252761    -0.0577039    0.0462859  -0.0801826   -0.171307      0.14704    -0.0286936  -0.137779     0.106524     0.132113    -0.0569303    0.00320421  -0.0549258    -0.0692341   -0.0370417    0.00502922
  0.0907069    0.141112     0.0701998   -0.0777109   -0.0646241    0.137849      0.104563      0.0267487   -0.0638034    0.117165     0.241747     0.0489926    0.0342484  -0.0660231   -0.0322215    -0.06162    -0.0310367  -0.0887036    0.108913     0.107333    -0.146632     0.0188116   -0.0893567    -0.103376     0.00577336   0.00568738
  0.0126273    0.0266316   -0.0820066    0.00793131   0.0550106    0.200694      0.0238311    -0.139854     0.0149826    0.0920652   -0.123943     0.159281    -0.0307059  -0.0576511   -0.00574938    0.0187508   0.0458342  -0.0723222    0.173456     0.0682695    0.0204154    0.0581064    0.0543683     0.225514     0.00426404   0.104974  
  0.0671325   -0.175698    -0.125779    -0.23435      0.0560776    0.0876598    -0.0386603     0.107739    -0.143028     0.116393    -0.0656153   -0.0053634    0.0401391   0.032627     0.0670993    -0.149645    0.0534769  -0.126678     0.233175     0.0204004    0.0483413    0.0341082    0.099929     -0.0879775   -0.0166496   -0.121401  
  0.137373    -0.0160306   -0.0802936    0.177863     0.218626     0.00917315   -0.0909463    -0.0258248   -0.073154    -0.00611761  -0.0548968    0.0428495   -0.0433798   0.0310888   -0.164122     -0.0314081  -0.0832773  -0.129959    -0.061853     0.0157602    0.036906     0.150823     0.170147     -0.0609843    0.202434     0.0974964 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 17 20 24
INFO: iteration 1, average log likelihood -1.083273
WARNING: Variances had to be floored 1 5 7 15 16 17 20 24 28 29
INFO: iteration 2, average log likelihood -1.033760
WARNING: Variances had to be floored 1 3 7 12 15 17 18 20 23 24 29
INFO: iteration 3, average log likelihood -1.021300
WARNING: Variances had to be floored 1 5 16 17 20 24 28
INFO: iteration 4, average log likelihood -1.068730
WARNING: Variances had to be floored 1 7 15 17 20 24
INFO: iteration 5, average log likelihood -1.043219
WARNING: Variances had to be floored 1 3 5 7 12 15 16 17 18 20 23 24 28 29
INFO: iteration 6, average log likelihood -1.004336
WARNING: Variances had to be floored 1 17 20 24
INFO: iteration 7, average log likelihood -1.079184
WARNING: Variances had to be floored 1 5 7 15 16 17 20 24 28 29
INFO: iteration 8, average log likelihood -1.025711
WARNING: Variances had to be floored 1 3 6 7 12 15 17 18 20 23 24 29
INFO: iteration 9, average log likelihood -1.010338
WARNING: Variances had to be floored 1 5 16 17 20 24 28
INFO: iteration 10, average log likelihood -1.063691
INFO: EM with 100000 data points 10 iterations avll -1.063691
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.165214     0.144642     0.00254511   0.0259447    0.00697848   0.0633115     0.030767    -0.019136     0.189483    -0.100501     0.0820336    0.0596154   -0.038234     0.0654722   -0.106198     0.0375286    0.109959    -0.0842469  -0.081947    -0.0591601   -0.0936373   -0.0253572   -0.0714721    0.144663     0.0364561   -0.0149523 
 -0.105706    -0.0735247   -0.0417711    0.0313837   -0.216915     0.053974      0.129454    -0.0494658   -0.0612725   -0.0553678    0.0966193    0.0246984   -0.054017     0.15677      0.023051     0.0398197    0.0449543    0.0743574  -0.0783986    0.098987     0.159646    -0.0629826    0.0708704    0.149586    -0.137769     0.223276  
 -0.0648223   -0.0117651   -0.0549871   -0.0345742    0.0293614   -0.0447449     0.0633089    0.0664009    0.185008     0.0581209   -0.113778     0.0172625    0.106238    -0.0857488    0.0412815   -0.0494118   -0.0567717   -0.199091   -0.106431    -0.0729618   -0.138       -0.0899045    0.0389049    0.0643383   -0.0239121    0.101434  
 -0.0755354    0.150559    -0.124637     0.137624     0.00529105  -0.110222     -0.0127511   -0.0467105   -0.176906     0.143928    -0.105051    -0.0711202   -0.147551    -0.20661      0.104894     0.0676491   -0.0708783    0.0819153  -0.0666795   -0.00858857  -0.0388089   -0.0363354   -0.0147448    0.0525744   -0.00918793  -0.0436387 
  0.149379     0.127113     0.0867726   -0.0808629    0.068994    -0.278021      0.0249018   -0.0974652    0.00439977  -0.136156    -0.121136    -0.112838     0.0802226   -0.20808     -0.100988    -0.00432161  -0.0871496   -0.127404    0.0113137   -0.0235597    0.0256043   -0.137286    -0.0660522    0.066832     0.0576201   -0.0289933 
 -0.144808    -0.0388651    0.179447    -0.105896     0.0976104    0.153675     -0.00774503  -0.155574    -0.220397     0.0798948    0.178265     0.0487794   -0.065173     0.13167     -0.0049055   -0.0091584    0.0125137    0.172879    0.00989566   0.313401    -0.00332524   0.00114745  -0.0452885   -0.00584942   0.134904     0.0337986 
  0.143773    -0.0420735   -0.176025     0.0230432    0.0914804    0.147331      0.0203024   -0.0306936    0.102234     0.0521393   -0.160326    -0.0141526   -0.00955119  -0.0209261    0.07673     -0.0716721   -0.066533     0.100159    0.0257186    0.0178809    0.194705    -0.0365614   -0.0463618   -0.0567154   -0.12589      0.0657583 
  0.00528826   0.013654    -0.0930474   -0.0714393   -0.0805213    0.000218537   0.0806684   -0.0504972    0.0426539    0.153348     0.101424    -0.058634    -0.138485    -0.0268737   -0.0443825    0.0391108    0.112779    -0.0837284   0.0477021   -0.0817064    0.113275     0.079524    -0.0492815   -0.0693366   -0.0182676   -0.0612003 
 -0.0229687    0.0649548    0.00251096  -0.0654118    0.16395      0.072828      0.0374441   -0.176564     0.0634172    0.0170273   -0.139286     0.033395     0.0840043   -0.0670286    0.00629288   0.0962319    0.0864876   -0.0338726   0.030248    -0.00789753  -0.00970273   0.00954869   0.137996     0.113465    -0.0714574   -0.0099674 
 -0.0909665   -0.00787863   0.0396079    0.00790608  -0.0898324   -0.0277134     0.0667544   -0.0711699    0.0178304   -0.0445579   -0.00628008   0.0751251    0.0532205    0.0938526    0.0145909   -0.13084     -0.0167972    0.155854    0.0628216   -0.121923    -0.078517     0.227074     0.0170698   -0.0503655    0.116102    -0.0872184 
  0.00363733   0.131903     0.102456    -0.00207583  -0.0492712   -0.0371604     0.0372064    0.0199729   -0.243054     0.147409    -0.0686788    0.045365    -0.158614     0.0150291    0.104413    -0.0748228   -0.158882    -0.153193   -0.0923511    0.180877    -0.112564    -0.125087    -0.00943718  -0.0483839    0.117556    -0.00703273
  0.00487434   0.00474133  -0.100787    -0.176944    -0.0163274   -0.169966      0.00979336   0.00191729  -0.0280947    0.0110461   -0.210724    -0.0499187    0.010201    -0.0524877   -0.0761869   -0.0440066    0.186132    -0.0574875  -0.118139     0.0993836    0.0433599   -0.0139118    0.112862    -0.0408383   -0.126855    -0.191621  
  0.0281434    0.206458     0.035173     0.0976666   -0.0475066    0.0443097    -0.0516404    0.0842542    0.152408    -0.168605    -0.15845     -0.0873385    0.218475     0.0800667    0.158105     0.193465    -0.00294334   0.0037179   0.0907753    0.00605065  -0.105547    -0.0546717   -0.102609     0.0757265   -0.0810494   -0.0423573 
 -0.158652    -0.0137598    0.0487243    0.0471026   -0.0155712   -0.211965      0.0577242   -0.0190336   -0.0662632    0.0620564    0.312211     0.0978428    0.116255     0.0466421    0.017614    -0.260895    -0.0433631   -0.106474    0.040937    -0.00256281  -0.0809271    0.00924837  -0.00382462  -0.0443994   -0.202994    -0.0734875 
  0.211731     0.0802392    0.0162594   -0.091007    -0.135956     0.0531173    -0.093235     0.0130226   -0.123792     0.0243038    0.0842422    0.149834     0.0697396    0.212644    -0.0779027    0.0638181    0.0496768   -0.01803     0.148257     0.0124538    0.0602871    0.163395    -0.102306     0.0689032    0.0469019    0.0262938 
 -0.0512515   -0.0568059    0.04118      0.0940146    0.0831617    0.0976748     0.130198     0.147791    -0.1551       0.0296323    0.0137366    0.00993533  -0.0144592    0.00840705  -0.0418733    0.00217837  -0.0384305   -0.137197    0.0530871    0.0348962   -0.0339585    0.187558     0.129487     0.00185385   0.0794257   -0.0478357 
  0.0341464    0.0153091   -0.0363961    0.174856    -0.178854    -0.137611      0.121858    -0.0869264   -0.135899     0.0271889    0.0375736    0.0467464    0.107928    -0.226828     0.113206     0.00990298  -0.122966     0.0711121   0.0323733   -0.140466    -0.185352    -0.0300216    0.0173183   -0.222648     0.163311     0.280357  
 -0.172512     0.155113     0.0885773   -0.0239309    0.0161594   -0.0436622    -0.0543843   -0.0524057   -0.116167    -0.0584272    0.292218    -0.0869859    0.0715885    0.16454     -0.208629    -0.0282977    0.0957662   -0.0124874  -0.0358563    0.00674143  -0.0840123   -0.0222601    0.00988099  -0.102294    -0.147413    -0.0928801 
  0.0824536    0.10661      0.240229     0.0643485   -0.0386306    0.0467572     0.0616433    0.0644263   -0.0543597   -0.0929596   -0.0903164   -0.130515     0.0785365   -0.0731393    0.0410904    0.0577432    0.0599878   -0.0248918  -0.0623962   -0.0561928   -0.00936406   0.0246087   -0.0172881   -0.0148938    0.0635419   -0.0363993 
  0.0859462    0.114114     0.0382141    0.0354758    0.0486381    0.159953      0.00862445   0.115375     0.0812237   -0.125183     0.0963608   -0.0824407   -0.0165618   -0.031518     0.0347836    0.0481811    0.0412883   -0.0349257  -0.0368497    0.0637074   -0.0319452   -0.176591    -0.0749602    0.106533    -0.0442767    0.00482392
  0.104778     0.147831     0.141221     0.147213    -0.0164783   -0.0596725    -0.030417    -0.0824931   -0.00341382   0.0029478   -0.156055     0.113824     0.248489     7.93723e-5   0.0686441   -0.036273     0.0368571    0.0583722   0.04257      0.0134536    0.140628     0.172351    -0.135513    -0.0387003   -0.0381397   -0.0574602 
  0.0281433   -0.129813    -0.0203536   -0.0115691   -0.0554594   -0.166111     -0.0281035   -0.0516742   -0.00344131   0.0203456    0.0821865    0.11165     -0.131494    -0.0235799   -0.116783     0.0770146    0.0293924    0.316468   -0.119979    -0.0913254   -0.107081    -0.0413561   -0.165057    -0.0143006    0.00686105   0.0460146 
  0.0129791    0.171651     0.0340864   -0.105198     0.123608    -0.067159     -0.0486258   -0.0462736    0.0932569    0.0226434   -0.0273141   -0.089623     0.0484411   -0.0348747   -0.127011    -0.0908162    0.100546     0.107504    0.029668    -0.0585427    0.160275    -0.0351839    0.224687    -0.150412     0.0458362    0.069548  
 -0.0933482    0.0805885   -0.0947055   -0.0276199   -0.0281308    0.0752884     0.0304521   -0.178712     0.132599    -0.00886711   0.106542    -0.0194893    0.107978    -0.0293494   -0.133979     0.0693122    0.246508     0.0181055  -0.0081659    0.0776618   -0.0815569    0.0652952   -0.0426945    0.0393768    0.0877575    0.00273524
 -0.044133     0.0580938    0.0439602   -0.0967063    0.0565085    0.0982185     0.113259     0.00352639  -0.0810352   -0.10652     -0.192628    -0.0498374   -0.0448972   -0.095804     0.0309211   -0.0291668   -0.0387394    0.097091   -0.0059969    0.106931    -0.0138429    0.00886742   0.0312084    0.105259    -0.107098     0.0507721 
  0.0248395    0.0365983    0.0608786   -0.0664818   -0.0172263   -0.0361525    -0.0233286    0.0695155    0.0189164   -0.0692141   -0.0806187   -0.0120657    0.120257     0.0376053    0.153339    -0.0761566    0.0231206    0.0486006  -0.0845073   -0.0779662   -0.0513857    0.0662454    0.227422     0.0178483   -0.0528145   -0.0060367 
  0.148342     0.0350456    0.0540739    0.0669387    0.0933366    0.0518238     0.0577921    0.00298494   0.00935254   0.158588    -0.179548     0.0658051    0.0105212   -0.0375074   -0.158928    -0.124833     0.0718482   -0.0776137  -0.206237     0.313795    -0.125742    -0.112558     0.0423501   -0.00707925  -0.010858     0.0942296 
 -0.0387394   -0.115819     0.131235    -0.0383113   -0.130363    -0.0189991     0.0530269    0.0193816    0.0728604   -0.129427    -0.0951648   -0.016042    -0.0164288   -0.117151     0.0242891   -0.00777501   0.0481405    0.0640063   0.00851015   0.0765105    0.00691981  -0.0536633    0.0636334    0.0872467    0.0284154    0.0247339 
 -0.0547679   -0.0448432   -0.0598142    0.156237     0.103935     0.274139     -0.146655     0.00236319   0.1221      -0.104867     0.077049    -0.0413523    0.00832622  -0.0200829   -0.100278    -0.0202607   -0.102652     0.145858   -0.105534     0.0421973   -0.0131132    0.0383181   -0.039139    -0.0151391    0.0382763   -0.00588529
  0.146844    -0.0513829    0.0697109    0.102276     0.113397     0.125742     -0.0402553   -0.0715045   -0.281944    -0.0646699    0.191455     0.106484     0.0743426    0.00776205   0.0175534    0.0459521   -0.140254    -0.0456109  -0.033418     0.014952    -0.137182    -0.00238277  -0.00383339  -0.102825     0.0699862    0.0540094 
  0.129958     0.0581016   -0.06456      0.034934     0.118242     0.142303      0.0749081    0.145459     0.054322     0.0678262   -0.0743546    0.0530708   -0.217919    -0.0893669   -0.0038763   -0.239439     0.077453     0.0559018  -0.0596626   -0.0306076    0.130563    -0.0851212    0.150368     0.00929351   0.267523     0.119643  
 -0.0297353   -0.151927    -0.0731466    0.00248462   0.049471    -0.145363     -0.0337059    0.221823    -0.0338956    0.048189     0.0347945   -0.144904     0.0402197   -0.0957385   -0.113767    -0.0302812   -0.0117191    0.0271065  -0.0972364    0.0211474    0.00195618   0.196532    -0.0649992    0.086252     0.110853    -0.171618  kind full, method split
0: avll = -1.4214722193885807
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.421492
INFO: iteration 2, average log likelihood -1.421417
INFO: iteration 3, average log likelihood -1.421364
INFO: iteration 4, average log likelihood -1.421306
INFO: iteration 5, average log likelihood -1.421238
INFO: iteration 6, average log likelihood -1.421161
INFO: iteration 7, average log likelihood -1.421077
INFO: iteration 8, average log likelihood -1.420993
INFO: iteration 9, average log likelihood -1.420913
INFO: iteration 10, average log likelihood -1.420834
INFO: iteration 11, average log likelihood -1.420744
INFO: iteration 12, average log likelihood -1.420615
INFO: iteration 13, average log likelihood -1.420391
INFO: iteration 14, average log likelihood -1.419981
INFO: iteration 15, average log likelihood -1.419284
INFO: iteration 16, average log likelihood -1.418299
INFO: iteration 17, average log likelihood -1.417267
INFO: iteration 18, average log likelihood -1.416506
INFO: iteration 19, average log likelihood -1.416093
INFO: iteration 20, average log likelihood -1.415905
INFO: iteration 21, average log likelihood -1.415825
INFO: iteration 22, average log likelihood -1.415792
INFO: iteration 23, average log likelihood -1.415777
INFO: iteration 24, average log likelihood -1.415771
INFO: iteration 25, average log likelihood -1.415768
INFO: iteration 26, average log likelihood -1.415767
INFO: iteration 27, average log likelihood -1.415766
INFO: iteration 28, average log likelihood -1.415766
INFO: iteration 29, average log likelihood -1.415766
INFO: iteration 30, average log likelihood -1.415765
INFO: iteration 31, average log likelihood -1.415765
INFO: iteration 32, average log likelihood -1.415765
INFO: iteration 33, average log likelihood -1.415765
INFO: iteration 34, average log likelihood -1.415765
INFO: iteration 35, average log likelihood -1.415765
INFO: iteration 36, average log likelihood -1.415765
INFO: iteration 37, average log likelihood -1.415765
INFO: iteration 38, average log likelihood -1.415764
INFO: iteration 39, average log likelihood -1.415764
INFO: iteration 40, average log likelihood -1.415764
INFO: iteration 41, average log likelihood -1.415764
INFO: iteration 42, average log likelihood -1.415764
INFO: iteration 43, average log likelihood -1.415764
INFO: iteration 44, average log likelihood -1.415764
INFO: iteration 45, average log likelihood -1.415764
INFO: iteration 46, average log likelihood -1.415764
INFO: iteration 47, average log likelihood -1.415764
INFO: iteration 48, average log likelihood -1.415764
INFO: iteration 49, average log likelihood -1.415764
INFO: iteration 50, average log likelihood -1.415764
INFO: EM with 100000 data points 50 iterations avll -1.415764
952.4 data points per parameter
1: avll = [-1.42149,-1.42142,-1.42136,-1.42131,-1.42124,-1.42116,-1.42108,-1.42099,-1.42091,-1.42083,-1.42074,-1.42061,-1.42039,-1.41998,-1.41928,-1.4183,-1.41727,-1.41651,-1.41609,-1.4159,-1.41583,-1.41579,-1.41578,-1.41577,-1.41577,-1.41577,-1.41577,-1.41577,-1.41577,-1.41577,-1.41577,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.415779
INFO: iteration 2, average log likelihood -1.415708
INFO: iteration 3, average log likelihood -1.415649
INFO: iteration 4, average log likelihood -1.415581
INFO: iteration 5, average log likelihood -1.415496
INFO: iteration 6, average log likelihood -1.415394
INFO: iteration 7, average log likelihood -1.415278
INFO: iteration 8, average log likelihood -1.415158
INFO: iteration 9, average log likelihood -1.415041
INFO: iteration 10, average log likelihood -1.414932
INFO: iteration 11, average log likelihood -1.414836
INFO: iteration 12, average log likelihood -1.414752
INFO: iteration 13, average log likelihood -1.414682
INFO: iteration 14, average log likelihood -1.414624
INFO: iteration 15, average log likelihood -1.414575
INFO: iteration 16, average log likelihood -1.414535
INFO: iteration 17, average log likelihood -1.414500
INFO: iteration 18, average log likelihood -1.414472
INFO: iteration 19, average log likelihood -1.414447
INFO: iteration 20, average log likelihood -1.414426
INFO: iteration 21, average log likelihood -1.414409
INFO: iteration 22, average log likelihood -1.414394
INFO: iteration 23, average log likelihood -1.414382
INFO: iteration 24, average log likelihood -1.414372
INFO: iteration 25, average log likelihood -1.414363
INFO: iteration 26, average log likelihood -1.414356
INFO: iteration 27, average log likelihood -1.414351
INFO: iteration 28, average log likelihood -1.414346
INFO: iteration 29, average log likelihood -1.414342
INFO: iteration 30, average log likelihood -1.414339
INFO: iteration 31, average log likelihood -1.414336
INFO: iteration 32, average log likelihood -1.414334
INFO: iteration 33, average log likelihood -1.414332
INFO: iteration 34, average log likelihood -1.414330
INFO: iteration 35, average log likelihood -1.414329
INFO: iteration 36, average log likelihood -1.414328
INFO: iteration 37, average log likelihood -1.414327
INFO: iteration 38, average log likelihood -1.414326
INFO: iteration 39, average log likelihood -1.414325
INFO: iteration 40, average log likelihood -1.414324
INFO: iteration 41, average log likelihood -1.414324
INFO: iteration 42, average log likelihood -1.414323
INFO: iteration 43, average log likelihood -1.414323
INFO: iteration 44, average log likelihood -1.414322
INFO: iteration 45, average log likelihood -1.414322
INFO: iteration 46, average log likelihood -1.414322
INFO: iteration 47, average log likelihood -1.414321
INFO: iteration 48, average log likelihood -1.414321
INFO: iteration 49, average log likelihood -1.414321
INFO: iteration 50, average log likelihood -1.414321
INFO: EM with 100000 data points 50 iterations avll -1.414321
473.9 data points per parameter
2: avll = [-1.41578,-1.41571,-1.41565,-1.41558,-1.4155,-1.41539,-1.41528,-1.41516,-1.41504,-1.41493,-1.41484,-1.41475,-1.41468,-1.41462,-1.41458,-1.41453,-1.4145,-1.41447,-1.41445,-1.41443,-1.41441,-1.41439,-1.41438,-1.41437,-1.41436,-1.41436,-1.41435,-1.41435,-1.41434,-1.41434,-1.41434,-1.41433,-1.41433,-1.41433,-1.41433,-1.41433,-1.41433,-1.41433,-1.41433,-1.41432,-1.41432,-1.41432,-1.41432,-1.41432,-1.41432,-1.41432,-1.41432,-1.41432,-1.41432,-1.41432]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.414331
INFO: iteration 2, average log likelihood -1.414272
INFO: iteration 3, average log likelihood -1.414218
INFO: iteration 4, average log likelihood -1.414154
INFO: iteration 5, average log likelihood -1.414072
INFO: iteration 6, average log likelihood -1.413971
INFO: iteration 7, average log likelihood -1.413854
INFO: iteration 8, average log likelihood -1.413728
INFO: iteration 9, average log likelihood -1.413604
INFO: iteration 10, average log likelihood -1.413491
INFO: iteration 11, average log likelihood -1.413394
INFO: iteration 12, average log likelihood -1.413314
INFO: iteration 13, average log likelihood -1.413250
INFO: iteration 14, average log likelihood -1.413199
INFO: iteration 15, average log likelihood -1.413161
INFO: iteration 16, average log likelihood -1.413131
INFO: iteration 17, average log likelihood -1.413108
INFO: iteration 18, average log likelihood -1.413091
INFO: iteration 19, average log likelihood -1.413077
INFO: iteration 20, average log likelihood -1.413065
INFO: iteration 21, average log likelihood -1.413055
INFO: iteration 22, average log likelihood -1.413046
INFO: iteration 23, average log likelihood -1.413038
INFO: iteration 24, average log likelihood -1.413030
INFO: iteration 25, average log likelihood -1.413023
INFO: iteration 26, average log likelihood -1.413016
INFO: iteration 27, average log likelihood -1.413009
INFO: iteration 28, average log likelihood -1.413002
INFO: iteration 29, average log likelihood -1.412995
INFO: iteration 30, average log likelihood -1.412989
INFO: iteration 31, average log likelihood -1.412982
INFO: iteration 32, average log likelihood -1.412975
INFO: iteration 33, average log likelihood -1.412969
INFO: iteration 34, average log likelihood -1.412962
INFO: iteration 35, average log likelihood -1.412955
INFO: iteration 36, average log likelihood -1.412948
INFO: iteration 37, average log likelihood -1.412941
INFO: iteration 38, average log likelihood -1.412934
INFO: iteration 39, average log likelihood -1.412928
INFO: iteration 40, average log likelihood -1.412921
INFO: iteration 41, average log likelihood -1.412914
INFO: iteration 42, average log likelihood -1.412907
INFO: iteration 43, average log likelihood -1.412900
INFO: iteration 44, average log likelihood -1.412893
INFO: iteration 45, average log likelihood -1.412886
INFO: iteration 46, average log likelihood -1.412879
INFO: iteration 47, average log likelihood -1.412873
INFO: iteration 48, average log likelihood -1.412866
INFO: iteration 49, average log likelihood -1.412859
INFO: iteration 50, average log likelihood -1.412853
INFO: EM with 100000 data points 50 iterations avll -1.412853
236.4 data points per parameter
3: avll = [-1.41433,-1.41427,-1.41422,-1.41415,-1.41407,-1.41397,-1.41385,-1.41373,-1.4136,-1.41349,-1.41339,-1.41331,-1.41325,-1.4132,-1.41316,-1.41313,-1.41311,-1.41309,-1.41308,-1.41306,-1.41305,-1.41305,-1.41304,-1.41303,-1.41302,-1.41302,-1.41301,-1.413,-1.413,-1.41299,-1.41298,-1.41298,-1.41297,-1.41296,-1.41296,-1.41295,-1.41294,-1.41293,-1.41293,-1.41292,-1.41291,-1.41291,-1.4129,-1.41289,-1.41289,-1.41288,-1.41287,-1.41287,-1.41286,-1.41285]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.412855
INFO: iteration 2, average log likelihood -1.412793
INFO: iteration 3, average log likelihood -1.412735
INFO: iteration 4, average log likelihood -1.412666
INFO: iteration 5, average log likelihood -1.412580
INFO: iteration 6, average log likelihood -1.412475
INFO: iteration 7, average log likelihood -1.412350
INFO: iteration 8, average log likelihood -1.412213
INFO: iteration 9, average log likelihood -1.412070
INFO: iteration 10, average log likelihood -1.411931
INFO: iteration 11, average log likelihood -1.411799
INFO: iteration 12, average log likelihood -1.411679
INFO: iteration 13, average log likelihood -1.411571
INFO: iteration 14, average log likelihood -1.411475
INFO: iteration 15, average log likelihood -1.411390
INFO: iteration 16, average log likelihood -1.411317
INFO: iteration 17, average log likelihood -1.411253
INFO: iteration 18, average log likelihood -1.411197
INFO: iteration 19, average log likelihood -1.411149
INFO: iteration 20, average log likelihood -1.411106
INFO: iteration 21, average log likelihood -1.411068
INFO: iteration 22, average log likelihood -1.411033
INFO: iteration 23, average log likelihood -1.411003
INFO: iteration 24, average log likelihood -1.410974
INFO: iteration 25, average log likelihood -1.410948
INFO: iteration 26, average log likelihood -1.410924
INFO: iteration 27, average log likelihood -1.410901
INFO: iteration 28, average log likelihood -1.410880
INFO: iteration 29, average log likelihood -1.410859
INFO: iteration 30, average log likelihood -1.410840
INFO: iteration 31, average log likelihood -1.410821
INFO: iteration 32, average log likelihood -1.410803
INFO: iteration 33, average log likelihood -1.410786
INFO: iteration 34, average log likelihood -1.410769
INFO: iteration 35, average log likelihood -1.410753
INFO: iteration 36, average log likelihood -1.410738
INFO: iteration 37, average log likelihood -1.410722
INFO: iteration 38, average log likelihood -1.410708
INFO: iteration 39, average log likelihood -1.410693
INFO: iteration 40, average log likelihood -1.410679
INFO: iteration 41, average log likelihood -1.410666
INFO: iteration 42, average log likelihood -1.410653
INFO: iteration 43, average log likelihood -1.410640
INFO: iteration 44, average log likelihood -1.410627
INFO: iteration 45, average log likelihood -1.410615
INFO: iteration 46, average log likelihood -1.410603
INFO: iteration 47, average log likelihood -1.410592
INFO: iteration 48, average log likelihood -1.410581
INFO: iteration 49, average log likelihood -1.410570
INFO: iteration 50, average log likelihood -1.410559
INFO: EM with 100000 data points 50 iterations avll -1.410559
118.1 data points per parameter
4: avll = [-1.41286,-1.41279,-1.41273,-1.41267,-1.41258,-1.41247,-1.41235,-1.41221,-1.41207,-1.41193,-1.4118,-1.41168,-1.41157,-1.41147,-1.41139,-1.41132,-1.41125,-1.4112,-1.41115,-1.41111,-1.41107,-1.41103,-1.411,-1.41097,-1.41095,-1.41092,-1.4109,-1.41088,-1.41086,-1.41084,-1.41082,-1.4108,-1.41079,-1.41077,-1.41075,-1.41074,-1.41072,-1.41071,-1.41069,-1.41068,-1.41067,-1.41065,-1.41064,-1.41063,-1.41062,-1.4106,-1.41059,-1.41058,-1.41057,-1.41056]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.410558
INFO: iteration 2, average log likelihood -1.410480
INFO: iteration 3, average log likelihood -1.410402
INFO: iteration 4, average log likelihood -1.410309
INFO: iteration 5, average log likelihood -1.410191
INFO: iteration 6, average log likelihood -1.410043
INFO: iteration 7, average log likelihood -1.409867
INFO: iteration 8, average log likelihood -1.409669
INFO: iteration 9, average log likelihood -1.409463
INFO: iteration 10, average log likelihood -1.409259
INFO: iteration 11, average log likelihood -1.409069
INFO: iteration 12, average log likelihood -1.408899
INFO: iteration 13, average log likelihood -1.408751
INFO: iteration 14, average log likelihood -1.408623
INFO: iteration 15, average log likelihood -1.408515
INFO: iteration 16, average log likelihood -1.408423
INFO: iteration 17, average log likelihood -1.408343
INFO: iteration 18, average log likelihood -1.408273
INFO: iteration 19, average log likelihood -1.408211
INFO: iteration 20, average log likelihood -1.408156
INFO: iteration 21, average log likelihood -1.408106
INFO: iteration 22, average log likelihood -1.408060
INFO: iteration 23, average log likelihood -1.408018
INFO: iteration 24, average log likelihood -1.407979
INFO: iteration 25, average log likelihood -1.407943
INFO: iteration 26, average log likelihood -1.407909
INFO: iteration 27, average log likelihood -1.407877
INFO: iteration 28, average log likelihood -1.407848
INFO: iteration 29, average log likelihood -1.407820
INFO: iteration 30, average log likelihood -1.407794
INFO: iteration 31, average log likelihood -1.407769
INFO: iteration 32, average log likelihood -1.407745
INFO: iteration 33, average log likelihood -1.407723
INFO: iteration 34, average log likelihood -1.407703
INFO: iteration 35, average log likelihood -1.407683
INFO: iteration 36, average log likelihood -1.407664
INFO: iteration 37, average log likelihood -1.407646
INFO: iteration 38, average log likelihood -1.407629
INFO: iteration 39, average log likelihood -1.407612
INFO: iteration 40, average log likelihood -1.407597
INFO: iteration 41, average log likelihood -1.407581
INFO: iteration 42, average log likelihood -1.407567
INFO: iteration 43, average log likelihood -1.407553
INFO: iteration 44, average log likelihood -1.407540
INFO: iteration 45, average log likelihood -1.407527
INFO: iteration 46, average log likelihood -1.407515
INFO: iteration 47, average log likelihood -1.407503
INFO: iteration 48, average log likelihood -1.407491
INFO: iteration 49, average log likelihood -1.407480
INFO: iteration 50, average log likelihood -1.407470
INFO: EM with 100000 data points 50 iterations avll -1.407470
59.0 data points per parameter
5: avll = [-1.41056,-1.41048,-1.4104,-1.41031,-1.41019,-1.41004,-1.40987,-1.40967,-1.40946,-1.40926,-1.40907,-1.4089,-1.40875,-1.40862,-1.40852,-1.40842,-1.40834,-1.40827,-1.40821,-1.40816,-1.40811,-1.40806,-1.40802,-1.40798,-1.40794,-1.40791,-1.40788,-1.40785,-1.40782,-1.40779,-1.40777,-1.40775,-1.40772,-1.4077,-1.40768,-1.40766,-1.40765,-1.40763,-1.40761,-1.4076,-1.40758,-1.40757,-1.40755,-1.40754,-1.40753,-1.40751,-1.4075,-1.40749,-1.40748,-1.40747]
[-1.42147,-1.42149,-1.42142,-1.42136,-1.42131,-1.42124,-1.42116,-1.42108,-1.42099,-1.42091,-1.42083,-1.42074,-1.42061,-1.42039,-1.41998,-1.41928,-1.4183,-1.41727,-1.41651,-1.41609,-1.4159,-1.41583,-1.41579,-1.41578,-1.41577,-1.41577,-1.41577,-1.41577,-1.41577,-1.41577,-1.41577,-1.41577,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41576,-1.41578,-1.41571,-1.41565,-1.41558,-1.4155,-1.41539,-1.41528,-1.41516,-1.41504,-1.41493,-1.41484,-1.41475,-1.41468,-1.41462,-1.41458,-1.41453,-1.4145,-1.41447,-1.41445,-1.41443,-1.41441,-1.41439,-1.41438,-1.41437,-1.41436,-1.41436,-1.41435,-1.41435,-1.41434,-1.41434,-1.41434,-1.41433,-1.41433,-1.41433,-1.41433,-1.41433,-1.41433,-1.41433,-1.41433,-1.41432,-1.41432,-1.41432,-1.41432,-1.41432,-1.41432,-1.41432,-1.41432,-1.41432,-1.41432,-1.41432,-1.41433,-1.41427,-1.41422,-1.41415,-1.41407,-1.41397,-1.41385,-1.41373,-1.4136,-1.41349,-1.41339,-1.41331,-1.41325,-1.4132,-1.41316,-1.41313,-1.41311,-1.41309,-1.41308,-1.41306,-1.41305,-1.41305,-1.41304,-1.41303,-1.41302,-1.41302,-1.41301,-1.413,-1.413,-1.41299,-1.41298,-1.41298,-1.41297,-1.41296,-1.41296,-1.41295,-1.41294,-1.41293,-1.41293,-1.41292,-1.41291,-1.41291,-1.4129,-1.41289,-1.41289,-1.41288,-1.41287,-1.41287,-1.41286,-1.41285,-1.41286,-1.41279,-1.41273,-1.41267,-1.41258,-1.41247,-1.41235,-1.41221,-1.41207,-1.41193,-1.4118,-1.41168,-1.41157,-1.41147,-1.41139,-1.41132,-1.41125,-1.4112,-1.41115,-1.41111,-1.41107,-1.41103,-1.411,-1.41097,-1.41095,-1.41092,-1.4109,-1.41088,-1.41086,-1.41084,-1.41082,-1.4108,-1.41079,-1.41077,-1.41075,-1.41074,-1.41072,-1.41071,-1.41069,-1.41068,-1.41067,-1.41065,-1.41064,-1.41063,-1.41062,-1.4106,-1.41059,-1.41058,-1.41057,-1.41056,-1.41056,-1.41048,-1.4104,-1.41031,-1.41019,-1.41004,-1.40987,-1.40967,-1.40946,-1.40926,-1.40907,-1.4089,-1.40875,-1.40862,-1.40852,-1.40842,-1.40834,-1.40827,-1.40821,-1.40816,-1.40811,-1.40806,-1.40802,-1.40798,-1.40794,-1.40791,-1.40788,-1.40785,-1.40782,-1.40779,-1.40777,-1.40775,-1.40772,-1.4077,-1.40768,-1.40766,-1.40765,-1.40763,-1.40761,-1.4076,-1.40758,-1.40757,-1.40755,-1.40754,-1.40753,-1.40751,-1.4075,-1.40749,-1.40748,-1.40747]
32×26 Array{Float64,2}:
 -0.00469869   0.0919095   0.053405     0.128293    0.0973853    0.0325405    0.17948     0.202951     0.0833356   0.0779008  -0.0637375   0.0703916  -0.0236762  -0.056455     0.121571   -0.234713    0.0915853  -0.00781729   -0.277674    -0.0553801  -0.239519    0.0328161   -0.155338   -0.0195531  -0.279026   -0.104117 
  0.0715077    0.0385492  -0.0183019   -0.0611374  -0.0647445    0.0311154   -0.0590793  -0.0825005   -0.1056     -0.0640099  -0.0148358  -0.0612393   0.0920786   0.0491575   -0.0637894   0.0856474  -0.0291101   0.000218686   0.205743     0.0516723   0.108755   -0.0151916    0.159237    0.0155838   0.171741    0.0706619
  0.125198     0.175666   -0.465744     0.170212    0.25506      0.0921427    0.599382   -0.282789    -0.0590257   0.476473    0.0111678  -0.411287   -0.189969   -0.0207784   -0.335907    0.0043797   0.205529   -0.164103     -0.163811    -0.0800436   0.160111    0.285263    -0.248813    0.0482278  -0.100574   -0.209564 
  0.739389     0.0508861   0.059475    -0.352966   -0.283065    -0.214512     0.13423     0.624189    -0.0851144   0.370871   -0.165966   -0.316723    0.124455   -0.0436796   -0.189234    0.279264    0.285646    0.334827     -0.217574     0.267328    0.303709    0.250484    -0.39724    -0.392477    0.108329   -0.3704   
 -0.0274953    0.199036   -0.202536     0.212391    0.215139    -0.0949391   -0.440985   -0.00700546   0.0335418  -0.553775   -0.0545622  -0.0912499  -0.366866    0.335827    -0.566105    0.0786543   0.567978    0.253244      0.032527     0.449732   -0.24849    -0.386292     0.28987    -0.152246    0.0809822  -0.820514 
 -0.0182439    0.025779    0.473518    -0.0807622   0.0679756   -0.201389    -0.272806   -0.0647901   -0.224161   -0.379897    0.265512    0.373073    0.0275151   0.00139583   0.768926    0.116714    0.32322     0.410104     -0.00443086   0.274318    0.246586    0.528596     0.345623   -0.487819   -0.255519   -0.393104 
 -0.0715284    0.595296   -0.217938    -0.372319    0.262369    -0.352854    -0.626691   -0.0273921    0.372719    0.0582834   0.0988032   0.0770067   0.33898    -0.120042    -0.510741   -0.622296   -0.341639    0.331428      0.0394935    0.452282   -0.0754245   0.221736     0.0359138   0.533781   -0.0850123  -0.466381 
 -0.197041     0.79144    -0.278995     0.0475908   0.709355     0.43976     -0.0254169   0.385157    -0.153785    0.573513    0.0828262   0.590855    0.315209   -0.0969171   -0.023251   -0.258654   -0.0566611   0.0597023     0.562734    -0.160532    0.184192    0.147516     0.33017    -0.0935105   0.272266   -0.401607 
 -0.119238    -0.359248   -0.0455943    0.152163   -0.68966     -0.172343     0.0277734   0.187097    -0.234942   -0.538786   -0.100728    0.0131597   0.0263902  -0.330428     0.225943   -0.126291   -0.107614    0.185192     -0.260003    -0.228013   -0.7537     -0.281338    -0.023277   -0.685648   -0.208952    0.747104 
  0.0797697   -0.48252     0.466534     0.131498   -0.214903    -0.475171    -0.127477   -0.27729      0.362307   -0.348651   -0.048968   -0.159047   -0.0678567  -0.0385695    0.0187486   0.138826   -0.160047   -0.0249038    -0.59195      0.278156   -0.296348   -0.238731    -0.463954    0.307133    0.0741646   0.455894 
 -0.050797     0.417145   -0.462886    -0.195381   -0.413647     0.274016     0.273916    0.407366     0.42673     0.41121     0.0477846   0.0489895   0.118448   -0.00546027  -0.63473    -0.140131   -0.201115   -0.031103     -0.00739766   0.027139   -0.318201   -0.167385    -0.154025    0.300814    0.323431    0.524859 
  0.0464312   -0.0091447   0.212929    -0.0804417  -0.450529     0.00275646   0.50471    -0.113316     0.14465    -0.109613    0.0154528  -0.190212    0.384895    0.286776     0.754701   -0.0681525  -0.257295   -0.467914     -0.175593    -0.40881     0.346331    0.0695642    0.13024     0.235052    0.299364    0.853046 
 -0.738015    -0.0370436  -0.0666104    0.178082    0.247088     0.270637    -0.444281   -0.383275    -0.519208   -0.256964   -0.137958    0.218143    0.110207   -0.0910188    0.140945   -0.441377   -0.318817   -0.306519     -0.00867383  -0.120387   -0.134733   -0.0504904   -0.0362193   0.131021   -0.284697    0.0561825
  0.186645    -0.298713   -0.015142     0.194952    0.134824    -0.0305971   -0.28418    -0.362406    -0.631439   -0.0492406  -0.963374   -0.400854    0.069768   -0.12565     -0.38459    -0.503909   -0.761957    0.0655986     0.126788    -0.0837347   0.673469   -0.137302     0.0383587   0.280957    0.442167    0.200323 
  0.403074    -0.28248     0.382913    -0.179035    0.00738927  -0.15021     -0.146903   -0.282007    -0.267095    0.295686   -0.0667582   0.293042    0.49908    -0.191976     0.572751    0.832254   -0.227295   -0.404811      0.454422     0.159585    0.179422   -0.290892     0.326199    0.235423   -0.51808     0.541906 
  0.0514921   -0.116652    0.18135      0.111161    0.0886961    0.351552     0.30679    -0.259806    -0.0452552   0.0910371   0.509799    0.247935    0.1054      0.115098    -0.067048    0.774418    0.0328749  -0.266834      0.364875     0.185224    0.319028   -0.0114235    0.0163708   0.0297414   0.795956    0.0275201
  0.0649061    0.166058    0.0421924   -0.0667421   0.158276    -0.421594     0.592451    0.00364816  -0.0929308   1.01085     0.126662    0.0267687   0.220609   -0.240101    -0.0711682   0.188151   -0.122221   -0.25937       0.517617    -0.303517    0.0320778   0.488535    -0.182437    0.175214    0.0796781   0.167523 
 -0.276671     0.222664   -0.0726377    0.16396     0.0423713    0.735603     0.204625    0.138573    -0.0112303   0.539067   -0.125884   -0.0188807   0.660992   -0.112568     0.164228   -0.18201    -0.500968    0.0652646    -0.712932     0.212751    0.349325    0.298756    -1.16889     0.0367285  -0.0575961   0.388353 
 -0.282121     0.167731    0.0110798   -0.493142    0.0420756    0.12411      0.0710137   0.53931     -0.0670624   0.370905    0.27143     0.409699   -0.535705   -0.437145    -0.173626    0.173253    0.789612    0.517671     -0.201266    -0.429349   -0.190351    0.396546    -0.665576   -0.134382   -0.399554   -0.732733 
  0.15411     -0.426401    0.556075     0.130182    0.455446    -0.777159     0.554283    0.119674     0.0220153   0.489492    0.189749    0.293177   -0.276388   -0.87265     -0.24478    -0.0300202   0.0203759   0.487037     -0.622769     0.137498    0.636904    0.323996    -0.488622    0.218237   -0.100648   -0.179581 
 -0.15565      0.519248   -0.0573235   -0.436541   -0.369654    -0.018819    -0.0949494   0.0941146    0.351668   -0.110079    0.383353    0.2544      0.787254    0.661474     0.166934    0.134391   -0.243776    0.0177901     0.162978     0.913538   -0.246362    0.359272    -0.0871264  -0.442559   -0.0165574  -0.0408313
  0.283125     0.527426    0.164603    -0.199136   -0.0105836   -0.0996438    0.307138    0.228142     0.803722    0.0113631   0.385858    0.206193    0.207479    0.298543     0.247789    0.037439    0.818769   -0.304251     -0.159282     0.603968   -0.451956   -0.271361     0.450573   -0.0335906  -0.364808    0.0551147
  0.782052     0.507875   -0.055732     0.440629    0.234853     0.215724     0.0760777   0.466461    -0.249732   -0.370097    0.49579    -0.670603    0.416609   -0.668837    -0.102258    0.099965    0.533828   -0.441456     -0.351081    -0.612718   -0.167133   -0.0891101   -0.370192   -0.5456     -0.074578   -0.243021 
  0.556072     0.551291   -0.400384     0.566774   -0.177854     0.440834     0.41691     0.553534    -0.719126    0.154171   -0.19969    -0.780396    0.0517619   0.281771    -0.0456543  -0.0819024  -0.197808    0.0644183     0.608254    -0.611636   -0.310915    0.525001     0.248185   -0.653447   -0.284699    0.0250264
 -0.113763    -0.0583073  -0.198039    -0.0839947  -0.476099     0.196387    -0.261791    0.572422     0.298607   -0.302377   -0.53578    -0.311149   -0.142969    0.404341    -0.31157    -0.539034    0.0156613   0.0388912    -0.231174    -0.418569   -0.883437   -0.465475    -0.204015    0.0218532   0.31989     0.103035 
 -0.0293698   -0.115522   -0.00520023  -0.245632    0.123721    -0.158502    -0.681426   -0.181597    -0.0979718  -0.353881   -0.112292    0.107757    0.0126629  -0.119392    -0.232703    0.0573029   0.268458    0.111442     -0.21758      0.346639    0.407256   -0.360914     0.0404156   0.152629   -0.0478181  -0.515347 
 -0.433644     0.144048    0.280192     0.762298   -0.0489277   -0.382378     0.179009   -0.46525     -0.192568   -0.323665    0.185742    0.240983   -0.568797    0.24908      0.0412788  -0.692067   -0.190359   -0.34871      -0.154348    -0.271498   -0.561727   -0.38192      0.868372    0.220563    0.0062681   0.428728 
 -0.820763    -0.534991    0.198948    -0.399037   -0.459856    -0.336198    -0.17362    -0.652028     0.491084    0.52385    -0.827776    0.50541    -0.383123   -0.063191     0.598082   -0.222013   -0.189513   -0.0490099    -0.390836     0.812238   -0.190822   -0.576853     0.221392    0.961564    0.264663    0.66525  
  0.483516    -0.389985   -0.274358    -0.568406   -0.45919     -0.884468     0.244634   -0.368086     0.436751   -0.448011   -0.0511035  -0.414328   -0.597465   -0.161742     0.047788    0.401845    0.507595    0.185784      0.307858    -0.542113   -0.555727   -0.00298771   0.598794    0.0597668  -0.269236    0.17984  
 -0.289802    -0.670014    0.304852    -0.244928   -0.35665      0.0591759   -0.568232    0.0265624   -0.316371   -0.348254   -0.0381517   0.313452   -0.716806   -0.091003    -0.288486    0.155694   -0.0451708   0.527479      0.418407    -0.172905    0.417674   -0.40336      0.296268    0.109785    0.427637    0.112467 
  0.010602    -1.14071     0.278392     0.267322   -0.136953     0.424467     0.571238   -0.436926    -0.471893   -0.264318    0.0284427  -0.268918   -0.530574   -0.0813647    0.523145    0.523552    0.453859   -0.21739      -0.497685    -0.259596    0.339579   -0.214722     0.0839221  -0.233008   -0.259791    0.514844 
  0.255754    -0.0096378  -0.0617212    0.450186    0.446728     0.834912     0.358788   -0.23612      0.727708   -0.8135      0.0995945  -0.389393   -0.308432    0.201761     0.3242     -0.127441   -0.226971    0.641727      0.205771    -0.0964867   0.0295821   0.387725     0.390137   -0.0451459   0.0746271  -0.0677318INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.407459
INFO: iteration 2, average log likelihood -1.407449
INFO: iteration 3, average log likelihood -1.407439
INFO: iteration 4, average log likelihood -1.407430
INFO: iteration 5, average log likelihood -1.407421
INFO: iteration 6, average log likelihood -1.407412
INFO: iteration 7, average log likelihood -1.407403
INFO: iteration 8, average log likelihood -1.407395
INFO: iteration 9, average log likelihood -1.407386
INFO: iteration 10, average log likelihood -1.407378
INFO: EM with 100000 data points 10 iterations avll -1.407378
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.351636e+05
      1       6.991373e+05      -2.360262e+05 |       32
      2       6.882639e+05      -1.087338e+04 |       32
      3       6.830542e+05      -5.209722e+03 |       32
      4       6.800782e+05      -2.976037e+03 |       32
      5       6.781781e+05      -1.900057e+03 |       32
      6       6.768377e+05      -1.340446e+03 |       32
      7       6.758423e+05      -9.953341e+02 |       32
      8       6.749853e+05      -8.570484e+02 |       32
      9       6.742353e+05      -7.500339e+02 |       32
     10       6.736140e+05      -6.213059e+02 |       32
     11       6.731291e+05      -4.848264e+02 |       32
     12       6.727399e+05      -3.892818e+02 |       32
     13       6.723902e+05      -3.496610e+02 |       32
     14       6.720953e+05      -2.949133e+02 |       32
     15       6.718194e+05      -2.758883e+02 |       32
     16       6.715666e+05      -2.527714e+02 |       32
     17       6.713365e+05      -2.300891e+02 |       32
     18       6.711418e+05      -1.947793e+02 |       32
     19       6.709414e+05      -2.003904e+02 |       32
     20       6.707532e+05      -1.881489e+02 |       32
     21       6.705895e+05      -1.637171e+02 |       32
     22       6.704407e+05      -1.487859e+02 |       32
     23       6.703033e+05      -1.374166e+02 |       32
     24       6.701813e+05      -1.220397e+02 |       32
     25       6.700583e+05      -1.229068e+02 |       32
     26       6.699502e+05      -1.081306e+02 |       32
     27       6.698440e+05      -1.061791e+02 |       32
     28       6.697295e+05      -1.145004e+02 |       32
     29       6.696298e+05      -9.973489e+01 |       32
     30       6.695379e+05      -9.191619e+01 |       32
     31       6.694543e+05      -8.358989e+01 |       32
     32       6.693797e+05      -7.455251e+01 |       32
     33       6.693030e+05      -7.675140e+01 |       32
     34       6.692248e+05      -7.821080e+01 |       32
     35       6.691562e+05      -6.860362e+01 |       32
     36       6.690909e+05      -6.527628e+01 |       32
     37       6.690305e+05      -6.041921e+01 |       32
     38       6.689727e+05      -5.782032e+01 |       32
     39       6.689149e+05      -5.775928e+01 |       32
     40       6.688573e+05      -5.757910e+01 |       32
     41       6.688077e+05      -4.965354e+01 |       32
     42       6.687673e+05      -4.033274e+01 |       32
     43       6.687233e+05      -4.404590e+01 |       32
     44       6.686739e+05      -4.936078e+01 |       32
     45       6.686315e+05      -4.243457e+01 |       32
     46       6.685954e+05      -3.606114e+01 |       32
     47       6.685612e+05      -3.425254e+01 |       32
     48       6.685293e+05      -3.188048e+01 |       32
     49       6.684947e+05      -3.455572e+01 |       32
     50       6.684596e+05      -3.516513e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 668459.5791057379)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.419769
INFO: iteration 2, average log likelihood -1.414740
INFO: iteration 3, average log likelihood -1.413401
INFO: iteration 4, average log likelihood -1.412403
INFO: iteration 5, average log likelihood -1.411347
INFO: iteration 6, average log likelihood -1.410342
INFO: iteration 7, average log likelihood -1.409606
INFO: iteration 8, average log likelihood -1.409167
INFO: iteration 9, average log likelihood -1.408909
INFO: iteration 10, average log likelihood -1.408739
INFO: iteration 11, average log likelihood -1.408612
INFO: iteration 12, average log likelihood -1.408508
INFO: iteration 13, average log likelihood -1.408421
INFO: iteration 14, average log likelihood -1.408346
INFO: iteration 15, average log likelihood -1.408279
INFO: iteration 16, average log likelihood -1.408220
INFO: iteration 17, average log likelihood -1.408167
INFO: iteration 18, average log likelihood -1.408118
INFO: iteration 19, average log likelihood -1.408075
INFO: iteration 20, average log likelihood -1.408035
INFO: iteration 21, average log likelihood -1.407998
INFO: iteration 22, average log likelihood -1.407965
INFO: iteration 23, average log likelihood -1.407933
INFO: iteration 24, average log likelihood -1.407905
INFO: iteration 25, average log likelihood -1.407878
INFO: iteration 26, average log likelihood -1.407853
INFO: iteration 27, average log likelihood -1.407829
INFO: iteration 28, average log likelihood -1.407807
INFO: iteration 29, average log likelihood -1.407786
INFO: iteration 30, average log likelihood -1.407767
INFO: iteration 31, average log likelihood -1.407748
INFO: iteration 32, average log likelihood -1.407730
INFO: iteration 33, average log likelihood -1.407714
INFO: iteration 34, average log likelihood -1.407698
INFO: iteration 35, average log likelihood -1.407683
INFO: iteration 36, average log likelihood -1.407668
INFO: iteration 37, average log likelihood -1.407655
INFO: iteration 38, average log likelihood -1.407642
INFO: iteration 39, average log likelihood -1.407629
INFO: iteration 40, average log likelihood -1.407617
INFO: iteration 41, average log likelihood -1.407605
INFO: iteration 42, average log likelihood -1.407594
INFO: iteration 43, average log likelihood -1.407583
INFO: iteration 44, average log likelihood -1.407572
INFO: iteration 45, average log likelihood -1.407562
INFO: iteration 46, average log likelihood -1.407552
INFO: iteration 47, average log likelihood -1.407542
INFO: iteration 48, average log likelihood -1.407532
INFO: iteration 49, average log likelihood -1.407523
INFO: iteration 50, average log likelihood -1.407514
INFO: EM with 100000 data points 50 iterations avll -1.407514
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.351074     0.24297     0.158029     -0.212977     0.155612   -0.0650081    0.30146      0.287675    0.418455    0.218397    0.189582     0.0905631   0.621285   -0.0773412   0.49594      0.0293581   0.217819    -0.691428    -0.172762    0.126327    -0.160099     -0.194048    0.069799   -0.0267494   -0.33142     0.438676  
 -0.0380305    0.463199   -0.664395     -0.185368    -0.315544    0.123263     0.0591197    0.768454    0.533371    0.43274    -0.112803     0.0479592   0.103574    0.256158   -0.778084    -0.232724    0.0239768   -0.0316389    0.0999792   0.00801259  -0.683097     -0.221504   -0.25566     0.0850413    0.326234   -0.0276996 
 -0.514599    -0.462183    0.32008      -0.68877     -0.579363   -0.265507    -0.0297372   -0.400048    0.531293    0.455115   -0.591156     0.363299   -0.240489   -0.0288076   0.223102    -0.0518679  -0.128962    -0.0227301   -0.216276    0.712673     0.0854904    -0.606212   -0.0901885   0.759506     0.608776    0.655741  
 -0.612725     0.0674333   0.0288091     0.551158     0.0612854  -0.518053    -0.0555002   -0.700891   -0.378022   -0.144833   -0.00768306   0.434678   -0.462531    0.29576     0.264588    -0.752052   -0.176815    -0.41516     -0.304483   -0.078555    -0.433795     -0.278271    0.900237    0.326705    -0.205781    0.374097  
 -0.00326551   0.481838   -0.0106822    -0.0117995    0.417275    0.579822     0.197487     0.0804565  -0.126355    0.466545    0.425355     0.299099    0.525648   -0.0182455   0.0692331    0.358812    0.00227954  -0.0207982    0.555922    0.12401      0.447304      0.20874     0.172724   -0.00950512   0.661871   -0.0864781 
  0.668046     0.258491   -0.1948        0.239671    -0.463727    0.12324      0.868335     0.36209    -0.418652    0.271034    0.0502174   -0.65419     0.120569   -0.141555    0.130166     0.240028    0.180864    -0.251231     0.156617   -0.710587    -0.18577       0.292884   -0.0869939  -0.54582     -0.0429025   0.466416  
  0.0115259    0.866036    0.0247862    -0.184874    -0.230811   -0.00500911  -0.0141523    0.0185629   0.674599   -0.164541    0.125514    -0.0670837   0.316159    0.869591    0.134451    -0.119234    0.421188    -0.294556     0.118948    0.854303    -0.632525     -0.134719    0.569946   -0.0596569   -0.299093   -0.0746349 
  0.350775    -0.316863    0.299134     -0.172059    -0.0100374  -0.0967201   -0.0564151   -0.591053   -0.117129    0.200064    0.105661     0.282607    0.522083   -0.0705226   0.731429     1.02947    -0.282863    -0.264498     0.413234    0.335525     0.294506      0.0132498   0.289584    0.243665    -0.446057    0.539275  
 -0.56152     -0.170271    0.286161      0.00708258  -0.398373    0.125478     0.471401    -0.102536    0.22792    -0.139334    0.368452    -0.0328833  -0.0391672   0.0699399   0.749849    -0.0975945  -0.663073    -0.434471     0.295913   -0.815124    -0.0662718     0.125951    0.0451696   0.12484      0.616082    0.874357  
 -0.22074      0.157883   -0.0605176     0.234773    -0.210087    0.375945     0.404428    -0.0753795   0.190493    0.65361    -0.151954    -0.121748    0.348218   -0.247353   -0.0529677   -0.259292   -0.680039     0.252201    -0.774888    0.24601      0.292297      0.513791   -0.927396    0.227266    -0.261846    0.65352   
  0.537533    -0.418381    0.0390063    -0.344763    -0.382168    0.217712    -0.338527     0.31592     0.792298   -0.97102    -0.158335    -0.290178   -0.322638   -0.0699479   0.209011     0.0202435   0.176833     0.49855     -0.577264   -0.276746    -0.198724     -0.605185    0.339134   -0.169077    -0.117631    0.316324  
 -0.536124     0.393092   -0.673513     -0.0680756    0.452976    0.655804     0.295546    -0.432929    0.830632   -0.668566   -0.172451    -0.397531   -0.207177    0.29568    -0.0115467   -0.351911   -0.10127      0.885493     0.10611    -0.0481852   -0.000329632   0.800774   -0.341634   -0.0989561    0.20517    -0.276152  
  0.247591    -0.0952776  -0.685099     -0.0350787    0.258519   -0.0630986    0.00154599  -0.0511938  -0.075814   -0.169308    0.0121423   -0.284055   -0.320495    0.770392   -0.478757    -0.0459442   0.550118    -0.12645     -0.142382    0.446621     0.170981     -0.304517    0.342227   -0.115068     0.357011   -0.910059  
  0.410886     0.665804   -0.347427      0.42818      0.522932    0.300533    -0.304793     0.528824   -0.550851   -0.246665    0.206366    -0.31361     0.180411   -0.148344   -0.0902403   -0.0683736   0.154091     0.109035     0.255403   -0.360204    -0.144808      0.340529    0.0424461  -0.728494    -0.557163   -0.804383  
  0.0413738    0.0085539   0.0294288    -0.0538003    0.109576   -0.0267618    0.0196683    0.0917021  -0.136276    0.189205   -0.0124604    0.036305    0.0738486  -0.153742   -0.238062     0.129734    0.166065     0.106239    -0.0304905   0.0688206    0.238767      0.0711952  -0.30182    -0.00917576   0.0826107  -0.285018  
 -0.194317    -0.228022    0.198354      0.199842    -0.515956    0.441843    -0.238243     0.0886491  -0.145047   -0.869255    0.0247908    0.282874    0.406131    0.305906    0.391985     0.268389    0.0983731   -0.00597014  -0.243362    0.584552    -0.319014     -0.113973   -0.165999   -0.558409     0.231904    0.21805   
 -0.0841334   -0.935552    0.339313      0.355994     0.356278    0.327703     0.150533    -0.558849   -0.680983   -0.256727   -0.0324844   -0.0437494  -0.522676   -0.322253    0.335707     0.169825    0.176723     0.025573    -0.201551   -0.247214     0.586175     -0.0748369   0.0594035  -0.0871071   -0.160842    0.100092  
 -0.535306     0.0948937  -0.109513     -0.199328     0.130217    0.182907    -0.757884     0.027855   -0.769379    0.125144   -0.225812     0.426667    0.46434    -0.122515   -0.00231588  -0.545667   -0.695581     0.01492     -0.0144775   0.0102983    0.434051      0.0598326  -0.396258   -0.0488367    0.0071561  -0.0563774 
  0.198274    -0.215083   -0.137183      0.519703     0.114941    0.211222    -0.200248    -0.408052   -0.523531   -0.324528   -1.03121     -0.629293    0.26571     0.207909   -0.17888     -0.424617   -0.569552    -0.253517     0.321982   -0.236448     0.197588     -0.0460971   0.297888    0.139886     0.325333    0.443133  
  0.0393306   -0.560469    0.156492      0.0394272   -0.517623   -0.364725     0.194917    -0.347046    0.224044   -0.415168    0.0524116   -0.378825   -0.32184     0.0790151   0.178288     0.180919   -0.0302095   -0.0531101   -0.271983   -0.124524    -0.265211     -0.100454   -0.0310994   0.136555     0.0392851   0.596606  
 -0.132872    -0.29705     0.231662     -0.204299    -0.259501   -0.237707    -0.576631    -0.0628529  -0.365122   -0.404024    0.0509556    0.253362   -0.612434    0.016224   -0.300068     0.319102    0.14575      0.292207     0.744232   -0.129631     0.0769144    -0.353102    0.614838   -0.0236835    0.295804   -0.108062  
 -0.587929     0.476488    0.0210698     0.240664    -0.132893    0.241754     0.0187486    0.401912   -0.24671    -0.257788   -0.238679     0.0793044  -0.200228   -0.284127    0.0413947   -0.911434   -0.0713357    0.253501    -0.0518029  -0.534486    -0.713938     -0.0654552   0.162074    0.290501    -0.386309    0.214129  
 -0.313343     0.232585    0.000486829  -0.114018     0.385687   -0.194149    -0.839914    -0.395495    0.37281    -0.602833   -0.0192676    0.128513    0.153688   -0.0975577  -0.488457    -0.362705   -0.0279517    0.246798    -0.298292    0.705265    -0.0236064    -0.195077    0.104327    0.597293    -0.04348    -0.616273  
  0.123623     0.446339   -0.196279     -0.310224     0.542277   -0.58685      0.0777497   -0.0834372   0.111279    0.953983    0.146885     0.0123108  -0.014414   -0.350198   -0.455895    -0.325364   -0.248153    -0.105555     0.449772   -0.181016     0.0349236     0.453355    0.032605    0.536883    -0.102662   -0.505033  
  0.301396     0.110649    0.173929      1.08124      0.637964    0.446697     0.833338    -0.191424    0.620911   -0.113966    0.491846    -0.0973584  -0.522612    0.104849   -0.213152     0.152909    0.0593637    0.110579     0.112622    0.041452    -0.494271      0.0342691   0.435816   -0.0662439    0.0758037   0.00235454
 -0.161755     0.0898813   0.0444623     0.0294696    0.129089    0.0448771   -0.155897    -0.10913    -0.0340123  -0.144465    0.0545898    0.135005    0.0552439  -0.0184854   0.00980359  -0.0852925   0.00734849  -0.0298174    0.0739404   0.157651    -0.0726027    -0.0195868   0.0804813   0.0245093   -0.0536236  -0.0683262 
 -0.258163     0.223549    0.25654      -0.491814     0.130032   -0.374464     0.243111     0.376451    0.134059    0.165892    0.658369     0.765372   -0.11728    -0.272171    0.387018     0.166172    0.863665     0.40737     -0.289922    0.222673     0.0489855     0.376075   -0.10727    -0.143812    -0.347215   -0.468701  
  0.0427669    0.0505732  -0.334569      0.00195354   0.0280624   0.222237     0.342569     0.0415306  -0.212064    0.315605   -0.194972    -0.213445   -0.236045   -0.102059   -0.104152     0.0318714   0.451896    -0.0982268   -0.193802   -0.21384      0.00933021    0.247709   -0.417864   -0.237922    -0.131532   -0.367392  
  0.450342     0.308044    0.321961     -0.0676473   -0.258202   -0.195639     0.201672     0.220619    0.24345    -0.0165154  -0.0684309   -0.344066    0.0418621   0.368955    0.301841    -0.170178    0.133357     0.587734    -0.0644997   0.0580413    0.175827      0.363583    0.280833   -0.308271    -0.148131   -0.17918   
  0.15933      0.0166379  -0.0887842     0.0245197   -0.0221405   0.0246723    0.205346     0.0581038  -0.0984827   0.323854   -0.162797    -0.0302352   0.137586   -0.0576936  -0.190717     0.0587685  -0.365173    -0.165759    -0.0199143  -0.21144      0.122264     -0.132879   -0.0961174   0.282883     0.221537    0.301975  
 -0.0559369   -0.585476    0.489605      0.427643    -0.384712   -0.761798     0.0558356   -0.113283   -0.299894    0.0526634  -0.0479792   -0.0873873  -0.0717888  -0.425      -0.36152     -0.100491    0.27005     -0.184613    -0.92036    -0.244802    -0.472028     -0.475725   -0.757781    0.0142373   -0.34068     0.224922  
  0.712454    -0.114256    0.18054      -0.203532    -0.0399964  -0.405815     0.099133     0.426839    0.13002     0.290456    0.0451509   -0.15585    -0.079121   -0.407573   -0.449344     0.41162     0.315566     0.365013    -0.362892    0.433357     0.240054      0.165012   -0.482103   -0.181024     0.192674   -0.441906  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.407505
INFO: iteration 2, average log likelihood -1.407496
INFO: iteration 3, average log likelihood -1.407488
INFO: iteration 4, average log likelihood -1.407479
INFO: iteration 5, average log likelihood -1.407471
INFO: iteration 6, average log likelihood -1.407463
INFO: iteration 7, average log likelihood -1.407455
INFO: iteration 8, average log likelihood -1.407447
INFO: iteration 9, average log likelihood -1.407439
INFO: iteration 10, average log likelihood -1.407432
INFO: EM with 100000 data points 10 iterations avll -1.407432
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
