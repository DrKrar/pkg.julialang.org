>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.7.0
INFO: Installing JLD v0.6.6
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.1
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.2.0
INFO: Installing StaticArrays v0.0.11
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1211
Commit 1ac30c3 (2016-11-08 21:35 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-100-generic #147-Ubuntu SMP Tue Oct 18 16:48:51 UTC 2016 x86_64 x86_64
Memory: 2.939289093017578 GB (665.02734375 MB free)
Uptime: 22210.0 sec
Load Avg:  1.00732421875  1.01513671875  1.04541015625
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3509 MHz    1390772 s       5354 s     140963 s     428291 s         11 s
#2  3509 MHz     496704 s        356 s      65090 s    1599341 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.3
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.7.0
 - JLD                           0.6.6
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.1
 - Rmath                         0.1.4
 - SHA                           0.2.1
 - ScikitLearnBase               0.2.0
 - StaticArrays                  0.0.11
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-1.2182403410676103e6,[6650.77,93349.2],
[9994.54 7815.85 -475.938; -9947.46 -7869.33 1173.39],

Array{Float64,2}[
[17992.9 9681.33 23.0372; 9681.33 13049.2 -488.968; 23.0372 -488.968 9993.24],

[82116.0 -9434.24 472.598; -9434.24 86549.9 247.195; 472.598 247.195 90668.3]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.182681e+03
      1       9.481548e+02      -2.345263e+02 |        4
      2       8.850511e+02      -6.310372e+01 |        2
      3       8.689707e+02      -1.608038e+01 |        0
      4       8.689707e+02       0.000000e+00 |        0
K-means converged with 4 iterations (objv = 868.9707388344718)
INFO: K-means with 272 data points using 4 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.065943
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.831295
INFO: iteration 2, lowerbound -3.708232
INFO: iteration 3, lowerbound -3.566191
INFO: iteration 4, lowerbound -3.396299
INFO: iteration 5, lowerbound -3.221260
INFO: iteration 6, lowerbound -3.064749
INFO: dropping number of Gaussions to 7
INFO: iteration 7, lowerbound -2.932935
INFO: dropping number of Gaussions to 6
INFO: iteration 8, lowerbound -2.807448
INFO: dropping number of Gaussions to 5
INFO: iteration 9, lowerbound -2.677855
INFO: dropping number of Gaussions to 4
INFO: iteration 10, lowerbound -2.551709
INFO: iteration 11, lowerbound -2.449264
INFO: iteration 12, lowerbound -2.380451
INFO: iteration 13, lowerbound -2.340022
INFO: dropping number of Gaussions to 3
INFO: iteration 14, lowerbound -2.316849
INFO: iteration 15, lowerbound -2.307468
INFO: dropping number of Gaussions to 2
INFO: iteration 16, lowerbound -2.302930
INFO: iteration 17, lowerbound -2.299262
INFO: iteration 18, lowerbound -2.299257
INFO: iteration 19, lowerbound -2.299255
INFO: iteration 20, lowerbound -2.299254
INFO: iteration 21, lowerbound -2.299253
INFO: iteration 22, lowerbound -2.299253
INFO: iteration 23, lowerbound -2.299253
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Thu 10 Nov 2016 11:38:56 AM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Thu 10 Nov 2016 11:38:58 AM UTC: K-means with 272 data points using 4 iterations
11.3 data points per parameter
,Thu 10 Nov 2016 11:38:59 AM UTC: EM with 272 data points 0 iterations avll -2.065943
5.8 data points per parameter
,Thu 10 Nov 2016 11:38:59 AM UTC: GMM converted to Variational GMM
,Thu 10 Nov 2016 11:39:01 AM UTC: iteration 1, lowerbound -3.831295
,Thu 10 Nov 2016 11:39:01 AM UTC: iteration 2, lowerbound -3.708232
,Thu 10 Nov 2016 11:39:01 AM UTC: iteration 3, lowerbound -3.566191
,Thu 10 Nov 2016 11:39:01 AM UTC: iteration 4, lowerbound -3.396299
,Thu 10 Nov 2016 11:39:01 AM UTC: iteration 5, lowerbound -3.221260
,Thu 10 Nov 2016 11:39:01 AM UTC: iteration 6, lowerbound -3.064749
,Thu 10 Nov 2016 11:39:02 AM UTC: dropping number of Gaussions to 7
,Thu 10 Nov 2016 11:39:02 AM UTC: iteration 7, lowerbound -2.932935
,Thu 10 Nov 2016 11:39:02 AM UTC: dropping number of Gaussions to 6
,Thu 10 Nov 2016 11:39:02 AM UTC: iteration 8, lowerbound -2.807448
,Thu 10 Nov 2016 11:39:02 AM UTC: dropping number of Gaussions to 5
,Thu 10 Nov 2016 11:39:02 AM UTC: iteration 9, lowerbound -2.677855
,Thu 10 Nov 2016 11:39:02 AM UTC: dropping number of Gaussions to 4
,Thu 10 Nov 2016 11:39:02 AM UTC: iteration 10, lowerbound -2.551709
,Thu 10 Nov 2016 11:39:02 AM UTC: iteration 11, lowerbound -2.449264
,Thu 10 Nov 2016 11:39:02 AM UTC: iteration 12, lowerbound -2.380451
,Thu 10 Nov 2016 11:39:02 AM UTC: iteration 13, lowerbound -2.340022
,Thu 10 Nov 2016 11:39:02 AM UTC: dropping number of Gaussions to 3
,Thu 10 Nov 2016 11:39:02 AM UTC: iteration 14, lowerbound -2.316849
,Thu 10 Nov 2016 11:39:02 AM UTC: iteration 15, lowerbound -2.307468
,Thu 10 Nov 2016 11:39:02 AM UTC: dropping number of Gaussions to 2
,Thu 10 Nov 2016 11:39:02 AM UTC: iteration 16, lowerbound -2.302930
,Thu 10 Nov 2016 11:39:02 AM UTC: iteration 17, lowerbound -2.299262
,Thu 10 Nov 2016 11:39:02 AM UTC: iteration 18, lowerbound -2.299257
,Thu 10 Nov 2016 11:39:02 AM UTC: iteration 19, lowerbound -2.299255
,Thu 10 Nov 2016 11:39:02 AM UTC: iteration 20, lowerbound -2.299254
,Thu 10 Nov 2016 11:39:02 AM UTC: iteration 21, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:02 AM UTC: iteration 22, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:02 AM UTC: iteration 23, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:02 AM UTC: iteration 24, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:03 AM UTC: iteration 25, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:03 AM UTC: iteration 26, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:03 AM UTC: iteration 27, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:03 AM UTC: iteration 28, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:03 AM UTC: iteration 29, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:03 AM UTC: iteration 30, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:03 AM UTC: iteration 31, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:03 AM UTC: iteration 32, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:03 AM UTC: iteration 33, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:03 AM UTC: iteration 34, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:03 AM UTC: iteration 35, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:03 AM UTC: iteration 36, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:03 AM UTC: iteration 37, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:03 AM UTC: iteration 38, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:03 AM UTC: iteration 39, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:03 AM UTC: iteration 40, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:03 AM UTC: iteration 41, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:03 AM UTC: iteration 42, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:03 AM UTC: iteration 43, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:03 AM UTC: iteration 44, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:03 AM UTC: iteration 45, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:04 AM UTC: iteration 46, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:04 AM UTC: iteration 47, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:04 AM UTC: iteration 48, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:04 AM UTC: iteration 49, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:04 AM UTC: iteration 50, lowerbound -2.299253
,Thu 10 Nov 2016 11:39:04 AM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.045,95.9549]
β = [178.045,95.9549]
m = [4.2503 79.2869; 2.00023 53.852]
ν = [180.045,97.9549]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.184042 -0.00764405; 0.0 0.00858171],

[0.375876 -0.00895312; 0.0 0.0127487]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.00000000001
avll from stats: -0.9990608759766518
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9990608759771811
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9990608759771814
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.00000000001
avll from stats: -1.0071381775195796
avll from llpg:  -1.0071381775195793
avll direct:     -1.0071381775195793
sum posterior: 100000.0
32×26 Array{Float64,2}:
  0.143211    0.182603      0.0563646    0.0246646   -0.0711008   -0.181138    -0.213995     0.0595863    0.0388743   -0.0471058    0.0952149   -0.0729835    0.156376    -0.0753759    -0.19644      0.120931     0.041887   -0.122363    -0.0203087    0.0828436  -0.166457    -0.0710855   -0.0793731    0.125828     0.0251763   -0.0945757 
 -0.0405733   0.150041     -0.00172428   0.243408     0.121478    -0.112931    -0.00900314   0.234038     0.0222758   -0.122173    -0.0716206    0.206448    -0.125377     0.0663508     0.0763159    0.234911     0.0313108  -0.054841    -0.047278     0.0297961   0.216811    -0.0899358    0.0351046   -0.177689     0.316827     0.236985  
 -0.0480214  -0.0822076     0.188798    -0.149109    -0.156161    -0.00962968   0.0681855   -0.045176     0.0825792    0.00890809   0.0445871    0.0548444    0.111974    -0.0553499    -0.136423     0.0542304    0.0571944  -0.135841    -0.0277427    0.0453429  -0.0284457   -0.0575496    0.20638     -0.0664973    0.057656     0.0395745 
 -0.0739732  -0.0442292     0.019014     0.113328     0.0774189   -0.0338427    0.0246649    0.189219     0.153392     0.0410527   -0.00815815   0.0110775    0.0316036   -0.0823558    -0.0278774    0.0361995    0.181778   -0.083513    -0.0427863    0.233483   -0.0170625    0.171049     0.101031     0.00590664   0.0617193    0.0821834 
  0.086494    0.148327     -0.0785755    0.0762917   -0.0418842   -0.00436158  -0.145023     0.0198848    0.107583     0.0225414    0.0805982    0.31008      0.0510256   -0.00636821   -0.156256     0.0532699    0.0667104  -0.0115979    0.0207396    0.0546045   0.10484      0.0501879    0.0805273    0.065085    -0.116155     0.0216964 
  0.0797844   0.00152652    0.0768399    0.134877     0.0954998    0.132885     0.00813677   0.06282     -0.0276002   -0.0225677    0.100663     0.0328007    0.118112    -0.000474425   0.0522159    0.0705265   -0.279186    0.0863849    0.0956942   -0.0458342   0.0748794    0.0547807   -0.0214397    0.104596    -0.213493     0.0596633 
  0.142268    0.00195381    0.00460948   0.0492679    0.0214867   -0.0351365   -0.0753648    0.0679532    0.0100769    0.0151746   -0.0595814   -0.035017    -0.163113     0.121241     -0.0334182   -0.0492308   -0.0860784   0.130607     0.141483     0.0677126  -0.0555502   -0.101115    -0.141228     0.108412     0.0325183   -0.0130867 
 -0.051774    0.00771804    0.0881991   -0.0231939   -0.0724381   -0.0831534   -0.162665    -0.00172366  -0.0210551   -0.0476447   -0.137229     0.1123       0.0305261    0.117919      0.0383252    0.101752     0.0687696  -0.00415719   0.030687     0.112358    0.00392975  -0.149426    -0.0776465    0.0146483    0.0620443   -0.00591504
  0.0117653   0.0414303    -0.00160865  -0.144743    -0.0822495   -0.14921      0.185029     0.124636    -0.124795     0.105193     0.132999    -0.0770115    0.0493405   -0.0792269    -0.0101639    0.232936     0.0325841   0.0791728    0.159842     0.0693818   0.0149236    4.68357e-5  -0.0382551   -0.130561    -0.200693    -0.0153391 
  0.0375686   0.0332507     0.298728    -0.0147661   -0.102089    -0.00931913   0.0119939    0.153489     0.0524035   -0.124076     0.0368569    0.0551246   -0.0769481    0.104729      0.259483    -0.10991     -0.123315    0.091316     0.00932125  -0.0795745  -0.0574816   -0.119214     0.148757    -0.0884404   -0.0168666    0.0606416 
  0.0927261  -0.045271     -0.172107     0.14416      0.045238    -0.0874868    0.0955299   -0.0852935   -0.216157    -0.0153139   -0.10596      0.00302196   0.0371666    0.0196486    -0.0438763    0.00985173  -0.0657425   0.013035     0.145776    -0.0243395   0.0310947    0.0238124   -0.125288    -0.0631572    0.0150136   -0.0403761 
 -0.0662134   0.0853644    -0.0939059   -0.0912496   -0.104056     0.00054592  -0.130057     0.0322653   -0.00890896  -0.1277      -0.122994     0.0224308    0.235125    -0.00966468   -0.0376343   -0.112386    -0.0621588   0.02976     -0.189672     0.239841   -0.0188658    0.146984    -0.0991185   -0.0910982    0.196262    -0.156779  
  0.0289773  -0.0386315     0.086075     0.0283045    0.00440516   0.0595496    0.0716928    0.00961736   0.0110869    0.0120541    0.109054    -0.0291095   -0.0964759    0.156297      0.0607329    0.0418288    0.0380967   0.0565547    0.167512    -0.0288789   0.175012     0.0913486   -0.00788133   0.138988     0.0368409   -0.0973829 
  0.0366739   0.0486947    -0.0760113   -0.184318     0.144951    -0.0507281   -0.0330743    0.104387    -0.114747     0.109832     0.01485     -0.0230891   -0.0892274    0.0431615     0.275929    -0.0592944   -0.0318612  -0.0156479    0.00308269  -0.0674655  -0.0206169   -0.0579818    0.0376929    0.0959363    0.0194327    0.111645  
 -0.102709    0.0659879    -0.0920525    0.131788     0.0437667    0.0064879   -0.0738283   -0.0689601    0.118765    -0.136735     0.0199148   -0.0573387   -0.126979     0.280872     -0.0798981   -0.105841     0.0208296   0.0295265   -0.134721    -0.118598   -0.0912359   -0.075703     0.0966412    0.11108     -0.28318     -0.06059   
 -0.0046465  -0.0314061    -0.0684697    0.0920773   -0.0951329    0.0524578   -0.0274275    0.0607336   -0.0610586   -0.026794    -0.057682     0.040521     0.0421461   -0.0528091     0.0693074   -0.129222     0.0197554  -0.0404696    0.148091     0.158695    0.0553974    0.0681743    0.177157     0.0336827   -0.111994    -0.0506675 
  0.108442    0.000292001   0.0661514   -0.0332307    0.10249      0.187493     0.0298286    0.0833146   -0.0582169   -0.0906824   -0.114009    -0.117291     0.120447    -0.0836271     0.04197      0.0730716   -0.205975   -0.048084    -0.0966387    0.088388    0.0216096    0.0915187   -0.13561     -0.0862239   -0.0742003    0.0366464 
 -0.153018    0.084136     -0.0280144    0.0362314   -0.0627429   -0.0670983   -0.0304246    0.0470728   -0.0791464    0.0672626   -0.0937105    0.242321     0.0140199   -0.052666     -0.0133693    0.11672      0.0625643   0.0751046   -0.0469058    0.180744   -0.0194554    0.223584    -0.00901354  -0.147984    -0.0295477    0.0592768 
 -0.177325   -0.12426      -0.0330663   -0.00227587  -0.0550675    0.0138567    0.0714688   -0.00120978  -0.0246945    0.0129546   -0.132725     0.0309724   -0.0681023    0.0271181    -0.0406645   -0.125443     0.0239453  -0.0135084    0.0915791    0.129888   -0.140276     0.111377     0.0441395   -0.0512749   -0.0143676    0.0819485 
  0.038952   -0.15001       0.0140011    0.0584511   -0.0386923    0.0619718   -0.0344815    0.0428081    0.0321062    0.0386698   -0.18084      0.0444       0.118412    -0.0489546    -0.0793033    0.092903     0.107028    0.0393461   -0.161027     0.0457966   0.0885203   -0.192507     0.00665078   0.164108     0.132707     0.0106886 
  0.109763    0.0271693     0.10619      0.0678508    0.0345466   -0.0706908    0.0538872    0.109181    -0.119028    -0.11904      0.0264942   -0.0487197   -0.0711349    0.00976651    0.00466667  -0.0499272    0.0363122   0.0960097   -0.0318146    0.171882   -0.128295    -0.185014     0.0242935    0.0298672    0.0531739   -0.0375577 
  0.0825727   0.00737719   -0.184487    -0.0345685   -0.0305034    0.141262     0.0389855   -0.0761364   -0.0243888   -0.051696    -0.316716    -0.110391     0.0435877    0.0608237    -0.00563335  -0.0642909   -0.0123383   0.104994     0.0237197   -0.0580432  -0.136641    -0.00401481   0.0833191    0.142365     0.00439798  -0.0276578 
 -0.0597893  -0.0121049     0.164976     0.0415626    0.00516683  -0.0319895   -0.00378156   0.114319     0.0102002   -0.0878868   -0.0193981   -0.108059    -0.111294     0.0264827    -0.0165064   -0.0557248    0.115562    0.0797853   -0.0798485   -0.167591   -0.00989079  -0.201486    -0.0221581   -0.0582512    0.0308023   -0.0766819 
  0.127865   -0.0807321     0.124105     0.0411828    0.0035534    0.0717916   -0.0371743    0.093961     0.238838    -0.13689      0.0736705    0.230782    -0.0894463    0.0431345     0.104316    -0.186536    -0.0795192  -0.316458     0.0608696   -0.128779    0.148127     0.0146797    0.0403328    0.0695996    0.0222949    0.191255  
 -0.156209    0.109323      0.0819958    0.0278468   -0.019791     0.0927823   -0.0560965    0.0670021   -0.0705483    0.117231     0.0650853    0.0068063    0.123352     0.0476874     0.0466156   -0.0240576   -0.105937   -0.0316512   -0.00355223   0.10519     0.217956     0.0160718    0.247384    -0.093022    -0.0460245   -0.0709695 
 -0.119535   -0.0103456    -0.112131    -0.0162113   -0.0421835   -0.130359     0.0320082   -0.199874     0.0239986   -0.046733    -0.0636876    0.0731977   -0.0849646    0.0214953    -0.0643464   -0.0986404   -0.100743    0.0414203    0.0261473    0.0258733  -0.0264998   -0.0576068    0.141143    -0.19675     -0.0227293   -0.164804  
  0.0100947  -0.0162431    -0.114315    -0.0490295   -0.0762241    0.0319954    0.0399276   -0.019142    -0.142266     0.154882     0.106137     0.0729491   -0.0853789   -0.103777     -0.0802194   -0.0163458   -0.0859078  -0.149636    -0.141834     0.0911468  -0.235483    -0.110766     0.0146919    0.0843477    0.17469      0.112506  
 -0.148799   -0.106331      0.0281528    0.123864     0.0348954   -0.0431751   -0.0695696    0.19959      0.14471      0.0122874    0.0794207    0.184746    -0.122015    -0.00808087   -0.0296183   -0.052785     0.0969158   0.0661292   -0.0263469   -0.0739317  -0.0352937   -0.119955     0.0370544    0.0162158    0.00460801   0.0764984 
 -0.0608555   0.18176       0.00594066  -0.0997797   -0.00909647   0.163642    -0.12371     -0.0101742   -0.211278    -0.079685    -0.0167883    0.127762    -0.00939362  -0.129234     -0.0311658    0.0154991    0.149209   -0.108036     0.0194117   -0.121522    0.0229556    0.00473428  -0.0198139   -0.135521     0.111289     0.0612137 
 -0.0333187  -0.0652901    -0.00390242   0.066011     0.0250347    0.0972081   -0.0324823    0.00526109  -0.0368257   -0.0997247    0.0576379   -0.18069      0.141252    -0.0260925    -0.016689    -0.065953    -0.0551614   0.121795    -0.0103897    0.173957   -0.0660071    0.0896081    0.0681972   -0.0417702    0.00959512   0.00491241
  0.131881    0.00925928    0.0782145    0.0332538    0.0506687   -0.118167     0.00617722   0.171151    -0.0635545   -0.103706     0.0560827    0.0582881   -0.138747    -0.108918     -0.194101     0.1305       0.125818    0.00495011  -0.137714     0.109077    0.122998    -0.131601     0.142372    -0.00772129   0.0673224    0.11084   
 -0.0720702   0.0771063     0.0645872    0.0511366   -0.035256    -0.0737309    0.00978382   0.0588788    0.0450461    0.193067     0.0517264    0.0458634    0.0425152    0.0442102    -0.0579759    0.169333     0.137696    0.0720179    0.0310757    0.0201072   0.154207     0.127863     0.139423    -0.165526     0.0950061   -0.185033  kind diag, method split
0: avll = -1.3817014449141314
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.381829
INFO: iteration 2, average log likelihood -1.381718
INFO: iteration 3, average log likelihood -1.380995
INFO: iteration 4, average log likelihood -1.372617
INFO: iteration 5, average log likelihood -1.353908
INFO: iteration 6, average log likelihood -1.348621
INFO: iteration 7, average log likelihood -1.347749
INFO: iteration 8, average log likelihood -1.347246
INFO: iteration 9, average log likelihood -1.346826
INFO: iteration 10, average log likelihood -1.346384
INFO: iteration 11, average log likelihood -1.345805
INFO: iteration 12, average log likelihood -1.345023
INFO: iteration 13, average log likelihood -1.344393
INFO: iteration 14, average log likelihood -1.344066
INFO: iteration 15, average log likelihood -1.343864
INFO: iteration 16, average log likelihood -1.343702
INFO: iteration 17, average log likelihood -1.343548
INFO: iteration 18, average log likelihood -1.343383
INFO: iteration 19, average log likelihood -1.343183
INFO: iteration 20, average log likelihood -1.342908
INFO: iteration 21, average log likelihood -1.342573
INFO: iteration 22, average log likelihood -1.342259
INFO: iteration 23, average log likelihood -1.341936
INFO: iteration 24, average log likelihood -1.341480
INFO: iteration 25, average log likelihood -1.340866
INFO: iteration 26, average log likelihood -1.340350
INFO: iteration 27, average log likelihood -1.340022
INFO: iteration 28, average log likelihood -1.339821
INFO: iteration 29, average log likelihood -1.339691
INFO: iteration 30, average log likelihood -1.339598
INFO: iteration 31, average log likelihood -1.339528
INFO: iteration 32, average log likelihood -1.339473
INFO: iteration 33, average log likelihood -1.339429
INFO: iteration 34, average log likelihood -1.339394
INFO: iteration 35, average log likelihood -1.339365
INFO: iteration 36, average log likelihood -1.339341
INFO: iteration 37, average log likelihood -1.339321
INFO: iteration 38, average log likelihood -1.339303
INFO: iteration 39, average log likelihood -1.339288
INFO: iteration 40, average log likelihood -1.339274
INFO: iteration 41, average log likelihood -1.339262
INFO: iteration 42, average log likelihood -1.339250
INFO: iteration 43, average log likelihood -1.339240
INFO: iteration 44, average log likelihood -1.339232
INFO: iteration 45, average log likelihood -1.339224
INFO: iteration 46, average log likelihood -1.339216
INFO: iteration 47, average log likelihood -1.339210
INFO: iteration 48, average log likelihood -1.339204
INFO: iteration 49, average log likelihood -1.339198
INFO: iteration 50, average log likelihood -1.339193
INFO: EM with 100000 data points 50 iterations avll -1.339193
952.4 data points per parameter
1: avll = [-1.38183,-1.38172,-1.38099,-1.37262,-1.35391,-1.34862,-1.34775,-1.34725,-1.34683,-1.34638,-1.34581,-1.34502,-1.34439,-1.34407,-1.34386,-1.3437,-1.34355,-1.34338,-1.34318,-1.34291,-1.34257,-1.34226,-1.34194,-1.34148,-1.34087,-1.34035,-1.34002,-1.33982,-1.33969,-1.3396,-1.33953,-1.33947,-1.33943,-1.33939,-1.33937,-1.33934,-1.33932,-1.3393,-1.33929,-1.33927,-1.33926,-1.33925,-1.33924,-1.33923,-1.33922,-1.33922,-1.33921,-1.3392,-1.3392,-1.33919]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.339361
INFO: iteration 2, average log likelihood -1.339196
INFO: iteration 3, average log likelihood -1.338445
INFO: iteration 4, average log likelihood -1.331472
INFO: iteration 5, average log likelihood -1.313808
INFO: iteration 6, average log likelihood -1.304636
INFO: iteration 7, average log likelihood -1.302169
INFO: iteration 8, average log likelihood -1.301002
INFO: iteration 9, average log likelihood -1.300267
INFO: iteration 10, average log likelihood -1.299751
INFO: iteration 11, average log likelihood -1.299364
INFO: iteration 12, average log likelihood -1.299056
INFO: iteration 13, average log likelihood -1.298798
INFO: iteration 14, average log likelihood -1.298572
INFO: iteration 15, average log likelihood -1.298364
INFO: iteration 16, average log likelihood -1.298169
INFO: iteration 17, average log likelihood -1.297980
INFO: iteration 18, average log likelihood -1.297810
INFO: iteration 19, average log likelihood -1.297668
INFO: iteration 20, average log likelihood -1.297562
INFO: iteration 21, average log likelihood -1.297486
INFO: iteration 22, average log likelihood -1.297433
INFO: iteration 23, average log likelihood -1.297395
INFO: iteration 24, average log likelihood -1.297369
INFO: iteration 25, average log likelihood -1.297349
INFO: iteration 26, average log likelihood -1.297334
INFO: iteration 27, average log likelihood -1.297322
INFO: iteration 28, average log likelihood -1.297312
INFO: iteration 29, average log likelihood -1.297304
INFO: iteration 30, average log likelihood -1.297298
INFO: iteration 31, average log likelihood -1.297292
INFO: iteration 32, average log likelihood -1.297287
INFO: iteration 33, average log likelihood -1.297282
INFO: iteration 34, average log likelihood -1.297278
INFO: iteration 35, average log likelihood -1.297275
INFO: iteration 36, average log likelihood -1.297271
INFO: iteration 37, average log likelihood -1.297268
INFO: iteration 38, average log likelihood -1.297265
INFO: iteration 39, average log likelihood -1.297262
INFO: iteration 40, average log likelihood -1.297258
INFO: iteration 41, average log likelihood -1.297255
INFO: iteration 42, average log likelihood -1.297252
INFO: iteration 43, average log likelihood -1.297249
INFO: iteration 44, average log likelihood -1.297246
INFO: iteration 45, average log likelihood -1.297242
INFO: iteration 46, average log likelihood -1.297239
INFO: iteration 47, average log likelihood -1.297235
INFO: iteration 48, average log likelihood -1.297231
INFO: iteration 49, average log likelihood -1.297226
INFO: iteration 50, average log likelihood -1.297222
INFO: EM with 100000 data points 50 iterations avll -1.297222
473.9 data points per parameter
2: avll = [-1.33936,-1.3392,-1.33844,-1.33147,-1.31381,-1.30464,-1.30217,-1.301,-1.30027,-1.29975,-1.29936,-1.29906,-1.2988,-1.29857,-1.29836,-1.29817,-1.29798,-1.29781,-1.29767,-1.29756,-1.29749,-1.29743,-1.2974,-1.29737,-1.29735,-1.29733,-1.29732,-1.29731,-1.2973,-1.2973,-1.29729,-1.29729,-1.29728,-1.29728,-1.29727,-1.29727,-1.29727,-1.29726,-1.29726,-1.29726,-1.29726,-1.29725,-1.29725,-1.29725,-1.29724,-1.29724,-1.29723,-1.29723,-1.29723,-1.29722]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.297424
INFO: iteration 2, average log likelihood -1.297224
INFO: iteration 3, average log likelihood -1.296601
INFO: iteration 4, average log likelihood -1.291165
INFO: iteration 5, average log likelihood -1.273689
INFO: iteration 6, average log likelihood -1.258320
INFO: iteration 7, average log likelihood -1.250859
INFO: iteration 8, average log likelihood -1.246769
INFO: iteration 9, average log likelihood -1.244728
INFO: iteration 10, average log likelihood -1.243301
INFO: iteration 11, average log likelihood -1.241782
INFO: iteration 12, average log likelihood -1.240031
INFO: iteration 13, average log likelihood -1.238378
INFO: iteration 14, average log likelihood -1.237003
INFO: iteration 15, average log likelihood -1.235751
INFO: iteration 16, average log likelihood -1.234565
INFO: iteration 17, average log likelihood -1.233605
INFO: iteration 18, average log likelihood -1.233048
INFO: iteration 19, average log likelihood -1.232772
INFO: iteration 20, average log likelihood -1.232607
INFO: iteration 21, average log likelihood -1.232482
INFO: iteration 22, average log likelihood -1.232376
INFO: iteration 23, average log likelihood -1.232281
INFO: iteration 24, average log likelihood -1.232199
INFO: iteration 25, average log likelihood -1.232127
INFO: iteration 26, average log likelihood -1.232068
INFO: iteration 27, average log likelihood -1.232022
INFO: iteration 28, average log likelihood -1.231988
INFO: iteration 29, average log likelihood -1.231965
INFO: iteration 30, average log likelihood -1.231948
INFO: iteration 31, average log likelihood -1.231935
INFO: iteration 32, average log likelihood -1.231926
INFO: iteration 33, average log likelihood -1.231918
INFO: iteration 34, average log likelihood -1.231912
INFO: iteration 35, average log likelihood -1.231906
INFO: iteration 36, average log likelihood -1.231902
INFO: iteration 37, average log likelihood -1.231898
INFO: iteration 38, average log likelihood -1.231894
INFO: iteration 39, average log likelihood -1.231891
INFO: iteration 40, average log likelihood -1.231889
INFO: iteration 41, average log likelihood -1.231887
INFO: iteration 42, average log likelihood -1.231885
INFO: iteration 43, average log likelihood -1.231884
INFO: iteration 44, average log likelihood -1.231882
INFO: iteration 45, average log likelihood -1.231881
INFO: iteration 46, average log likelihood -1.231880
INFO: iteration 47, average log likelihood -1.231880
INFO: iteration 48, average log likelihood -1.231879
INFO: iteration 49, average log likelihood -1.231878
INFO: iteration 50, average log likelihood -1.231878
INFO: EM with 100000 data points 50 iterations avll -1.231878
236.4 data points per parameter
3: avll = [-1.29742,-1.29722,-1.2966,-1.29116,-1.27369,-1.25832,-1.25086,-1.24677,-1.24473,-1.2433,-1.24178,-1.24003,-1.23838,-1.237,-1.23575,-1.23457,-1.23361,-1.23305,-1.23277,-1.23261,-1.23248,-1.23238,-1.23228,-1.2322,-1.23213,-1.23207,-1.23202,-1.23199,-1.23196,-1.23195,-1.23194,-1.23193,-1.23192,-1.23191,-1.23191,-1.2319,-1.2319,-1.23189,-1.23189,-1.23189,-1.23189,-1.23189,-1.23188,-1.23188,-1.23188,-1.23188,-1.23188,-1.23188,-1.23188,-1.23188]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.232122
INFO: iteration 2, average log likelihood -1.231818
INFO: iteration 3, average log likelihood -1.228892
WARNING: Variances had to be floored 2 7
INFO: iteration 4, average log likelihood -1.207076
WARNING: Variances had to be floored 1 2 7
INFO: iteration 5, average log likelihood -1.181179
WARNING: Variances had to be floored 3 4 7
INFO: iteration 6, average log likelihood -1.171001
WARNING: Variances had to be floored 2 7
INFO: iteration 7, average log likelihood -1.167953
WARNING: Variances had to be floored 1 2 4 7
INFO: iteration 8, average log likelihood -1.155313
WARNING: Variances had to be floored 3
INFO: iteration 9, average log likelihood -1.163453
WARNING: Variances had to be floored 2 4 7
INFO: iteration 10, average log likelihood -1.154206
WARNING: Variances had to be floored 1 2
INFO: iteration 11, average log likelihood -1.147500
WARNING: Variances had to be floored 3 4 7
INFO: iteration 12, average log likelihood -1.148492
WARNING: Variances had to be floored 2
INFO: iteration 13, average log likelihood -1.156992
WARNING: Variances had to be floored 1 2 4 7
INFO: iteration 14, average log likelihood -1.143635
WARNING: Variances had to be floored 3
INFO: iteration 15, average log likelihood -1.156308
WARNING: Variances had to be floored 2 4 7
INFO: iteration 16, average log likelihood -1.148437
WARNING: Variances had to be floored 1 2
INFO: iteration 17, average log likelihood -1.143082
WARNING: Variances had to be floored 3 4 7
INFO: iteration 18, average log likelihood -1.144713
WARNING: Variances had to be floored 2
INFO: iteration 19, average log likelihood -1.152937
WARNING: Variances had to be floored 1 2 4 7
INFO: iteration 20, average log likelihood -1.139483
WARNING: Variances had to be floored 3 5
INFO: iteration 21, average log likelihood -1.152230
WARNING: Variances had to be floored 2 4 7
INFO: iteration 22, average log likelihood -1.149948
WARNING: Variances had to be floored 1 2 7
INFO: iteration 23, average log likelihood -1.142377
WARNING: Variances had to be floored 3 4
INFO: iteration 24, average log likelihood -1.147186
WARNING: Variances had to be floored 2 7
INFO: iteration 25, average log likelihood -1.149181
WARNING: Variances had to be floored 1 2 4
INFO: iteration 26, average log likelihood -1.142459
WARNING: Variances had to be floored 3 7
INFO: iteration 27, average log likelihood -1.148796
WARNING: Variances had to be floored 2 4
INFO: iteration 28, average log likelihood -1.147622
WARNING: Variances had to be floored 1 2 7
INFO: iteration 29, average log likelihood -1.136399
WARNING: Variances had to be floored 3 4 5
INFO: iteration 30, average log likelihood -1.145133
WARNING: Variances had to be floored 2 7
INFO: iteration 31, average log likelihood -1.152980
WARNING: Variances had to be floored 1 2 4 7
INFO: iteration 32, average log likelihood -1.143794
WARNING: Variances had to be floored 3
INFO: iteration 33, average log likelihood -1.153100
WARNING: Variances had to be floored 2 4 7
INFO: iteration 34, average log likelihood -1.145309
WARNING: Variances had to be floored 1 2
INFO: iteration 35, average log likelihood -1.140197
WARNING: Variances had to be floored 3 4 7
INFO: iteration 36, average log likelihood -1.142024
WARNING: Variances had to be floored 2
INFO: iteration 37, average log likelihood -1.150685
WARNING: Variances had to be floored 1 2 4 7
INFO: iteration 38, average log likelihood -1.137717
WARNING: Variances had to be floored 3 5 7
INFO: iteration 39, average log likelihood -1.150523
WARNING: Variances had to be floored 2 4 7
INFO: iteration 40, average log likelihood -1.151029
WARNING: Variances had to be floored 1 2 7
INFO: iteration 41, average log likelihood -1.138882
WARNING: Variances had to be floored 3 4 7
INFO: iteration 42, average log likelihood -1.143299
WARNING: Variances had to be floored 2
INFO: iteration 43, average log likelihood -1.148505
WARNING: Variances had to be floored 1 2 4 7
INFO: iteration 44, average log likelihood -1.135252
WARNING: Variances had to be floored 3 7
INFO: iteration 45, average log likelihood -1.148011
WARNING: Variances had to be floored 2 4
INFO: iteration 46, average log likelihood -1.143738
WARNING: Variances had to be floored 1 2 7
INFO: iteration 47, average log likelihood -1.132517
WARNING: Variances had to be floored 3 4 5 7
INFO: iteration 48, average log likelihood -1.141213
WARNING: Variances had to be floored 2 7
INFO: iteration 49, average log likelihood -1.152274
WARNING: Variances had to be floored 1 2 4 7
INFO: iteration 50, average log likelihood -1.139774
INFO: EM with 100000 data points 50 iterations avll -1.139774
118.1 data points per parameter
4: avll = [-1.23212,-1.23182,-1.22889,-1.20708,-1.18118,-1.171,-1.16795,-1.15531,-1.16345,-1.15421,-1.1475,-1.14849,-1.15699,-1.14364,-1.15631,-1.14844,-1.14308,-1.14471,-1.15294,-1.13948,-1.15223,-1.14995,-1.14238,-1.14719,-1.14918,-1.14246,-1.1488,-1.14762,-1.1364,-1.14513,-1.15298,-1.14379,-1.1531,-1.14531,-1.1402,-1.14202,-1.15069,-1.13772,-1.15052,-1.15103,-1.13888,-1.1433,-1.1485,-1.13525,-1.14801,-1.14374,-1.13252,-1.14121,-1.15227,-1.13977]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 5 6 13 14
INFO: iteration 1, average log likelihood -1.149383
WARNING: Variances had to be floored 3 4 5 6 7 8 13 14
INFO: iteration 2, average log likelihood -1.133142
WARNING: Variances had to be floored 1 2 3 4 5 6 13
INFO: iteration 3, average log likelihood -1.132645
WARNING: Variances had to be floored 5 6 7 8 14
INFO: iteration 4, average log likelihood -1.129770
WARNING: Variances had to be floored 3 4 5 6 9 14
INFO: iteration 5, average log likelihood -1.093506
WARNING: Variances had to be floored 1 2 3 4 5 6 7 8 10 13 22 31
INFO: iteration 6, average log likelihood -1.065223
WARNING: Variances had to be floored 5 6 14 21
INFO: iteration 7, average log likelihood -1.090246
WARNING: Variances had to be floored 3 4 5 6 7 8 14
INFO: iteration 8, average log likelihood -1.070154
WARNING: Variances had to be floored 1 2 3 4 5 6 10 12 13 22 31
INFO: iteration 9, average log likelihood -1.054360
WARNING: Variances had to be floored 5 6 7 8 13 14
INFO: iteration 10, average log likelihood -1.080165
WARNING: Variances had to be floored 3 4 5 6 9 13 14 21
INFO: iteration 11, average log likelihood -1.053563
WARNING: Variances had to be floored 1 2 3 4 5 6 7 8 10 22 31
INFO: iteration 12, average log likelihood -1.062256
WARNING: Variances had to be floored 5 6 13 14
INFO: iteration 13, average log likelihood -1.084002
WARNING: Variances had to be floored 3 4 5 6 7 8 13 14
INFO: iteration 14, average log likelihood -1.050280
WARNING: Variances had to be floored 1 2 3 4 5 6 9 10 13 14 21 22 31
INFO: iteration 15, average log likelihood -1.042341
WARNING: Variances had to be floored 5 6 7 8 14
INFO: iteration 16, average log likelihood -1.086654
WARNING: Variances had to be floored 3 4 5 6 13
INFO: iteration 17, average log likelihood -1.066545
WARNING: Variances had to be floored 1 2 3 4 5 6 7 8 12 13 31
INFO: iteration 18, average log likelihood -1.049623
WARNING: Variances had to be floored 5 6 10 13 14 22
INFO: iteration 19, average log likelihood -1.077128
WARNING: Variances had to be floored 3 4 5 6 7 8 13 14 21
INFO: iteration 20, average log likelihood -1.055207
WARNING: Variances had to be floored 1 2 3 4 5 6 9 13 14 31
INFO: iteration 21, average log likelihood -1.055840
WARNING: Variances had to be floored 5 6 7 8 10 22
INFO: iteration 22, average log likelihood -1.074979
WARNING: Variances had to be floored 3 4 5 6 13 14
INFO: iteration 23, average log likelihood -1.061177
WARNING: Variances had to be floored 1 2 3 4 5 6 7 8 13 14 21 31
INFO: iteration 24, average log likelihood -1.042507
WARNING: Variances had to be floored 5 6 10 13 14 22
INFO: iteration 25, average log likelihood -1.073706
WARNING: Variances had to be floored 3 4 5 6 7 8 9 12 13 14
INFO: iteration 26, average log likelihood -1.051839
WARNING: Variances had to be floored 1 2 3 4 5 6 31
INFO: iteration 27, average log likelihood -1.058929
WARNING: Variances had to be floored 5 6 7 8 10 13 14 21 22
INFO: iteration 28, average log likelihood -1.066184
WARNING: Variances had to be floored 3 4 5 6 13 14
INFO: iteration 29, average log likelihood -1.070397
WARNING: Variances had to be floored 1 2 3 4 5 6 7 8 13 14 31
INFO: iteration 30, average log likelihood -1.053582
WARNING: Variances had to be floored 5 6 9 10 13 14
INFO: iteration 31, average log likelihood -1.069554
WARNING: Variances had to be floored 3 4 5 6 7 8 22
INFO: iteration 32, average log likelihood -1.053610
WARNING: Variances had to be floored 1 2 3 4 5 6 13 14 21 31
INFO: iteration 33, average log likelihood -1.052895
WARNING: Variances had to be floored 5 6 7 8 12 13 14
INFO: iteration 34, average log likelihood -1.078240
WARNING: Variances had to be floored 3 4 5 6 10 13 14 22
INFO: iteration 35, average log likelihood -1.062340
WARNING: Variances had to be floored 1 2 3 4 5 6 7 8 13 14 31
INFO: iteration 36, average log likelihood -1.051233
WARNING: Variances had to be floored 5 6 9 13 14 21
INFO: iteration 37, average log likelihood -1.066022
WARNING: Variances had to be floored 3 4 5 6 7 8 10 14 22
INFO: iteration 38, average log likelihood -1.063084
WARNING: Variances had to be floored 1 2 3 4 5 6 13 31
INFO: iteration 39, average log likelihood -1.059533
WARNING: Variances had to be floored 5 6 7 8 13 14
INFO: iteration 40, average log likelihood -1.061689
WARNING: Variances had to be floored 2 3 4 5 6 7 9 10 13 14 21 22
INFO: iteration 41, average log likelihood -1.034563
WARNING: Variances had to be floored 1 3 4 5 6 8 12 14 31
INFO: iteration 42, average log likelihood -1.054830
WARNING: Variances had to be floored 2 5 6 7 13
INFO: iteration 43, average log likelihood -1.079192
WARNING: Variances had to be floored 1 3 4 5 6 8 10 13
INFO: iteration 44, average log likelihood -1.048269
WARNING: Variances had to be floored 2 5 6 7 13 22 31
INFO: iteration 45, average log likelihood -1.054901
WARNING: Variances had to be floored 1 3 4 5 6 8 13 21
INFO: iteration 46, average log likelihood -1.045094
WARNING: Variances had to be floored 2 5 6 7 9 10 12 13 14
INFO: iteration 47, average log likelihood -1.055559
WARNING: Variances had to be floored 1 3 4 5 6 8 13 22 31
INFO: iteration 48, average log likelihood -1.052250
WARNING: Variances had to be floored 2 5 6 7 14
INFO: iteration 49, average log likelihood -1.070155
WARNING: Variances had to be floored 1 3 4 5 6 8 10 14 21
INFO: iteration 50, average log likelihood -1.041099
INFO: EM with 100000 data points 50 iterations avll -1.041099
59.0 data points per parameter
5: avll = [-1.14938,-1.13314,-1.13265,-1.12977,-1.09351,-1.06522,-1.09025,-1.07015,-1.05436,-1.08016,-1.05356,-1.06226,-1.084,-1.05028,-1.04234,-1.08665,-1.06654,-1.04962,-1.07713,-1.05521,-1.05584,-1.07498,-1.06118,-1.04251,-1.07371,-1.05184,-1.05893,-1.06618,-1.0704,-1.05358,-1.06955,-1.05361,-1.05289,-1.07824,-1.06234,-1.05123,-1.06602,-1.06308,-1.05953,-1.06169,-1.03456,-1.05483,-1.07919,-1.04827,-1.0549,-1.04509,-1.05556,-1.05225,-1.07016,-1.0411]
[-1.3817,-1.38183,-1.38172,-1.38099,-1.37262,-1.35391,-1.34862,-1.34775,-1.34725,-1.34683,-1.34638,-1.34581,-1.34502,-1.34439,-1.34407,-1.34386,-1.3437,-1.34355,-1.34338,-1.34318,-1.34291,-1.34257,-1.34226,-1.34194,-1.34148,-1.34087,-1.34035,-1.34002,-1.33982,-1.33969,-1.3396,-1.33953,-1.33947,-1.33943,-1.33939,-1.33937,-1.33934,-1.33932,-1.3393,-1.33929,-1.33927,-1.33926,-1.33925,-1.33924,-1.33923,-1.33922,-1.33922,-1.33921,-1.3392,-1.3392,-1.33919,-1.33936,-1.3392,-1.33844,-1.33147,-1.31381,-1.30464,-1.30217,-1.301,-1.30027,-1.29975,-1.29936,-1.29906,-1.2988,-1.29857,-1.29836,-1.29817,-1.29798,-1.29781,-1.29767,-1.29756,-1.29749,-1.29743,-1.2974,-1.29737,-1.29735,-1.29733,-1.29732,-1.29731,-1.2973,-1.2973,-1.29729,-1.29729,-1.29728,-1.29728,-1.29727,-1.29727,-1.29727,-1.29726,-1.29726,-1.29726,-1.29726,-1.29725,-1.29725,-1.29725,-1.29724,-1.29724,-1.29723,-1.29723,-1.29723,-1.29722,-1.29742,-1.29722,-1.2966,-1.29116,-1.27369,-1.25832,-1.25086,-1.24677,-1.24473,-1.2433,-1.24178,-1.24003,-1.23838,-1.237,-1.23575,-1.23457,-1.23361,-1.23305,-1.23277,-1.23261,-1.23248,-1.23238,-1.23228,-1.2322,-1.23213,-1.23207,-1.23202,-1.23199,-1.23196,-1.23195,-1.23194,-1.23193,-1.23192,-1.23191,-1.23191,-1.2319,-1.2319,-1.23189,-1.23189,-1.23189,-1.23189,-1.23189,-1.23188,-1.23188,-1.23188,-1.23188,-1.23188,-1.23188,-1.23188,-1.23188,-1.23212,-1.23182,-1.22889,-1.20708,-1.18118,-1.171,-1.16795,-1.15531,-1.16345,-1.15421,-1.1475,-1.14849,-1.15699,-1.14364,-1.15631,-1.14844,-1.14308,-1.14471,-1.15294,-1.13948,-1.15223,-1.14995,-1.14238,-1.14719,-1.14918,-1.14246,-1.1488,-1.14762,-1.1364,-1.14513,-1.15298,-1.14379,-1.1531,-1.14531,-1.1402,-1.14202,-1.15069,-1.13772,-1.15052,-1.15103,-1.13888,-1.1433,-1.1485,-1.13525,-1.14801,-1.14374,-1.13252,-1.14121,-1.15227,-1.13977,-1.14938,-1.13314,-1.13265,-1.12977,-1.09351,-1.06522,-1.09025,-1.07015,-1.05436,-1.08016,-1.05356,-1.06226,-1.084,-1.05028,-1.04234,-1.08665,-1.06654,-1.04962,-1.07713,-1.05521,-1.05584,-1.07498,-1.06118,-1.04251,-1.07371,-1.05184,-1.05893,-1.06618,-1.0704,-1.05358,-1.06955,-1.05361,-1.05289,-1.07824,-1.06234,-1.05123,-1.06602,-1.06308,-1.05953,-1.06169,-1.03456,-1.05483,-1.07919,-1.04827,-1.0549,-1.04509,-1.05556,-1.05225,-1.07016,-1.0411]
32×26 Array{Float64,2}:
 -0.132068    -0.0113509    -0.112694    -0.035371    -0.0360698    -0.128413     0.0322668   -0.191468     -0.00515141  -0.0151507   -0.0705502    0.0747253   -0.0613568    0.0172505   -0.062629    -0.0953437   -0.102828    0.0410096   0.00954201   0.0158327  -0.0302046    -0.060262      0.136914    -0.196111   -0.0215085   -0.173894  
  0.0188423    0.119622     -0.0497017    0.00871107  -0.00430336   -0.112078     0.0291734    0.188055      0.177827    -0.124501    -0.0728543    0.207843     0.0244783   -0.124639    -0.122803     0.211103     0.0668938  -0.0999894  -0.369502     0.0304965   0.170908      0.00689427    0.0521212   -0.104343   -0.0148016    0.12194   
 -0.00497053   0.154607     -0.228632     0.165708    -0.655229     -0.10648     -0.00742683   0.199602      0.0193013   -0.120693    -0.0890629    0.206669    -0.154816     0.175885     0.286526     0.272531     0.0318061  -0.124887   -0.0599735    0.0282685   0.222514     -0.117983      0.0438083   -0.152163    0.182133     0.410354  
 -0.00488401   0.14703       0.247315     0.23102      0.77746      -0.118606    -0.0142335    0.24992       0.0277443   -0.119171    -0.0642557    0.206301    -0.0990909   -0.0261295   -0.153365     0.228479     0.0261274  -0.0322468  -0.0143082    0.032469    0.216692     -0.0679527     0.0416429   -0.193307    0.428872     0.0883992 
  0.0136418    0.0431755     0.01462     -0.118988     0.105935     -0.136637     0.173241     0.125856     -0.124062     0.105357     0.1275      -0.0186969    0.06824     -0.0777998   -0.0882327    0.238839     0.0375374   0.0500589   0.139455     0.0704072   0.000534335   0.00044358   -0.196295    -0.411403   -0.19999     -0.0142908 
  0.00821601   0.0422193    -0.0117703   -0.18066     -0.308843     -0.171579     0.184376     0.121456     -0.124423     0.106363     0.133257    -0.139011     0.0296956   -0.0696651    0.115964     0.220891     0.0105944   0.105507    0.18706      0.0775413   0.0454757    -0.00110045    0.201955     0.190126   -0.198599    -0.0140005 
 -0.0641488    0.12522       0.00585131   0.185431    -0.0556408     0.208772    -0.0980566   -0.00161735    0.297325    -0.167818    -0.494548     0.0860672   -0.00339847  -0.119464    -0.0381627    0.0468781    0.154048   -0.0603724   0.0480185   -0.082151   -0.00671028   -0.107838     -0.053021    -0.0839936   0.147806    -0.00888951
 -0.0592752    0.186137      0.0058429   -0.086033    -0.0171357     0.165527    -0.116808    -0.0165533    -0.176928    -0.0855049    0.00096733   0.128231    -0.0100775   -0.124219    -0.0365583    0.0156349    0.14109    -0.115888    0.021113    -0.129339    0.0309392     0.00368281    0.0309502   -0.118854    0.0912255    0.0627439 
  0.113609    -0.0159578     0.030876    -0.00116649   0.0721497     0.0761507    0.0972867   -0.000621757  -0.0630081   -0.1065      -0.102775    -0.126486     0.100856    -0.0580382    0.0563576    0.00603298  -0.139941   -0.0715779  -0.0601905    0.0888208   0.0149957     0.0618707    -0.128143    -0.193487   -0.0542104    0.0223899 
  0.100773    -0.0501271    -0.206495     0.176111    -9.89969e-5   -0.0866737    0.103458    -0.0931285    -0.264374    -0.0148865   -0.111293     0.00803528   0.0140901    0.00941346  -0.0714883   -0.00287005  -0.0654002   0.044523    0.19755     -0.040866    0.0311524     0.0268985    -0.116671    -0.0532232   0.0410376   -0.0759088 
 -0.113083    -0.102374      0.021417     0.125224     0.0458879    -0.0484934   -0.0654862    0.210041      0.153724    -0.0353367    0.0791128    0.203251    -0.113967    -0.0135921   -0.101868    -0.0548518    0.0982936   0.0669369  -0.0319067   -0.0689687  -0.0534154    -0.127924      0.01091      0.0588062   0.00415711   0.0663883 
  0.0591361    0.00421365    0.072007     0.134013     0.105954      0.121159    -0.0098819    0.073569     -0.00285391  -0.0189324    0.0971075    0.0324262    0.119292     0.00241035   0.0306826    0.0620973   -0.278563    0.0875857   0.0980029   -0.0491399   0.0715428     0.0803276    -0.0210589    0.111208   -0.201904     0.0702396 
  0.00982205  -0.0443709    -0.0406859    0.0588611   -0.450207      0.0560075   -0.0197564    0.0541631    -0.121565     0.112422    -0.0626241   -0.0128124    0.0760318   -0.0370123    0.059916    -0.108807    -0.0858923  -0.20696     0.111581     0.129273    0.0659764     0.0739886     0.125379    -0.172354   -0.0732642   -0.0456492 
 -0.0529656   -0.0414612    -0.0812346    0.0998095   -0.000843207   0.0519563   -0.0293263    0.0813859    -0.0308582    0.042835    -0.0504543    0.0257209    0.0401965   -0.047491     0.0689002   -0.173385     0.013914    0.0103227   0.13696      0.155213    0.0533901     0.0633591     0.187161     0.0722972  -0.107342    -0.0507725 
  0.0268137   -0.0336132     0.0791669    0.0609729   -0.038838      0.0656669    0.0721771   -0.0246766    -0.00792491  -0.00542439   0.108467    -0.0530732   -0.0912024    0.156812     0.0767354    0.0488828    0.0277903   0.051717    0.177691    -0.0433069   0.159691      0.0945423    -0.0454739    0.140614    0.0568398   -0.0958297 
 -0.0707228    0.0695249     0.0700773    0.0434911   -0.0345184    -0.075812     0.0269874    0.0567372     0.0507997    0.176616     0.047703     0.0541962    0.0418732    0.0761638   -0.0645029    0.167445     0.13642     0.0709947   0.0251478    0.0178479   0.16011       0.116683      0.141635    -0.146226    0.100081    -0.195086  
 -0.121381     0.0271292     0.00323888  -0.0099372   -0.0265607     0.0665557    0.01655      0.0266897    -0.0778325    0.12458      0.0725713    0.035109     0.0395974   -0.025429    -0.00869465  -0.0413409   -0.101097   -0.0870263  -0.0846768    0.106071    0.00582546   -0.0317204     0.112893    -0.0087577   0.0508954    0.0151238 
 -0.152729     0.100752     -0.0110673    0.033795    -0.0959894    -0.0736873   -0.0294774    0.0302301    -0.0785343    0.0673187   -0.0954503    0.248583     0.0169425   -0.0518696   -0.00831964   0.109348     0.0594534   0.0751443  -0.0438975    0.169923   -0.0107122     0.208818     -0.00433151  -0.146942   -0.0095535    0.101838  
 -0.0746398   -0.232189      0.0220325    0.0170848   -0.0100196     0.344994     0.0670802    0.118958      0.00281342   0.0768639   -0.174334    -0.00956474   0.118262    -0.121701    -0.065843     0.0954012    0.131974   -0.278046   -0.179439     0.0416764   0.01296      -0.173298      0.1573       0.172232    0.077508    -0.025866  
  0.259599    -0.0380143     0.0322958    0.100734    -0.090204     -0.180261    -0.165019    -0.0351594     0.0682585   -0.00772021  -0.190504     0.102933     0.118526    -0.0128368   -0.0797037    0.0903797    0.148637    0.379896   -0.133663     0.0537034   0.196331     -0.190131     -0.0636327    0.152602    0.219344     0.0771103 
 -0.0298046   -0.0889612     0.188582    -0.150823    -0.18434      -0.0185156    0.0389247   -0.0443039     0.0656284   -0.0019789    0.0687328    0.0542668    0.114224    -0.0584207   -0.117036     0.0108141    0.0339077  -0.134788   -0.0619355    0.0558846  -0.0855195    -0.0558815     0.195989    -0.0597625   0.0662937    0.0372942 
  0.0649476    0.000325975   0.285971    -0.036806    -0.0636022    -0.00828776   0.0131113    0.151697      0.0569358   -0.120743     0.0145775    0.0539628   -0.0673394    0.0426896    0.286926    -0.0987719   -0.131918    0.0951926   0.019355    -0.0565703  -0.0586763    -0.134293      0.145954    -0.0839514  -0.0238982    0.0418201 
  0.209968    -0.0778253     0.125318     0.10454      0.00465358    0.0934809   -0.0297594    0.0938219     0.242738    -0.120721    -0.122061     0.226155    -0.0880539   -0.0998182    0.231563    -0.31515     -0.0357687  -0.431003    0.161927    -0.130405   -0.180992      0.0336246     0.0174659    0.134679    0.0328842    0.223049  
  0.110494    -0.0800739     0.122442    -0.00441113   0.00280272    0.052319    -0.0501983    0.10467       0.236385    -0.16689      0.261474     0.23873     -0.0903838    0.231817    -0.0987403   -0.131996    -0.14743    -0.217311   -0.0786087   -0.126732    0.42706       0.000137494   0.0495118    0.021739    0.033939     0.163945  
  0.0944523    0.0638946     0.0603672    0.0468526    0.0434965    -0.0694648   -0.0721898    0.127867      0.0128331   -0.0651491    0.0298013   -0.0110566    0.027641    -0.0924024   -0.138798     0.0960485    0.0603019  -0.0566099  -0.105592     0.125453   -0.0239556    -0.0176616     0.0305266    0.0120781   0.0400733    0.025743  
  0.065387     0.078631     -0.0814348   -0.0543244    0.0458909    -0.0820899   -0.0986647    0.0674058     0.0166126    0.0666416    0.0602143    0.129087    -0.0257885    0.0257761    0.0418295    0.00229669   0.0578205  -0.0611617   0.0369232    0.0350866   0.0486325    -0.000178478   0.0650164    0.0614466  -0.0383525    0.0747517 
 -0.147675    -0.0671697     0.0177241   -0.0284758   -0.0701606    -0.0351244   -0.0417894   -0.0268079    -0.0209143   -0.0122863   -0.134936     0.0619765   -0.0215817    0.067985    -0.0118771   -0.0219376    0.0605047   0.0106521   0.0529429    0.120571   -0.0842353    -0.0228779    -0.0126964   -0.0127075   0.0241822    0.037497  
  0.0350419    0.00119603   -0.00836358   0.00314679  -0.0126952     0.0541332    0.0259118    0.035148     -0.00257086  -0.0510316   -0.170513    -0.102003    -0.0159172    0.0334413   -0.00357597  -0.0633143    0.0762695   0.0691222  -0.0315133   -0.0902665  -0.0784497    -0.0894787     0.0300204    0.0233121   0.0178938   -0.0557283 
  0.101353     0.0434861     0.107424     0.0639106    0.0384714    -0.0565536    0.0513163    0.113956     -0.113565    -0.119865     0.0109415   -0.0496287   -0.0715619   -0.0214311    0.00810284  -0.0649905    0.0322529   0.0973299  -0.0321123    0.174057   -0.122268     -0.182589      0.0306408    0.033466    0.0398799   -0.0332068 
  0.00585529   0.0218539    -0.0479086    0.0994629    0.0351749    -0.021762    -0.0735304    0.00857717    0.0575728   -0.0381694   -0.0394337   -0.0425066   -0.124842     0.191068    -0.0690529   -0.0507219   -0.0463325   0.064638   -0.00336612  -0.0202694  -0.0690423    -0.057126     -0.0305358    0.105132   -0.11738     -0.00998663
 -0.0270598   -0.0501598     0.0119332    0.0337637    0.025794      0.11713     -0.026841     0.0216556    -0.0228535   -0.0939141    0.0823837   -0.179319     0.136605    -0.0286126   -0.0131358   -0.065574    -0.0393058   0.107615    0.0443031    0.193149   -0.0537425     0.0886144     0.0545981   -0.0411998   0.00886107  -0.00463135
 -0.0678638    0.0740719    -0.11201     -0.0914468   -0.124413      0.051994    -0.13806      0.029404      0.0169508   -0.149036    -0.148045     0.0608717    0.225897    -0.00599556  -0.0369998   -0.106737    -0.063084    0.0155668  -0.189842     0.241657   -0.0173564     0.162074     -0.0976617   -0.091485    0.197486    -0.170606  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 2 5 6 7 14 22 31
INFO: iteration 1, average log likelihood -1.065603
WARNING: Variances had to be floored 1 3 4 5 6 8 12 13 14 22 31
INFO: iteration 2, average log likelihood -1.034020
WARNING: Variances had to be floored 5 6 7 9 10 13 14 21 22 31
INFO: iteration 3, average log likelihood -1.044633
WARNING: Variances had to be floored 1 2 3 4 5 6 8 12 14 22 31
INFO: iteration 4, average log likelihood -1.047476
WARNING: Variances had to be floored 2 5 6 7 13 22 31
INFO: iteration 5, average log likelihood -1.049766
WARNING: Variances had to be floored 1 3 4 5 6 8 9 10 12 13 14 21 22 31
INFO: iteration 6, average log likelihood -1.026514
WARNING: Variances had to be floored 2 5 6 14 22 31
INFO: iteration 7, average log likelihood -1.064135
WARNING: Variances had to be floored 1 3 4 5 6 7 8 12 13 22 31
INFO: iteration 8, average log likelihood -1.032281
WARNING: Variances had to be floored 2 5 6 7 9 10 13 14 21 22 31
INFO: iteration 9, average log likelihood -1.040717
WARNING: Variances had to be floored 1 2 3 4 5 6 8 12 14 22 31
INFO: iteration 10, average log likelihood -1.047054
INFO: EM with 100000 data points 10 iterations avll -1.047054
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.565014e+05
      1       6.448496e+05      -2.116517e+05 |       32
      2       6.108467e+05      -3.400290e+04 |       32
      3       5.949840e+05      -1.586271e+04 |       32
      4       5.867504e+05      -8.233677e+03 |       32
      5       5.819629e+05      -4.787425e+03 |       32
      6       5.782320e+05      -3.730918e+03 |       32
      7       5.762747e+05      -1.957265e+03 |       32
      8       5.752509e+05      -1.023805e+03 |       32
      9       5.744744e+05      -7.765106e+02 |       32
     10       5.738660e+05      -6.084397e+02 |       32
     11       5.734647e+05      -4.013352e+02 |       32
     12       5.731859e+05      -2.787878e+02 |       32
     13       5.729446e+05      -2.412917e+02 |       32
     14       5.727165e+05      -2.280592e+02 |       32
     15       5.725103e+05      -2.061771e+02 |       32
     16       5.723004e+05      -2.099021e+02 |       32
     17       5.720749e+05      -2.255674e+02 |       31
     18       5.718277e+05      -2.471338e+02 |       31
     19       5.715645e+05      -2.632152e+02 |       32
     20       5.713257e+05      -2.387995e+02 |       31
     21       5.710992e+05      -2.264958e+02 |       32
     22       5.709463e+05      -1.529605e+02 |       31
     23       5.708661e+05      -8.015851e+01 |       30
     24       5.708296e+05      -3.649534e+01 |       29
     25       5.708171e+05      -1.247439e+01 |       28
     26       5.708118e+05      -5.346981e+00 |       26
     27       5.708095e+05      -2.271039e+00 |       25
     28       5.708073e+05      -2.195908e+00 |       22
     29       5.708060e+05      -1.316898e+00 |       21
     30       5.708051e+05      -9.350879e-01 |       16
     31       5.708043e+05      -7.592635e-01 |       16
     32       5.708037e+05      -6.379521e-01 |        8
     33       5.708035e+05      -1.743059e-01 |        6
     34       5.708034e+05      -1.371025e-01 |        7
     35       5.708032e+05      -1.383637e-01 |        5
     36       5.708031e+05      -1.256326e-01 |        2
     37       5.708031e+05      -3.040939e-02 |        0
     38       5.708031e+05       0.000000e+00 |        0
K-means converged with 38 iterations (objv = 570803.0719361434)
INFO: K-means with 32000 data points using 38 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.289882
INFO: iteration 2, average log likelihood -1.256587
INFO: iteration 3, average log likelihood -1.223907
INFO: iteration 4, average log likelihood -1.182972
INFO: iteration 5, average log likelihood -1.127361
WARNING: Variances had to be floored 2 8 17
INFO: iteration 6, average log likelihood -1.061950
WARNING: Variances had to be floored 1 9 20 21 26
INFO: iteration 7, average log likelihood -1.051697
WARNING: Variances had to be floored 5
INFO: iteration 8, average log likelihood -1.091491
WARNING: Variances had to be floored 16 28
INFO: iteration 9, average log likelihood -1.047112
WARNING: Variances had to be floored 4 8 10 18 25
INFO: iteration 10, average log likelihood -1.018605
WARNING: Variances had to be floored 1 2 3 9 21 26 32
INFO: iteration 11, average log likelihood -1.023197
WARNING: Variances had to be floored 20
INFO: iteration 12, average log likelihood -1.094298
WARNING: Variances had to be floored 5
INFO: iteration 13, average log likelihood -1.048102
WARNING: Variances had to be floored 1 8 10 16 21 25 28
INFO: iteration 14, average log likelihood -1.007531
WARNING: Variances had to be floored 3 9 26
INFO: iteration 15, average log likelihood -1.051439
WARNING: Variances had to be floored 2 4 18 20
INFO: iteration 16, average log likelihood -1.048861
WARNING: Variances had to be floored 5 8 15 32
INFO: iteration 17, average log likelihood -1.028662
WARNING: Variances had to be floored 1 10
INFO: iteration 18, average log likelihood -1.041602
WARNING: Variances had to be floored 3 9 16 21 25 28
INFO: iteration 19, average log likelihood -1.012398
WARNING: Variances had to be floored 8 26
INFO: iteration 20, average log likelihood -1.050051
WARNING: Variances had to be floored 2 4 5 20
INFO: iteration 21, average log likelihood -1.032745
WARNING: Variances had to be floored 1 10 22
INFO: iteration 22, average log likelihood -1.036064
WARNING: Variances had to be floored 3 8 9 21
INFO: iteration 23, average log likelihood -1.019852
WARNING: Variances had to be floored 5 25 26 28 32
INFO: iteration 24, average log likelihood -1.025314
WARNING: Variances had to be floored 2 15
INFO: iteration 25, average log likelihood -1.060867
WARNING: Variances had to be floored 1 4 8 16
INFO: iteration 26, average log likelihood -1.033560
WARNING: Variances had to be floored 5 20 22
INFO: iteration 27, average log likelihood -1.026153
WARNING: Variances had to be floored 3 9 10 21 25 26 28
INFO: iteration 28, average log likelihood -1.005847
WARNING: Variances had to be floored 2 32
INFO: iteration 29, average log likelihood -1.081943
WARNING: Variances had to be floored 8
INFO: iteration 30, average log likelihood -1.050165
WARNING: Variances had to be floored 1 5 16 22
INFO: iteration 31, average log likelihood -1.001697
WARNING: Variances had to be floored 3 4 9 10 21 25 26 28
INFO: iteration 32, average log likelihood -1.001443
WARNING: Variances had to be floored 2 20 32
INFO: iteration 33, average log likelihood -1.081408
WARNING: Variances had to be floored 8
INFO: iteration 34, average log likelihood -1.070793
WARNING: Variances had to be floored 5 15
INFO: iteration 35, average log likelihood -1.017881
WARNING: Variances had to be floored 1 3 9 10 16 21 25 26 28
INFO: iteration 36, average log likelihood -0.980736
WARNING: Variances had to be floored 2 20
INFO: iteration 37, average log likelihood -1.080498
WARNING: Variances had to be floored 4 8 22 32
INFO: iteration 38, average log likelihood -1.048656
WARNING: Variances had to be floored 5 10
INFO: iteration 39, average log likelihood -1.034630
WARNING: Variances had to be floored 1 3 9 15 21 25 26 28
INFO: iteration 40, average log likelihood -0.995924
WARNING: Variances had to be floored 2 20
INFO: iteration 41, average log likelihood -1.074426
WARNING: Variances had to be floored 5 8 16
INFO: iteration 42, average log likelihood -1.047137
WARNING: Variances had to be floored 9 10 22 32
INFO: iteration 43, average log likelihood -1.022523
WARNING: Variances had to be floored 1 3 4 21 25 26 28
INFO: iteration 44, average log likelihood -1.006601
WARNING: Variances had to be floored 2 20
INFO: iteration 45, average log likelihood -1.072491
WARNING: Variances had to be floored 5 8
INFO: iteration 46, average log likelihood -1.056971
WARNING: Variances had to be floored 9 16
INFO: iteration 47, average log likelihood -1.035775
WARNING: Variances had to be floored 1 3 21 25 26 28
INFO: iteration 48, average log likelihood -1.002060
WARNING: Variances had to be floored 2 20 32
INFO: iteration 49, average log likelihood -1.052239
WARNING: Variances had to be floored 4 5 8 22
INFO: iteration 50, average log likelihood -1.043854
INFO: EM with 100000 data points 50 iterations avll -1.043854
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0689206    0.00663665   0.283112     -0.0379499   -0.067102     -0.012313     0.0117819    0.148639     0.0489281   -0.115839     0.0156264    0.055792    -0.0638805    0.050797     0.291966    -0.0943411  -0.126645     0.0924831    0.0183915  -0.0532693   -0.054336     -0.129683     0.151987   -0.0845213   -0.0229184    0.0429574
  0.0235941    0.031854     0.0145667    -0.134266    -0.0876158    -0.143409     0.191876     0.110705    -0.103952     0.0339469    0.0527485   -0.142354     0.0580098   -0.0649804    0.0330439    0.291335   -0.0352626    0.0637991    0.150714    0.0715711    0.0217681     0.00680296  -0.0524801  -0.191882    -0.177063    -0.005989 
  0.160315     0.172175     0.0446474     0.0299187   -0.062867     -0.173934    -0.213428     0.0691446    0.031934    -0.0521306    0.0789514   -0.0736072    0.155325    -0.0890916   -0.185693     0.129336    0.0315846   -0.119818    -0.0177626   0.0833005   -0.153647     -0.0580623   -0.0828174   0.15058      0.00502325  -0.0792452
  0.0581135    0.00460254   0.0776495     0.138238     0.109272      0.119253    -0.00880753   0.069545    -0.00228534  -0.0203036    0.0987331    0.0351786    0.124167     0.00192      0.0328567    0.0651534  -0.28554      0.0875593    0.100669   -0.0490237    0.0748035     0.0797892   -0.021427    0.110812    -0.21047      0.0724981
  0.0407838    0.037037    -0.0898451    -0.187817     0.145185     -0.0472953   -0.0593836    0.0694335   -0.121271     0.104171     0.0287812   -0.0256912   -0.0492755    0.0840905    0.247144    -0.0587531  -0.0241133   -0.118068     0.0224642  -0.0650202   -0.0181064    -0.0790741    0.034588    0.0988521    0.00398057   0.0962805
  0.0895466   -0.0161987    0.0274564     0.07554     -0.00208937    0.0241833   -0.0159943    0.0361202    0.00165763  -0.00585786   0.0194976   -0.0406732   -0.12363      0.141368    -0.00300372   0.0020181  -0.0440752    0.0906028    0.157681    0.0124602    0.0252636    -0.00279675  -0.11318     0.123833     0.0368878   -0.0242722
  0.0886404   -0.135274     0.0275993     0.0588948   -0.050385      0.0855676   -0.0445241    0.0428689    0.0356488    0.0352854   -0.18224      0.046086     0.118333    -0.0670717   -0.0727737    0.0925193   0.139828     0.0455379   -0.157132    0.0481484    0.102162     -0.180343     0.0491689   0.162386     0.147046     0.0240219
 -0.168227    -0.125772    -0.0368928    -4.1736e-5   -0.055558      0.0183787    0.072495    -0.0171107   -0.0124562    0.0261989   -0.134174     0.0377735   -0.0675431    0.0267883   -0.0555595   -0.159116    0.0250697    0.0193351    0.0821933   0.130546    -0.161443      0.107496     0.0425846  -0.0465785   -0.0138709    0.0794985
 -0.0236313   -0.0770338    0.176856     -0.146707    -0.164326     -0.0208156    0.0460266   -0.0345341    0.0545031   -0.0019099    0.0625272    0.0500771    0.110251    -0.0601329   -0.108828     0.0279429   0.0221344   -0.128498    -0.0607989   0.0544753   -0.079499     -0.0506883    0.182771   -0.0691579    0.0555889    0.0329807
 -0.142687     0.0109757    0.0468984     0.0229775    0.0128112     0.096493    -0.0301535    0.0446041   -0.0322064    0.0195398    0.0631069   -0.0806576    0.131979     0.00757649   0.0163726   -0.0437637  -0.0752821    0.0292438    0.0216796   0.152342     0.085167      0.058469     0.155694   -0.0728155   -0.0233009   -0.0320312
  0.144723     0.0178237    0.110083      0.0363079    0.0694546    -0.116318    -0.0145329    0.169424    -0.0629954   -0.103128     0.0430458    0.0680691   -0.110831    -0.104718    -0.187876     0.122115    0.121235     0.0281027   -0.187388    0.126531     0.13112      -0.123833     0.134171   -0.025375     0.0664047    0.101906 
 -0.067853     0.0684393    0.0664184     0.0414489   -0.0359138    -0.0759781    0.0280635    0.0543327    0.0421001    0.184236     0.0472554    0.0609739    0.0428091    0.0673237   -0.0611678    0.161859    0.13101      0.0710464    0.0275302   0.0199101    0.165685      0.110097     0.136847   -0.150481     0.100271    -0.193766 
 -0.152617     0.100526    -0.010892      0.033959    -0.0947342    -0.0738485   -0.0286791    0.0301333   -0.0785746    0.0673078   -0.0952887    0.247196     0.0169712   -0.0519402   -0.00811866   0.10947     0.0591545    0.0764718   -0.0424549   0.168799    -0.00946111    0.208055    -0.0042971  -0.146799    -0.01033      0.100011 
  0.0908001    0.148137    -0.085329      0.0301652   -0.0418227    -0.095797    -0.144608     0.0528593    0.110474     0.0205466    0.0813193    0.306064     0.00561344  -0.0173785   -0.140723     0.0602444   0.0829526   -0.00483502   0.063238    0.083445     0.110546      0.0491614    0.076898    0.0618698   -0.11719      0.0385876
 -0.16011     -0.00276637   0.0738715    -0.00848676  -0.0446365    -0.0692607   -0.0961971    0.0152139   -0.00796742  -0.0164214   -0.113208     0.0690242    0.0283747    0.0983701    0.0518117    0.108827   -0.0399097   -0.0206517    0.0200689   0.100002     0.025921     -0.0445564   -0.0803346   0.0402042    0.0270771   -0.022985 
  0.107582    -0.0421667   -0.167997      0.13926      0.0121221    -0.0475792    0.108952    -0.0822487   -0.238565    -0.0393522   -0.109307    -0.0223224    0.0305108   -0.00568379  -0.047988    -0.0122033  -0.0754243    0.0277274    0.164882   -0.0202064    0.0300455     0.0319107   -0.119658   -0.0924215    0.0285038   -0.0585923
 -0.0394864   -0.0352707    0.00286074    0.119336     0.0823524    -0.0255687    0.0231853    0.132109     0.148494     0.0277793    0.00314717   0.0190123    0.0325677   -0.067529    -0.0341964    0.0322128   0.137878    -0.0860764   -0.0383269   0.212291    -0.0174976     0.151023     0.1017     -0.0130157    0.0590177    0.0738086
 -4.38135e-5  -0.0181852   -0.103402     -0.0491149   -0.0742091     0.033634     0.0530183   -0.0293233   -0.112402     0.145247     0.116002     0.0763653   -0.0851424   -0.103871    -0.0806116   -0.060089   -0.0856416   -0.125324    -0.159209    0.0871532   -0.232845     -0.116046    -0.0134669   0.0997046    0.173169     0.0933395
  0.1017       0.0436405    0.107663      0.0636364    0.0385946    -0.056879     0.0515308    0.113758    -0.114214    -0.119313     0.0106233   -0.04991     -0.0716553   -0.0216127    0.00876292  -0.0642484   0.0319151    0.0978815   -0.032044    0.171037    -0.122849     -0.18101      0.0305324   0.0343308    0.0399127   -0.032844 
 -0.223692    -0.0179657    0.0820853    -0.0730668   -0.09575      -0.0833648   -0.153637    -0.115623    -0.0218532   -0.0574625   -0.110957     0.113528     0.0405154    0.13797      0.0472622    0.13291     0.0993126    0.0113308    0.0291597   0.100113     0.0135726    -0.157088    -0.0830644   0.0637593    0.0617167   -0.023777 
 -0.0240414    0.121116    -0.000804481   0.19056      0.0893927    -0.116583    -0.00630621   0.153463     0.0196557   -0.112585    -0.0746987    0.175022    -0.124722     0.0669448    0.0236956    0.190484    0.00257426  -0.052524    -0.0291709   0.0316449    0.194711     -0.0831961    0.0455214  -0.174795     0.25758      0.163613 
 -0.0249071   -0.024981    -0.0609674     0.0525114   -0.116864      0.0100606   -0.00208631   0.0970324   -0.0714102    0.0816063   -0.0181297    0.00822575   0.0505386   -0.0477716    0.0608277   -0.0781684   0.0221518   -0.00708185   0.14578     0.139057     0.0556549     0.0504013    0.157583    0.00194023  -0.123609    -0.0449568
 -0.0340785   -0.012136     0.159767      0.0433792    0.000675935  -0.0333586   -0.0173395    0.118973    -0.00570958  -0.0501626   -0.0329363   -0.094227    -0.0935393    0.0260182    0.0119015   -0.0505844   0.132558     0.0634714   -0.0800432  -0.131233    -0.0135635    -0.200105    -0.0249097  -0.0656655    0.031405    -0.104622 
  0.102639     0.0158413   -0.165254     -0.0282134   -0.0454934     0.138175     0.0285678   -0.0702611   -0.0100379   -0.0492298   -0.298935    -0.106069     0.0487461    0.0578772   -0.0420378   -0.077747    0.0312911    0.0913703    0.0231662  -0.0446362   -0.13711      -0.00559237   0.0860613   0.060885     0.00369836  -0.0238346
 -0.0487272    0.177987     0.00125362   -0.0955778   -0.0111649     0.137106    -0.0990237   -0.00367492  -0.241755    -0.0775153    0.012948     0.116316    -0.00572283  -0.170139    -0.0350462    0.0254539   0.1712      -0.112087     0.0374904  -0.10769      0.0481755    -0.0040292   -0.0589721  -0.0958927    0.0901574    0.0800785
 -0.126799     0.023882    -0.0728604    -0.126135    -0.0375931    -0.0679592    0.0340515   -0.135217     0.0455044   -0.00112323  -0.0588081    0.0752531   -0.0590399    0.0364123   -0.0543736   -0.0490562  -0.0864755    0.0230136    0.0234706  -0.00619355  -0.0359946    -0.045043     0.172213   -0.210506    -0.0273567   -0.166252 
 -0.0680741    0.0701137   -0.108345     -0.0915386   -0.125828      0.0511148   -0.138112     0.0304756    0.0150348   -0.149455    -0.143216     0.0593772    0.225594    -0.00624252  -0.0367077   -0.107222   -0.0635297    0.018283    -0.192221    0.242118    -0.0204099     0.159951    -0.097624   -0.0912622    0.197543    -0.169786 
  0.0393074    0.00778669  -0.0148707     0.0348905   -0.106521      0.00720413   0.0307213    0.0946834   -0.0655019    0.00341798  -0.00845119  -0.0157251    0.0577989   -0.0136711    0.0521909   -0.144359   -0.056688    -0.015536     0.142517    0.0537149    0.102224      0.0553936    0.181953    0.175507    -0.0927742   -0.0207785
 -0.124467     0.062526    -0.094645      0.145161     0.0303241    -0.0170217   -0.0754736   -0.0722533    0.109778    -0.113197    -0.0159078   -0.0668851   -0.117758     0.287899    -0.0722779   -0.0904655   0.0185065    0.0307088   -0.139619   -0.133905    -0.0860332    -0.0463666    0.0961848   0.108324    -0.266587    -0.0686153
 -0.119297    -0.098713     0.0193988     0.123403     0.040976     -0.0534768   -0.059491     0.197886     0.150047    -0.0301471    0.0765038    0.198147    -0.114283    -0.0123289   -0.0948466   -0.0532239   0.0964152    0.0669003   -0.0242703  -0.0665272   -0.0481354    -0.123662     0.0117081   0.06193      0.00515884   0.0621776
  0.162361    -0.0787233    0.124339      0.0523856    0.00347811    0.0731683   -0.040036     0.0993703    0.239744    -0.143021     0.0630923    0.232392    -0.0893247    0.0594578    0.0752358   -0.227853   -0.08792     -0.327453     0.0464626  -0.128008     0.111066      0.0169272    0.0331499   0.0797447    0.0331633    0.193961 
  0.131276    -0.0193648    0.0626904    -0.0167704    0.165269      0.207197     0.0213269    0.0792828   -0.0503915   -0.0842666   -0.100783    -0.139985     0.101931    -0.0455798    0.0400714    0.0138868  -0.17438     -0.0288482   -0.146449    0.0891086    0.000283173   0.104589    -0.129775   -0.142984    -0.0394736    0.0214061INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 9
INFO: iteration 1, average log likelihood -1.048672
WARNING: Variances had to be floored 1 3 9 16 21 25 26 28
INFO: iteration 2, average log likelihood -0.985449
WARNING: Variances had to be floored 2 3 5 8 9 15 20 21 25 32
INFO: iteration 3, average log likelihood -0.977905
WARNING: Variances had to be floored 1 9 16 21 26
INFO: iteration 4, average log likelihood -1.021202
WARNING: Variances had to be floored 3 4 9 21 25 28
INFO: iteration 5, average log likelihood -0.986430
WARNING: Variances had to be floored 1 2 3 8 9 15 16 20 21 26 32
INFO: iteration 6, average log likelihood -0.972900
WARNING: Variances had to be floored 5 9 22 25
INFO: iteration 7, average log likelihood -1.022008
WARNING: Variances had to be floored 1 3 9 16 21 26 28
INFO: iteration 8, average log likelihood -0.978384
WARNING: Variances had to be floored 2 3 8 9 20 21 25 32
INFO: iteration 9, average log likelihood -0.983281
WARNING: Variances had to be floored 1 4 9 15 16 21 22 26
INFO: iteration 10, average log likelihood -0.998941
INFO: EM with 100000 data points 10 iterations avll -0.998941
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0776969    0.0591634    0.0351661   -0.0783127    0.0873373    0.121871      0.0179715   -0.122585      0.0124779    0.0762478    0.147649     0.155763     0.0178869   -0.0529984   -0.070855    -0.097288     0.0805234   0.0492532   -0.0260035   -0.0774243   -0.113475    -0.00964727  -0.105123    0.101926    -0.000797949   0.0745093 
  0.0290313   -0.170416     0.12261     -0.141386    -0.0416443    0.0178226     0.0038683    0.0749077     0.0593977   -0.0728384    0.0317015    0.0770899   -0.0410815    0.15555      0.0268727   -0.0370644   -0.0252053   0.0131043   -0.20487      0.11301     -0.0924221    0.0466718    0.11882     0.10919     -0.0525142    -0.11482   
 -0.129876     0.0510029   -0.16111      0.00942277  -0.0149403   -0.0929618     0.068477    -0.00588666    0.056691    -0.0678569    0.0851263   -0.0753724    0.00520738   0.0442248    0.138724     0.0680225   -0.214228   -0.0240299    0.0402652   -0.0708538    0.105921    -0.0509406   -0.105717    0.00920096   0.107448     -0.0827097 
 -0.0460159   -0.0220342   -0.0248247    0.105819    -0.200657     0.0183464     0.128119     0.230791     -0.0990277   -0.138004     0.0125571   -0.0756497   -0.00750929   0.00460224   0.182481     0.0490931    0.0731906   0.00529226   0.0472965   -0.0189047    0.061845     0.111768     0.0754442   0.0270224    0.109736      0.040113  
 -0.188018     0.0289519    0.133587    -0.208626    -0.157692     0.000280395   0.0256606    0.000575187  -0.032536    -0.129434    -0.154184     0.105712    -0.0218135    0.12763      0.178015    -0.0248727    0.187361    0.0369437    0.0326101    0.0419792    0.0165048    0.0175023    0.0904432   0.0124448   -0.0241417    -0.0112666 
  0.127106     0.0509425    0.00982381  -0.0774222   -0.173495    -0.121085      0.00992991  -0.0878208     0.0908725   -0.218736     0.206955     0.137346     0.064162     0.0445423    0.154826    -0.0504059   -0.0164614   0.0900434    0.0623106   -0.137613    -0.103377     0.0106843    0.10025     0.0340199    0.00710773   -0.042677  
  0.133069    -0.0960602    0.143426    -0.0396352    0.0168602   -0.0414578    -0.133501     0.0887551    -0.0797991   -0.134748     0.00811907   0.0444799   -0.053361    -0.0695457   -0.0344242    0.301901    -0.117044   -0.0721877   -0.161403    -0.042376     0.0197663    0.0263357   -0.13372     0.033869     0.199423     -0.00370607
  0.0826101   -0.12765      0.0354881    0.0592906   -0.128144     0.178535      0.08774      0.0388712     0.0842225    0.0517989    0.217023    -0.102316    -0.108622     0.0416266    0.104335    -0.109508    -0.0498616   0.0675645   -0.0192769    0.231985     0.0345788    0.217728     0.0203297   0.0426471   -0.016786      0.0827706 
  0.00854998  -0.0238964   -0.0276783   -0.0643933   -0.151456    -0.0145367    -0.0988257   -0.316601      0.00314535  -0.169049     0.0547409    0.040981    -0.167797    -0.133737    -0.138915     0.209014     0.0812038  -0.146571     0.0344401    0.0694522   -0.0801745    0.00758855   0.0704566  -0.0175977    0.117531     -0.133914  
 -0.00415955   0.196391     0.222827     0.048609    -0.183525     0.065267      0.076327     0.0407781    -0.045161    -0.0232862   -0.067485    -0.117782    -0.117557     0.0702974    0.00365889   0.247784     0.0523681  -0.00609737   0.0243884    0.0922682   -0.0343609    0.0303324   -0.0345963   0.127177    -0.100458     -0.074871  
  0.0519596   -0.0738481   -0.0608585   -0.0875492   -0.170786     0.0109784     0.103455     0.0575468     0.0869275   -0.0687399   -0.0336221   -0.0219791    0.0875696    0.107214     0.0739458    0.196954    -0.106299   -0.0370198    0.0938238    0.0712475    0.00815072  -0.0140957   -0.133686   -0.0327891    0.0677814    -0.0461479 
 -0.0696567   -0.0240959    0.0672931    0.0236373    0.0943837   -0.122185     -0.00344658   0.162279      0.254973    -0.102186     0.00076212   0.0332693    0.0794335   -0.0652964    0.0353856   -0.0899508   -0.11379    -0.0235358   -0.00582421   0.0888634   -0.0419167   -0.141447    -0.0107651   0.0224044   -0.0868017     0.0743802 
 -0.0685164   -0.0247508   -0.0880019   -0.150414     0.235215    -0.174491     -0.076367     0.0132053     0.0460724    0.0582996   -0.00708761  -0.0809118    0.0611508   -0.0686418    0.0835834   -0.0282456   -0.25893    -0.0987099   -0.0606456   -0.00609789   0.0393922    0.0115181    0.0837406   0.0625614    0.0489812     0.0763691 
 -0.117703    -0.0458121    0.0419339   -0.053116    -0.00709399  -0.044538     -0.13743      0.0185901     0.0605315    0.0483587   -0.0394144    0.0397624    0.0309474    0.00622142  -0.0189725   -0.0661242   -0.0124247  -0.0306907    0.154436    -0.0526997    0.116409     0.0755959   -0.0168893   0.0193877    0.204184      0.126396  
  0.0545722    0.0436873    0.038519     0.155448     0.0607486    0.00264085    0.00660845   0.0428695     0.0244236   -0.0887948    0.0247304   -0.174077    -0.106097     0.0460338   -0.0888931    0.159611     0.0470695  -0.0264008   -0.0592179    0.0211512   -0.115755     0.0246072    0.0247113  -0.0717082   -0.106599      0.127477  
 -0.0789225   -0.140973     0.0973525    0.0967994    0.212771    -0.00028343   -0.00268534   0.0115357     0.0164354    0.0653056    0.0573864   -0.0467245    0.0476837   -0.0356426   -0.173163    -0.0250386   -0.202063   -0.205876     0.0581586   -0.00260034   0.12057     -0.0812022    0.058348   -0.136204     0.0565186    -0.0545239 
 -0.270225     0.0544347   -0.118737    -0.0576936   -0.0989617   -0.163833     -0.0406341    0.00178068    0.00478789   0.179313    -0.105338    -0.0374773    0.0034853   -0.02155      0.127559    -0.0364009    0.0876718  -0.220519     0.00112421  -0.157464    -0.0403917    0.0249339    0.15494     0.0267764    0.108689      0.0417753 
 -0.0270418    0.0077516    0.0389717   -0.0884948    0.130207    -0.0711236     0.210371     0.245263     -0.00818088   0.0497065   -0.0635176    0.0873327   -0.0366003   -0.091764    -0.0178757   -0.112153     0.0844471   0.0631295   -0.041619     0.169393     0.0213105    0.0365689    0.228072    0.00401421   0.0205518     0.0476301 
 -0.122187    -0.0775384    0.0216758    0.0340636    0.098059     0.0597073    -0.105343     0.0507686     0.156848     0.0446798   -0.0491953   -0.134188    -0.0447341    0.194828     0.0788752    0.0355623   -0.130887    0.138796     0.225092    -0.0585158    0.0337677    0.0939532   -0.220327    0.0317785    0.115668     -0.00121703
  0.0746481    0.004024    -0.0126931   -0.0558794    0.0619258    0.150346     -0.121593    -0.122429      0.0866068   -0.0320936    0.0496569   -0.128249    -0.0946223    0.011973     0.139915    -0.0113235    0.0196556   0.0242751    0.112026     0.0878191   -0.0238853   -0.140456    -0.0634753   0.126898    -0.134511      0.03671   
  0.0460591   -0.0229804    0.0886999    0.023157    -0.0765243   -0.0657335    -0.0161858    0.0134101    -0.0728524    0.145741    -0.0332753   -0.0952133   -0.0403294    0.0280281   -0.107367     0.10765      0.0480005   0.00942341  -0.211276    -0.0802509   -0.0996505   -0.0452266   -0.0270559  -0.0172411    0.00959187   -0.0221457 
  0.193118    -0.00806005  -0.0236873   -0.013337     0.0172356    0.013319      0.137463     0.125724     -0.029998    -0.185631     0.041376    -0.0491273    0.0261559   -0.123242    -0.0575639    0.0247733   -0.0559125  -0.0298984    0.0609706    0.103665     0.0624684    0.0332804   -0.108393   -0.00985707  -0.0719581     0.200804  
 -0.024329     0.0589034    0.121903     0.0948339   -0.0188164    0.0159265    -0.0310347   -0.0258977    -0.0869787   -0.130728    -0.00190393  -0.118485    -0.181978     0.149011    -0.0519166   -0.0816031    0.0402771   0.067309     0.0715443    0.105664     0.025873    -0.0536118   -0.0675945  -0.0294074    0.00970714   -0.0254987 
  0.0139351   -0.0115426    0.0991544    0.0502819    0.0344373   -0.208929      0.144038     0.0268186     0.101033     0.0550116   -0.0781476    0.0415506   -0.0133323    0.0979765    0.0571512   -0.0107447   -0.038747    0.0208354    0.0701434   -0.164089    -0.119451     0.0285996    0.138422   -0.113869     0.0900143    -0.00744164
  0.126301     0.0732289   -0.0851289    0.0728367    0.125717     0.0573924    -0.121853    -0.0493044    -0.0729078    0.00405859  -0.0922911   -0.118898     0.0711347   -0.00256373   0.0623096   -0.00306048   0.134691   -0.0810733    0.0179571    0.0413898   -0.0271168   -0.0957804    0.0516286   0.10683     -0.0255226     0.139861  
  0.0401098    0.171832     0.167622     0.108577     0.0718991   -0.0292881     0.0335722   -0.0130122    -0.0814187    0.0582319   -0.00369341   0.00574064  -0.193508     0.107909    -0.115672    -0.0187952   -0.10511     0.0480179   -0.0677261    0.0733483    0.0321786   -0.0256519    0.028759   -0.0800898    0.0615651    -0.090665  
  0.0544512    0.0785125    0.0452938   -0.108773    -0.0894489    0.0589005     0.0794915    0.0760865     0.047377     0.0974225   -0.0770559   -0.0265888   -0.0520424   -0.160447     0.0676013   -0.101186    -0.0274561  -0.167594    -0.0534035   -0.110786    -0.0280195   -0.0444592    0.129004   -0.0409546    0.0421234    -0.0954177 
  0.164531    -0.0721512    0.116212    -0.0016746   -0.107243     0.0445407    -0.0782113    0.0337889     0.0489926    0.10701      0.093288    -0.0168102   -0.0372016    0.00500466   0.14622     -0.0108802    0.0913573   0.106043     0.120941    -0.0549128    0.10837      0.254394    -0.0913823  -0.0442826    0.304954      0.0323713 
 -0.0969652    0.0169       0.198568    -0.0551353   -0.0213441    0.134739      0.0825735    0.0417102    -0.164458    -0.168263     0.0943856    0.0934137   -0.0393014   -0.136102     0.127028     0.102401    -0.159596    0.0764273    0.0529214    0.00877608  -0.0590646   -0.0163264    0.0356046   0.051485     0.0657002    -0.150068  
 -0.0077325   -0.107976    -0.044902     0.0070297   -0.0288367    0.116252      0.0504604    0.0687361    -0.091176    -0.00130071   0.0130891   -0.0163324    0.120916    -0.227719     0.117543    -0.127801     0.039506    0.0770485   -0.0229183    0.11414      0.0168123   -0.00763931  -0.0963423  -0.00609028  -0.0299926    -0.207424  
 -0.125123     0.150226    -0.0690565    0.109373     0.037101    -0.0302731     0.0309051    0.0106688    -0.103862    -0.104738    -0.21339     -0.205719     0.133378    -0.178518    -0.0341054   -0.0261643   -0.0819684   0.0519015    0.0385779   -0.0614236   -0.108801    -0.0854152   -0.0258717  -0.086316    -0.0731176     0.00478626
 -0.0817141   -0.0409158    0.172975     0.04249     -0.159149     0.0502936    -0.00177856   0.00520352    0.0237277   -0.0241821   -0.0777433   -0.0365145    0.061091     0.128713    -0.235763     0.063537     0.096907    0.031955     0.0889317   -0.090557    -0.124764    -0.0964403   -0.0126909   0.132285    -0.015892     -0.0529973 kind full, method split
0: avll = -1.4206320619366137
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.420650
INFO: iteration 2, average log likelihood -1.420589
INFO: iteration 3, average log likelihood -1.420548
INFO: iteration 4, average log likelihood -1.420506
INFO: iteration 5, average log likelihood -1.420459
INFO: iteration 6, average log likelihood -1.420409
INFO: iteration 7, average log likelihood -1.420357
INFO: iteration 8, average log likelihood -1.420302
INFO: iteration 9, average log likelihood -1.420241
INFO: iteration 10, average log likelihood -1.420159
INFO: iteration 11, average log likelihood -1.420010
INFO: iteration 12, average log likelihood -1.419697
INFO: iteration 13, average log likelihood -1.419057
INFO: iteration 14, average log likelihood -1.417984
INFO: iteration 15, average log likelihood -1.416733
INFO: iteration 16, average log likelihood -1.415816
INFO: iteration 17, average log likelihood -1.415370
INFO: iteration 18, average log likelihood -1.415200
INFO: iteration 19, average log likelihood -1.415139
INFO: iteration 20, average log likelihood -1.415117
INFO: iteration 21, average log likelihood -1.415109
INFO: iteration 22, average log likelihood -1.415105
INFO: iteration 23, average log likelihood -1.415104
INFO: iteration 24, average log likelihood -1.415104
INFO: iteration 25, average log likelihood -1.415103
INFO: iteration 26, average log likelihood -1.415103
INFO: iteration 27, average log likelihood -1.415103
INFO: iteration 28, average log likelihood -1.415103
INFO: iteration 29, average log likelihood -1.415103
INFO: iteration 30, average log likelihood -1.415102
INFO: iteration 31, average log likelihood -1.415102
INFO: iteration 32, average log likelihood -1.415102
INFO: iteration 33, average log likelihood -1.415102
INFO: iteration 34, average log likelihood -1.415102
INFO: iteration 35, average log likelihood -1.415102
INFO: iteration 36, average log likelihood -1.415102
INFO: iteration 37, average log likelihood -1.415102
INFO: iteration 38, average log likelihood -1.415102
INFO: iteration 39, average log likelihood -1.415102
INFO: iteration 40, average log likelihood -1.415102
INFO: iteration 41, average log likelihood -1.415102
INFO: iteration 42, average log likelihood -1.415102
INFO: iteration 43, average log likelihood -1.415102
INFO: iteration 44, average log likelihood -1.415102
INFO: iteration 45, average log likelihood -1.415102
INFO: iteration 46, average log likelihood -1.415102
INFO: iteration 47, average log likelihood -1.415102
INFO: iteration 48, average log likelihood -1.415102
INFO: iteration 49, average log likelihood -1.415102
INFO: iteration 50, average log likelihood -1.415102
INFO: EM with 100000 data points 50 iterations avll -1.415102
952.4 data points per parameter
1: avll = [-1.42065,-1.42059,-1.42055,-1.42051,-1.42046,-1.42041,-1.42036,-1.4203,-1.42024,-1.42016,-1.42001,-1.4197,-1.41906,-1.41798,-1.41673,-1.41582,-1.41537,-1.4152,-1.41514,-1.41512,-1.41511,-1.41511,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.415116
INFO: iteration 2, average log likelihood -1.415052
INFO: iteration 3, average log likelihood -1.415005
INFO: iteration 4, average log likelihood -1.414953
INFO: iteration 5, average log likelihood -1.414895
INFO: iteration 6, average log likelihood -1.414827
INFO: iteration 7, average log likelihood -1.414752
INFO: iteration 8, average log likelihood -1.414670
INFO: iteration 9, average log likelihood -1.414587
INFO: iteration 10, average log likelihood -1.414505
INFO: iteration 11, average log likelihood -1.414430
INFO: iteration 12, average log likelihood -1.414365
INFO: iteration 13, average log likelihood -1.414313
INFO: iteration 14, average log likelihood -1.414273
INFO: iteration 15, average log likelihood -1.414243
INFO: iteration 16, average log likelihood -1.414220
INFO: iteration 17, average log likelihood -1.414203
INFO: iteration 18, average log likelihood -1.414189
INFO: iteration 19, average log likelihood -1.414178
INFO: iteration 20, average log likelihood -1.414167
INFO: iteration 21, average log likelihood -1.414158
INFO: iteration 22, average log likelihood -1.414149
INFO: iteration 23, average log likelihood -1.414142
INFO: iteration 24, average log likelihood -1.414134
INFO: iteration 25, average log likelihood -1.414127
INFO: iteration 26, average log likelihood -1.414121
INFO: iteration 27, average log likelihood -1.414115
INFO: iteration 28, average log likelihood -1.414109
INFO: iteration 29, average log likelihood -1.414104
INFO: iteration 30, average log likelihood -1.414099
INFO: iteration 31, average log likelihood -1.414094
INFO: iteration 32, average log likelihood -1.414089
INFO: iteration 33, average log likelihood -1.414085
INFO: iteration 34, average log likelihood -1.414080
INFO: iteration 35, average log likelihood -1.414075
INFO: iteration 36, average log likelihood -1.414071
INFO: iteration 37, average log likelihood -1.414066
INFO: iteration 38, average log likelihood -1.414061
INFO: iteration 39, average log likelihood -1.414056
INFO: iteration 40, average log likelihood -1.414051
INFO: iteration 41, average log likelihood -1.414045
INFO: iteration 42, average log likelihood -1.414040
INFO: iteration 43, average log likelihood -1.414034
INFO: iteration 44, average log likelihood -1.414027
INFO: iteration 45, average log likelihood -1.414021
INFO: iteration 46, average log likelihood -1.414014
INFO: iteration 47, average log likelihood -1.414007
INFO: iteration 48, average log likelihood -1.413999
INFO: iteration 49, average log likelihood -1.413992
INFO: iteration 50, average log likelihood -1.413984
INFO: EM with 100000 data points 50 iterations avll -1.413984
473.9 data points per parameter
2: avll = [-1.41512,-1.41505,-1.415,-1.41495,-1.41489,-1.41483,-1.41475,-1.41467,-1.41459,-1.41451,-1.41443,-1.41437,-1.41431,-1.41427,-1.41424,-1.41422,-1.4142,-1.41419,-1.41418,-1.41417,-1.41416,-1.41415,-1.41414,-1.41413,-1.41413,-1.41412,-1.41412,-1.41411,-1.4141,-1.4141,-1.41409,-1.41409,-1.41408,-1.41408,-1.41408,-1.41407,-1.41407,-1.41406,-1.41406,-1.41405,-1.41405,-1.41404,-1.41403,-1.41403,-1.41402,-1.41401,-1.41401,-1.414,-1.41399,-1.41398]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.413985
INFO: iteration 2, average log likelihood -1.413924
INFO: iteration 3, average log likelihood -1.413871
INFO: iteration 4, average log likelihood -1.413811
INFO: iteration 5, average log likelihood -1.413742
INFO: iteration 6, average log likelihood -1.413662
INFO: iteration 7, average log likelihood -1.413575
INFO: iteration 8, average log likelihood -1.413485
INFO: iteration 9, average log likelihood -1.413398
INFO: iteration 10, average log likelihood -1.413320
INFO: iteration 11, average log likelihood -1.413251
INFO: iteration 12, average log likelihood -1.413191
INFO: iteration 13, average log likelihood -1.413140
INFO: iteration 14, average log likelihood -1.413095
INFO: iteration 15, average log likelihood -1.413057
INFO: iteration 16, average log likelihood -1.413025
INFO: iteration 17, average log likelihood -1.412997
INFO: iteration 18, average log likelihood -1.412974
INFO: iteration 19, average log likelihood -1.412955
INFO: iteration 20, average log likelihood -1.412939
INFO: iteration 21, average log likelihood -1.412926
INFO: iteration 22, average log likelihood -1.412915
INFO: iteration 23, average log likelihood -1.412905
INFO: iteration 24, average log likelihood -1.412896
INFO: iteration 25, average log likelihood -1.412888
INFO: iteration 26, average log likelihood -1.412881
INFO: iteration 27, average log likelihood -1.412874
INFO: iteration 28, average log likelihood -1.412867
INFO: iteration 29, average log likelihood -1.412861
INFO: iteration 30, average log likelihood -1.412854
INFO: iteration 31, average log likelihood -1.412848
INFO: iteration 32, average log likelihood -1.412841
INFO: iteration 33, average log likelihood -1.412835
INFO: iteration 34, average log likelihood -1.412829
INFO: iteration 35, average log likelihood -1.412822
INFO: iteration 36, average log likelihood -1.412816
INFO: iteration 37, average log likelihood -1.412809
INFO: iteration 38, average log likelihood -1.412803
INFO: iteration 39, average log likelihood -1.412796
INFO: iteration 40, average log likelihood -1.412789
INFO: iteration 41, average log likelihood -1.412783
INFO: iteration 42, average log likelihood -1.412776
INFO: iteration 43, average log likelihood -1.412769
INFO: iteration 44, average log likelihood -1.412762
INFO: iteration 45, average log likelihood -1.412755
INFO: iteration 46, average log likelihood -1.412748
INFO: iteration 47, average log likelihood -1.412741
INFO: iteration 48, average log likelihood -1.412733
INFO: iteration 49, average log likelihood -1.412726
INFO: iteration 50, average log likelihood -1.412719
INFO: EM with 100000 data points 50 iterations avll -1.412719
236.4 data points per parameter
3: avll = [-1.41398,-1.41392,-1.41387,-1.41381,-1.41374,-1.41366,-1.41357,-1.41348,-1.4134,-1.41332,-1.41325,-1.41319,-1.41314,-1.4131,-1.41306,-1.41302,-1.413,-1.41297,-1.41295,-1.41294,-1.41293,-1.41291,-1.4129,-1.4129,-1.41289,-1.41288,-1.41287,-1.41287,-1.41286,-1.41285,-1.41285,-1.41284,-1.41284,-1.41283,-1.41282,-1.41282,-1.41281,-1.4128,-1.4128,-1.41279,-1.41278,-1.41278,-1.41277,-1.41276,-1.41275,-1.41275,-1.41274,-1.41273,-1.41273,-1.41272]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.412721
INFO: iteration 2, average log likelihood -1.412661
INFO: iteration 3, average log likelihood -1.412608
INFO: iteration 4, average log likelihood -1.412548
INFO: iteration 5, average log likelihood -1.412476
INFO: iteration 6, average log likelihood -1.412390
INFO: iteration 7, average log likelihood -1.412290
INFO: iteration 8, average log likelihood -1.412178
INFO: iteration 9, average log likelihood -1.412060
INFO: iteration 10, average log likelihood -1.411942
INFO: iteration 11, average log likelihood -1.411831
INFO: iteration 12, average log likelihood -1.411728
INFO: iteration 13, average log likelihood -1.411635
INFO: iteration 14, average log likelihood -1.411552
INFO: iteration 15, average log likelihood -1.411477
INFO: iteration 16, average log likelihood -1.411411
INFO: iteration 17, average log likelihood -1.411353
INFO: iteration 18, average log likelihood -1.411301
INFO: iteration 19, average log likelihood -1.411254
INFO: iteration 20, average log likelihood -1.411213
INFO: iteration 21, average log likelihood -1.411175
INFO: iteration 22, average log likelihood -1.411140
INFO: iteration 23, average log likelihood -1.411108
INFO: iteration 24, average log likelihood -1.411078
INFO: iteration 25, average log likelihood -1.411049
INFO: iteration 26, average log likelihood -1.411021
INFO: iteration 27, average log likelihood -1.410994
INFO: iteration 28, average log likelihood -1.410967
INFO: iteration 29, average log likelihood -1.410941
INFO: iteration 30, average log likelihood -1.410916
INFO: iteration 31, average log likelihood -1.410891
INFO: iteration 32, average log likelihood -1.410868
INFO: iteration 33, average log likelihood -1.410845
INFO: iteration 34, average log likelihood -1.410823
INFO: iteration 35, average log likelihood -1.410801
INFO: iteration 36, average log likelihood -1.410781
INFO: iteration 37, average log likelihood -1.410762
INFO: iteration 38, average log likelihood -1.410743
INFO: iteration 39, average log likelihood -1.410725
INFO: iteration 40, average log likelihood -1.410708
INFO: iteration 41, average log likelihood -1.410691
INFO: iteration 42, average log likelihood -1.410675
INFO: iteration 43, average log likelihood -1.410660
INFO: iteration 44, average log likelihood -1.410644
INFO: iteration 45, average log likelihood -1.410630
INFO: iteration 46, average log likelihood -1.410615
INFO: iteration 47, average log likelihood -1.410601
INFO: iteration 48, average log likelihood -1.410587
INFO: iteration 49, average log likelihood -1.410573
INFO: iteration 50, average log likelihood -1.410559
INFO: EM with 100000 data points 50 iterations avll -1.410559
118.1 data points per parameter
4: avll = [-1.41272,-1.41266,-1.41261,-1.41255,-1.41248,-1.41239,-1.41229,-1.41218,-1.41206,-1.41194,-1.41183,-1.41173,-1.41163,-1.41155,-1.41148,-1.41141,-1.41135,-1.4113,-1.41125,-1.41121,-1.41118,-1.41114,-1.41111,-1.41108,-1.41105,-1.41102,-1.41099,-1.41097,-1.41094,-1.41092,-1.41089,-1.41087,-1.41084,-1.41082,-1.4108,-1.41078,-1.41076,-1.41074,-1.41073,-1.41071,-1.41069,-1.41068,-1.41066,-1.41064,-1.41063,-1.41061,-1.4106,-1.41059,-1.41057,-1.41056]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.410554
INFO: iteration 2, average log likelihood -1.410481
INFO: iteration 3, average log likelihood -1.410412
INFO: iteration 4, average log likelihood -1.410333
INFO: iteration 5, average log likelihood -1.410237
INFO: iteration 6, average log likelihood -1.410118
INFO: iteration 7, average log likelihood -1.409976
INFO: iteration 8, average log likelihood -1.409816
INFO: iteration 9, average log likelihood -1.409647
INFO: iteration 10, average log likelihood -1.409478
INFO: iteration 11, average log likelihood -1.409319
INFO: iteration 12, average log likelihood -1.409173
INFO: iteration 13, average log likelihood -1.409042
INFO: iteration 14, average log likelihood -1.408928
INFO: iteration 15, average log likelihood -1.408828
INFO: iteration 16, average log likelihood -1.408742
INFO: iteration 17, average log likelihood -1.408667
INFO: iteration 18, average log likelihood -1.408602
INFO: iteration 19, average log likelihood -1.408545
INFO: iteration 20, average log likelihood -1.408496
INFO: iteration 21, average log likelihood -1.408452
INFO: iteration 22, average log likelihood -1.408414
INFO: iteration 23, average log likelihood -1.408379
INFO: iteration 24, average log likelihood -1.408348
INFO: iteration 25, average log likelihood -1.408319
INFO: iteration 26, average log likelihood -1.408293
INFO: iteration 27, average log likelihood -1.408269
INFO: iteration 28, average log likelihood -1.408246
INFO: iteration 29, average log likelihood -1.408224
INFO: iteration 30, average log likelihood -1.408204
INFO: iteration 31, average log likelihood -1.408184
INFO: iteration 32, average log likelihood -1.408165
INFO: iteration 33, average log likelihood -1.408146
INFO: iteration 34, average log likelihood -1.408127
INFO: iteration 35, average log likelihood -1.408109
INFO: iteration 36, average log likelihood -1.408091
INFO: iteration 37, average log likelihood -1.408073
INFO: iteration 38, average log likelihood -1.408055
INFO: iteration 39, average log likelihood -1.408037
INFO: iteration 40, average log likelihood -1.408019
INFO: iteration 41, average log likelihood -1.408001
INFO: iteration 42, average log likelihood -1.407983
INFO: iteration 43, average log likelihood -1.407965
INFO: iteration 44, average log likelihood -1.407947
INFO: iteration 45, average log likelihood -1.407929
INFO: iteration 46, average log likelihood -1.407911
INFO: iteration 47, average log likelihood -1.407893
INFO: iteration 48, average log likelihood -1.407876
INFO: iteration 49, average log likelihood -1.407859
INFO: iteration 50, average log likelihood -1.407843
INFO: EM with 100000 data points 50 iterations avll -1.407843
59.0 data points per parameter
5: avll = [-1.41055,-1.41048,-1.41041,-1.41033,-1.41024,-1.41012,-1.40998,-1.40982,-1.40965,-1.40948,-1.40932,-1.40917,-1.40904,-1.40893,-1.40883,-1.40874,-1.40867,-1.4086,-1.40855,-1.4085,-1.40845,-1.40841,-1.40838,-1.40835,-1.40832,-1.40829,-1.40827,-1.40825,-1.40822,-1.4082,-1.40818,-1.40816,-1.40815,-1.40813,-1.40811,-1.40809,-1.40807,-1.40805,-1.40804,-1.40802,-1.408,-1.40798,-1.40796,-1.40795,-1.40793,-1.40791,-1.40789,-1.40788,-1.40786,-1.40784]
[-1.42063,-1.42065,-1.42059,-1.42055,-1.42051,-1.42046,-1.42041,-1.42036,-1.4203,-1.42024,-1.42016,-1.42001,-1.4197,-1.41906,-1.41798,-1.41673,-1.41582,-1.41537,-1.4152,-1.41514,-1.41512,-1.41511,-1.41511,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.4151,-1.41512,-1.41505,-1.415,-1.41495,-1.41489,-1.41483,-1.41475,-1.41467,-1.41459,-1.41451,-1.41443,-1.41437,-1.41431,-1.41427,-1.41424,-1.41422,-1.4142,-1.41419,-1.41418,-1.41417,-1.41416,-1.41415,-1.41414,-1.41413,-1.41413,-1.41412,-1.41412,-1.41411,-1.4141,-1.4141,-1.41409,-1.41409,-1.41408,-1.41408,-1.41408,-1.41407,-1.41407,-1.41406,-1.41406,-1.41405,-1.41405,-1.41404,-1.41403,-1.41403,-1.41402,-1.41401,-1.41401,-1.414,-1.41399,-1.41398,-1.41398,-1.41392,-1.41387,-1.41381,-1.41374,-1.41366,-1.41357,-1.41348,-1.4134,-1.41332,-1.41325,-1.41319,-1.41314,-1.4131,-1.41306,-1.41302,-1.413,-1.41297,-1.41295,-1.41294,-1.41293,-1.41291,-1.4129,-1.4129,-1.41289,-1.41288,-1.41287,-1.41287,-1.41286,-1.41285,-1.41285,-1.41284,-1.41284,-1.41283,-1.41282,-1.41282,-1.41281,-1.4128,-1.4128,-1.41279,-1.41278,-1.41278,-1.41277,-1.41276,-1.41275,-1.41275,-1.41274,-1.41273,-1.41273,-1.41272,-1.41272,-1.41266,-1.41261,-1.41255,-1.41248,-1.41239,-1.41229,-1.41218,-1.41206,-1.41194,-1.41183,-1.41173,-1.41163,-1.41155,-1.41148,-1.41141,-1.41135,-1.4113,-1.41125,-1.41121,-1.41118,-1.41114,-1.41111,-1.41108,-1.41105,-1.41102,-1.41099,-1.41097,-1.41094,-1.41092,-1.41089,-1.41087,-1.41084,-1.41082,-1.4108,-1.41078,-1.41076,-1.41074,-1.41073,-1.41071,-1.41069,-1.41068,-1.41066,-1.41064,-1.41063,-1.41061,-1.4106,-1.41059,-1.41057,-1.41056,-1.41055,-1.41048,-1.41041,-1.41033,-1.41024,-1.41012,-1.40998,-1.40982,-1.40965,-1.40948,-1.40932,-1.40917,-1.40904,-1.40893,-1.40883,-1.40874,-1.40867,-1.4086,-1.40855,-1.4085,-1.40845,-1.40841,-1.40838,-1.40835,-1.40832,-1.40829,-1.40827,-1.40825,-1.40822,-1.4082,-1.40818,-1.40816,-1.40815,-1.40813,-1.40811,-1.40809,-1.40807,-1.40805,-1.40804,-1.40802,-1.408,-1.40798,-1.40796,-1.40795,-1.40793,-1.40791,-1.40789,-1.40788,-1.40786,-1.40784]
32×26 Array{Float64,2}:
 -0.406       -0.109967    0.339546    0.360288    -0.200002     0.203364    -0.343341     0.118094     0.0763686    0.35411    -0.148925   -0.115595     0.271811    0.242428    0.4499      0.832911    0.241523    -0.472314   -0.197222    -0.107912    -0.0670049   -0.417922    0.228059    0.483025    -0.569872    -0.0245596 
 -0.559556    -0.139003    0.512433   -0.446946     0.569391     0.129064    -0.780057    -0.190599    -0.17494      0.104107   -0.247532   -0.190947     0.547801   -0.225965    0.0560573   0.225496   -0.312089    -0.534639    0.16233     -0.266057    -0.356166     0.0263764   0.374953   -0.462689    -0.271585     0.124504  
 -0.308039    -0.172151    0.206036    0.0472664    0.0599995    1.02878      0.0689761   -0.16114     -0.0474396    0.252128    0.893254   -0.259075    -0.702919   -0.260594    0.873573    0.0640617  -0.384086     0.633359    0.251719     0.845169    -0.476701    -0.119625   -0.362697   -0.264875    -0.467702     0.295678  
  0.404674    -0.199921   -0.218929    0.35801     -0.460618     0.661708     0.365252     0.321841     0.505753     0.103939    0.490404   -0.0207229   -0.161316    0.397316    0.149983   -0.160006    0.122976     0.966708    0.0849677   -0.0829834   -0.375058     0.916708    0.293942    0.75562     -0.40685      0.00733159
  0.10951      0.811532    0.370004   -0.25839     -0.206896    -0.454847    -0.238636     0.579996    -0.240638     0.526496   -0.451376   -0.360047     0.597874    0.125142    0.351502   -0.520383    0.344229     0.0463843  -0.543901    -0.266332    -0.576436    -0.663695   -0.510944   -0.440559    -0.307631    -0.0566099 
  0.264007     0.0310398   0.0731116  -0.212953     0.228777    -0.436       -0.0673385   -0.0768701   -0.259716     0.111851    0.597532   -0.264905     0.370651   -0.0245266   0.303282   -0.704918    0.00299268  -0.0589485  -0.309015    -0.41806     -0.0191871   -0.751477    0.37575    -0.310905     0.573672    -0.8968    
  0.0351483    0.615639   -0.0217424  -0.0376189   -0.853252    -0.24997     -0.361282    -0.00733729   0.39168     -0.16141    -0.2913     -0.0701058    0.342724    0.376522    0.0280176  -0.672259    0.156868    -0.0428071   0.323617    -0.802419     0.473375     0.539872    0.137501   -0.178161     0.00583137  -0.094378  
  0.110807    -0.049599   -0.163914    0.0377856    0.866964     0.0273134   -0.318564     0.366127     0.364261     0.209959   -0.46657     0.0891924    0.259918    0.681319   -0.280292   -0.469517    0.267585    -0.501642    0.273154    -0.46137      0.42472      0.930236   -0.399386   -0.0807802    0.631472     0.13288   
 -0.063038    -0.443947    0.0899872   0.05818     -0.161227    -0.0888954    0.165447    -0.646858    -0.420606     0.308916   -0.110863    0.416676    -0.02328    -0.212439   -0.0785115   0.0948692   0.237277    -0.879569   -0.0786233   -0.0489265    0.0446406   -0.175083   -0.456264   -0.261426    -0.301968    -0.525417  
  0.605739    -0.261195   -0.12403    -0.0857303    0.00345783  -0.2949      -0.246724    -0.256509     0.277309     0.545152   -0.0529322   0.489       -0.200565    0.204034    0.142508   -0.132374   -0.461551     0.138302    0.457195     0.893572     0.17961     -0.0370436  -0.957614   -0.33489      0.258815    -0.300443  
 -0.371142    -0.117923   -0.109071   -0.00148628  -0.181957    -0.128275     0.00522917  -0.258385    -0.168892    -0.303249   -0.070006   -0.00973329  -0.323168    0.138834   -0.352449    0.248685   -0.250541     0.0162491   0.105452     0.140863     0.101025    -0.0638288   0.0450571   0.230833     0.176701     0.311395  
 -0.209943     0.311293    0.440959   -0.127783     0.0502004   -0.0552346    0.142416     0.836188     0.104655    -0.23029     0.285421   -0.517446    -0.0533411  -0.329078    0.427227    0.201724   -0.357206    -0.0805469   0.216303    -0.113635     0.169026    -0.47163     0.37031     0.12001      0.419854     0.0914934 
  0.561297     0.175219    0.334398   -0.22488      1.38944      0.249728     0.277235    -0.529176     0.790122     0.23943     0.171227    0.242474     0.363115   -0.242069   -0.0782502  -0.189279    0.679191    -0.286038   -0.405059     0.475205    -0.587253     0.367219    0.951936    0.029663    -0.348318    -0.0996529 
  0.661353    -0.336734   -0.556291   -0.784336     0.550886    -0.131175     0.271987    -0.204227    -0.0519581   -0.219375   -0.139676   -0.169962    -0.589742   -0.405141   -0.427553   -0.555647    0.0993685    0.794075   -0.34067      0.296439     0.758719     0.0523279   0.108903    0.525973     0.188035    -0.21799   
 -0.130115    -0.141725    0.104027    0.788354    -0.105916     0.0985646    0.612612    -0.0705956   -0.406397    -0.452806   -0.191458    0.0363015   -0.01749     0.0686547   0.0879955   0.0299623   0.306395    -0.104878   -0.0530586    0.517732     0.204764     0.164562   -0.103949   -0.640825     0.413466     0.289392  
  0.253907     0.637272    0.069803    0.445537    -0.0793955    0.10263      0.773934     0.0110402   -0.438854    -0.388396   -0.178543    0.0466186   -0.083678   -0.0993778  -1.0687      0.236749    0.190534     0.0726749  -0.722783    -0.10376     -0.230903    -0.168249   -0.3802      0.395443     0.12223      0.307679  
 -0.144193    -0.251585   -0.458554   -0.0625523   -0.145122    -0.00637699   0.233097     0.568812    -0.249375    -0.293167    0.209124    0.53808      0.0730288  -0.369859    0.490534    0.316881    0.0523658   -0.0198969   0.0773604   -0.757124    -0.641751     0.53307     0.299925    0.246353    -0.0545685   -0.316657  
  0.103104    -0.559506   -0.1552      0.0226855   -0.0677029    0.064041    -0.15751     -0.109742    -0.431254    -0.370473   -0.363203    0.231873     0.681185    0.278313    0.203319    0.370955   -0.0333363    0.417214   -0.0604101   -0.00296582  -0.321573    -0.128      -0.321739   -0.138906    -0.40669     -0.725302  
 -0.106505    -0.282139   -0.12062    -0.218806    -0.161576    -0.328966     0.213378     0.221909    -0.0880396    0.077871    0.163457    0.200425    -0.132627   -0.0896456   0.305427    0.491623   -0.283275     0.328693   -0.180316     0.613517    -0.516298    -0.490326    0.0397851   0.264012    -0.0193802   -0.0743938 
 -0.0968474   -0.271069    0.499979   -0.277775     0.00620501  -0.0737823    0.310503    -0.319079    -0.0699005   -0.321951   -0.394606    0.0655365   -0.296243   -0.138981   -0.138321    0.181453   -0.208386     0.131933   -0.0837196    0.814874     0.55346     -0.398206   -0.16643    -0.024281     0.228207    -0.00471858
  0.097951    -0.0144432  -0.1994     -0.231986    -0.396322    -0.504564    -0.21237      0.100736     0.0308967    0.115275    0.0212281  -0.0116048    0.0967721   0.239899    0.0995137  -0.10833    -0.483077    -0.0703214   0.496932    -0.229722     0.199471    -0.141907   -0.426286   -0.241929     0.282565    -0.165057  
 -0.205832    -0.0610392   0.453509   -0.162079    -0.340932    -0.253161    -0.126811    -0.0704193   -0.287817    -0.23333     0.178634   -0.0663284   -0.0215936  -0.268976   -0.302057    0.2604     -0.455774    -0.282829    0.0622796   -0.647614     0.376465     0.0218903  -0.092627    0.258879     0.229182    -0.511862  
 -0.0935366    0.0580927   0.107448   -0.0543791   -0.134324    -0.0415929   -0.03527      0.0470932    0.00984592  -0.0825562  -0.0498729   0.00263828   0.127816    0.0705129   0.081022    0.0550314  -0.0143906    0.0668098   0.0220476   -0.0366804   -0.050685    -0.0126203   0.0384317  -0.00547065  -0.0533781   -0.0388557 
  0.00257353  -0.120067   -0.0515825   0.191242     0.488327     0.0312527    0.0903644   -0.378558    -0.117813     0.15487     0.0821462   0.0011907   -0.170936   -0.0675738  -0.166686    0.0480987   0.220128    -0.41179    -0.0960333    0.197133    -0.0957579   -0.102715    0.152211    0.0276955    0.219712    -0.00961339
  0.127354     0.771652   -0.0286015  -0.614859     0.0164428    0.146519    -0.0296115    0.194754     0.444345     0.32092     0.199063    0.178975    -0.171934   -0.221436   -0.0222756  -0.458936   -0.23501      0.145706    0.292589    -0.390881    -0.344074     0.210016    0.336269    0.300911    -0.501216     0.631217  
 -0.425147     0.44152    -0.103961   -0.139901     0.0998802   -0.0364078   -0.0748327    0.173121     0.292903    -0.314944    0.276959   -0.362996    -0.044791    0.170964   -0.317592   -0.213258    0.0946571    0.194089    0.0226476   -0.367737    -0.144517     0.284966    0.5854      0.487994     0.562802     0.332634  
 -0.531703    -0.076761   -0.280337    0.442995    -0.0825874    0.445285    -0.0298584   -0.0544804    0.0713572   -0.611796   -0.547697   -0.147553    -0.351703    0.153118   -0.380928    0.714462   -0.00253371  -0.22027     0.0726256    0.110565    -0.0443199    0.500367    0.189353    0.347859    -0.225379     0.782673  
 -0.109173    -0.571137   -0.254827    0.224457     0.286047     0.309506    -0.368481    -0.0905128    0.499087     0.0241112   0.48568     0.0285641   -0.681866   -0.244833    0.136749    0.211635   -0.165624    -0.571685    0.646617     0.187286     0.60742      0.602804    0.331129    0.259049     0.0337603    0.429403  
  0.130132     0.410792    0.509633    0.0987351    0.022895     0.0621058   -0.390702     0.228968     0.354015     0.302876   -0.311773   -0.530192     0.141141   -0.494703   -0.113042   -0.290877    0.364109     0.096919   -0.216529     0.143242     0.00964842  -0.146576   -0.236205   -0.50011     -0.490769     0.276282  
  0.138959     0.174604   -0.260444   -0.121864    -0.141334     0.0399635   -0.491902    -0.275187     0.174815     0.220759   -0.073925   -0.162263     0.0371196   0.655666   -0.199414   -0.320786    0.234918     0.438048   -0.223825     0.182694    -0.396431    -0.0782145  -0.140006   -0.438176    -0.404333     0.267642  
  0.415422     0.0289104   0.109261    0.150252    -0.00992302   0.125987     0.40891      0.485969     0.197822     0.0571224   0.167813    0.0233349   -0.21103    -0.561982    0.216084   -0.183682    0.0383298    0.102625    0.00447434  -0.00122884   0.180042     0.362266   -0.0766801   0.145103     0.30434      0.0728    
  0.477074     0.239501   -0.117905    0.568929    -0.0578775    0.529878     0.460896     0.142585     0.459334     0.145266    0.0470838  -0.382575    -0.27795     0.450213    0.418557   -0.573911    0.578034     0.295271    0.00149146   0.253319     0.127984     0.323329    0.140687    0.0414628    0.044048     0.17488   INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.407826
INFO: iteration 2, average log likelihood -1.407811
INFO: iteration 3, average log likelihood -1.407795
INFO: iteration 4, average log likelihood -1.407780
INFO: iteration 5, average log likelihood -1.407766
INFO: iteration 6, average log likelihood -1.407752
INFO: iteration 7, average log likelihood -1.407738
INFO: iteration 8, average log likelihood -1.407724
INFO: iteration 9, average log likelihood -1.407711
INFO: iteration 10, average log likelihood -1.407698
INFO: EM with 100000 data points 10 iterations avll -1.407698
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.887892e+05
      1       6.986802e+05      -1.901090e+05 |       32
      2       6.851309e+05      -1.354929e+04 |       32
      3       6.796167e+05      -5.514227e+03 |       32
      4       6.769047e+05      -2.711947e+03 |       32
      5       6.751885e+05      -1.716243e+03 |       32
      6       6.739927e+05      -1.195750e+03 |       32
      7       6.730592e+05      -9.334944e+02 |       32
      8       6.722605e+05      -7.986954e+02 |       32
      9       6.715928e+05      -6.677214e+02 |       32
     10       6.710456e+05      -5.472192e+02 |       32
     11       6.706266e+05      -4.189841e+02 |       32
     12       6.702710e+05      -3.555860e+02 |       32
     13       6.699599e+05      -3.111591e+02 |       32
     14       6.696983e+05      -2.615958e+02 |       32
     15       6.694676e+05      -2.306706e+02 |       32
     16       6.692616e+05      -2.060440e+02 |       32
     17       6.690816e+05      -1.799918e+02 |       32
     18       6.689338e+05      -1.477796e+02 |       32
     19       6.687888e+05      -1.450210e+02 |       32
     20       6.686502e+05      -1.385911e+02 |       32
     21       6.685145e+05      -1.356221e+02 |       32
     22       6.683910e+05      -1.235505e+02 |       32
     23       6.682773e+05      -1.136467e+02 |       32
     24       6.681846e+05      -9.270121e+01 |       32
     25       6.681051e+05      -7.952988e+01 |       32
     26       6.680341e+05      -7.105273e+01 |       32
     27       6.679742e+05      -5.989186e+01 |       32
     28       6.679087e+05      -6.548024e+01 |       32
     29       6.678404e+05      -6.830119e+01 |       32
     30       6.677764e+05      -6.399162e+01 |       32
     31       6.677132e+05      -6.321383e+01 |       32
     32       6.676527e+05      -6.052468e+01 |       32
     33       6.676001e+05      -5.251761e+01 |       32
     34       6.675508e+05      -4.933742e+01 |       32
     35       6.675038e+05      -4.699199e+01 |       32
     36       6.674573e+05      -4.652924e+01 |       32
     37       6.674240e+05      -3.328812e+01 |       32
     38       6.673909e+05      -3.313963e+01 |       32
     39       6.673550e+05      -3.581549e+01 |       32
     40       6.673190e+05      -3.603248e+01 |       32
     41       6.672838e+05      -3.520452e+01 |       32
     42       6.672529e+05      -3.086191e+01 |       32
     43       6.672266e+05      -2.634517e+01 |       32
     44       6.672060e+05      -2.058950e+01 |       32
     45       6.671901e+05      -1.592042e+01 |       32
     46       6.671747e+05      -1.536965e+01 |       32
     47       6.671614e+05      -1.330324e+01 |       32
     48       6.671489e+05      -1.255257e+01 |       32
     49       6.671337e+05      -1.515100e+01 |       32
     50       6.671201e+05      -1.364637e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 667120.0654669738)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.419319
INFO: iteration 2, average log likelihood -1.414332
INFO: iteration 3, average log likelihood -1.413005
INFO: iteration 4, average log likelihood -1.412009
INFO: iteration 5, average log likelihood -1.410959
INFO: iteration 6, average log likelihood -1.410001
INFO: iteration 7, average log likelihood -1.409343
INFO: iteration 8, average log likelihood -1.408965
INFO: iteration 9, average log likelihood -1.408743
INFO: iteration 10, average log likelihood -1.408594
INFO: iteration 11, average log likelihood -1.408482
INFO: iteration 12, average log likelihood -1.408392
INFO: iteration 13, average log likelihood -1.408317
INFO: iteration 14, average log likelihood -1.408254
INFO: iteration 15, average log likelihood -1.408199
INFO: iteration 16, average log likelihood -1.408152
INFO: iteration 17, average log likelihood -1.408110
INFO: iteration 18, average log likelihood -1.408073
INFO: iteration 19, average log likelihood -1.408040
INFO: iteration 20, average log likelihood -1.408010
INFO: iteration 21, average log likelihood -1.407982
INFO: iteration 22, average log likelihood -1.407957
INFO: iteration 23, average log likelihood -1.407933
INFO: iteration 24, average log likelihood -1.407911
INFO: iteration 25, average log likelihood -1.407890
INFO: iteration 26, average log likelihood -1.407871
INFO: iteration 27, average log likelihood -1.407852
INFO: iteration 28, average log likelihood -1.407834
INFO: iteration 29, average log likelihood -1.407817
INFO: iteration 30, average log likelihood -1.407801
INFO: iteration 31, average log likelihood -1.407785
INFO: iteration 32, average log likelihood -1.407770
INFO: iteration 33, average log likelihood -1.407755
INFO: iteration 34, average log likelihood -1.407741
INFO: iteration 35, average log likelihood -1.407727
INFO: iteration 36, average log likelihood -1.407714
INFO: iteration 37, average log likelihood -1.407701
INFO: iteration 38, average log likelihood -1.407688
INFO: iteration 39, average log likelihood -1.407676
INFO: iteration 40, average log likelihood -1.407664
INFO: iteration 41, average log likelihood -1.407652
INFO: iteration 42, average log likelihood -1.407640
INFO: iteration 43, average log likelihood -1.407629
INFO: iteration 44, average log likelihood -1.407618
INFO: iteration 45, average log likelihood -1.407607
INFO: iteration 46, average log likelihood -1.407597
INFO: iteration 47, average log likelihood -1.407586
INFO: iteration 48, average log likelihood -1.407576
INFO: iteration 49, average log likelihood -1.407566
INFO: iteration 50, average log likelihood -1.407556
INFO: EM with 100000 data points 50 iterations avll -1.407556
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.129328     0.0681139   0.301768     0.131022    0.11705      0.285018    0.508191    -0.00845481   0.0085041   0.0331891    0.0113015  -0.350056    -0.696889   -0.0115927    0.457341      0.143788    0.0279035    0.395856     0.0101089   1.14637    -0.122039   -0.392181   -0.281545    -0.337341     0.188965    0.586496  
 -0.0224252    0.132533    0.260696    -0.133036   -0.525432    -0.127865   -0.241332    -0.123107    -0.162607   -0.0944122    0.246928    0.0659337    0.152756   -0.00317542  -0.265129      0.118492   -0.227126    -0.198529     0.0625048  -0.861263    0.319407    0.184038    0.045886     0.538137    -0.0400091  -0.694979  
  0.264727    -0.0381796  -0.48227      0.59382     0.357337     0.355437   -0.464786     0.197945     0.122321    0.329547     0.197853   -0.390244     0.0190658   0.185051     0.283118     -0.354036    0.260768    -0.738031     0.0821057  -0.495292   -0.2106      0.235505    0.181128    -0.215471     0.110332   -0.207576  
  0.0421135    0.0670493   0.0254804    0.0279195  -0.167303    -0.523841   -0.00217754   0.667385     0.526771    0.00881112   0.497311   -0.243643    -0.596794   -0.212952     0.24523       0.155341   -0.632612     0.160268     0.0448955  -0.028064    0.221051   -0.324652    0.208449     0.602898     0.696842    0.138957  
  0.688367    -0.548284    0.410386    -0.159352    0.344894     0.880497   -0.0923291    0.105945     0.59862    -0.235756    -0.359478   -0.01473      0.716491    0.140319     0.196288      0.466049    0.306671     0.596928    -0.437084    0.0892999   0.386008    0.290436   -0.00371253  -0.146799    -0.315029   -0.182694  
  0.218973     0.823401    0.19491     -0.189347   -0.293988     0.382011    0.2482       0.507867    -0.1249     -0.132762     0.1459      0.145876    -0.0925547  -0.570844    -0.344992     -0.748041   -0.259053     0.37881      0.234784   -0.294309   -0.0257036   0.6617     -0.221005    -0.809832     0.176411    0.369204  
  0.212504     0.866408    0.26397      0.278345   -0.134933    -0.130505    0.72433      0.145898    -0.493153   -0.50975     -0.395228   -0.187121     0.120271    0.0290811   -0.917283      0.302154    0.0492447    0.210137    -0.544575   -0.049199   -0.23973    -0.386711   -0.22492      0.378958     0.166751    0.267249  
  0.366497    -0.390301    0.0806636   -0.442156    0.298494    -0.203802    0.157177    -0.396696    -0.133375   -0.15412     -0.120803   -0.136007    -0.496567   -0.213646    -0.379412     -0.281623   -0.127925     0.304705    -0.288477    0.538731    0.71646    -0.391766   -0.0952717    0.0332576    0.307659   -0.281959  
  0.0356685   -0.0657258  -0.0220011   -0.372845   -0.255228    -0.461357   -0.235836     0.00824447  -0.117783   -0.238488    -0.226117    0.0505506    0.549807    0.144113     0.200722      0.0229521  -0.40664      0.0960103    0.510793   -0.176087   -0.0342553  -0.0929344  -0.128417    -0.308906     0.113334   -0.373036  
  0.153671     0.121765    0.00261634  -0.0207137  -0.17365      0.587859   -0.0277283    0.149769     0.44683     0.2072       0.473953   -0.031135    -0.289335    0.235598     0.432269     -0.343149   -0.11411      0.972438     0.27807     0.0811245  -0.582184    0.367461    0.220784     0.355318    -0.790671    0.230325  
 -0.246682     0.0149417   0.187517    -0.128463    0.0154403   -0.0668531  -0.0329407    0.0102687   -0.0141558  -0.161349     0.0753935  -0.176069     0.0196775  -0.148065     0.000362793   0.113659   -0.18704     -0.295656     0.137465   -0.0485405   0.215509   -0.0953954   0.183717     0.00825685   0.271631    0.0304636 
 -0.12002     -0.363592    0.232056     0.117172    0.0230319    0.0970517  -0.290524    -0.20274      0.354015    0.224909     0.367015    0.0891737   -0.40164    -0.190637     0.515938     -0.0800275  -0.200427    -0.489099     0.536912    0.100851    0.547925    0.322491   -0.00972596  -0.466622    -0.0859145   0.338502  
  0.336584     0.2153     -0.435914    -0.898376    0.256407    -0.150488    0.123119     0.451424     0.129907    0.115342     0.23222     0.264681    -0.305887   -0.503762     0.191875     -0.247172    0.0459237    0.205706     0.449584   -0.19695    -0.203541    0.278458    0.216489     0.619046    -0.22332     0.158253  
 -0.683271     0.111041   -0.063145     0.0658027  -0.124814     0.32006     0.164336     0.642306    -0.0930572  -0.541017     0.0995909   0.0487664    0.12989    -0.336448     0.413597      0.292377    0.20881      0.0631035    0.0137202  -1.08629    -0.513273    0.551818    0.612798     0.278376    -0.143235    0.158352  
 -0.80212     -0.256851    0.430942    -0.274673    0.0898191   -0.0495818   0.100232    -0.162442    -0.0730665  -0.40158     -0.126653    0.219903    -0.0554041  -0.082794    -0.309402      0.608909   -0.379488    -0.314939    -0.0411042   0.237021   -0.0173655  -0.0276463   0.366337     0.274295     0.333323    0.349629  
 -0.654431    -0.0930298   0.46125     -0.553576    0.439375    -0.0194338  -0.887052    -0.304789    -0.24229     0.123839    -0.372164   -0.130239     0.2803     -0.220255    -0.106851      0.310655   -0.329444    -0.528372     0.123374   -0.11045    -0.430371   -0.222047    0.0897629   -0.459175    -0.452133    0.187475  
 -0.00178075   0.0200391  -0.319913    -0.163111    0.203544    -0.127714    0.0447166    0.111854    -0.0581662   0.172641    -0.0163123   0.297227     0.414672    0.057931    -0.259754      0.340993    0.0294088    0.0937092   -0.483969   -0.0505501  -0.654817   -0.159505    0.0267706    0.317788     0.19686    -0.190947  
  0.605071     0.0836201  -0.139314     0.593535   -0.242386     0.574822    0.889987     0.323578     0.577484    0.160611     0.149686   -0.20345     -0.158889    0.127344     0.391751     -0.562226    0.513222     0.490474    -0.0499806   0.174636    0.13533     0.745446    0.220689     0.382093     0.319032    0.177241  
  0.140214     0.594055    0.64018     -0.0435809  -0.205838    -0.20382    -0.4488       0.472255     0.131366    0.566563    -0.506707   -0.359867     0.494375   -0.258448     0.281505     -0.279546    0.402485    -0.0895163   -0.479925   -0.0823072  -0.342611   -0.484392   -0.476569    -0.502959    -0.605092    0.203203  
 -0.523043    -0.387971   -0.226953     0.37998    -1.14195     -0.272654   -0.0587715    0.400889    -0.684586   -0.248146    -0.0745971  -0.286922    -0.483184    0.172508    -0.212884      0.243479   -0.288902     0.00739705   0.322321   -0.434798    0.825554   -0.257126   -1.19416     -0.221658     0.400633    0.11108   
 -0.475591    -0.27271     0.0222625    0.38558     0.00224638   0.108631   -0.231715     0.127559     0.111676    0.0903757   -0.427217   -0.341226     0.105587    0.391406     0.456367      1.24273     0.336469    -0.462475    -0.0743769   0.0410306   0.0810276  -0.447583    0.188479     0.896468    -0.474826    0.169911  
  0.133172     0.30047     0.1766      -0.33331     0.0969576   -0.550432    0.115302     0.101191    -0.315851    0.133255     0.3559     -0.281149     0.562748   -0.00133289   0.369182     -0.734074    0.00810863  -0.0568459   -0.226746   -0.455815    0.0104297  -0.785156    0.299486    -0.406971     0.556174   -0.707214  
 -0.0734681   -0.234649    0.0604023    0.524006    0.0329353    0.0146817   0.266541    -0.0726722   -0.256807   -0.299598    -0.244447    0.0598722    0.0108967   0.00109168   0.0363297     0.0998699   0.177133    -0.136475    -0.0785809   0.273341    0.131283    0.0598623  -0.0505452   -0.189        0.142748   -0.00264878
  0.0137228    0.165916    0.115297    -0.0661009  -0.0202503    0.0295891  -0.0259754    0.0372434    0.15588     0.11611      0.084679   -0.166052    -0.119025   -0.0182064    0.0320962    -0.0755492   0.0326723    0.0568558    0.0316915   0.0280431   0.0245608  -0.0331481   0.0261519    0.0380491   -0.0193239   0.100678  
  0.0112198   -0.413476    0.203288    -0.0946323  -0.235114     0.0418201   0.290152     0.111097    -0.454644   -0.0696499    0.261529    0.118658    -0.0536404  -0.595795     0.515193      0.68492    -0.24311      0.278435    -0.108711    0.44988    -0.384008   -0.347845    0.0232677    0.207256    -0.34979    -0.465945  
 -0.0687318    0.205508   -0.0445013    0.0210591   0.320797    -0.222175   -0.301722     0.273637     0.511256    0.0816015   -0.536413   -0.00750209   0.239711    0.700977    -0.307523     -0.463573    0.257117    -0.422481     0.410092   -0.549213    0.556192    0.82196    -0.139558     0.0501456    0.703312    0.315457  
 -0.103166    -0.14059    -0.557435     0.503298   -0.0263279    0.393557    0.263335    -0.296804     0.122836   -0.674641     0.0551168  -0.0178065   -0.673297    0.163718    -0.566844      0.408588   -0.0365908   -0.00667978   0.369938    0.388774    0.190009    0.611753    0.456032     0.554818     0.145535    0.532754  
  0.166704     0.415724   -0.422688    -0.153058   -0.130982    -0.0417593  -0.309419    -0.304888     0.354061   -0.0361753   -0.173286   -0.188429    -0.06365     0.532714    -0.361744     -0.673827    0.461982     0.510983    -0.405781   -0.159655   -0.213749    0.187249    0.0388524   -0.0638465   -0.370004    0.30989   
  0.54293     -0.205975   -0.17311     -0.174046   -0.083291    -0.324133   -0.239548    -0.269329     0.263762    0.655142    -0.111459    0.458721    -0.102654    0.197687     0.0559875    -0.0401921  -0.469097     0.108437     0.522554    0.745606    0.122917   -0.10512    -1.03787     -0.304554     0.211104   -0.320622  
  0.193832     0.385852    0.30886      0.110581    0.86753      0.275084    0.393754    -0.564844     0.366146    0.465674     0.188535    0.0286571   -0.161757   -0.342767    -0.37137      -0.198413    0.627161    -0.72647     -0.339858    0.300818   -0.247403   -0.0203235   0.465301     0.148831    -0.130284    0.218435  
 -0.00783416  -0.62702    -0.222757     0.342981    0.0673064   -0.006236    0.292232    -0.421013    -0.663674   -0.163361    -0.321328    0.613424     0.194841    0.143926    -0.00449766    0.145117    0.211367    -0.199865    -0.25136     0.282056   -0.244125    0.055613   -0.404708    -0.236054    -0.199986   -0.569052  
 -0.226432    -0.0489338  -0.183719     0.186933   -0.522264    -0.0124447  -0.35351      0.329551     0.0822343   0.205732     0.416593   -0.0391597    0.250711    0.741664     0.445336      0.0043628  -0.252191     0.229294    -0.0253698   0.171692   -0.964068   -0.278876   -0.0500507   -0.315239    -0.320068   -0.142555  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.407547
INFO: iteration 2, average log likelihood -1.407537
INFO: iteration 3, average log likelihood -1.407528
INFO: iteration 4, average log likelihood -1.407519
INFO: iteration 5, average log likelihood -1.407510
INFO: iteration 6, average log likelihood -1.407502
INFO: iteration 7, average log likelihood -1.407493
INFO: iteration 8, average log likelihood -1.407485
INFO: iteration 9, average log likelihood -1.407476
INFO: iteration 10, average log likelihood -1.407468
INFO: EM with 100000 data points 10 iterations avll -1.407468
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
