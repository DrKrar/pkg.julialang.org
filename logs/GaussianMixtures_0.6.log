>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.7.0
INFO: Installing JLD v0.6.6
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.1
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.3.0
INFO: Installing ScikitLearnBase v0.2.1
INFO: Installing StaticArrays v0.1.0
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
WARNING: takebuf_string(b) is deprecated, use String(take!(b)) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in takebuf_string(::Base.AbstractIOBuffer{Array{UInt8,1}}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/BinDeps/src/dependencies.jl:887 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::SubString{String}) at ./sysimg.jl:14
 in evalfile(::SubString{String}, ::Array{String,1}) at ./loading.jl:572 (repeats 2 times)
 in cd(::##2#4, ::String) at ./file.jl:69
 in (::##1#3)(::IOStream) at ./none:13
 in open(::##1#3, ::String, ::String) at ./iostream.jl:152
 in eval(::Module, ::Any) at ./boot.jl:236
 in process_options(::Base.JLOptions) at ./client.jl:248
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/Rmath/deps/build.jl, in expression starting on line 39
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1297
Commit 416f5f2 (2016-11-23 13:12 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-101-generic #148-Ubuntu SMP Thu Oct 20 22:08:32 UTC 2016 x86_64 x86_64
Memory: 2.939281463623047 GB (655.32421875 MB free)
Uptime: 27716.0 sec
Load Avg:  0.9892578125  0.998046875  0.99169921875
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3499 MHz    1670966 s       1488 s     179410 s     625088 s         49 s
#2  3499 MHz     760055 s       6744 s      88864 s    1814245 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.4
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.7.0
 - JLD                           0.6.6
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.1
 - Rmath                         0.1.4
 - SHA                           0.3.0
 - ScikitLearnBase               0.2.1
 - StaticArrays                  0.1.0
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: takebuf_string(b) is deprecated, use String(take!(b)) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in takebuf_string(::Base.AbstractIOBuffer{Array{UInt8,1}}) at ./deprecated.jl:50
 in #_write#17(::Array{Any,1}, ::Function, ::JLD.JldFile, ::String, ::Array{Any,1}, ::JLD.JldWriteSession) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:587
 in #write#14(::Array{Any,1}, ::Function, ::JLD.JldFile, ::String, ::Array{Any,1}, ::JLD.JldWriteSession) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:529
 in #jldopen#9(::Bool, ::Bool, ::Bool, ::Function, ::String, ::Bool, ::Bool, ::Bool, ::Bool, ::Bool) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:198
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::String, ::Bool, ::Bool, ::Bool, ::Bool, ::Bool) at ./<missing>:0
 in #jldopen#10(::Bool, ::Bool, ::Bool, ::Function, ::String, ::String) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:253
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::String, ::String) at ./<missing>:0
 in #jldopen#11(::Array{Any,1}, ::Function, ::JLD.##34#35{String,Array{Float64,2},Tuple{}}, ::String, ::Vararg{String,N}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:263
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::Function, ::String, ::String) at ./<missing>:0
 in #save#33(::Bool, ::Bool, ::Function, ::FileIO.File{FileIO.DataFormat{:JLD}}, ::String, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:1217
 in save(::FileIO.File{FileIO.DataFormat{:JLD}}, ::String, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:1214
 in #save#14(::Array{Any,1}, ::Function, ::String, ::String, ::Vararg{Any,N}) at /home/vagrant/.julia/v0.6/FileIO/src/loadsave.jl:54
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:8 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##790#792{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-2.672833896981033e6,[3055.76,96944.2],
[-5291.67 -1782.25 -1731.82; 5688.99 1170.09 1735.24],

Array{Float64,2}[
[9832.06 3195.75 2040.06; 3195.75 2493.01 1235.37; 2040.06 1235.37 2970.9],

[89599.2 -3410.19 -2131.74; -3410.19 97356.0 -781.549; -2131.74 -781.549 97642.9]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.500490e+03
      1       9.715789e+02      -5.289112e+02 |        7
      2       8.460306e+02      -1.255483e+02 |        4
      3       8.303698e+02      -1.566073e+01 |        2
      4       8.234973e+02      -6.872492e+00 |        3
      5       8.150919e+02      -8.405457e+00 |        0
      6       8.150919e+02       0.000000e+00 |        0
K-means converged with 6 iterations (objv = 815.091882939284)
INFO: K-means with 272 data points using 6 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.052558
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.802675
INFO: iteration 2, lowerbound -3.701170
INFO: iteration 3, lowerbound -3.594590
INFO: iteration 4, lowerbound -3.476748
INFO: iteration 5, lowerbound -3.362877
INFO: iteration 6, lowerbound -3.264492
INFO: dropping number of Gaussions to 7
INFO: iteration 7, lowerbound -3.174850
INFO: iteration 8, lowerbound -3.088097
INFO: dropping number of Gaussions to 5
INFO: iteration 9, lowerbound -2.990736
INFO: iteration 10, lowerbound -2.886758
INFO: iteration 11, lowerbound -2.801724
INFO: dropping number of Gaussions to 4
INFO: iteration 12, lowerbound -2.730996
INFO: iteration 13, lowerbound -2.666618
INFO: dropping number of Gaussions to 3
INFO: iteration 14, lowerbound -2.603180
INFO: iteration 15, lowerbound -2.531556
INFO: iteration 16, lowerbound -2.464862
INFO: iteration 17, lowerbound -2.408817
INFO: iteration 18, lowerbound -2.366050
INFO: iteration 19, lowerbound -2.334630
INFO: iteration 20, lowerbound -2.314265
INFO: iteration 21, lowerbound -2.307396
INFO: dropping number of Gaussions to 2
INFO: iteration 22, lowerbound -2.302940
INFO: iteration 23, lowerbound -2.299261
INFO: iteration 24, lowerbound -2.299257
INFO: iteration 25, lowerbound -2.299255
INFO: iteration 26, lowerbound -2.299254
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Fri 25 Nov 2016 01:11:51 PM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Fri 25 Nov 2016 01:11:52 PM UTC: K-means with 272 data points using 6 iterations
11.3 data points per parameter
,Fri 25 Nov 2016 01:11:54 PM UTC: EM with 272 data points 0 iterations avll -2.052558
5.8 data points per parameter
,Fri 25 Nov 2016 01:11:55 PM UTC: GMM converted to Variational GMM
,Fri 25 Nov 2016 01:11:57 PM UTC: iteration 1, lowerbound -3.802675
,Fri 25 Nov 2016 01:11:57 PM UTC: iteration 2, lowerbound -3.701170
,Fri 25 Nov 2016 01:11:57 PM UTC: iteration 3, lowerbound -3.594590
,Fri 25 Nov 2016 01:11:57 PM UTC: iteration 4, lowerbound -3.476748
,Fri 25 Nov 2016 01:11:57 PM UTC: iteration 5, lowerbound -3.362877
,Fri 25 Nov 2016 01:11:57 PM UTC: iteration 6, lowerbound -3.264492
,Fri 25 Nov 2016 01:11:57 PM UTC: dropping number of Gaussions to 7
,Fri 25 Nov 2016 01:11:57 PM UTC: iteration 7, lowerbound -3.174850
,Fri 25 Nov 2016 01:11:57 PM UTC: iteration 8, lowerbound -3.088097
,Fri 25 Nov 2016 01:11:57 PM UTC: dropping number of Gaussions to 5
,Fri 25 Nov 2016 01:11:57 PM UTC: iteration 9, lowerbound -2.990736
,Fri 25 Nov 2016 01:11:57 PM UTC: iteration 10, lowerbound -2.886758
,Fri 25 Nov 2016 01:11:58 PM UTC: iteration 11, lowerbound -2.801724
,Fri 25 Nov 2016 01:11:58 PM UTC: dropping number of Gaussions to 4
,Fri 25 Nov 2016 01:11:58 PM UTC: iteration 12, lowerbound -2.730996
,Fri 25 Nov 2016 01:11:58 PM UTC: iteration 13, lowerbound -2.666618
,Fri 25 Nov 2016 01:11:58 PM UTC: dropping number of Gaussions to 3
,Fri 25 Nov 2016 01:11:58 PM UTC: iteration 14, lowerbound -2.603180
,Fri 25 Nov 2016 01:11:58 PM UTC: iteration 15, lowerbound -2.531556
,Fri 25 Nov 2016 01:11:58 PM UTC: iteration 16, lowerbound -2.464862
,Fri 25 Nov 2016 01:11:58 PM UTC: iteration 17, lowerbound -2.408817
,Fri 25 Nov 2016 01:11:58 PM UTC: iteration 18, lowerbound -2.366050
,Fri 25 Nov 2016 01:11:58 PM UTC: iteration 19, lowerbound -2.334630
,Fri 25 Nov 2016 01:11:58 PM UTC: iteration 20, lowerbound -2.314265
,Fri 25 Nov 2016 01:11:58 PM UTC: iteration 21, lowerbound -2.307396
,Fri 25 Nov 2016 01:11:58 PM UTC: dropping number of Gaussions to 2
,Fri 25 Nov 2016 01:11:58 PM UTC: iteration 22, lowerbound -2.302940
,Fri 25 Nov 2016 01:11:58 PM UTC: iteration 23, lowerbound -2.299261
,Fri 25 Nov 2016 01:11:58 PM UTC: iteration 24, lowerbound -2.299257
,Fri 25 Nov 2016 01:11:58 PM UTC: iteration 25, lowerbound -2.299255
,Fri 25 Nov 2016 01:11:59 PM UTC: iteration 26, lowerbound -2.299254
,Fri 25 Nov 2016 01:11:59 PM UTC: iteration 27, lowerbound -2.299253
,Fri 25 Nov 2016 01:11:59 PM UTC: iteration 28, lowerbound -2.299253
,Fri 25 Nov 2016 01:11:59 PM UTC: iteration 29, lowerbound -2.299253
,Fri 25 Nov 2016 01:11:59 PM UTC: iteration 30, lowerbound -2.299253
,Fri 25 Nov 2016 01:11:59 PM UTC: iteration 31, lowerbound -2.299253
,Fri 25 Nov 2016 01:11:59 PM UTC: iteration 32, lowerbound -2.299253
,Fri 25 Nov 2016 01:11:59 PM UTC: iteration 33, lowerbound -2.299253
,Fri 25 Nov 2016 01:11:59 PM UTC: iteration 34, lowerbound -2.299253
,Fri 25 Nov 2016 01:11:59 PM UTC: iteration 35, lowerbound -2.299253
,Fri 25 Nov 2016 01:11:59 PM UTC: iteration 36, lowerbound -2.299253
,Fri 25 Nov 2016 01:11:59 PM UTC: iteration 37, lowerbound -2.299253
,Fri 25 Nov 2016 01:11:59 PM UTC: iteration 38, lowerbound -2.299253
,Fri 25 Nov 2016 01:11:59 PM UTC: iteration 39, lowerbound -2.299253
,Fri 25 Nov 2016 01:12:00 PM UTC: iteration 40, lowerbound -2.299253
,Fri 25 Nov 2016 01:12:00 PM UTC: iteration 41, lowerbound -2.299253
,Fri 25 Nov 2016 01:12:00 PM UTC: iteration 42, lowerbound -2.299253
,Fri 25 Nov 2016 01:12:00 PM UTC: iteration 43, lowerbound -2.299253
,Fri 25 Nov 2016 01:12:00 PM UTC: iteration 44, lowerbound -2.299253
,Fri 25 Nov 2016 01:12:00 PM UTC: iteration 45, lowerbound -2.299253
,Fri 25 Nov 2016 01:12:00 PM UTC: iteration 46, lowerbound -2.299253
,Fri 25 Nov 2016 01:12:00 PM UTC: iteration 47, lowerbound -2.299253
,Fri 25 Nov 2016 01:12:00 PM UTC: iteration 48, lowerbound -2.299253
,Fri 25 Nov 2016 01:12:00 PM UTC: iteration 49, lowerbound -2.299253
,Fri 25 Nov 2016 01:12:00 PM UTC: iteration 50, lowerbound -2.299253
,Fri 25 Nov 2016 01:12:00 PM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.045,95.9549]
β = [178.045,95.9549]
m = [4.2503 79.2869; 2.00023 53.852]
ν = [180.045,97.9549]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.184042 -0.00764405; 0.0 0.00858171],

[0.375876 -0.00895312; 0.0 0.0127487]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.00000000012
avll from stats: -0.9653370990665522
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9653370990665501
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.96533709906655
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 99999.99999999999
avll from stats: -0.9682789432798359
avll from llpg:  -0.9682789432798354
avll direct:     -0.9682789432798355
sum posterior: 100000.0
32×26 Array{Float64,2}:
  0.0752603   -0.0412153   -0.0534871  -0.0184666    0.0619318    0.0712824   -0.033025     -0.006407    -0.195359     0.0189529   0.0222104   0.116732     -0.0938152     0.0733615   -0.0781579   -0.0937988   -0.10426     -0.0845731   -0.0218034  -0.0157522    0.18398      0.00443841   0.0410795    0.0765577  -0.0467943    0.174145 
  0.0405778   -0.0330278   -0.110346   -0.0052529    0.0469724    0.0768993    0.243075      0.101076    -0.104595     0.0945922  -0.188898    0.0412249    -0.0380004     0.178029     0.0735628    0.0502517    0.0260215    0.0996867    0.0638946  -0.00746006  -0.206228     0.0201383    0.141242    -0.0680891  -0.148824    -0.0234203
 -0.0343446   -0.0769851    0.074116   -0.0759972    0.114526    -0.114725    -0.0512365    -0.00978256  -0.0645176    0.17295     0.0751673   0.0608756    -0.0189168     0.0471346    0.0370369   -0.0902217    0.112007    -0.200735     0.132011    0.0656599    0.0209727    0.0838576    0.038918    -0.0936763  -0.147764     0.0880219
  0.193058     0.222269     0.0908681   0.0554837    0.16729     -0.0412715   -0.222152      0.0357352   -0.0971188   -0.0284692   0.206873   -0.0218656     0.0390332    -0.0114105   -0.150161     0.00158352   0.00267858  -0.0462157   -0.0693079  -0.0927659   -0.0315538    0.0644936    0.110447     0.0154134   0.047286    -0.0679678
  0.0145233    0.0298689   -0.103422    0.059076    -0.0132376   -0.184803     0.0601758     0.0878195    0.0293126   -0.03854     0.187567   -0.0874425    -0.0472065    -0.00249516  -0.0249717   -0.0997814    0.0728663    0.257722     0.203199   -0.0141304   -0.0275335    0.0801134   -0.153007    -0.0904923  -0.0170903    0.0556896
 -0.24772     -0.0946262    0.195848    0.0696807   -0.154813    -0.0742762   -0.0516229     0.0276604   -0.150628    -0.0210201  -0.0430324  -0.0505303     0.128087      0.0204299    0.101458    -0.119214    -0.177803     0.0416929   -0.0317141   0.0196866   -0.0704791   -0.268592    -0.127924    -0.0419029  -0.0398217    0.193632 
 -0.032763    -0.0323381    0.0560381  -0.0154592    0.0686573    0.00610318  -0.000373214  -0.0620624   -0.0511964    0.0900138  -0.0573158   0.133366      0.118162      0.0714157   -0.0921934   -0.018367     0.0757877   -0.034643     0.046076   -0.00545191  -0.0829006    0.0737251   -0.159163     0.0647374   0.0211326   -0.0249653
 -0.0884323    0.0859163    0.0104287   0.122406    -0.0260372    0.124331    -0.133828      0.0142571    0.140546     0.0616382   0.236721   -0.100895      0.00688631   -0.128512     0.0348057   -0.0570214    0.0840221    0.059612     0.259341   -0.00567533  -0.120206     0.0364781    0.0522125   -0.111798   -0.140403     0.0675654
  0.169681     0.0154203    0.120838   -0.0281965    0.0319251    0.1206      -0.0761628     0.137727    -0.134567     0.0113398   0.0384029  -0.212408      0.11736       0.160613     0.0437663   -0.0141058    0.0903162   -0.0670252    0.0257664   0.0295741    0.0356278   -0.0871734    0.12626      0.0204366  -0.174778    -0.0454138
 -0.0631312   -0.0880263    0.0398936   0.0297331    0.108472    -0.00180808  -0.0383788     0.166274    -0.0648326   -0.0201829  -0.0718783  -0.0825819     0.0304497    -0.0874567   -0.13126     -0.118498    -0.0249471    0.168475     0.0448788   0.0275168   -0.167045    -0.120505    -0.217411     0.181507   -0.0748719    0.0621479
 -0.00990731   0.00457872  -0.0195462   0.0448018   -0.0973743    0.130706    -0.0632229    -0.0340377    0.0196379    0.0114006  -0.218573    0.0285396     0.00504801    0.115248     0.0430472   -0.0451169   -0.0856955   -0.213404    -0.0977542  -0.120226     0.0945648    0.0269085    0.0409074    0.0286824  -0.0495849    0.19419  
  0.0842434    0.0359605   -0.0330689   0.016426    -0.137315    -0.136296    -0.000499499   0.0231513    0.189656     0.0915847  -0.0165195  -0.16117      -0.0420603     0.0318653   -0.136126    -0.0578468    0.128614    -0.0230162    0.098844   -0.0798824    0.0428569    0.114275     0.00172086   0.0499898  -0.126084     0.0421999
  0.117735    -0.210862     0.05335    -0.0391865   -0.0448842    0.0274293   -0.0296333    -0.144362    -0.0625085   -0.0476481  -0.0869855   0.087283     -0.0051004     0.03022     -0.087402    -0.0221047    0.0562934   -0.0894963   -0.110054   -0.0945206    0.00263891   0.172018     0.0663903   -0.0972763   0.0657519    0.260777 
  0.0925715   -0.0659982   -0.0434063   0.223591    -0.0104703    0.0395708    0.0985672     0.145449     0.210637    -0.205123    0.132586    0.0545914    -0.0642788     0.021112    -0.0166419    0.00642328   0.0871188    0.0147668   -0.202209   -0.0762891    0.0336343   -0.00870919  -0.0882592   -0.22564    -0.0555419   -0.0464069
 -0.0224322   -0.112654    -0.0531243  -0.0120352    0.0196294   -0.0257468    0.0400763    -0.11281      0.0672156   -0.060989   -0.145718    0.0203637    -0.165495      0.130113    -0.203348    -0.0293586    0.0514856   -0.0304894    0.0443673  -0.0528641    0.182828    -0.087532     0.0538857   -0.132552    0.072492     0.109217 
 -0.084517     0.0790497    0.0108132  -0.0826333    0.150431     0.022539     0.18524      -0.0468371    0.00295529  -0.0565875   0.0746262  -0.0388801     0.0356756    -0.214466     0.0654654    0.0215799    0.0211355   -0.0159834   -0.062559    0.0307209    0.0370164   -0.0917728    0.0411055   -0.0946961  -0.0525258    0.143553 
 -0.128206     0.113923     0.0781825  -0.0867777    0.0560048    0.0282061   -0.014359     -0.0260574    0.00901953  -0.228059   -0.176296    0.10598      -0.0614847     0.0327122    0.0461809   -0.226757    -0.0518774   -0.00795879   0.0806186  -0.191373     0.0651935   -0.130634    -0.139682    -0.0232014  -0.119145     0.0932727
  0.0908657    0.0155992   -0.12548    -0.118937    -0.079619    -0.0298659   -0.0505891     0.0474859   -0.0499747    0.0504879   0.0228998   0.037954     -0.0119928     0.0494909    0.0502773   -0.149007     0.00474218  -0.0961879   -0.0404784   0.131663    -0.0268676   -0.153272     0.0376947   -0.0415953  -0.0725968    0.0364165
 -0.0348067    0.0650998   -0.0359921   0.140787     0.0959828    0.031134     0.0193441     0.0651373   -0.196213    -0.0670986  -0.145492    0.131558     -0.0379797     0.184462    -0.0110607    0.167476     0.191647    -0.0912302    0.144801    0.0992035    0.123861    -0.0367044    0.0481758   -0.0867763  -0.0377748    0.0734834
 -0.0965944   -0.128033    -0.116562    0.0520211    0.00857503  -0.126209    -0.0892844    -0.0578984    0.0541113   -0.0857255   0.0408496  -0.14736       0.0256758     0.0612519    0.0857821   -0.0125781   -0.0176173    0.191625    -0.0092339   0.0193718   -0.0415299    0.09863     -0.0809691    0.134105   -0.00312458  -0.0323207
 -0.100664    -0.00415489   0.169697   -0.234731    -0.0869288    0.0204202   -0.0439912     0.210903    -0.260554    -0.0129959   0.165861   -0.279652     -0.183208      0.0931993   -0.0975065   -0.212839     0.219222     0.106283    -0.0316386  -0.0580676   -0.117016    -0.0370447    0.0169979   -0.0994513  -0.0217576    0.0215145
  0.109777     0.032897    -0.0339728   0.172683     0.0544522    0.107337     0.055395     -0.00949873   0.0905963   -0.0498122   0.0666067  -0.0552076    -0.187549     -0.0306637    0.00443729  -0.0622977   -0.00991064  -0.0245411    0.0378966  -0.00343698  -0.057584     0.181236     0.00983459   0.0233131   0.0967987   -0.0126169
 -0.0760667    0.0770755    0.0773702   0.00741137  -0.20849      0.0540243    0.0363455    -0.0393274    0.0572802   -0.0693165  -0.14847    -0.182058     -0.0743623    -0.018334    -0.135743     0.12526      0.0322019    0.077082    -0.0159734   0.0568649   -0.00121223   0.00541581  -0.0233495    0.0153942   0.0583477    0.0984621
  0.143029     0.0322965   -0.0352055  -0.0310898    0.0119816   -0.005077     0.122273      0.168207    -0.183868    -0.036916    0.108004   -0.0262051    -0.000552914   0.018571    -0.0108939    0.102629     0.00136842  -0.131163     0.0973941  -0.0239315   -0.216792    -0.0807735   -0.0670023    0.0340239  -0.0986056    0.0878936
 -0.11478      0.125995    -0.0276924   0.0445796   -0.0908562   -0.211952     0.161046      0.00253933   0.107055     0.0972843   0.0379844  -0.000852353   0.118178     -0.332999    -0.0175772   -0.086144     0.0172211   -0.0969954   -0.174634   -0.0318684   -0.100093    -0.075744     0.0807235   -0.0548841  -0.0948196    0.0721932
 -0.15449      0.159693    -0.0707733  -0.0784576   -0.0830395    0.0255518   -0.181857     -0.0141069    0.12689      0.123438   -0.0319056  -0.0284379    -0.183275     -0.219595     0.0598295    0.0343114    0.00599802  -0.184916     0.139706   -0.0160646    0.189646     0.0139228    0.177787     0.107606    0.0311221   -0.0573598
  0.0777617    0.130194     0.0418616   0.0651522   -0.223744    -0.0886292   -0.000694739   0.0215653    0.107494    -0.111739   -0.161175    0.169838     -0.027796     -0.0489672   -0.0872262    0.0241333    0.145502     0.059817    -0.138445   -0.055671     0.0561665    0.00304412   0.11778     -0.141666   -0.0969771    0.116364 
 -0.100268     0.0308637    0.0285906   0.00239527   0.153807     0.0281366    0.175991     -0.0318118    0.120707    -0.0927214   0.02139    -0.0229716    -0.0823215    -0.0330777    0.0324985   -0.057803     0.145127    -0.0682965   -0.0487288   0.190244    -0.07663     -0.0149758   -0.157345     0.0599322  -0.111426     0.0563117
  0.0016989   -0.0158648   -0.0205263   0.0355273    0.0435758   -0.0565758    0.0641861    -0.146629     0.0117503   -0.0252888  -0.0259503   0.0254552     0.0729424    -0.0128507    0.00810441  -0.0634827    0.19611      0.144158     0.115799    0.0254705   -0.0776108   -0.0561015    0.17461     -0.0769044   0.0675072   -0.216185 
  0.00656973   0.0172484   -0.147727   -0.221144     0.0413183   -0.161264     0.168045     -0.0674635   -0.0804931    0.0149914   0.0886517   0.231676     -0.115055      0.0700154    0.00612766   0.0479846   -0.0174846    0.0455801    0.0134043   0.0843953    0.058512    -0.12462      0.0169466   -0.0593728  -0.127707    -0.0018332
  0.0610552   -0.0760005    0.0283502   0.0598083    0.0965617   -0.1165      -0.00370011    0.0524733   -0.0729688    0.0864976  -0.0522156   0.00716484    0.0787954     0.17543      0.0520216    0.155912     0.0350552   -0.00286757  -0.132106    0.0164109   -0.105        0.18176      0.0163887    0.0496902   0.122692     0.169776 
 -0.0356766    0.0553163    0.0906999  -0.163727     0.0823695    0.145531     0.00905685    0.00154784   0.0947139    0.137366   -0.0581368  -0.0843022    -0.162451     -0.0570612   -0.165335    -0.0808743   -0.181563    -0.063482    -0.0942034  -0.119718     0.068484    -0.0857537   -0.0660659    0.0153041  -0.0370731   -0.0835913kind diag, method split
0: avll = -1.4136039231165398
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.413734
INFO: iteration 2, average log likelihood -1.413640
INFO: iteration 3, average log likelihood -1.413399
INFO: iteration 4, average log likelihood -1.410846
INFO: iteration 5, average log likelihood -1.400209
INFO: iteration 6, average log likelihood -1.390628
INFO: iteration 7, average log likelihood -1.387722
INFO: iteration 8, average log likelihood -1.386485
INFO: iteration 9, average log likelihood -1.385694
INFO: iteration 10, average log likelihood -1.385050
INFO: iteration 11, average log likelihood -1.384467
INFO: iteration 12, average log likelihood -1.383961
INFO: iteration 13, average log likelihood -1.383565
INFO: iteration 14, average log likelihood -1.383277
INFO: iteration 15, average log likelihood -1.383074
INFO: iteration 16, average log likelihood -1.382930
INFO: iteration 17, average log likelihood -1.382824
INFO: iteration 18, average log likelihood -1.382743
INFO: iteration 19, average log likelihood -1.382679
INFO: iteration 20, average log likelihood -1.382624
INFO: iteration 21, average log likelihood -1.382573
INFO: iteration 22, average log likelihood -1.382524
INFO: iteration 23, average log likelihood -1.382477
INFO: iteration 24, average log likelihood -1.382434
INFO: iteration 25, average log likelihood -1.382400
INFO: iteration 26, average log likelihood -1.382375
INFO: iteration 27, average log likelihood -1.382357
INFO: iteration 28, average log likelihood -1.382342
INFO: iteration 29, average log likelihood -1.382330
INFO: iteration 30, average log likelihood -1.382320
INFO: iteration 31, average log likelihood -1.382312
INFO: iteration 32, average log likelihood -1.382306
INFO: iteration 33, average log likelihood -1.382301
INFO: iteration 34, average log likelihood -1.382296
INFO: iteration 35, average log likelihood -1.382292
INFO: iteration 36, average log likelihood -1.382288
INFO: iteration 37, average log likelihood -1.382284
INFO: iteration 38, average log likelihood -1.382281
INFO: iteration 39, average log likelihood -1.382277
INFO: iteration 40, average log likelihood -1.382272
INFO: iteration 41, average log likelihood -1.382267
INFO: iteration 42, average log likelihood -1.382261
INFO: iteration 43, average log likelihood -1.382252
INFO: iteration 44, average log likelihood -1.382240
INFO: iteration 45, average log likelihood -1.382222
INFO: iteration 46, average log likelihood -1.382196
INFO: iteration 47, average log likelihood -1.382155
INFO: iteration 48, average log likelihood -1.382095
INFO: iteration 49, average log likelihood -1.381999
INFO: iteration 50, average log likelihood -1.381822
INFO: EM with 100000 data points 50 iterations avll -1.381822
952.4 data points per parameter
1: avll = [-1.41373,-1.41364,-1.4134,-1.41085,-1.40021,-1.39063,-1.38772,-1.38649,-1.38569,-1.38505,-1.38447,-1.38396,-1.38357,-1.38328,-1.38307,-1.38293,-1.38282,-1.38274,-1.38268,-1.38262,-1.38257,-1.38252,-1.38248,-1.38243,-1.3824,-1.38238,-1.38236,-1.38234,-1.38233,-1.38232,-1.38231,-1.38231,-1.3823,-1.3823,-1.38229,-1.38229,-1.38228,-1.38228,-1.38228,-1.38227,-1.38227,-1.38226,-1.38225,-1.38224,-1.38222,-1.3822,-1.38215,-1.38209,-1.382,-1.38182]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.381690
INFO: iteration 2, average log likelihood -1.381302
INFO: iteration 3, average log likelihood -1.380840
INFO: iteration 4, average log likelihood -1.378057
INFO: iteration 5, average log likelihood -1.365652
INFO: iteration 6, average log likelihood -1.349899
INFO: iteration 7, average log likelihood -1.343540
INFO: iteration 8, average log likelihood -1.340993
INFO: iteration 9, average log likelihood -1.339554
INFO: iteration 10, average log likelihood -1.338560
INFO: iteration 11, average log likelihood -1.337806
INFO: iteration 12, average log likelihood -1.337156
INFO: iteration 13, average log likelihood -1.336496
INFO: iteration 14, average log likelihood -1.335794
INFO: iteration 15, average log likelihood -1.335156
INFO: iteration 16, average log likelihood -1.334672
INFO: iteration 17, average log likelihood -1.334328
INFO: iteration 18, average log likelihood -1.334085
INFO: iteration 19, average log likelihood -1.333914
INFO: iteration 20, average log likelihood -1.333791
INFO: iteration 21, average log likelihood -1.333697
INFO: iteration 22, average log likelihood -1.333620
INFO: iteration 23, average log likelihood -1.333551
INFO: iteration 24, average log likelihood -1.333484
INFO: iteration 25, average log likelihood -1.333417
INFO: iteration 26, average log likelihood -1.333346
INFO: iteration 27, average log likelihood -1.333272
INFO: iteration 28, average log likelihood -1.333193
INFO: iteration 29, average log likelihood -1.333113
INFO: iteration 30, average log likelihood -1.333033
INFO: iteration 31, average log likelihood -1.332955
INFO: iteration 32, average log likelihood -1.332879
INFO: iteration 33, average log likelihood -1.332807
INFO: iteration 34, average log likelihood -1.332738
INFO: iteration 35, average log likelihood -1.332674
INFO: iteration 36, average log likelihood -1.332614
INFO: iteration 37, average log likelihood -1.332561
INFO: iteration 38, average log likelihood -1.332517
INFO: iteration 39, average log likelihood -1.332483
INFO: iteration 40, average log likelihood -1.332459
INFO: iteration 41, average log likelihood -1.332443
INFO: iteration 42, average log likelihood -1.332433
INFO: iteration 43, average log likelihood -1.332426
INFO: iteration 44, average log likelihood -1.332422
INFO: iteration 45, average log likelihood -1.332419
INFO: iteration 46, average log likelihood -1.332417
INFO: iteration 47, average log likelihood -1.332416
INFO: iteration 48, average log likelihood -1.332416
INFO: iteration 49, average log likelihood -1.332415
INFO: iteration 50, average log likelihood -1.332415
INFO: EM with 100000 data points 50 iterations avll -1.332415
473.9 data points per parameter
2: avll = [-1.38169,-1.3813,-1.38084,-1.37806,-1.36565,-1.3499,-1.34354,-1.34099,-1.33955,-1.33856,-1.33781,-1.33716,-1.3365,-1.33579,-1.33516,-1.33467,-1.33433,-1.33409,-1.33391,-1.33379,-1.3337,-1.33362,-1.33355,-1.33348,-1.33342,-1.33335,-1.33327,-1.33319,-1.33311,-1.33303,-1.33295,-1.33288,-1.33281,-1.33274,-1.33267,-1.33261,-1.33256,-1.33252,-1.33248,-1.33246,-1.33244,-1.33243,-1.33243,-1.33242,-1.33242,-1.33242,-1.33242,-1.33242,-1.33242,-1.33241]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.332691
INFO: iteration 2, average log likelihood -1.332430
INFO: iteration 3, average log likelihood -1.331755
INFO: iteration 4, average log likelihood -1.326197
INFO: iteration 5, average log likelihood -1.309577
INFO: iteration 6, average log likelihood -1.294486
INFO: iteration 7, average log likelihood -1.287491
INFO: iteration 8, average log likelihood -1.283847
INFO: iteration 9, average log likelihood -1.281535
INFO: iteration 10, average log likelihood -1.280198
INFO: iteration 11, average log likelihood -1.279553
INFO: iteration 12, average log likelihood -1.279201
INFO: iteration 13, average log likelihood -1.278953
INFO: iteration 14, average log likelihood -1.278738
INFO: iteration 15, average log likelihood -1.278532
INFO: iteration 16, average log likelihood -1.278331
INFO: iteration 17, average log likelihood -1.278140
INFO: iteration 18, average log likelihood -1.277955
INFO: iteration 19, average log likelihood -1.277768
INFO: iteration 20, average log likelihood -1.277576
INFO: iteration 21, average log likelihood -1.277385
INFO: iteration 22, average log likelihood -1.277208
INFO: iteration 23, average log likelihood -1.277052
INFO: iteration 24, average log likelihood -1.276920
INFO: iteration 25, average log likelihood -1.276818
INFO: iteration 26, average log likelihood -1.276740
INFO: iteration 27, average log likelihood -1.276679
INFO: iteration 28, average log likelihood -1.276630
INFO: iteration 29, average log likelihood -1.276589
INFO: iteration 30, average log likelihood -1.276555
INFO: iteration 31, average log likelihood -1.276527
INFO: iteration 32, average log likelihood -1.276503
INFO: iteration 33, average log likelihood -1.276484
INFO: iteration 34, average log likelihood -1.276468
INFO: iteration 35, average log likelihood -1.276454
INFO: iteration 36, average log likelihood -1.276442
INFO: iteration 37, average log likelihood -1.276430
INFO: iteration 38, average log likelihood -1.276417
INFO: iteration 39, average log likelihood -1.276401
INFO: iteration 40, average log likelihood -1.276383
INFO: iteration 41, average log likelihood -1.276358
INFO: iteration 42, average log likelihood -1.276326
INFO: iteration 43, average log likelihood -1.276283
INFO: iteration 44, average log likelihood -1.276225
INFO: iteration 45, average log likelihood -1.276146
INFO: iteration 46, average log likelihood -1.276044
INFO: iteration 47, average log likelihood -1.275915
INFO: iteration 48, average log likelihood -1.275762
INFO: iteration 49, average log likelihood -1.275589
INFO: iteration 50, average log likelihood -1.275393
INFO: EM with 100000 data points 50 iterations avll -1.275393
236.4 data points per parameter
3: avll = [-1.33269,-1.33243,-1.33176,-1.3262,-1.30958,-1.29449,-1.28749,-1.28385,-1.28154,-1.2802,-1.27955,-1.2792,-1.27895,-1.27874,-1.27853,-1.27833,-1.27814,-1.27795,-1.27777,-1.27758,-1.27738,-1.27721,-1.27705,-1.27692,-1.27682,-1.27674,-1.27668,-1.27663,-1.27659,-1.27656,-1.27653,-1.2765,-1.27648,-1.27647,-1.27645,-1.27644,-1.27643,-1.27642,-1.2764,-1.27638,-1.27636,-1.27633,-1.27628,-1.27622,-1.27615,-1.27604,-1.27592,-1.27576,-1.27559,-1.27539]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.275447
INFO: iteration 2, average log likelihood -1.274895
INFO: iteration 3, average log likelihood -1.273491
INFO: iteration 4, average log likelihood -1.261081
WARNING: Variances had to be floored 1
INFO: iteration 5, average log likelihood -1.227977
WARNING: Variances had to be floored 9
INFO: iteration 6, average log likelihood -1.219481
WARNING: Variances had to be floored 1
INFO: iteration 7, average log likelihood -1.213042
INFO: iteration 8, average log likelihood -1.211816
INFO: iteration 9, average log likelihood -1.198778
WARNING: Variances had to be floored 1 9
INFO: iteration 10, average log likelihood -1.190190
INFO: iteration 11, average log likelihood -1.207694
WARNING: Variances had to be floored 1
INFO: iteration 12, average log likelihood -1.193880
INFO: iteration 13, average log likelihood -1.198181
WARNING: Variances had to be floored 1 9
INFO: iteration 14, average log likelihood -1.189106
INFO: iteration 15, average log likelihood -1.203452
WARNING: Variances had to be floored 1
INFO: iteration 16, average log likelihood -1.191219
INFO: iteration 17, average log likelihood -1.196627
WARNING: Variances had to be floored 1 9
INFO: iteration 18, average log likelihood -1.187991
INFO: iteration 19, average log likelihood -1.203049
WARNING: Variances had to be floored 1
INFO: iteration 20, average log likelihood -1.190692
WARNING: Variances had to be floored 9
INFO: iteration 21, average log likelihood -1.196333
WARNING: Variances had to be floored 1
INFO: iteration 22, average log likelihood -1.195434
INFO: iteration 23, average log likelihood -1.198247
WARNING: Variances had to be floored 1 9
INFO: iteration 24, average log likelihood -1.188863
INFO: iteration 25, average log likelihood -1.203015
WARNING: Variances had to be floored 1
INFO: iteration 26, average log likelihood -1.190558
WARNING: Variances had to be floored 9
INFO: iteration 27, average log likelihood -1.196227
WARNING: Variances had to be floored 1
INFO: iteration 28, average log likelihood -1.195375
INFO: iteration 29, average log likelihood -1.198137
WARNING: Variances had to be floored 1 9
INFO: iteration 30, average log likelihood -1.188699
INFO: iteration 31, average log likelihood -1.202909
WARNING: Variances had to be floored 1
INFO: iteration 32, average log likelihood -1.190321
WARNING: Variances had to be floored 9
INFO: iteration 33, average log likelihood -1.195888
WARNING: Variances had to be floored 1
INFO: iteration 34, average log likelihood -1.194906
INFO: iteration 35, average log likelihood -1.197398
WARNING: Variances had to be floored 1 9
INFO: iteration 36, average log likelihood -1.187635
INFO: iteration 37, average log likelihood -1.201730
WARNING: Variances had to be floored 1
INFO: iteration 38, average log likelihood -1.189021
WARNING: Variances had to be floored 9
INFO: iteration 39, average log likelihood -1.194635
WARNING: Variances had to be floored 1
INFO: iteration 40, average log likelihood -1.193807
INFO: iteration 41, average log likelihood -1.196577
WARNING: Variances had to be floored 1 9
INFO: iteration 42, average log likelihood -1.187004
INFO: iteration 43, average log likelihood -1.201334
WARNING: Variances had to be floored 1
INFO: iteration 44, average log likelihood -1.188677
WARNING: Variances had to be floored 9
INFO: iteration 45, average log likelihood -1.194184
WARNING: Variances had to be floored 1
INFO: iteration 46, average log likelihood -1.193127
INFO: iteration 47, average log likelihood -1.195485
WARNING: Variances had to be floored 1 9
INFO: iteration 48, average log likelihood -1.185409
INFO: iteration 49, average log likelihood -1.199402
WARNING: Variances had to be floored 1
INFO: iteration 50, average log likelihood -1.186678
INFO: EM with 100000 data points 50 iterations avll -1.186678
118.1 data points per parameter
4: avll = [-1.27545,-1.27489,-1.27349,-1.26108,-1.22798,-1.21948,-1.21304,-1.21182,-1.19878,-1.19019,-1.20769,-1.19388,-1.19818,-1.18911,-1.20345,-1.19122,-1.19663,-1.18799,-1.20305,-1.19069,-1.19633,-1.19543,-1.19825,-1.18886,-1.20301,-1.19056,-1.19623,-1.19537,-1.19814,-1.1887,-1.20291,-1.19032,-1.19589,-1.19491,-1.1974,-1.18763,-1.20173,-1.18902,-1.19464,-1.19381,-1.19658,-1.187,-1.20133,-1.18868,-1.19418,-1.19313,-1.19549,-1.18541,-1.1994,-1.18668]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 17 18
INFO: iteration 1, average log likelihood -1.192724
WARNING: Variances had to be floored 1 2 17 18
INFO: iteration 2, average log likelihood -1.184700
WARNING: Variances had to be floored 17 18
INFO: iteration 3, average log likelihood -1.190978
WARNING: Variances had to be floored 1 2 17 18
INFO: iteration 4, average log likelihood -1.164689
WARNING: Variances had to be floored 11 17 18
INFO: iteration 5, average log likelihood -1.114684
WARNING: Variances had to be floored 1 2 17 18 21 26
INFO: iteration 6, average log likelihood -1.091609
WARNING: Variances had to be floored 5 17 18
INFO: iteration 7, average log likelihood -1.097931
WARNING: Variances had to be floored 1 2 11 17 18
INFO: iteration 8, average log likelihood -1.076908
WARNING: Variances had to be floored 17 18 21 26
INFO: iteration 9, average log likelihood -1.091608
WARNING: Variances had to be floored 1 2 11 17 18
INFO: iteration 10, average log likelihood -1.088406
WARNING: Variances had to be floored 5 17 18
INFO: iteration 11, average log likelihood -1.093735
WARNING: Variances had to be floored 1 2 11 17 18 21 26
INFO: iteration 12, average log likelihood -1.077549
WARNING: Variances had to be floored 17 18
INFO: iteration 13, average log likelihood -1.109766
WARNING: Variances had to be floored 1 2 11 17 18
INFO: iteration 14, average log likelihood -1.076017
WARNING: Variances had to be floored 5 17 18 21 26
INFO: iteration 15, average log likelihood -1.086530
WARNING: Variances had to be floored 1 2 11 17 18
INFO: iteration 16, average log likelihood -1.096345
WARNING: Variances had to be floored 17 18
INFO: iteration 17, average log likelihood -1.097393
WARNING: Variances had to be floored 1 2 11 17 18 21 26
INFO: iteration 18, average log likelihood -1.068644
WARNING: Variances had to be floored 5 17 18
INFO: iteration 19, average log likelihood -1.105676
WARNING: Variances had to be floored 1 2 11 17 18
INFO: iteration 20, average log likelihood -1.084112
WARNING: Variances had to be floored 17 18 21 26
INFO: iteration 21, average log likelihood -1.090399
WARNING: Variances had to be floored 1 2 11 17 18
INFO: iteration 22, average log likelihood -1.087871
WARNING: Variances had to be floored 5 17 18
INFO: iteration 23, average log likelihood -1.093403
WARNING: Variances had to be floored 1 2 11 17 18 21 26
INFO: iteration 24, average log likelihood -1.076963
WARNING: Variances had to be floored 17 18
INFO: iteration 25, average log likelihood -1.109666
WARNING: Variances had to be floored 1 2 11 17 18
INFO: iteration 26, average log likelihood -1.075737
WARNING: Variances had to be floored 5 17 18 21 26
INFO: iteration 27, average log likelihood -1.086428
WARNING: Variances had to be floored 1 2 11 17 18
INFO: iteration 28, average log likelihood -1.096200
WARNING: Variances had to be floored 17 18
INFO: iteration 29, average log likelihood -1.097357
WARNING: Variances had to be floored 1 2 11 17 18 21 26
INFO: iteration 30, average log likelihood -1.068535
WARNING: Variances had to be floored 5 17 18
INFO: iteration 31, average log likelihood -1.105662
WARNING: Variances had to be floored 1 2 11 17 18
INFO: iteration 32, average log likelihood -1.084054
WARNING: Variances had to be floored 17 18 21 26
INFO: iteration 33, average log likelihood -1.090374
WARNING: Variances had to be floored 1 2 11 17 18
INFO: iteration 34, average log likelihood -1.087834
WARNING: Variances had to be floored 5 17 18
INFO: iteration 35, average log likelihood -1.093389
WARNING: Variances had to be floored 1 2 11 17 18 21 26
INFO: iteration 36, average log likelihood -1.076930
WARNING: Variances had to be floored 17 18
INFO: iteration 37, average log likelihood -1.109655
WARNING: Variances had to be floored 1 2 11 17 18
INFO: iteration 38, average log likelihood -1.075715
WARNING: Variances had to be floored 5 17 18 21 26
INFO: iteration 39, average log likelihood -1.086413
WARNING: Variances had to be floored 1 2 11 17 18
INFO: iteration 40, average log likelihood -1.096181
WARNING: Variances had to be floored 17 18
INFO: iteration 41, average log likelihood -1.097345
WARNING: Variances had to be floored 1 2 11 17 18 21 26
INFO: iteration 42, average log likelihood -1.068517
WARNING: Variances had to be floored 5 17 18
INFO: iteration 43, average log likelihood -1.105652
WARNING: Variances had to be floored 1 2 11 17 18
INFO: iteration 44, average log likelihood -1.084039
WARNING: Variances had to be floored 17 18 21 26
INFO: iteration 45, average log likelihood -1.090363
WARNING: Variances had to be floored 1 2 11 17 18
INFO: iteration 46, average log likelihood -1.087820
WARNING: Variances had to be floored 5 17 18
INFO: iteration 47, average log likelihood -1.093378
WARNING: Variances had to be floored 1 2 11 17 18 21 26
INFO: iteration 48, average log likelihood -1.076917
WARNING: Variances had to be floored 17 18
INFO: iteration 49, average log likelihood -1.109646
WARNING: Variances had to be floored 1 2 11 17 18
INFO: iteration 50, average log likelihood -1.075703
INFO: EM with 100000 data points 50 iterations avll -1.075703
59.0 data points per parameter
5: avll = [-1.19272,-1.1847,-1.19098,-1.16469,-1.11468,-1.09161,-1.09793,-1.07691,-1.09161,-1.08841,-1.09373,-1.07755,-1.10977,-1.07602,-1.08653,-1.09635,-1.09739,-1.06864,-1.10568,-1.08411,-1.0904,-1.08787,-1.0934,-1.07696,-1.10967,-1.07574,-1.08643,-1.0962,-1.09736,-1.06853,-1.10566,-1.08405,-1.09037,-1.08783,-1.09339,-1.07693,-1.10965,-1.07571,-1.08641,-1.09618,-1.09734,-1.06852,-1.10565,-1.08404,-1.09036,-1.08782,-1.09338,-1.07692,-1.10965,-1.0757]
[-1.4136,-1.41373,-1.41364,-1.4134,-1.41085,-1.40021,-1.39063,-1.38772,-1.38649,-1.38569,-1.38505,-1.38447,-1.38396,-1.38357,-1.38328,-1.38307,-1.38293,-1.38282,-1.38274,-1.38268,-1.38262,-1.38257,-1.38252,-1.38248,-1.38243,-1.3824,-1.38238,-1.38236,-1.38234,-1.38233,-1.38232,-1.38231,-1.38231,-1.3823,-1.3823,-1.38229,-1.38229,-1.38228,-1.38228,-1.38228,-1.38227,-1.38227,-1.38226,-1.38225,-1.38224,-1.38222,-1.3822,-1.38215,-1.38209,-1.382,-1.38182,-1.38169,-1.3813,-1.38084,-1.37806,-1.36565,-1.3499,-1.34354,-1.34099,-1.33955,-1.33856,-1.33781,-1.33716,-1.3365,-1.33579,-1.33516,-1.33467,-1.33433,-1.33409,-1.33391,-1.33379,-1.3337,-1.33362,-1.33355,-1.33348,-1.33342,-1.33335,-1.33327,-1.33319,-1.33311,-1.33303,-1.33295,-1.33288,-1.33281,-1.33274,-1.33267,-1.33261,-1.33256,-1.33252,-1.33248,-1.33246,-1.33244,-1.33243,-1.33243,-1.33242,-1.33242,-1.33242,-1.33242,-1.33242,-1.33242,-1.33241,-1.33269,-1.33243,-1.33176,-1.3262,-1.30958,-1.29449,-1.28749,-1.28385,-1.28154,-1.2802,-1.27955,-1.2792,-1.27895,-1.27874,-1.27853,-1.27833,-1.27814,-1.27795,-1.27777,-1.27758,-1.27738,-1.27721,-1.27705,-1.27692,-1.27682,-1.27674,-1.27668,-1.27663,-1.27659,-1.27656,-1.27653,-1.2765,-1.27648,-1.27647,-1.27645,-1.27644,-1.27643,-1.27642,-1.2764,-1.27638,-1.27636,-1.27633,-1.27628,-1.27622,-1.27615,-1.27604,-1.27592,-1.27576,-1.27559,-1.27539,-1.27545,-1.27489,-1.27349,-1.26108,-1.22798,-1.21948,-1.21304,-1.21182,-1.19878,-1.19019,-1.20769,-1.19388,-1.19818,-1.18911,-1.20345,-1.19122,-1.19663,-1.18799,-1.20305,-1.19069,-1.19633,-1.19543,-1.19825,-1.18886,-1.20301,-1.19056,-1.19623,-1.19537,-1.19814,-1.1887,-1.20291,-1.19032,-1.19589,-1.19491,-1.1974,-1.18763,-1.20173,-1.18902,-1.19464,-1.19381,-1.19658,-1.187,-1.20133,-1.18868,-1.19418,-1.19313,-1.19549,-1.18541,-1.1994,-1.18668,-1.19272,-1.1847,-1.19098,-1.16469,-1.11468,-1.09161,-1.09793,-1.07691,-1.09161,-1.08841,-1.09373,-1.07755,-1.10977,-1.07602,-1.08653,-1.09635,-1.09739,-1.06864,-1.10568,-1.08411,-1.0904,-1.08787,-1.0934,-1.07696,-1.10967,-1.07574,-1.08643,-1.0962,-1.09736,-1.06853,-1.10566,-1.08405,-1.09037,-1.08783,-1.09339,-1.07693,-1.10965,-1.07571,-1.08641,-1.09618,-1.09734,-1.06852,-1.10565,-1.08404,-1.09036,-1.08782,-1.09338,-1.07692,-1.10965,-1.0757]
32×26 Array{Float64,2}:
  0.00574469  -0.0170592   1.25621      0.0368233    0.0484419   -0.0346572     0.0637897   -0.136899     0.0438364   -0.0549788    -0.0503308    0.0212669    0.0717297   -0.0182153    0.00536334  -0.0999118    0.19573      0.136734     0.119652     0.0442792   -0.0118775   -0.0463199    0.172761   -0.0859367    0.067593    -0.216726  
 -0.00298988  -0.0151441  -0.966578     0.0300775    0.0518346   -0.0686799     0.0565597   -0.145252    -0.0233339   -0.000156838  -0.0149607    0.0250768    0.0674184   -0.018178     0.00802439  -0.0323529    0.194251     0.140662     0.0941136    0.0297124   -0.180488    -0.0603748    0.166814   -0.0720847    0.06788     -0.227049  
  0.206744     0.229396    0.129812     0.0729917    0.27545     -0.0337185    -0.222953     0.0325318   -0.104117    -0.476746      0.203053    -0.0485649    0.136507    -0.036884    -0.158464    -0.0240578    0.0321774    0.0508669   -0.16773     -0.0830268   -0.023393     0.272866     0.110913    0.0334806    0.0433113   -0.0354473 
  0.195433     0.219427    0.0820148    0.0449394    0.0972937   -0.0344955    -0.206358     0.0485291   -0.119202     0.448427      0.209053    -0.00833107  -0.012704     0.00610835  -0.147942     0.0253057    0.0330927   -0.12584      0.0803865   -0.162866    -0.0398494   -0.0561438    0.102613    0.010605     0.0467677   -0.0937915 
 -0.0605295    0.0432674   0.096911    -0.127902     0.0577209    0.124733      0.00748374  -0.0418773    0.0906438    0.134444     -0.0606333   -0.0500829   -0.162865    -0.0613484   -0.169758    -0.068512    -0.181327    -0.068812    -0.0935541   -0.118693     0.0686093   -0.0875046   -0.0693697   0.0138637   -0.0608847   -0.0799447 
 -0.0410243   -0.12172    -0.0532534   -0.0382682    0.0184071    0.000927239   0.0408505   -0.10504      0.0588129   -0.0565075    -0.153425     0.0151458   -0.199241     0.13267     -0.212489    -0.0399076    0.0634726   -0.022788     0.0430984   -0.050072     0.151434    -0.145073     0.0565492  -0.12711      0.0673312    0.114792  
 -0.260024    -0.0520445   0.196712     0.0558898   -0.065387    -0.095125     -0.0511623    0.0509844   -0.181062    -0.0313905    -0.030572    -0.0580295    0.0305276   -0.271701     0.0786985   -0.0907786   -0.166494     0.0220181   -0.0339971    0.022346    -0.0431573   -0.293968    -0.0686596  -0.0224855   -0.0945257    0.301058  
 -0.304649    -0.122066    0.19664      0.0688897   -0.182049    -0.103686     -0.0483984    0.101644    -0.123853    -0.0115824    -0.0763287   -0.0320923    0.223299     0.460282     0.136569    -0.129499    -0.185812     0.0629746   -0.0314795    0.0160023   -0.102481    -0.252021    -0.175994   -0.057926     0.0464451    0.0765406 
  0.0911889   -0.114732   -0.00548734  -0.0296682    0.00923357   0.0343592    -0.0251381   -0.0765114   -0.140192    -0.0460263    -0.0379699    0.115423    -0.053316     0.0589337   -0.0710882   -0.0622772   -0.0279193   -0.0845927   -0.0587649   -0.0719116    0.0997882    0.0923328    0.0705822   0.00162536  -0.00885416   0.218987  
 -0.125159     0.166982   -0.0554203   -0.0618301   -0.0269123    0.0237079    -0.173698    -0.00164609   0.116228     0.116557     -0.00899193  -0.0287252   -0.104159    -0.169054     0.071156     0.0375954   -0.0245807   -0.191623     0.12884      0.00282355   0.181094     0.0216479    0.2036      0.110399     0.0467672   -0.0440133 
 -0.0541198   -0.100313    0.0379259    0.0283998    0.109385    -0.0427807    -0.0260962    0.163842    -0.0608575   -0.0487008    -0.0757313   -0.070817     0.0307139   -0.088457    -0.130568    -0.0979967    0.0227538    0.169557     0.00174773   0.0303919   -0.165869    -0.0953736   -0.209994    0.180899    -0.0549878    0.0627222 
 -0.0946106    0.0185866   0.0256701    0.005235     0.146937     0.0292188     0.183354    -0.00295927   0.0850464   -0.0999295     0.142654    -0.0361072   -0.0608982   -0.0337602    0.0692879   -0.057621     0.143436    -0.068811    -0.0438506    0.193219    -0.0719591    0.00258626  -0.155702    0.0696111   -0.1214       0.0647999 
  0.129947     0.0274013  -0.0871418   -0.216482    -0.304274    -0.145692      0.144116    -0.038447     0.185324     0.0353044     0.0052615   -0.0627023   -0.0960539    0.0514795   -0.217658    -0.162101     0.140845    -0.0423737    0.112813    -0.0868002   -0.306686     0.115189    -0.140231    0.00846874  -0.0794359    0.0425138 
  0.00547004  -0.0475372   0.0341026    0.226668     0.0106308    0.00316992   -0.0407313    0.0835495    0.154224     0.161431     -0.0428873   -0.165073    -0.0102475    0.0167205   -0.0953085   -0.00737718   0.153836     0.0124757    0.0978034    0.00618043   0.208847     0.136232     0.13188     0.0916426   -0.233661     0.0417425 
  0.0438495   -0.0384305  -0.0521071    0.0311209   -0.0217749    0.0768459     0.115775     0.0316234   -0.0317608    0.061163     -0.179367     0.042014    -0.00663164   0.145172     0.0709673    0.00119718  -0.0287371   -0.0423685   -0.018841    -0.028013    -0.044167    -0.00455758   0.0888408  -0.0180532   -0.139684     0.0873213 
 -0.103294    -0.0393945   0.12954     -0.220747    -0.0106503    0.032048     -0.0461767    0.1799      -0.258323    -0.012422      0.158998    -0.27199     -0.184511     0.0866873   -0.077457    -0.212091     0.21974      0.106314    -0.0320223   -0.0521808   -0.102836    -0.0417174    0.0120141  -0.0823771    0.00795864   0.0177427 
 -0.099847    -0.132342   -0.0443945    0.0531978    0.00873999  -0.199589     -0.0798223   -0.0529171   -0.337618    -0.0760207     0.0341569   -0.143921    -0.077048    -0.00716313   0.0760242    0.0234486   -0.0140054    0.176137     0.0511572    0.145537    -0.129111     0.0843889   -0.234167    0.0243161   -0.00490362  -0.0816907 
 -0.095123    -0.120936   -0.128924     0.0534727    0.00869822  -0.12533      -0.0741956   -0.0640136    0.433025    -0.0940617     0.0428275   -0.14707      0.0761237    0.21174      0.0884198   -0.0366598   -0.119238     0.201358    -0.0298539   -0.104364     0.0426254    0.163139     0.0747944   0.217518    -0.00343956   0.00992646
  0.0843736   -0.0799643  -0.567516    -0.0716241    0.0908072   -0.121122     -0.0642715    0.053528    -0.132236     0.169854      0.07562      0.0131253    0.143209     0.0427212    0.033392    -0.0433074    0.228563    -0.21327      0.262043     0.0605765    0.0163519    0.0753446   -0.0346474  -0.169957    -0.101807     0.074205  
 -0.137282    -0.091748    0.680548    -0.0769838    0.131137    -0.111281     -0.043373    -0.0654882   -0.0170795    0.178842      0.0807334    0.109123    -0.0927184    0.0861692    0.037541    -0.124774     0.0382031   -0.199363    -0.0104373    0.0710912   -0.0048411    0.0728116    0.0686685  -0.0937338   -0.168329     0.186251  
  0.104776     0.0322723  -0.0260433    0.175752     0.052681     0.100872      0.0540097   -0.00930185   0.102734    -0.0419153     0.0488144   -0.0470279   -0.192284    -0.0330006   -0.0133525   -0.0543243    0.00598014  -0.0256422    0.0483273   -0.0382758   -0.0539021    0.193193    -0.0304545   0.0155378    0.0950294   -0.0199291 
  0.173257     0.0224416   0.115824    -0.0146408    0.0273389    0.123416     -0.0726559    0.14523     -0.127804     0.0100145     0.0143823   -0.215427     0.101347     0.156709     0.0456365   -0.00582231   0.0578987   -0.0640532    0.051802     0.038533     0.00907281  -0.105015     0.0748295   0.0249602   -0.174603    -0.0448239 
  0.0598694    0.0190818  -0.0991128   -0.0558703    0.0209356   -0.117929      0.118836     0.067408    -0.0564402   -0.0221893     0.126014     0.0311233   -0.0631431    0.0169412   -0.0184805    0.0187268    0.013985     0.0519903    0.100594     0.0160672   -0.0780539   -0.0299967   -0.0607535  -0.031634    -0.0624007    0.053774  
 -0.0710733    0.0763108   0.0051702    0.12059     -0.0341706    0.103327     -0.131907     0.0133091    0.107409     0.0633839     0.227399    -0.100989     0.0116228   -0.145021     0.0315762   -0.0442591    0.0850456    0.0412369    0.307718    -0.0149391   -0.100925     0.0494452    0.0468109  -0.111482    -0.102977     0.08487   
 -0.138891     0.144586    0.0835348   -0.089382     0.0481031   -0.000186282   0.00518777  -0.00788764  -0.00855665  -0.226452     -0.187793     0.110957    -0.0614922    0.0379079    0.0458454   -0.229091    -0.0872685   -0.047287     0.0969953   -0.191503     0.0819769   -0.125947    -0.123269   -0.019894    -0.0941973    0.098296  
 -0.0945609    0.0616014   0.075713     0.00606035  -0.197639     0.0425958     0.0242776   -0.0652664    0.0436493   -0.0689698    -0.133546    -0.158306    -0.0953806   -0.0227569   -0.13678      0.121898     0.0576553    0.0782544   -0.0214518    0.0611724   -0.0204196    0.0168347    0.0117521   0.0418942    0.0605113    0.103526  
 -0.0556707   -0.0894032   0.14992      0.0919601    0.0519652   -0.0167241     0.179529     0.266172    -0.2141      -0.0639188    -0.129313     0.118924    -0.0971781   -0.164327    -0.0127979    0.152707     0.223909    -0.165742     0.151571    -0.0870555    0.141358    -0.031453     0.0784638  -0.0721177    0.0143123    0.0747986 
 -0.00521575   0.196701   -0.222009     0.154523     0.107329     0.0582968    -0.1224      -0.0952326   -0.150552    -0.0745878    -0.158007     0.0974301    0.0177595    0.630825    -0.00915683   0.204156     0.14903     -0.0885892    0.147566     0.239379     0.0900952   -0.044592     0.0287142  -0.10122     -0.0116354    0.0816595 
  0.0561592   -0.0443711   0.018853     0.081395     0.057688    -0.0293218     0.0592718    0.0415551    0.0422287   -0.0278954     0.0194188    0.0806603    0.0558449    0.0809041   -0.0383285    0.0284664    0.0193564   -0.00434979  -0.0865844   -0.0168555   -0.054021     0.0658327   -0.0957647  -0.0076453   -0.0169329    0.00289904
 -0.0408864    0.0366299  -0.0613014   -0.0298549   -0.0837361   -0.100978      0.0370545    0.0397419    0.0219221    0.0655616     0.0376976    0.0158429    0.0326729   -0.117868     0.0250483   -0.125726     0.0164673   -0.0972785   -0.100708     0.0992437   -0.0773243   -0.113214     0.0581148  -0.0517108   -0.110084     0.0635657 
  0.0773048    0.1334      0.0418098    0.0996253   -0.190486    -0.122618     -0.00189052   0.0205783    0.109964    -0.113879     -0.161549     0.176388    -0.0341883   -0.0609823   -0.0931407    0.0429846    0.140343     0.064828    -0.14244     -0.0606374    0.0214097    0.00392313   0.079596   -0.13187     -0.0950018    0.112335  
 -0.136495     0.0826881   0.0126697   -0.092115     0.129629     0.0258468     0.179532    -0.0487187    0.00196115  -0.0545578     0.0763298   -0.0443745    0.0383498   -0.215052     0.0663088    0.0415435    0.00100095  -0.0333401   -0.0645531    0.0520589    0.0481292   -0.0977627    0.044198   -0.100829    -0.0263397    0.122198  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 5 17 18 21 26
INFO: iteration 1, average log likelihood -1.086404
WARNING: Variances had to be floored 1 2 5 11 17 18 21 26
INFO: iteration 2, average log likelihood -1.066968
WARNING: Variances had to be floored 5 17 18 21 26
INFO: iteration 3, average log likelihood -1.086384
WARNING: Variances had to be floored 1 2 5 11 17 18 21 26
INFO: iteration 4, average log likelihood -1.066890
WARNING: Variances had to be floored 5 17 18 21 26
INFO: iteration 5, average log likelihood -1.086383
WARNING: Variances had to be floored 1 2 5 11 17 18 21 26
INFO: iteration 6, average log likelihood -1.066882
WARNING: Variances had to be floored 5 17 18 21 26
INFO: iteration 7, average log likelihood -1.086382
WARNING: Variances had to be floored 1 2 5 11 17 18 21 26
INFO: iteration 8, average log likelihood -1.066880
WARNING: Variances had to be floored 5 17 18 21 26
INFO: iteration 9, average log likelihood -1.086381
WARNING: Variances had to be floored 1 2 5 11 17 18 21 26
INFO: iteration 10, average log likelihood -1.066879
INFO: EM with 100000 data points 10 iterations avll -1.066879
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.432371e+05
      1       6.976853e+05      -2.455518e+05 |       32
      2       6.664207e+05      -3.126460e+04 |       32
      3       6.444367e+05      -2.198405e+04 |       32
      4       6.288221e+05      -1.561451e+04 |       32
      5       6.204143e+05      -8.407800e+03 |       32
      6       6.158361e+05      -4.578213e+03 |       32
      7       6.131868e+05      -2.649345e+03 |       32
      8       6.120041e+05      -1.182668e+03 |       32
      9       6.114778e+05      -5.263574e+02 |       32
     10       6.111728e+05      -3.049717e+02 |       32
     11       6.109112e+05      -2.616305e+02 |       32
     12       6.106818e+05      -2.293260e+02 |       32
     13       6.104443e+05      -2.375298e+02 |       32
     14       6.102449e+05      -1.994309e+02 |       32
     15       6.100635e+05      -1.813746e+02 |       32
     16       6.098986e+05      -1.649257e+02 |       32
     17       6.097427e+05      -1.559142e+02 |       32
     18       6.096062e+05      -1.364632e+02 |       32
     19       6.094709e+05      -1.353224e+02 |       32
     20       6.092896e+05      -1.812490e+02 |       32
     21       6.090515e+05      -2.380763e+02 |       32
     22       6.088846e+05      -1.669646e+02 |       32
     23       6.088013e+05      -8.326117e+01 |       32
     24       6.087442e+05      -5.711359e+01 |       31
     25       6.086856e+05      -5.865212e+01 |       32
     26       6.086274e+05      -5.812061e+01 |       32
     27       6.085698e+05      -5.761491e+01 |       32
     28       6.085112e+05      -5.860100e+01 |       32
     29       6.084552e+05      -5.605897e+01 |       32
     30       6.084048e+05      -5.038276e+01 |       32
     31       6.083566e+05      -4.814214e+01 |       32
     32       6.083093e+05      -4.732161e+01 |       31
     33       6.082632e+05      -4.612596e+01 |       31
     34       6.082238e+05      -3.943784e+01 |       32
     35       6.081838e+05      -3.992816e+01 |       31
     36       6.081479e+05      -3.593681e+01 |       29
     37       6.081202e+05      -2.765521e+01 |       31
     38       6.081007e+05      -1.948890e+01 |       29
     39       6.080841e+05      -1.665651e+01 |       30
     40       6.080680e+05      -1.609729e+01 |       29
     41       6.080568e+05      -1.115868e+01 |       29
     42       6.080495e+05      -7.341608e+00 |       25
     43       6.080447e+05      -4.756184e+00 |       22
     44       6.080418e+05      -2.890339e+00 |       22
     45       6.080401e+05      -1.754681e+00 |       15
     46       6.080389e+05      -1.201419e+00 |       15
     47       6.080377e+05      -1.190803e+00 |       19
     48       6.080361e+05      -1.558017e+00 |       19
     49       6.080348e+05      -1.340291e+00 |       13
     50       6.080340e+05      -7.561767e-01 |       17
K-means terminated without convergence after 50 iterations (objv = 608034.0394522387)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.326317
INFO: iteration 2, average log likelihood -1.297103
INFO: iteration 3, average log likelihood -1.268562
INFO: iteration 4, average log likelihood -1.237908
INFO: iteration 5, average log likelihood -1.206386
INFO: iteration 6, average log likelihood -1.161529
WARNING: Variances had to be floored 19
INFO: iteration 7, average log likelihood -1.102096
WARNING: Variances had to be floored 8 10 13 15 21 27
INFO: iteration 8, average log likelihood -1.046139
WARNING: Variances had to be floored 12 16
INFO: iteration 9, average log likelihood -1.104873
INFO: iteration 10, average log likelihood -1.085696
WARNING: Variances had to be floored 8 10 19 21 29
INFO: iteration 11, average log likelihood -1.029336
WARNING: Variances had to be floored 12 15
INFO: iteration 12, average log likelihood -1.069613
WARNING: Variances had to be floored 13 16 27
INFO: iteration 13, average log likelihood -1.065484
WARNING: Variances had to be floored 19
INFO: iteration 14, average log likelihood -1.061656
WARNING: Variances had to be floored 8 10 12 15 21 29
INFO: iteration 15, average log likelihood -1.016634
INFO: iteration 16, average log likelihood -1.084106
WARNING: Variances had to be floored 13 16 19 27
INFO: iteration 17, average log likelihood -1.042388
WARNING: Variances had to be floored 8 12
INFO: iteration 18, average log likelihood -1.058184
WARNING: Variances had to be floored 10 15 21
INFO: iteration 19, average log likelihood -1.033781
WARNING: Variances had to be floored 13 16 19 29
INFO: iteration 20, average log likelihood -1.054324
WARNING: Variances had to be floored 8 12 27
INFO: iteration 21, average log likelihood -1.061457
WARNING: Variances had to be floored 15 21
INFO: iteration 22, average log likelihood -1.055841
WARNING: Variances had to be floored 10 13 16 19
INFO: iteration 23, average log likelihood -1.049539
WARNING: Variances had to be floored 8 12 29
INFO: iteration 24, average log likelihood -1.059566
WARNING: Variances had to be floored 15 21 27
INFO: iteration 25, average log likelihood -1.054701
WARNING: Variances had to be floored 19
INFO: iteration 26, average log likelihood -1.069254
WARNING: Variances had to be floored 8 10 12 13 16
INFO: iteration 27, average log likelihood -1.022586
WARNING: Variances had to be floored 15 21 27 29
INFO: iteration 28, average log likelihood -1.051120
WARNING: Variances had to be floored 19
INFO: iteration 29, average log likelihood -1.091062
WARNING: Variances had to be floored 8
INFO: iteration 30, average log likelihood -1.050092
WARNING: Variances had to be floored 10 12 13 15 16 27
INFO: iteration 31, average log likelihood -1.009435
WARNING: Variances had to be floored 19 21
INFO: iteration 32, average log likelihood -1.071498
WARNING: Variances had to be floored 8 29
INFO: iteration 33, average log likelihood -1.062462
WARNING: Variances had to be floored 13 15
INFO: iteration 34, average log likelihood -1.034332
WARNING: Variances had to be floored 10 12 16 19 21 27
INFO: iteration 35, average log likelihood -1.022640
WARNING: Variances had to be floored 8
INFO: iteration 36, average log likelihood -1.088246
WARNING: Variances had to be floored 15 29
INFO: iteration 37, average log likelihood -1.044822
WARNING: Variances had to be floored 12 13 16 19 21
INFO: iteration 38, average log likelihood -1.022409
WARNING: Variances had to be floored 8 10 27
INFO: iteration 39, average log likelihood -1.073488
WARNING: Variances had to be floored 15
INFO: iteration 40, average log likelihood -1.065679
WARNING: Variances had to be floored 12 13 16 19 21 29
INFO: iteration 41, average log likelihood -1.024055
WARNING: Variances had to be floored 8 10
INFO: iteration 42, average log likelihood -1.079264
WARNING: Variances had to be floored 14 15 27
INFO: iteration 43, average log likelihood -1.055192
WARNING: Variances had to be floored 12 13 19
INFO: iteration 44, average log likelihood -1.055429
WARNING: Variances had to be floored 8 21 29
INFO: iteration 45, average log likelihood -1.037937
WARNING: Variances had to be floored 10 15 16
INFO: iteration 46, average log likelihood -1.044640
WARNING: Variances had to be floored 13 19 27
INFO: iteration 47, average log likelihood -1.063368
WARNING: Variances had to be floored 8 12 14
INFO: iteration 48, average log likelihood -1.048896
WARNING: Variances had to be floored 15 21 29
INFO: iteration 49, average log likelihood -1.036488
WARNING: Variances had to be floored 10 16 19
INFO: iteration 50, average log likelihood -1.052069
INFO: EM with 100000 data points 50 iterations avll -1.052069
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.118289    -0.198071     0.0547101   -0.0431816    -0.0426647     0.0285722    -0.0256459   -0.146216    -0.0536336   -0.0538743   -0.0805828    0.081915    -0.00797895   0.032575    -0.0661203   -0.0240067    0.0685095   -0.0919472   -0.089593    -0.0909756     0.00952103    0.155058     0.0567966   -0.0695104    0.0410638     0.232528 
 -0.0454811   -0.125844    -0.0468353   -0.0382981     0.0235938    -0.00119225    0.0396872   -0.102419     0.055851    -0.0518935   -0.149102     0.0267131   -0.194723     0.121293    -0.212714    -0.0535222    0.0716153   -0.00625849   0.0639      -0.0449815     0.144377     -0.14715      0.0523316   -0.113887     0.0682125     0.114173 
  0.13524      0.0423027   -0.0454687   -0.0499555     0.016651      0.00697463    0.128537     0.174235    -0.189672    -0.0428635    0.0993243   -0.0476682   -0.029508     0.00849776  -0.00454324   0.104746     0.00283817  -0.133182     0.123627    -0.0116174    -0.221832     -0.0839297   -0.0628054    0.0341721   -0.0962663     0.0885498
 -0.098841     0.0103319    0.0233218    0.00381939    0.147714      0.0363846     0.174288     0.00373527   0.0892474   -0.115561     0.147043    -0.037888    -0.0594706   -0.0378811    0.0496738   -0.0755781    0.139837    -0.0533246   -0.0440475    0.187734     -0.0787028    -0.0188867   -0.160257     0.0773855   -0.138989      0.058194 
 -0.282045    -0.0863634    0.196782     0.0622746    -0.121548     -0.0997118    -0.0497995    0.076526    -0.154808    -0.0222787   -0.05184     -0.0458507    0.12216      0.0737181    0.10629     -0.109286    -0.175665     0.041358    -0.0326183    0.0193268    -0.0711416    -0.275079    -0.119855    -0.0397071   -0.0273587     0.194207 
 -0.133606     0.16598     -0.0598247   -0.0695158    -0.0360138     0.0348158    -0.183578    -0.00651227   0.118794     0.113619    -0.0178526   -0.0340236   -0.110791    -0.176962     0.0637453    0.03633     -0.00765849  -0.187904     0.14225      0.000144908   0.194509      0.0218285    0.220526     0.126291     0.0509939    -0.0502261
 -0.068896     0.0822019    0.00676318   0.128069     -0.0362755     0.104889     -0.136423     0.0131833    0.123683     0.0594004    0.232912    -0.0973429    0.0101521   -0.1571       0.0429546   -0.0543128    0.0856174    0.0399191    0.322899    -0.0249083    -0.0976562     0.0478102    0.0458591   -0.116668    -0.110676      0.081513 
  0.0073176    0.0152869   -0.176613    -0.218238      0.0718293    -0.150752      0.168764    -0.0485986   -0.0770278    0.024862     0.128655     0.251526    -0.147625     0.0671833   -0.00441999   0.0457849   -0.0144311    0.0608914    0.00859757   0.110092      0.0556393    -0.120288     0.0138105   -0.0562346   -0.135402     -0.0524022
 -0.00812119   0.013593     0.0635143   -0.0179624     0.0804922    -0.00798858    0.0193231   -0.0609997   -0.0490491    0.0967485   -0.0173518    0.121777     0.12731      0.068388    -0.0897031   -0.016862     0.0917333   -0.0294425    0.0398851    0.0172699    -0.0827597     0.0794212   -0.157599     0.176316    -0.0256784    -0.0486539
 -0.0976622   -0.124661    -0.0851915    0.0536511     0.00892256   -0.158283     -0.0788695   -0.0582564    0.0451434   -0.0857619    0.0378121   -0.145987     0.002284     0.102078     0.0811366   -0.00833009  -0.0657909    0.18754      0.0114086    0.0223336    -0.038835      0.123144    -0.079022     0.12042     -0.00501819   -0.0385214
 -0.103953    -0.0414396    0.129259    -0.221021     -0.0118386     0.0331523    -0.0446236    0.180993    -0.258162    -0.0128807    0.158257    -0.275963    -0.184035     0.0859654   -0.077891    -0.211166     0.221503     0.10646     -0.0321209   -0.0503159    -0.101836     -0.0409636    0.0119465   -0.082064     0.00753777    0.0174433
  0.0299965    0.00162547  -0.105779    -0.0700421    -0.0667046    -0.0293094    -0.0495597    0.0379917   -0.0662436    0.0339582    0.0113698    0.026922    -0.00911553   0.0251192    0.0269551   -0.142722     0.0257006   -0.0651682   -0.0468034    0.151215     -0.0639694    -0.142301     0.0134905   -0.0215198   -0.0831089     0.027572 
  0.0964547   -0.0490117   -0.106881     0.000933154   0.0487357     0.0529804     0.249405     0.0876042   -0.0955877    0.0939841   -0.170298     0.0409188   -0.0372268    0.174807     0.0844728    0.0446225    0.0308002    0.0990004    0.0590832    0.0629238    -0.167227     -0.0110906    0.12932     -0.0651324   -0.187381     -0.0231152
 -0.0404923    0.0091111    0.097672     0.0160143    -0.125445      0.00410308    0.0523649   -0.107001    -0.00235268  -0.0268726   -0.152198     0.0273557   -0.0488678    0.0130313   -0.0624152    0.0602465   -0.0972596   -0.0471443   -0.0209865   -0.0767922     0.00906465   -0.0212969    0.0533857   -0.00679503  -0.0228808     0.122727 
  0.090517     0.03539     -0.0172772    0.18786       0.0220601     0.0965659     0.0861724   -0.0207993    0.102889    -0.0502602    0.0278535   -0.0677914   -0.297265    -0.0401219   -0.0325237   -0.0428186   -0.00292512  -0.0254703    0.0611149   -0.0345039    -0.0339825     0.216469    -0.0376326    0.025092     0.0906063    -0.0136531
 -0.0132655   -0.0370735    0.00773689   0.0569783    -0.102235      0.118391     -0.0145103   -0.0184132    0.043673     0.0172723   -0.207901     0.0286734    0.0227848    0.114428     0.0363248   -0.0443222   -0.0843185   -0.179018    -0.091466    -0.117423      0.0845329    -0.00255262   0.0411618    0.028101    -0.0640148     0.195326 
  0.0687797   -0.00621992  -0.0265573    0.0178399    -0.158585     -0.0747886     0.04238      0.0174005    0.171336     0.0922383   -0.0221681   -0.139423    -0.0484282    0.0286124   -0.158716    -0.0795164    0.15525     -0.0182737    0.10875     -0.0355135    -0.0413294     0.125692     0.00238711   0.0553234   -0.164651      0.0416364
  0.0293142   -0.0316978    0.0152538    0.0476338     0.0717002    -0.0848911     0.0240085   -0.0390279   -0.0369707    0.0135274   -0.0522332    0.0259842    0.0710241    0.0823232    0.0468341    0.0518788    0.115417     0.0631836   -0.00924307   0.0320051    -0.129847      0.0691757    0.0905513   -0.0129463    0.0974279    -0.0208307
  0.017071    -0.00262215  -0.0955305    0.0653778    -0.0085431    -0.185768      0.0655091    0.0551703    0.0797763   -0.0364831    0.187016    -0.0934591   -0.0602805   -0.00496131  -0.0844401   -0.10052      0.0578982    0.257546     0.191296    -0.0274846    -0.0325864     0.0778423   -0.151761    -0.0830495   -0.000377356   0.0677747
  0.112188    -0.103791    -0.038356     0.206831      0.000914039   0.0259169     0.0997015    0.147424     0.211862    -0.200428     0.118156     0.0558376   -0.0553442    0.0184411   -0.0293374   -0.00692584  -0.0954347    0.0427407   -0.173305    -0.0712208     0.0424926    -0.0173463   -0.0874852   -0.219626    -0.0733821    -0.0360916
 -0.0498157   -0.127662     0.041577     0.031701      0.0635074    -0.0326269    -0.0275098    0.177079    -0.0230129   -0.0413207   -0.0865219   -0.0670623    0.0949261   -0.0677184   -0.132833    -0.141085     0.022987     0.181656    -0.00294632   0.0470629    -0.171223     -0.109995    -0.235082     0.178859    -0.086596      0.0700009
 -0.140222     0.126069     0.0134563    0.0345974    -0.0686283    -0.209883      0.156748     0.0164002    0.129505     0.0935558    0.0445983   -0.00283378   0.114697    -0.330662    -0.0193196   -0.100709     0.00208999  -0.106132    -0.162251     0.0100026    -0.123984     -0.064492     0.0904222   -0.0500423   -0.10441       0.0930619
 -0.0334686    0.0485834   -0.0322798    0.123006      0.0782858     0.0256671     0.0331871    0.0892583   -0.184234    -0.0693004   -0.144068     0.109457    -0.0403809    0.224307    -0.0111046    0.178257     0.188594    -0.130605     0.152122     0.0757145     0.116881     -0.0382355    0.054412    -0.0876294   -0.000116609   0.0776089
  0.0664616   -0.0273122   -0.049869    -0.0252219     0.0554459     0.0519362    -0.0256911   -0.0030413   -0.196529    -0.0574744   -0.00891278   0.145903    -0.0948001    0.0662129   -0.0822198   -0.0899234   -0.0957232   -0.0776727   -0.0239402   -0.061459      0.175999      0.0254767    0.0733903    0.065635    -0.055533      0.193174 
  0.197605     0.216112     0.102093     0.0582499     0.172418     -0.0280218    -0.214963     0.0378013   -0.107455     0.0175237    0.199925    -0.0299449    0.0575596   -0.0145395   -0.154861     0.00241067   0.030589    -0.0407874   -0.0345565   -0.114052     -0.0348127     0.0937821    0.103663     0.0204939    0.0488927    -0.0692077
 -0.134431     0.0840078    0.0126045   -0.0898831     0.129121      0.0269309     0.176625    -0.0486245    0.00187705  -0.0545411    0.0752093   -0.0428532    0.0389177   -0.214568     0.0636768    0.0408925   -0.0030452   -0.0320269   -0.0640253    0.0518417     0.0438006    -0.0980813    0.0417996   -0.0972025   -0.0268825     0.121078 
 -0.0674448    0.0417594    0.0963491   -0.121882      0.0577937     0.128998      0.00632325  -0.0313827    0.0927384    0.134063    -0.0639472   -0.0566031   -0.158374    -0.0626113   -0.168113    -0.0736134   -0.183202    -0.060256    -0.0926667   -0.11515       0.0626544    -0.0934761   -0.0767514    0.0198487   -0.0582728    -0.0790893
 -0.0399484   -0.0830474    0.0764113   -0.0722776     0.111979     -0.114613     -0.0538175   -0.00342349  -0.0722487    0.171959     0.0745239    0.0462353    0.013422     0.0596721    0.0290739   -0.0822782    0.1318      -0.204333     0.12676      0.0662956    -0.000835648   0.0677168    0.0112756   -0.122226    -0.137798      0.132941 
 -0.0934478    0.0751541    0.0859869    0.011747     -0.159193      0.0323414     0.0114216   -0.0295559    0.0194957   -0.0790037   -0.116948    -0.268263    -0.104249    -0.045215    -0.134928     0.163887     0.109392     0.127623    -0.0378317    0.0721126    -0.0485259     0.0270454   -0.00626958   0.0696997    0.101335      0.0872696
 -0.139913     0.144632     0.0836672   -0.0900002     0.0477452     0.000700172   0.00365044  -0.00558235  -0.00832972  -0.226239    -0.186994     0.111139    -0.0615091    0.0377406    0.0457524   -0.229163    -0.0882555   -0.0454254    0.0978986   -0.191545      0.0825019    -0.125825    -0.123493    -0.0174325   -0.094675      0.0985798
  0.172729     0.0383606    0.11835     -0.0159021     0.0303398     0.123449     -0.0730511    0.145753    -0.128255     0.00922074   0.0187885   -0.233257     0.110249     0.158162     0.0453731   -0.00184391   0.0654674   -0.0607895    0.0686846    0.0416356     0.0151602    -0.106946     0.0735785    0.0324827   -0.172673     -0.046286 
  0.0775901    0.133898     0.041754     0.0994821    -0.192373     -0.122878     -0.00195401   0.0206136    0.109718    -0.113209    -0.161602     0.176088    -0.0342877   -0.0599061   -0.0923807    0.0426932    0.14103      0.0649702   -0.14221     -0.0617139     0.022582      0.00337522   0.0796204   -0.132505    -0.095366      0.111731 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 8 12 13 27
INFO: iteration 1, average log likelihood -1.058269
WARNING: Variances had to be floored 8 12 13 15 27
INFO: iteration 2, average log likelihood -1.016530
WARNING: Variances had to be floored 8 10 12 13 16 19 21 27 29
INFO: iteration 3, average log likelihood -0.991017
WARNING: Variances had to be floored 8 12 13 15 27
INFO: iteration 4, average log likelihood -1.048030
WARNING: Variances had to be floored 8 12 13 27
INFO: iteration 5, average log likelihood -1.019431
WARNING: Variances had to be floored 8 10 12 13 14 15 19 21 27 29
INFO: iteration 6, average log likelihood -0.979190
WARNING: Variances had to be floored 8 12 13 27
INFO: iteration 7, average log likelihood -1.056491
WARNING: Variances had to be floored 8 12 13 15 16 27
INFO: iteration 8, average log likelihood -1.008993
WARNING: Variances had to be floored 8 10 12 13 19 21 27 29
INFO: iteration 9, average log likelihood -1.003315
WARNING: Variances had to be floored 8 12 13 15 27
INFO: iteration 10, average log likelihood -1.040735
INFO: EM with 100000 data points 10 iterations avll -1.040735
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0501195    0.013646    0.146242    -0.115727      0.0296399    -0.179598    -0.137993    -0.00605387   0.104104     0.0557027   -0.0174974    -0.182053    -0.15291    -0.0612988   -0.0160794    0.0200162    0.108497     -0.161184     0.125836     0.0897178    0.0115773    0.19791      0.114093    -0.128732      0.0485817   -0.221531  
 -0.0970112    0.0766397   0.073487     0.0369447     0.0309556     0.112034     0.073568     0.140596    -0.0774226   -0.057152    -0.000327674  -0.127644     0.052309    0.0779501    0.0417979    0.0659388   -0.115654     -0.0141946   -0.044322     0.0590115   -0.0382247   -0.0906843    0.0229812    0.16925       0.15249      0.226005  
 -0.012793     0.0328398   0.0370049   -0.011008     -0.0705889     0.081462     0.130837     0.0265069    0.145004    -0.0367496    0.00841066    0.0204681   -0.123857    0.143252    -0.0746663    0.188549     0.0855627    -0.118778    -0.0888621   -0.0076344    0.0166846    0.208494    -0.113874    -0.182162     -0.226551    -0.198121  
  0.117472     0.122864   -0.0542294   -0.0361152     0.076494      0.0946032   -0.0389834   -0.102326     0.0480019   -0.0683736    0.107274     -0.0123382    0.0217271   0.0141989    0.0567959    0.0375476    0.0585035    -0.027978    -0.24657      0.0276394   -0.0150172   -0.0196154    0.184369    -0.165024      0.0242113   -0.00835547
  0.196713     0.0306259   0.291346    -0.0862398     0.0678598    -0.101946     0.124798    -0.0902223    0.0644226    0.0102413   -0.115531     -0.00121886   0.0788208   0.0819115   -0.0695275    0.0662078    0.00950866   -0.0485252    0.0550843    0.050248    -0.0177147    0.0474659    0.152303    -0.184079     -0.0363643   -0.0689466 
 -0.11782      0.185443    0.0831587    0.0836491     0.000215912  -0.219254    -0.0483561    0.136132    -0.159396     0.162663     0.0139437    -0.151317     0.0392504   0.078023     0.0911427   -0.0346717   -0.155412      0.00823289   0.062377     0.189231     0.0743862    0.119043     0.0900235   -0.0455564    -0.120067     0.0096912 
  0.140174    -0.184159    0.201071    -0.071498      0.00763718    0.0630821    0.0202925    0.178105     0.0498538   -0.133926     0.0847079    -0.116368    -0.0205851  -0.157124     0.00418664   0.265804    -0.122402      0.0348714    0.0481153    0.0256236   -0.116082    -0.114354     0.297783    -0.0070315     0.0407224   -0.0325453 
 -0.085871     0.23592     0.107646     0.0739924    -0.0980076    -0.169311     0.00821801  -0.0526773   -0.0980594   -0.119455    -0.125812     -0.0194067    0.0250685   0.111812    -0.0597105    0.0647031    0.0747744     0.0172551   -0.0952807    0.116631     0.00527997   0.0302263   -0.0085709    0.044817      0.108697     0.00890018
  0.0380513   -0.0899449   0.0313796   -0.167259     -0.211915      0.0197441   -0.24702     -0.00922562  -0.168936    -0.120953    -0.189801     -0.189591    -0.08977    -0.0584226    0.0423219    0.0776091   -0.0664057    -0.12687      0.137474     0.101422    -0.0104257    0.00193727  -0.0136761   -0.197017      0.00965461  -0.178856  
 -0.0733821   -0.0785777  -0.132551    -0.0606061     0.0410314    -0.0011142   -0.0542823   -0.0221008   -0.0666001    0.00574755   0.257809      0.1634       0.0393567  -0.169909     0.0179823   -0.0582935    0.125622     -0.093289     0.00486034   0.15455     -0.026405    -0.0728268    0.108386     0.12929       0.0342864   -0.0938246 
 -0.00319445   0.0839451  -0.079753    -0.153749      0.0779602     0.0560343   -0.111572     0.12741     -0.134153    -0.126165     0.0365699    -0.00264953   0.0151213   0.194968     0.0346176   -0.0114549   -0.200642     -0.083742    -0.0412864   -0.0485654    0.0832307    0.0282193    0.069915     0.0549772    -0.0190235    0.0665976 
  0.103567    -0.145548   -0.0107117    0.0899981     0.111241      0.0552969    0.0696935    0.147367     0.070042    -0.0145032    0.0444647     0.149432     0.0274375   0.071385    -0.0554609   -0.0369435   -0.123572     -0.164308     0.0129073   -0.151339    -0.0698928   -0.133677    -0.0356308   -0.110714     -0.086615    -0.383455  
 -0.0621905   -0.0134138  -0.0167666    0.154023      0.0262516     0.0672865    0.119062    -0.00552962   0.0237782    0.00522506   0.198407     -0.109552     0.0557523   0.055485     0.135787    -0.153752     0.0131405    -0.0158019   -0.0506433   -0.0351077   -0.00874289   0.0528186    0.0554772   -0.281536     -0.0235938    0.196683  
  0.0849792    0.0948847  -0.165813     0.0697671    -0.154195     -0.0715048    0.0485455    0.0495733   -0.165881    -0.00944004  -0.0250065     0.268478     0.0114669  -0.277525    -0.00173583  -0.00175613   0.0563933     0.146145     0.0355538    0.0384158   -0.0839776   -0.0117102    0.0812283    0.156798      0.0694473    0.154417  
 -0.0233708   -0.0304726  -0.0148491   -0.00782716    0.139867      0.118118     0.158839     0.0308785   -0.046888     0.0949142    0.0388034     0.0284696   -0.0508195  -0.115292     0.0960647   -0.0927122    0.0105367    -0.174859    -0.0551847   -0.00527777   0.00211804   0.0193916    0.0235457    0.125392     -0.115372    -0.0760794 
 -0.0500313    0.119422   -0.0162821    0.000567027   0.148995      0.218113    -0.0352567   -0.0215765    0.0348224    0.0554319   -0.0655057    -0.160889    -0.0979236  -0.0743582    0.0847846    0.15002     -0.0124316     0.0272233    0.144429     0.0023266   -0.164559    -0.159496    -0.206052     0.202025     -0.0436323   -0.147274  
  0.0685623   -0.0533259   0.0331407    0.090046     -0.075501     -0.177329     0.0245679   -0.0707283   -0.130147     0.119239    -0.0133148    -0.00207968   0.0881545   0.0468027   -0.0571803   -0.0100314    0.0492973    -0.107753    -0.0461901    0.122134    -0.0526318    0.0582444    0.0547789   -0.0969498    -0.0521269   -0.148439  
  0.0340297    0.0170673   0.0469781    0.144396      0.0063886    -0.16081     -0.038781     0.0784564    0.00760457  -0.0794875    0.277849      0.0201717   -0.0495673  -0.0801127   -0.0441537    0.0719431   -0.0929509    -0.0349259   -0.0943165    0.0820131    0.0217383   -0.158032     0.087754     0.117994     -0.0447293    0.181235  
  0.0138204   -0.139574   -0.0600933   -0.116059     -0.0867339    -0.0607047    0.084817    -0.0356071    0.0568171    0.0267997    0.078491     -0.0850322    0.101697   -0.0464242   -0.0583747   -0.165014    -0.0765044     0.047263    -0.125322     0.0160845    0.0251037    0.00285066   0.144481    -0.0198738     0.0701149    0.134301  
  0.0555496   -0.0343956  -0.075776    -0.0111147     0.104419      0.00433629   0.063954     0.00768199   0.0412834   -0.135597    -0.000351525  -0.131257    -0.150145   -0.0643027   -0.130785     0.0907157    0.179685     -0.0308781   -0.109997     0.0957407    0.19747      0.134789     0.0329619    0.015123      0.0765804   -0.0150648 
 -0.0142896    0.143861    0.0132182   -0.0087654     0.102004      0.0106316    0.0503207   -0.044833     0.00301461  -0.0692409    0.0435361    -0.0915739   -0.110309   -0.150232    -0.0899217   -0.0446437    0.0148605    -0.131381     0.0885383    0.0545121    0.0733401    0.033945     0.0706091   -0.000708667   0.0192228   -0.0295184 
  0.0215735    0.132114    0.0739538   -0.0249641    -0.0980882     0.00919187   0.0713614   -0.0265073   -0.0453428   -0.00965597   0.0165811     0.0994621   -0.0984115  -0.00888279  -0.0424111   -0.00265906  -0.00972708    0.00670017   0.0528116    0.100429    -0.0594311   -0.196992     0.0267229   -0.0532081     0.0161714    0.1795    
  0.0727942   -0.124279   -0.00601599   0.151229     -0.00926667   -0.110934     0.149887     0.265693     0.0766098    0.0931141   -0.0900091     0.0426355   -0.0258416   0.0247749    0.0720207   -0.108864    -0.0665484    -0.021137     0.085472    -0.0850218   -0.0830005   -0.0883564   -0.154068     0.0631204    -0.120894    -0.1206    
  0.0727079   -0.207322   -0.0664126    0.101838     -0.019868     -0.0192199   -0.102434     0.234218     0.110793    -0.0505167   -0.166444      0.035241     0.110968    0.0470928   -0.0348733   -0.0717677   -0.102025     -0.0727091   -0.0345925   -0.0366915   -0.206648     0.208902     0.10869     -0.187307      0.111451    -0.0772334 
  0.0277961   -0.165298   -0.0684572   -0.125059     -0.0850404    -0.097019    -0.167694     0.0400358   -0.030582    -0.0732694    0.0982604     0.0566651    0.123903    0.072451     0.0485818    0.0590184    0.121929     -0.0480228   -0.0746876    0.0313367    0.065799    -0.0870121    0.0962482    0.0653427    -0.0129225   -0.0433779 
 -0.0604617   -0.190264   -0.117328    -0.00954391    0.0453641    -0.0208718   -0.0297251   -0.163697    -0.145782    -0.133235    -0.00709255    0.129798    -0.169519    0.105894     0.0987913   -0.0896838    0.0114326    -0.268924     0.053129     0.0468728    0.089725     0.0302627   -0.013853     0.139143      0.13455      0.0604273 
  0.0654611    0.0381378   0.00433114  -0.105124     -0.0152379    -0.109883     0.0955891    0.00423192   0.033429     0.0348298   -0.043283     -0.0993294    0.260223    0.0316686   -0.133678     0.032898    -0.00276759   -0.0209226    0.0375829    0.236532    -0.126097     0.0440789   -0.087342    -0.00875623    0.415387     0.141287  
  0.0675877   -0.0146002   0.00579183  -0.192262     -0.112862     -0.0858992   -0.00863491   0.108122    -0.0637069    0.133199    -0.095091      0.070603     0.0566379  -0.0607835    0.00808582  -0.216277     0.0571952    -0.117957    -0.075        0.142912    -0.0735895   -0.0883088   -0.114739     0.0548158    -0.0098663    0.029241  
  0.0668224   -0.142689    0.0201694    0.124686      0.11405      -0.281919     0.0229287   -0.132524    -0.229736    -0.0538675    0.221967     -0.228815     0.0885042   0.0149259   -0.037826    -0.0967266    0.0675777    -0.0401979   -0.0882439    0.0043453   -0.0570951    0.0359024   -0.0944827    0.0203068     0.114719     0.0873084 
  0.037599     0.0854439   0.0374995   -0.00672817   -0.289606     -0.0454909    0.104503    -0.00484073   0.172123    -0.156367    -0.0452211    -0.158428    -0.0652125   0.0796568   -0.120236     0.0663025    0.116123     -0.0778093    0.164924     0.165733    -0.0660037    0.037597     0.168895     0.0635566    -0.0116086   -0.0723496 
 -0.246537     0.0609566  -0.139053    -0.0639067    -0.127482     -0.0883114    0.13005     -0.0418919   -0.27076     -0.24241      0.130443     -0.0810036    0.120394    0.0605928   -0.10787      0.104688     0.0167557    -0.208139    -0.135997    -0.168725     0.170411     0.0179826    0.00562805  -0.0588655     0.123254     0.0357928 
 -0.0116195   -0.0101192  -0.236455    -0.0064        0.0253503     0.0713289   -0.0882611    0.0331642   -0.0665176    0.0313098    0.0780507     0.0810977    0.238772    0.0479537    0.0253053   -0.054168     0.000868948   0.0835976    0.0771151   -0.151564    -0.0420257    0.038361     0.0298064   -0.129866     -0.0525679    0.00951489kind full, method split
0: avll = -1.423639710892187
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.423660
INFO: iteration 2, average log likelihood -1.423579
INFO: iteration 3, average log likelihood -1.423511
INFO: iteration 4, average log likelihood -1.423427
INFO: iteration 5, average log likelihood -1.423325
INFO: iteration 6, average log likelihood -1.423209
INFO: iteration 7, average log likelihood -1.423092
INFO: iteration 8, average log likelihood -1.422989
INFO: iteration 9, average log likelihood -1.422912
INFO: iteration 10, average log likelihood -1.422859
INFO: iteration 11, average log likelihood -1.422827
INFO: iteration 12, average log likelihood -1.422806
INFO: iteration 13, average log likelihood -1.422792
INFO: iteration 14, average log likelihood -1.422781
INFO: iteration 15, average log likelihood -1.422771
INFO: iteration 16, average log likelihood -1.422760
INFO: iteration 17, average log likelihood -1.422746
INFO: iteration 18, average log likelihood -1.422725
INFO: iteration 19, average log likelihood -1.422688
INFO: iteration 20, average log likelihood -1.422620
INFO: iteration 21, average log likelihood -1.422489
INFO: iteration 22, average log likelihood -1.422239
INFO: iteration 23, average log likelihood -1.421787
INFO: iteration 24, average log likelihood -1.421061
INFO: iteration 25, average log likelihood -1.420129
INFO: iteration 26, average log likelihood -1.419251
INFO: iteration 27, average log likelihood -1.418653
INFO: iteration 28, average log likelihood -1.418336
INFO: iteration 29, average log likelihood -1.418189
INFO: iteration 30, average log likelihood -1.418124
INFO: iteration 31, average log likelihood -1.418095
INFO: iteration 32, average log likelihood -1.418083
INFO: iteration 33, average log likelihood -1.418077
INFO: iteration 34, average log likelihood -1.418074
INFO: iteration 35, average log likelihood -1.418073
INFO: iteration 36, average log likelihood -1.418072
INFO: iteration 37, average log likelihood -1.418072
INFO: iteration 38, average log likelihood -1.418071
INFO: iteration 39, average log likelihood -1.418071
INFO: iteration 40, average log likelihood -1.418071
INFO: iteration 41, average log likelihood -1.418071
INFO: iteration 42, average log likelihood -1.418070
INFO: iteration 43, average log likelihood -1.418070
INFO: iteration 44, average log likelihood -1.418070
INFO: iteration 45, average log likelihood -1.418070
INFO: iteration 46, average log likelihood -1.418070
INFO: iteration 47, average log likelihood -1.418070
INFO: iteration 48, average log likelihood -1.418070
INFO: iteration 49, average log likelihood -1.418070
INFO: iteration 50, average log likelihood -1.418070
INFO: EM with 100000 data points 50 iterations avll -1.418070
952.4 data points per parameter
1: avll = [-1.42366,-1.42358,-1.42351,-1.42343,-1.42333,-1.42321,-1.42309,-1.42299,-1.42291,-1.42286,-1.42283,-1.42281,-1.42279,-1.42278,-1.42277,-1.42276,-1.42275,-1.42272,-1.42269,-1.42262,-1.42249,-1.42224,-1.42179,-1.42106,-1.42013,-1.41925,-1.41865,-1.41834,-1.41819,-1.41812,-1.4181,-1.41808,-1.41808,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.418090
INFO: iteration 2, average log likelihood -1.418005
INFO: iteration 3, average log likelihood -1.417933
INFO: iteration 4, average log likelihood -1.417844
INFO: iteration 5, average log likelihood -1.417735
INFO: iteration 6, average log likelihood -1.417616
INFO: iteration 7, average log likelihood -1.417501
INFO: iteration 8, average log likelihood -1.417405
INFO: iteration 9, average log likelihood -1.417334
INFO: iteration 10, average log likelihood -1.417284
INFO: iteration 11, average log likelihood -1.417247
INFO: iteration 12, average log likelihood -1.417219
INFO: iteration 13, average log likelihood -1.417196
INFO: iteration 14, average log likelihood -1.417176
INFO: iteration 15, average log likelihood -1.417157
INFO: iteration 16, average log likelihood -1.417139
INFO: iteration 17, average log likelihood -1.417121
INFO: iteration 18, average log likelihood -1.417102
INFO: iteration 19, average log likelihood -1.417082
INFO: iteration 20, average log likelihood -1.417061
INFO: iteration 21, average log likelihood -1.417039
INFO: iteration 22, average log likelihood -1.417015
INFO: iteration 23, average log likelihood -1.416989
INFO: iteration 24, average log likelihood -1.416961
INFO: iteration 25, average log likelihood -1.416933
INFO: iteration 26, average log likelihood -1.416904
INFO: iteration 27, average log likelihood -1.416876
INFO: iteration 28, average log likelihood -1.416849
INFO: iteration 29, average log likelihood -1.416824
INFO: iteration 30, average log likelihood -1.416801
INFO: iteration 31, average log likelihood -1.416781
INFO: iteration 32, average log likelihood -1.416763
INFO: iteration 33, average log likelihood -1.416747
INFO: iteration 34, average log likelihood -1.416733
INFO: iteration 35, average log likelihood -1.416721
INFO: iteration 36, average log likelihood -1.416711
INFO: iteration 37, average log likelihood -1.416702
INFO: iteration 38, average log likelihood -1.416694
INFO: iteration 39, average log likelihood -1.416687
INFO: iteration 40, average log likelihood -1.416681
INFO: iteration 41, average log likelihood -1.416675
INFO: iteration 42, average log likelihood -1.416670
INFO: iteration 43, average log likelihood -1.416666
INFO: iteration 44, average log likelihood -1.416661
INFO: iteration 45, average log likelihood -1.416657
INFO: iteration 46, average log likelihood -1.416654
INFO: iteration 47, average log likelihood -1.416650
INFO: iteration 48, average log likelihood -1.416647
INFO: iteration 49, average log likelihood -1.416644
INFO: iteration 50, average log likelihood -1.416641
INFO: EM with 100000 data points 50 iterations avll -1.416641
473.9 data points per parameter
2: avll = [-1.41809,-1.41801,-1.41793,-1.41784,-1.41774,-1.41762,-1.4175,-1.41741,-1.41733,-1.41728,-1.41725,-1.41722,-1.4172,-1.41718,-1.41716,-1.41714,-1.41712,-1.4171,-1.41708,-1.41706,-1.41704,-1.41701,-1.41699,-1.41696,-1.41693,-1.4169,-1.41688,-1.41685,-1.41682,-1.4168,-1.41678,-1.41676,-1.41675,-1.41673,-1.41672,-1.41671,-1.4167,-1.41669,-1.41669,-1.41668,-1.41668,-1.41667,-1.41667,-1.41666,-1.41666,-1.41665,-1.41665,-1.41665,-1.41664,-1.41664]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.416649
INFO: iteration 2, average log likelihood -1.416596
INFO: iteration 3, average log likelihood -1.416550
INFO: iteration 4, average log likelihood -1.416496
INFO: iteration 5, average log likelihood -1.416430
INFO: iteration 6, average log likelihood -1.416349
INFO: iteration 7, average log likelihood -1.416254
INFO: iteration 8, average log likelihood -1.416148
INFO: iteration 9, average log likelihood -1.416038
INFO: iteration 10, average log likelihood -1.415932
INFO: iteration 11, average log likelihood -1.415834
INFO: iteration 12, average log likelihood -1.415747
INFO: iteration 13, average log likelihood -1.415671
INFO: iteration 14, average log likelihood -1.415607
INFO: iteration 15, average log likelihood -1.415553
INFO: iteration 16, average log likelihood -1.415507
INFO: iteration 17, average log likelihood -1.415468
INFO: iteration 18, average log likelihood -1.415435
INFO: iteration 19, average log likelihood -1.415407
INFO: iteration 20, average log likelihood -1.415382
INFO: iteration 21, average log likelihood -1.415359
INFO: iteration 22, average log likelihood -1.415339
INFO: iteration 23, average log likelihood -1.415321
INFO: iteration 24, average log likelihood -1.415305
INFO: iteration 25, average log likelihood -1.415289
INFO: iteration 26, average log likelihood -1.415275
INFO: iteration 27, average log likelihood -1.415262
INFO: iteration 28, average log likelihood -1.415250
INFO: iteration 29, average log likelihood -1.415239
INFO: iteration 30, average log likelihood -1.415228
INFO: iteration 31, average log likelihood -1.415218
INFO: iteration 32, average log likelihood -1.415208
INFO: iteration 33, average log likelihood -1.415199
INFO: iteration 34, average log likelihood -1.415190
INFO: iteration 35, average log likelihood -1.415182
INFO: iteration 36, average log likelihood -1.415174
INFO: iteration 37, average log likelihood -1.415167
INFO: iteration 38, average log likelihood -1.415159
INFO: iteration 39, average log likelihood -1.415152
INFO: iteration 40, average log likelihood -1.415145
INFO: iteration 41, average log likelihood -1.415139
INFO: iteration 42, average log likelihood -1.415132
INFO: iteration 43, average log likelihood -1.415126
INFO: iteration 44, average log likelihood -1.415119
INFO: iteration 45, average log likelihood -1.415113
INFO: iteration 46, average log likelihood -1.415107
INFO: iteration 47, average log likelihood -1.415101
INFO: iteration 48, average log likelihood -1.415095
INFO: iteration 49, average log likelihood -1.415090
INFO: iteration 50, average log likelihood -1.415084
INFO: EM with 100000 data points 50 iterations avll -1.415084
236.4 data points per parameter
3: avll = [-1.41665,-1.4166,-1.41655,-1.4165,-1.41643,-1.41635,-1.41625,-1.41615,-1.41604,-1.41593,-1.41583,-1.41575,-1.41567,-1.41561,-1.41555,-1.41551,-1.41547,-1.41544,-1.41541,-1.41538,-1.41536,-1.41534,-1.41532,-1.4153,-1.41529,-1.41528,-1.41526,-1.41525,-1.41524,-1.41523,-1.41522,-1.41521,-1.4152,-1.41519,-1.41518,-1.41517,-1.41517,-1.41516,-1.41515,-1.41515,-1.41514,-1.41513,-1.41513,-1.41512,-1.41511,-1.41511,-1.4151,-1.4151,-1.41509,-1.41508]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.415088
INFO: iteration 2, average log likelihood -1.415033
INFO: iteration 3, average log likelihood -1.414985
INFO: iteration 4, average log likelihood -1.414932
INFO: iteration 5, average log likelihood -1.414868
INFO: iteration 6, average log likelihood -1.414791
INFO: iteration 7, average log likelihood -1.414701
INFO: iteration 8, average log likelihood -1.414599
INFO: iteration 9, average log likelihood -1.414490
INFO: iteration 10, average log likelihood -1.414378
INFO: iteration 11, average log likelihood -1.414270
INFO: iteration 12, average log likelihood -1.414168
INFO: iteration 13, average log likelihood -1.414075
INFO: iteration 14, average log likelihood -1.413994
INFO: iteration 15, average log likelihood -1.413923
INFO: iteration 16, average log likelihood -1.413861
INFO: iteration 17, average log likelihood -1.413808
INFO: iteration 18, average log likelihood -1.413762
INFO: iteration 19, average log likelihood -1.413722
INFO: iteration 20, average log likelihood -1.413687
INFO: iteration 21, average log likelihood -1.413655
INFO: iteration 22, average log likelihood -1.413626
INFO: iteration 23, average log likelihood -1.413599
INFO: iteration 24, average log likelihood -1.413574
INFO: iteration 25, average log likelihood -1.413550
INFO: iteration 26, average log likelihood -1.413528
INFO: iteration 27, average log likelihood -1.413506
INFO: iteration 28, average log likelihood -1.413485
INFO: iteration 29, average log likelihood -1.413464
INFO: iteration 30, average log likelihood -1.413444
INFO: iteration 31, average log likelihood -1.413424
INFO: iteration 32, average log likelihood -1.413404
INFO: iteration 33, average log likelihood -1.413385
INFO: iteration 34, average log likelihood -1.413367
INFO: iteration 35, average log likelihood -1.413349
INFO: iteration 36, average log likelihood -1.413331
INFO: iteration 37, average log likelihood -1.413315
INFO: iteration 38, average log likelihood -1.413298
INFO: iteration 39, average log likelihood -1.413283
INFO: iteration 40, average log likelihood -1.413268
INFO: iteration 41, average log likelihood -1.413254
INFO: iteration 42, average log likelihood -1.413240
INFO: iteration 43, average log likelihood -1.413228
INFO: iteration 44, average log likelihood -1.413215
INFO: iteration 45, average log likelihood -1.413204
INFO: iteration 46, average log likelihood -1.413193
INFO: iteration 47, average log likelihood -1.413182
INFO: iteration 48, average log likelihood -1.413172
INFO: iteration 49, average log likelihood -1.413162
INFO: iteration 50, average log likelihood -1.413153
INFO: EM with 100000 data points 50 iterations avll -1.413153
118.1 data points per parameter
4: avll = [-1.41509,-1.41503,-1.41499,-1.41493,-1.41487,-1.41479,-1.4147,-1.4146,-1.41449,-1.41438,-1.41427,-1.41417,-1.41408,-1.41399,-1.41392,-1.41386,-1.41381,-1.41376,-1.41372,-1.41369,-1.41365,-1.41363,-1.4136,-1.41357,-1.41355,-1.41353,-1.41351,-1.41348,-1.41346,-1.41344,-1.41342,-1.4134,-1.41339,-1.41337,-1.41335,-1.41333,-1.41331,-1.4133,-1.41328,-1.41327,-1.41325,-1.41324,-1.41323,-1.41322,-1.4132,-1.41319,-1.41318,-1.41317,-1.41316,-1.41315]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.413152
INFO: iteration 2, average log likelihood -1.413087
INFO: iteration 3, average log likelihood -1.413024
INFO: iteration 4, average log likelihood -1.412949
INFO: iteration 5, average log likelihood -1.412854
INFO: iteration 6, average log likelihood -1.412734
INFO: iteration 7, average log likelihood -1.412588
INFO: iteration 8, average log likelihood -1.412422
INFO: iteration 9, average log likelihood -1.412245
INFO: iteration 10, average log likelihood -1.412067
INFO: iteration 11, average log likelihood -1.411896
INFO: iteration 12, average log likelihood -1.411735
INFO: iteration 13, average log likelihood -1.411590
INFO: iteration 14, average log likelihood -1.411461
INFO: iteration 15, average log likelihood -1.411349
INFO: iteration 16, average log likelihood -1.411252
INFO: iteration 17, average log likelihood -1.411169
INFO: iteration 18, average log likelihood -1.411097
INFO: iteration 19, average log likelihood -1.411035
INFO: iteration 20, average log likelihood -1.410980
INFO: iteration 21, average log likelihood -1.410933
INFO: iteration 22, average log likelihood -1.410891
INFO: iteration 23, average log likelihood -1.410854
INFO: iteration 24, average log likelihood -1.410820
INFO: iteration 25, average log likelihood -1.410790
INFO: iteration 26, average log likelihood -1.410762
INFO: iteration 27, average log likelihood -1.410737
INFO: iteration 28, average log likelihood -1.410714
INFO: iteration 29, average log likelihood -1.410693
INFO: iteration 30, average log likelihood -1.410673
INFO: iteration 31, average log likelihood -1.410654
INFO: iteration 32, average log likelihood -1.410637
INFO: iteration 33, average log likelihood -1.410621
INFO: iteration 34, average log likelihood -1.410605
INFO: iteration 35, average log likelihood -1.410591
INFO: iteration 36, average log likelihood -1.410577
INFO: iteration 37, average log likelihood -1.410564
INFO: iteration 38, average log likelihood -1.410552
INFO: iteration 39, average log likelihood -1.410541
INFO: iteration 40, average log likelihood -1.410530
INFO: iteration 41, average log likelihood -1.410519
INFO: iteration 42, average log likelihood -1.410509
INFO: iteration 43, average log likelihood -1.410500
INFO: iteration 44, average log likelihood -1.410491
INFO: iteration 45, average log likelihood -1.410482
INFO: iteration 46, average log likelihood -1.410473
INFO: iteration 47, average log likelihood -1.410465
INFO: iteration 48, average log likelihood -1.410457
INFO: iteration 49, average log likelihood -1.410449
INFO: iteration 50, average log likelihood -1.410442
INFO: EM with 100000 data points 50 iterations avll -1.410442
59.0 data points per parameter
5: avll = [-1.41315,-1.41309,-1.41302,-1.41295,-1.41285,-1.41273,-1.41259,-1.41242,-1.41225,-1.41207,-1.4119,-1.41174,-1.41159,-1.41146,-1.41135,-1.41125,-1.41117,-1.4111,-1.41103,-1.41098,-1.41093,-1.41089,-1.41085,-1.41082,-1.41079,-1.41076,-1.41074,-1.41071,-1.41069,-1.41067,-1.41065,-1.41064,-1.41062,-1.41061,-1.41059,-1.41058,-1.41056,-1.41055,-1.41054,-1.41053,-1.41052,-1.41051,-1.4105,-1.41049,-1.41048,-1.41047,-1.41046,-1.41046,-1.41045,-1.41044]
[-1.42364,-1.42366,-1.42358,-1.42351,-1.42343,-1.42333,-1.42321,-1.42309,-1.42299,-1.42291,-1.42286,-1.42283,-1.42281,-1.42279,-1.42278,-1.42277,-1.42276,-1.42275,-1.42272,-1.42269,-1.42262,-1.42249,-1.42224,-1.42179,-1.42106,-1.42013,-1.41925,-1.41865,-1.41834,-1.41819,-1.41812,-1.4181,-1.41808,-1.41808,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41807,-1.41809,-1.41801,-1.41793,-1.41784,-1.41774,-1.41762,-1.4175,-1.41741,-1.41733,-1.41728,-1.41725,-1.41722,-1.4172,-1.41718,-1.41716,-1.41714,-1.41712,-1.4171,-1.41708,-1.41706,-1.41704,-1.41701,-1.41699,-1.41696,-1.41693,-1.4169,-1.41688,-1.41685,-1.41682,-1.4168,-1.41678,-1.41676,-1.41675,-1.41673,-1.41672,-1.41671,-1.4167,-1.41669,-1.41669,-1.41668,-1.41668,-1.41667,-1.41667,-1.41666,-1.41666,-1.41665,-1.41665,-1.41665,-1.41664,-1.41664,-1.41665,-1.4166,-1.41655,-1.4165,-1.41643,-1.41635,-1.41625,-1.41615,-1.41604,-1.41593,-1.41583,-1.41575,-1.41567,-1.41561,-1.41555,-1.41551,-1.41547,-1.41544,-1.41541,-1.41538,-1.41536,-1.41534,-1.41532,-1.4153,-1.41529,-1.41528,-1.41526,-1.41525,-1.41524,-1.41523,-1.41522,-1.41521,-1.4152,-1.41519,-1.41518,-1.41517,-1.41517,-1.41516,-1.41515,-1.41515,-1.41514,-1.41513,-1.41513,-1.41512,-1.41511,-1.41511,-1.4151,-1.4151,-1.41509,-1.41508,-1.41509,-1.41503,-1.41499,-1.41493,-1.41487,-1.41479,-1.4147,-1.4146,-1.41449,-1.41438,-1.41427,-1.41417,-1.41408,-1.41399,-1.41392,-1.41386,-1.41381,-1.41376,-1.41372,-1.41369,-1.41365,-1.41363,-1.4136,-1.41357,-1.41355,-1.41353,-1.41351,-1.41348,-1.41346,-1.41344,-1.41342,-1.4134,-1.41339,-1.41337,-1.41335,-1.41333,-1.41331,-1.4133,-1.41328,-1.41327,-1.41325,-1.41324,-1.41323,-1.41322,-1.4132,-1.41319,-1.41318,-1.41317,-1.41316,-1.41315,-1.41315,-1.41309,-1.41302,-1.41295,-1.41285,-1.41273,-1.41259,-1.41242,-1.41225,-1.41207,-1.4119,-1.41174,-1.41159,-1.41146,-1.41135,-1.41125,-1.41117,-1.4111,-1.41103,-1.41098,-1.41093,-1.41089,-1.41085,-1.41082,-1.41079,-1.41076,-1.41074,-1.41071,-1.41069,-1.41067,-1.41065,-1.41064,-1.41062,-1.41061,-1.41059,-1.41058,-1.41056,-1.41055,-1.41054,-1.41053,-1.41052,-1.41051,-1.4105,-1.41049,-1.41048,-1.41047,-1.41046,-1.41046,-1.41045,-1.41044]
32×26 Array{Float64,2}:
 -0.187584     0.281853    -0.117626     0.125775   -0.249243    0.0489469   -0.129657    -0.09736    -0.192643    0.102622   -0.403515   -0.292938    -0.00390136   0.315862   -0.244496   -0.0291066   -0.201981    -0.395951    -0.0327491  -0.0465094  -0.298155   -0.381961    0.0652506    0.294355    0.259078    0.162686 
 -0.147183    -0.23952     -0.270058     0.158398    0.130133   -0.269738     0.247384    -0.066636   -0.268499    0.0514204  -0.062795    0.0176196    0.219522    -0.0994137  -0.0225264  -0.276261    -0.233528     0.414088    -0.240968   -0.322782   -0.389126   -0.0799354   0.0538412   -0.0288266   0.200745   -0.0139744
  0.00585309   0.194988    -0.220655    -0.159165   -0.0304081   0.220331     0.17458      0.155046   -0.0881335   0.0921419   0.262828    0.00216771  -0.0193208    0.0568014  -0.028455   -0.0695382    0.142524    -0.0344325    0.182441    0.340934   -0.0919944   0.0343333  -0.0223049   -0.04404     0.0162088  -0.0158798
 -0.0350707   -0.065476     0.379236    -0.0468012  -0.0224528  -0.114866    -0.0837493    0.146323    0.176615   -0.134817   -0.117387    0.107087    -0.0465022   -0.102553   -0.0384404   0.0706975   -0.160678    -0.00157982  -0.191412    0.0581859   0.194794   -0.113435    0.00775596  -0.0409537   0.0593292   0.0321505
  0.223361     0.143945     0.0906371   -0.0597619  -0.46364    -0.609449    -0.187437     0.451015    0.517485    0.0269638  -0.769299   -0.350692    -0.10404      0.0239401  -0.681087    0.416228    -0.00985011   0.268405    -0.807231   -0.067446   -0.0402209  -0.261157    0.359206    -0.269705    0.143566   -0.463771 
  0.172796    -0.599273     0.572002     0.246015   -0.423896    0.324416    -0.14254     -0.387565    0.179188    0.19129    -0.156562   -0.306145     0.236501    -0.676783   -0.458953   -0.025288     0.599076     0.39306     -0.778327    0.827528    0.0138403  -0.445073   -0.103944    -0.452664    0.43966     0.401488 
  0.0482455    0.143307    -0.0664514   -0.689839    0.132972   -0.0185106    0.210802     0.675528    0.309556   -0.0162143   0.0677917  -0.0960119    0.557356    -0.240129    0.378896    0.00937506  -0.301971     0.319295    -0.547068   -0.0376669  -0.100736   -0.797096   -0.0234795   -0.418908    0.386649    0.211646 
 -0.0670668   -0.383898     0.85147     -0.360197    0.0449984  -0.677934    -0.00708986  -0.351364    0.232543    0.151246    0.0246926   0.0130793    0.732788     0.073492   -0.015437    0.182632    -0.168626    -0.498367    -0.578425   -0.49492     0.121235    0.379332   -0.474493    -0.107117    0.168783    0.348018 
  0.108037     0.25402     -0.37147      0.160036    0.283743    0.0854376   -0.02248      0.306182   -0.675087    0.25798     0.293751   -0.474105     0.0996761   -0.0591085  -0.0940142   0.22039      0.390431     0.284758     0.578756    0.462833   -0.329413    0.370695   -0.0219822   -0.118293   -0.186439   -0.682411 
 -0.0360812    0.185516     0.144944    -0.26411     0.373831    0.161385     0.0833098    0.071762    0.507969   -0.319103    0.125697   -0.353536    -0.194622     0.439833   -0.500093    0.779818    -0.0664401    0.145538     0.63161     0.703173    0.205874    0.203186    0.220478    -0.921987   -0.0783842  -0.376295 
  0.0154092   -0.453936    -0.23429      0.267851    0.0228051   0.0683092    0.297016    -0.980846    0.210952    0.0649174   0.0341582  -0.553519     0.0129454    0.206275   -0.108979   -0.0731174   -0.120281     0.167005     0.293331   -0.299227    0.214428    0.75958     0.0884007   -0.431041    0.266576    0.0886955
  0.315656    -0.0929144    0.0767814    0.222357   -0.0126155  -0.185551    -0.0754262   -0.424076   -0.0215948  -0.346759    0.234445    0.360723    -0.185628     0.315369   -0.368998    0.123262     0.363745    -0.20508      0.158897    0.136448    0.285114    0.518057    0.0723799   -0.37576    -0.050147    0.320385 
  0.660041    -0.0636712   -0.125333    -0.0479623   0.257849   -0.3135      -0.447764    -0.259668   -0.437174   -0.772889   -0.399934    0.145866    -0.25296     -0.301787   -0.0114683   0.136165    -0.521685    -0.179803     0.561073   -0.293188    0.254604    0.0254947   0.0392725    0.175719   -0.258609   -0.313255 
  0.104592     0.0210544    0.0234219   -0.069567    0.0692851   0.196257    -0.520747     1.07549    -0.150166   -0.767714   -0.138235    0.320379    -0.238142    -0.128525   -0.0994155  -0.125304    -0.153107    -0.192026     0.0855191   0.392634    0.119347   -0.794227    0.22995      0.223759   -0.446401   -0.0633245
  0.0883283    0.0795378    0.00458368   0.14223     0.0810219  -0.0160197   -0.286011    -0.0653395  -0.402116    0.0621482   0.309164   -0.251936     0.29259     -0.0235324   0.418243    0.0528919    0.543849    -0.685246     0.0442661  -0.353603   -0.0247644   0.251225    0.151532     0.147021   -0.483291   -0.141155 
  0.704042    -0.147746     0.0687071    0.0208512   0.144119   -0.0563208   -0.159236     0.206651    0.178952    0.170944   -0.0485652  -0.0115496   -0.459844    -0.682089    0.312362    0.0504913    0.399627     0.030956     0.0761132   0.352178    0.401176    0.144998    0.289735     0.382396   -0.222269   -0.440038 
  0.209145     0.0657978    0.0714682   -0.22066     0.115775   -0.278025    -0.128991    -0.067485   -0.303766   -0.130001   -0.100173   -0.126489     0.415071    -0.154172    0.308285    0.153148    -0.108402    -0.191408     0.0361556  -0.418628    0.13029     0.0454266  -0.129945    -0.0303843  -0.106497   -0.158362 
 -0.0415277   -0.13645     -0.144208     0.309712   -0.12361     0.573505     0.293374     0.10824     0.294497    0.0491062   0.297739    0.162188    -0.323476     0.44721    -0.277164   -0.0185188    0.0341131    0.274917     0.203843    0.727877   -0.0816368   0.150295    0.225853    -0.090712   -0.0805793  -0.103344 
 -0.0921686   -0.193576    -0.488281     0.459895   -0.378526    0.0334677   -0.228705    -0.354694   -0.30818     0.12273    -0.477317   -0.0766532   -0.739723    -0.0118591   0.161818   -0.62346     -0.348051    -0.318559    -0.0724062  -0.154224   -0.210687   -0.0789017   0.230414     0.628774   -0.403329    0.236967 
  0.278608     0.259629     0.448952     0.35711     0.211449   -0.347278    -0.487749    -0.117817    0.0563498   0.143297   -0.0629232  -0.175805    -0.0912514    0.0382781  -0.283352    0.392564     0.353487    -0.560937     0.726403    0.202261    0.300488    0.242056    0.107945     0.655818    0.203077   -0.358959 
  0.286753    -0.604718     0.189252     0.35889    -0.20364    -0.244154    -0.379219    -0.126636   -0.0441418  -0.350852   -0.0684788  -0.499613     0.329078    -0.406978   -0.203926    0.119418    -0.0191435    0.110536    -0.0576394  -0.779146    0.0404216  -0.506929    1.15142     -0.492556    0.335115    0.0102578
  0.0210292   -0.014503    -0.236505    -0.0248574  -0.569875    0.285727     0.794448    -0.047584    0.40439     0.0962947   0.226516   -0.155241     0.374148    -0.173917    0.265757   -0.0870651    0.372331     0.22848      0.164268   -0.515648   -0.70551    -0.156756    0.747449    -0.130742    0.0230758   0.174745 
 -0.421438    -0.399525    -0.323879     0.491373   -0.214161    0.0621399    0.589134     0.236834    0.144688   -0.130098    0.469762    0.314408     0.226883     0.302838    0.358989   -0.0574994    0.343342    -0.194855    -0.618704   -0.58778     0.117678    0.488367    0.513489    -0.536918   -0.802991    0.0214907
 -0.263466    -0.120197     0.399298     0.884495    0.751693   -0.55604     -0.41157      0.233      -0.0782323   0.0633423  -0.0511389   0.0362318   -0.247766    -0.037428    0.48556     0.660542     0.379236    -0.0804972   -0.420492   -0.640948   -0.29221     0.296039    0.497487     0.1708     -0.812082   -0.0403649
  0.366876    -0.0869938   -0.333139    -0.450531   -0.636938    0.599915     0.455158     0.290672    0.341467   -0.394245   -0.526637    0.175829    -0.233083     0.215697   -0.645285   -0.734268    -0.846337     0.45054      0.439248    0.745682    0.238995   -0.271796   -0.342165    -0.450955    0.987314   -0.318038 
 -0.695494     0.00967776   0.0268821    0.172856   -0.195007   -0.461633     0.725489    -0.193532    0.167563   -0.259282   -0.182362    0.0407756    0.0639056    0.0827947  -0.739619   -0.21303     -0.639667     0.363152    -0.174639   -0.130472   -0.355061    0.0861658  -0.275969    -0.704965    0.256761    0.685844 
  0.274516     0.575822    -0.00217633  -0.420576   -0.356766    0.00638629   0.24853     -0.250845   -0.313968    0.113964   -0.0835416  -0.496627     0.249623    -0.127105   -0.347205   -0.428195    -0.610978     0.219157     0.365031    0.493828   -0.21249    -0.619961   -0.595284     0.196159    0.899293   -0.085892 
 -0.12917      0.162181    -0.345212    -0.519867    0.0540653   0.0460273    0.470847     0.277376    0.297242    1.20779     0.31952    -0.128621     0.0252393   -0.0300131   0.286922   -0.207544     0.312279     0.103516    -0.498928    0.538936    0.243185    0.135508   -0.285795     0.652293    0.432165    0.106126 
 -0.113169     0.245325    -0.54543     -0.563898    0.260997   -0.103325     0.380651     0.502763   -0.391409   -0.256802    1.05283     0.683505    -0.198859     0.181822    0.523888   -0.451266    -0.205429    -0.609225     0.585144   -0.333916   -0.317326    0.301645    0.116119     0.388796   -0.0661831   0.23384  
 -0.152556     0.408565    -0.225273    -0.0421804   0.464504    0.709827     0.222449    -0.207786   -0.19625    -0.0291102   0.18744     0.288129    -0.306554     0.33759     0.618543   -0.283313    -0.160293    -0.468175     0.552538    0.695197   -0.145018    0.135092   -0.87787      0.89664    -0.190447    0.330187 
 -0.238196     0.262498     0.334033    -0.883149   -0.226064   -0.0242028   -0.221678     0.169398   -0.356876   -0.195002    0.394862    0.387602    -0.0802541   -0.722723    0.363618   -0.0521775    0.0737092   -0.206269    -0.475627    0.237867    0.0218449  -0.154022   -0.453633    -0.0474535   0.10352     0.344785 
 -0.687502    -0.12104      0.133806    -0.251921   -0.12737     0.418552    -0.035024     0.058227    0.0100154  -0.0605122   0.169142    0.433955     0.245166     0.92237     0.0849597  -0.135611    -0.116017    -0.316861    -0.412953    0.168667    0.0373939  -0.14607    -0.499926    -0.159459    0.15574    -0.106377 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.410434
INFO: iteration 2, average log likelihood -1.410427
INFO: iteration 3, average log likelihood -1.410420
INFO: iteration 4, average log likelihood -1.410413
INFO: iteration 5, average log likelihood -1.410407
INFO: iteration 6, average log likelihood -1.410400
INFO: iteration 7, average log likelihood -1.410393
INFO: iteration 8, average log likelihood -1.410387
INFO: iteration 9, average log likelihood -1.410380
INFO: iteration 10, average log likelihood -1.410374
INFO: EM with 100000 data points 10 iterations avll -1.410374
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.358190e+05
      1       7.058615e+05      -2.299575e+05 |       32
      2       6.912471e+05      -1.461439e+04 |       32
      3       6.856042e+05      -5.642859e+03 |       32
      4       6.827468e+05      -2.857447e+03 |       32
      5       6.809345e+05      -1.812251e+03 |       32
      6       6.797469e+05      -1.187670e+03 |       32
      7       6.789121e+05      -8.347671e+02 |       32
      8       6.782819e+05      -6.302230e+02 |       32
      9       6.777713e+05      -5.105343e+02 |       32
     10       6.773425e+05      -4.288871e+02 |       32
     11       6.769568e+05      -3.856301e+02 |       32
     12       6.765964e+05      -3.604611e+02 |       32
     13       6.762757e+05      -3.207019e+02 |       32
     14       6.760078e+05      -2.678674e+02 |       32
     15       6.757602e+05      -2.476393e+02 |       32
     16       6.755225e+05      -2.376836e+02 |       32
     17       6.753099e+05      -2.126041e+02 |       32
     18       6.750979e+05      -2.119424e+02 |       32
     19       6.749095e+05      -1.884075e+02 |       32
     20       6.746976e+05      -2.119351e+02 |       32
     21       6.744739e+05      -2.236789e+02 |       32
     22       6.742907e+05      -1.832090e+02 |       32
     23       6.741249e+05      -1.657836e+02 |       32
     24       6.739855e+05      -1.393884e+02 |       32
     25       6.738559e+05      -1.296422e+02 |       32
     26       6.737275e+05      -1.283638e+02 |       32
     27       6.735905e+05      -1.370250e+02 |       32
     28       6.734640e+05      -1.264703e+02 |       32
     29       6.733509e+05      -1.131173e+02 |       32
     30       6.732520e+05      -9.889005e+01 |       32
     31       6.731666e+05      -8.539380e+01 |       32
     32       6.730776e+05      -8.905045e+01 |       32
     33       6.729945e+05      -8.305156e+01 |       32
     34       6.729219e+05      -7.257050e+01 |       32
     35       6.728548e+05      -6.713371e+01 |       32
     36       6.727867e+05      -6.814449e+01 |       32
     37       6.727198e+05      -6.687340e+01 |       32
     38       6.726608e+05      -5.900589e+01 |       32
     39       6.726050e+05      -5.582330e+01 |       32
     40       6.725565e+05      -4.850900e+01 |       32
     41       6.725185e+05      -3.794032e+01 |       32
     42       6.724820e+05      -3.653934e+01 |       32
     43       6.724428e+05      -3.920277e+01 |       32
     44       6.724061e+05      -3.663643e+01 |       32
     45       6.723657e+05      -4.047144e+01 |       32
     46       6.723238e+05      -4.184374e+01 |       32
     47       6.722847e+05      -3.912992e+01 |       32
     48       6.722505e+05      -3.419593e+01 |       32
     49       6.722215e+05      -2.897566e+01 |       32
     50       6.721965e+05      -2.503271e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 672196.4845247624)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.422231
INFO: iteration 2, average log likelihood -1.417245
INFO: iteration 3, average log likelihood -1.415891
INFO: iteration 4, average log likelihood -1.414879
INFO: iteration 5, average log likelihood -1.413811
INFO: iteration 6, average log likelihood -1.412816
INFO: iteration 7, average log likelihood -1.412117
INFO: iteration 8, average log likelihood -1.411724
INFO: iteration 9, average log likelihood -1.411507
INFO: iteration 10, average log likelihood -1.411372
INFO: iteration 11, average log likelihood -1.411274
INFO: iteration 12, average log likelihood -1.411195
INFO: iteration 13, average log likelihood -1.411128
INFO: iteration 14, average log likelihood -1.411069
INFO: iteration 15, average log likelihood -1.411015
INFO: iteration 16, average log likelihood -1.410966
INFO: iteration 17, average log likelihood -1.410920
INFO: iteration 18, average log likelihood -1.410877
INFO: iteration 19, average log likelihood -1.410837
INFO: iteration 20, average log likelihood -1.410799
INFO: iteration 21, average log likelihood -1.410763
INFO: iteration 22, average log likelihood -1.410729
INFO: iteration 23, average log likelihood -1.410697
INFO: iteration 24, average log likelihood -1.410666
INFO: iteration 25, average log likelihood -1.410637
INFO: iteration 26, average log likelihood -1.410610
INFO: iteration 27, average log likelihood -1.410584
INFO: iteration 28, average log likelihood -1.410560
INFO: iteration 29, average log likelihood -1.410536
INFO: iteration 30, average log likelihood -1.410515
INFO: iteration 31, average log likelihood -1.410494
INFO: iteration 32, average log likelihood -1.410475
INFO: iteration 33, average log likelihood -1.410457
INFO: iteration 34, average log likelihood -1.410440
INFO: iteration 35, average log likelihood -1.410424
INFO: iteration 36, average log likelihood -1.410409
INFO: iteration 37, average log likelihood -1.410395
INFO: iteration 38, average log likelihood -1.410382
INFO: iteration 39, average log likelihood -1.410369
INFO: iteration 40, average log likelihood -1.410358
INFO: iteration 41, average log likelihood -1.410346
INFO: iteration 42, average log likelihood -1.410336
INFO: iteration 43, average log likelihood -1.410326
INFO: iteration 44, average log likelihood -1.410317
INFO: iteration 45, average log likelihood -1.410308
INFO: iteration 46, average log likelihood -1.410300
INFO: iteration 47, average log likelihood -1.410291
INFO: iteration 48, average log likelihood -1.410284
INFO: iteration 49, average log likelihood -1.410276
INFO: iteration 50, average log likelihood -1.410269
INFO: EM with 100000 data points 50 iterations avll -1.410269
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.113257    0.366916    0.0864262   -0.185556    -0.252343     0.130716     0.00813471   0.616706     0.174724   -0.190348   -0.260733     0.113593   -0.233518     0.188668    -0.311324    0.0052318  -0.314526     0.0455328   -0.179686     0.380648    -0.14262      -0.55885      0.0402662     0.0334764   0.244622     -0.016361   
  0.267118    0.0510012   0.118082     0.435085     0.309883    -0.19037     -0.17808     -0.245175     0.439934   -0.373372   -0.980938    -0.390641    0.0354711    0.491773     0.146682    0.472464   -0.669234     0.393232    -0.152481    -0.427452     0.473398      0.317333    -0.0433068    -0.257043   -0.325312      0.052176   
 -0.401192   -0.215206    0.282047     0.900829     0.636428    -0.440149    -0.425298     0.182718    -0.201052   -0.275903   -0.171115     0.255386   -0.511346    -0.173082    -0.0757868   0.609432    0.151469     0.271746    -0.175473    -0.72401     -0.586692      0.110927     0.680426     -0.0570892  -0.534398     -0.0636377  
 -0.487769    0.11257    -0.49619      0.469959    -0.431748    -0.142002     0.68624      0.0410883    0.0979271  -0.33723    -0.228396    -0.0689254   0.259982     0.498416    -0.398815   -0.230879   -0.135551     0.262791    -0.385619    -0.608941    -0.667259     -0.0879426    0.412647     -0.711848   -0.00542716    0.236756   
 -0.13425     0.0685445  -0.169093     0.140671    -0.0562745   -0.0752286    0.0803916   -0.0617556   -0.26097     0.0347983   0.0233135   -0.306611    0.234352     0.0934242    0.0158207   0.0719015   0.0258311   -0.0749637   -0.00846926  -0.267759    -0.326013      0.00448391   0.188593     -0.0303897  -0.0914166    -0.135871   
  0.269101   -0.160239    0.217821     0.036749     0.0552034   -0.0337521   -0.109476     0.222225     0.204957    0.0710289  -0.0760385   -0.0144064  -0.171761    -0.39773      0.0904499   0.0317421   0.163823     0.132408    -0.124554     0.247044     0.295443      0.0167301    0.163506      0.11059    -0.0616041    -0.220225   
  0.282357    0.268991    0.499404     0.763217     0.273569    -1.05165     -0.0547897    0.604072    -0.58494     0.733254    0.0165505   -0.400145    0.221152    -0.457125     0.526324   -0.0391641   0.533989    -0.531314    -0.0330718   -0.686408    -0.363888     -0.0375086    0.404411      0.807682   -0.429335      0.0730699  
 -0.191743    0.472263   -0.0274953   -0.590417     0.0264492   -0.118999     0.782457     0.724973     0.183771    0.408202    0.702194    -0.028532    0.713349    -0.0766927    0.252348    0.27789     0.287763     0.288876    -0.247521     0.0802669   -0.0555159     0.123499    -0.049743     -0.317437    0.0336726     0.103438   
  0.306266    0.230507   -0.1828      -0.486728    -0.624582     0.292674     0.5043      -0.103638     0.0687789  -0.184431   -0.331915    -0.15118     0.106111     0.0867736   -0.667072   -0.658323   -0.829524     0.328638     0.497874     0.591746     0.072975     -0.403672    -0.467503     -0.293185    0.999806     -0.211086   
 -0.0825436  -0.51916    -0.19157      0.504957    -0.206756    -0.136708     0.55227     -0.604483     0.211094   -0.0197371   0.595849    -0.300682   -0.0854677    0.289623    -0.516798    0.0316165   0.106093     0.128871     0.586994    -0.153076     0.15158       0.834887     0.455713     -0.260567    0.0404713     0.171907   
 -0.287551    0.37574    -0.253427    -0.175598     0.500809     0.544985     0.406459    -0.0389052   -0.259315   -0.0864261   0.328082     0.39824    -0.283737     0.427795     0.625273   -0.417409   -0.264247    -0.461989     0.53812      0.340094    -0.219506      0.149817    -0.736924      0.737521   -0.124899      0.329098   
 -0.16548     0.382143    0.129063    -0.711631    -0.453984     0.490572    -0.548951     0.317152    -0.417441   -0.195132    0.174963    -0.0934543   0.342975    -0.179138     0.629571   -0.0720156   0.0318029   -0.717215    -0.299816    -0.0800684    0.0648764    -0.39389     -0.198904      0.340748   -0.139752     -0.102671   
 -0.37294    -0.570468   -0.214239    -0.309429    -0.123099     0.0645318    0.00936832   0.267436     0.218084   -0.553633    0.5683       1.28054     0.0106564    0.229639     0.233891   -0.386598    0.0311523   -0.0641909   -0.383635    -0.142921     0.189077      0.134546    -0.047662     -0.363152   -0.16224       0.016085   
  0.22364    -0.264993    0.125443    -0.318852    -0.609099     0.349441     0.421426    -0.355407     0.0328941  -0.756576    0.269932    -0.270931    0.0974468   -0.963379     0.0616143  -0.345446    0.180423     0.413372    -0.334211     0.382046    -0.36958      -0.0738862   -0.0665545    -0.440192    0.108303      0.777928   
 -0.309872   -0.0657893  -0.139118    -0.913757     0.423787     0.262958     0.0960827   -0.213395     0.531312   -0.217782    0.263336    -0.400231   -0.440583     0.584624    -0.747096    0.788949   -0.149204     0.295825     0.531318     0.984676     0.0746039     0.163659     0.212177     -1.51531     0.0586642    -0.552395   
  0.0779536   0.243014   -0.437435    -0.351421    -0.0480833    0.179947     0.228216     0.0435171   -0.0810366   0.679026    0.125236    -0.0410572  -0.0515987    0.00146013   0.317212   -0.216694    0.127117    -0.00517736  -0.110248     0.355122     0.148587      0.0203176   -0.284636      0.580368    0.195321     -0.000255701
  0.136352    0.393139    0.441041     0.145751     0.110564    -0.0184541   -0.488943     0.00994408   0.266294    0.0176852   0.00925211  -0.039755   -0.24157      0.282445    -0.34564     0.532876    0.467531    -0.721887     0.393247     0.45108      0.454383      0.235345     0.11524       0.230282   -0.000942011  -0.0834666  
 -0.290283   -0.512222    0.31904      0.66462     -0.208088     0.417845     0.59935      0.202334     0.524895    0.566548    0.0311624    0.0991806  -0.5256       0.407116    -0.282455    0.0343276  -0.183576     0.800612    -0.52604      0.983234    -0.246224      0.0577697   -0.188547     -0.175151   -0.200822      0.0283512  
 -0.240353   -0.326711   -0.280485     0.433327    -0.386391     0.00128503  -0.272617    -0.360746    -0.402971   -0.0734495  -0.4143      -0.111976   -0.768083     0.196709    -0.0313886  -0.505969   -0.336676    -0.734524    -0.0464978   -0.00590954  -0.0905034    -0.0738579    0.0684637     0.478891   -0.422306      0.214639   
 -0.101396   -0.0442889   0.601867    -0.461942     0.212889    -0.668302    -0.123585    -0.52225     -0.252175    0.143345    0.210946     0.055627    0.509858     0.0435289   -0.108794    0.219202   -0.174556    -0.384601    -0.12877      0.0320083   -0.00383868    0.444695    -0.847007     -0.23949     0.404236      0.254678   
  0.842501   -0.105999   -0.114792    -0.00200133   0.269629    -0.282155    -0.450202     0.0634083   -0.45679    -0.152228   -0.285138    -0.141872   -0.320473    -0.518151    -0.022455    0.427954    0.132856     0.0466313    0.320932     0.3599       0.355482      0.227828    -0.203829      0.146276   -0.494478     -0.668347   
  0.261179   -0.262107    0.502353    -0.182512    -0.358429    -0.734713    -0.466254     0.236769     0.322586   -0.366417   -0.435618    -0.347641    0.353425    -0.485259    -0.400632    0.416773    0.00128319   0.187678    -0.768365    -0.292985     0.289089     -0.662019     0.460342     -0.473048    0.206198     -0.0893098  
  0.0833927  -0.0749426   0.204218     0.182965     0.204989     0.0642002   -0.335006     0.10751     -0.272916   -0.712524    0.0904753    0.072799    0.00869208  -0.0533384   -0.0890592   0.109226   -0.1989      -0.304429     0.552375     0.0259168    0.0285133    -0.153061     0.308008     -0.0326991  -0.237394     -0.216989   
  0.0521713   0.194251   -0.0801215   -0.301728     0.160457     0.0333675    0.127123     0.260949    -0.259269   -0.0289596   0.544458     0.35522    -0.0658264    0.0034457    0.253821   -0.125202    0.123135    -0.383401     0.219695     0.163823    -0.000470646   0.149104    -0.103278      0.0236355  -0.180437      0.0756484  
 -0.0496355  -0.165879   -0.377218    -0.401586     0.509074    -0.0915651    0.0396415    0.326955    -0.201568    0.0977761  -0.0202881   -0.0583653   0.232491    -0.310092     0.535295   -0.356617   -0.72269      0.686087    -0.557002    -0.0342383   -0.221524     -0.796893    -0.226685     -0.140882    0.417913      0.174594   
  0.0462977   0.367305   -0.504419     0.294489     0.148507     0.404063     0.0344938    0.280096    -0.40287     0.51068     0.211291    -0.4911     -0.103288     0.245942    -0.0565355  -0.0200289   0.48033      0.286945     0.570667     0.746826    -0.501121      0.204808     0.0109135     0.0819037   0.0514936    -0.381146   
 -0.0101331  -0.130789   -0.171861     0.00138855  -0.00203027   0.104577    -0.0875798   -0.668563    -0.0997543  -0.0962127  -0.112264    -0.0096278  -0.0443975    0.175224    -0.219595   -0.216886   -0.0949937    0.0244391    0.140279     0.12253      0.201786      0.147458    -0.29225      -0.0899625   0.41263       0.253264   
 -0.187128   -0.437855    0.165183     0.519522     0.30143      0.0237642    0.0274646   -0.118794    -0.0403994   0.0906796   0.272524    -0.0102818   0.193831     0.214743     0.696706    0.163583    0.509628    -0.559038    -0.576705    -0.555592     0.188474      0.669142     0.33256      -0.127984   -0.873365     -0.0563409  
  0.572524    0.356373   -0.477747     0.126591     0.14265     -0.198305    -0.0645023   -0.131589    -0.107853   -0.538857    0.0804182    0.18553    -0.101402    -0.278498     0.0973613  -0.190175   -0.0282154   -0.201613     0.902124    -0.413796    -0.115909     -0.0772846    0.68353       0.270855    0.0385145    -0.129402   
 -0.0728078  -0.228154    0.408362    -0.143441    -0.146308    -0.366508     0.208283    -0.0593192    0.227113   -0.0968419  -0.139087     0.120928    0.476273     0.0970353   -0.298309   -0.037446   -0.154611    -0.0823713   -0.40556     -0.320573    -0.0175624    -0.143543     0.000972508  -0.289699    0.311315      0.275533   
 -0.395903   -0.0563598   0.0119814   -0.313868    -0.405244    -0.0386407   -0.0780339   -0.145419     0.322093    0.943673   -0.329778    -0.212522   -0.036515    -0.169387    -0.169699   -0.147183    0.138129    -0.0905621   -0.675783     0.0194659   -0.358677     -0.209501    -0.145746      0.604985    0.60691       0.133395   
  0.394934   -0.596058    1.98088e-5   0.362734    -0.236763     0.143176    -0.0736729   -0.386839    -0.0148768   0.404582    0.0685129   -0.603387    0.298899    -0.284924    -0.173675    0.138221    0.531924     0.114376    -0.0343628   -0.515279    -0.0630637    -0.170645     0.980471     -0.565902    0.475043     -0.201281   INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.410263
INFO: iteration 2, average log likelihood -1.410256
INFO: iteration 3, average log likelihood -1.410250
INFO: iteration 4, average log likelihood -1.410244
INFO: iteration 5, average log likelihood -1.410238
INFO: iteration 6, average log likelihood -1.410233
INFO: iteration 7, average log likelihood -1.410227
INFO: iteration 8, average log likelihood -1.410222
INFO: iteration 9, average log likelihood -1.410217
INFO: iteration 10, average log likelihood -1.410212
INFO: EM with 100000 data points 10 iterations avll -1.410212
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
