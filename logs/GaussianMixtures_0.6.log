>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.7.0
INFO: Installing JLD v0.6.6
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.1
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.2.0
INFO: Installing StaticArrays v0.1.0
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated
INFO: METADATA is out-of-date â€” you may not have the latest version of GaussianMixtures
INFO: Use `Pkg.update()` to get the latest versions of your packages

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1235
Commit 271b31d (2016-11-13 22:02 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-101-generic #148-Ubuntu SMP Thu Oct 20 22:08:32 UTC 2016 x86_64 x86_64
Memory: 2.939281463623047 GB (653.38671875 MB free)
Uptime: 24000.0 sec
Load Avg:  1.130859375  1.09326171875  1.06689453125
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3500 MHz    1471213 s       6642 s     145468 s     522529 s         54 s
#2  3500 MHz     653754 s         39 s      78452 s    1589925 s          1 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.3
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.7.0
 - JLD                           0.6.6
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.1
 - Rmath                         0.1.4
 - SHA                           0.2.1
 - ScikitLearnBase               0.2.0
 - StaticArrays                  0.1.0
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##775#777{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-734068.5878071178,[79164.0,20836.0],
[22120.6 3435.77 -9381.29; -22013.4 -3651.2 9357.6],

Array{Float64,2}[
[67752.5 253.194 7579.38; 253.194 86036.6 8089.87; 7579.38 8089.87 80209.7],

[32469.8 256.821 -7064.28; 256.821 14384.2 -7760.86; -7064.28 -7760.86 19880.3]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       2.405840e+03
      1       1.382563e+03      -1.023277e+03 |        6
      2       1.190081e+03      -1.924813e+02 |        6
      3       1.064645e+03      -1.254365e+02 |        6
      4       9.369801e+02      -1.276648e+02 |        4
      5       8.871112e+02      -4.986890e+01 |        5
      6       8.469303e+02      -4.018088e+01 |        2
      7       8.441928e+02      -2.737565e+00 |        0
      8       8.441928e+02       0.000000e+00 |        0
K-means converged with 8 iterations (objv = 844.1927848175046)
INFO: K-means with 272 data points using 8 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.058483
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.841633
INFO: iteration 2, lowerbound -3.760648
INFO: iteration 3, lowerbound -3.673402
INFO: iteration 4, lowerbound -3.560586
INFO: iteration 5, lowerbound -3.425188
INFO: iteration 6, lowerbound -3.278137
INFO: dropping number of Gaussions to 7
INFO: iteration 7, lowerbound -3.121268
INFO: iteration 8, lowerbound -2.956804
INFO: dropping number of Gaussions to 6
INFO: iteration 9, lowerbound -2.793405
INFO: iteration 10, lowerbound -2.646529
INFO: dropping number of Gaussions to 5
INFO: iteration 11, lowerbound -2.535225
INFO: dropping number of Gaussions to 4
INFO: iteration 12, lowerbound -2.452804
INFO: iteration 13, lowerbound -2.397300
INFO: dropping number of Gaussions to 3
INFO: iteration 14, lowerbound -2.360408
INFO: iteration 15, lowerbound -2.329711
INFO: iteration 16, lowerbound -2.311973
INFO: iteration 17, lowerbound -2.307667
INFO: dropping number of Gaussions to 2
INFO: iteration 18, lowerbound -2.302919
INFO: iteration 19, lowerbound -2.299260
INFO: iteration 20, lowerbound -2.299256
INFO: iteration 21, lowerbound -2.299254
INFO: iteration 22, lowerbound -2.299254
INFO: iteration 23, lowerbound -2.299253
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: 48 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Mon 14 Nov 2016 12:10:10 PM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Mon 14 Nov 2016 12:10:11 PM UTC: K-means with 272 data points using 8 iterations
11.3 data points per parameter
,Mon 14 Nov 2016 12:10:13 PM UTC: EM with 272 data points 0 iterations avll -2.058483
5.8 data points per parameter
,Mon 14 Nov 2016 12:10:13 PM UTC: GMM converted to Variational GMM
,Mon 14 Nov 2016 12:10:15 PM UTC: iteration 1, lowerbound -3.841633
,Mon 14 Nov 2016 12:10:15 PM UTC: iteration 2, lowerbound -3.760648
,Mon 14 Nov 2016 12:10:15 PM UTC: iteration 3, lowerbound -3.673402
,Mon 14 Nov 2016 12:10:15 PM UTC: iteration 4, lowerbound -3.560586
,Mon 14 Nov 2016 12:10:16 PM UTC: iteration 5, lowerbound -3.425188
,Mon 14 Nov 2016 12:10:16 PM UTC: iteration 6, lowerbound -3.278137
,Mon 14 Nov 2016 12:10:16 PM UTC: dropping number of Gaussions to 7
,Mon 14 Nov 2016 12:10:16 PM UTC: iteration 7, lowerbound -3.121268
,Mon 14 Nov 2016 12:10:16 PM UTC: iteration 8, lowerbound -2.956804
,Mon 14 Nov 2016 12:10:16 PM UTC: dropping number of Gaussions to 6
,Mon 14 Nov 2016 12:10:16 PM UTC: iteration 9, lowerbound -2.793405
,Mon 14 Nov 2016 12:10:16 PM UTC: iteration 10, lowerbound -2.646529
,Mon 14 Nov 2016 12:10:16 PM UTC: dropping number of Gaussions to 5
,Mon 14 Nov 2016 12:10:16 PM UTC: iteration 11, lowerbound -2.535225
,Mon 14 Nov 2016 12:10:16 PM UTC: dropping number of Gaussions to 4
,Mon 14 Nov 2016 12:10:16 PM UTC: iteration 12, lowerbound -2.452804
,Mon 14 Nov 2016 12:10:16 PM UTC: iteration 13, lowerbound -2.397300
,Mon 14 Nov 2016 12:10:16 PM UTC: dropping number of Gaussions to 3
,Mon 14 Nov 2016 12:10:16 PM UTC: iteration 14, lowerbound -2.360408
,Mon 14 Nov 2016 12:10:16 PM UTC: iteration 15, lowerbound -2.329711
,Mon 14 Nov 2016 12:10:17 PM UTC: iteration 16, lowerbound -2.311973
,Mon 14 Nov 2016 12:10:17 PM UTC: iteration 17, lowerbound -2.307667
,Mon 14 Nov 2016 12:10:17 PM UTC: dropping number of Gaussions to 2
,Mon 14 Nov 2016 12:10:17 PM UTC: iteration 18, lowerbound -2.302919
,Mon 14 Nov 2016 12:10:17 PM UTC: iteration 19, lowerbound -2.299260
,Mon 14 Nov 2016 12:10:17 PM UTC: iteration 20, lowerbound -2.299256
,Mon 14 Nov 2016 12:10:17 PM UTC: iteration 21, lowerbound -2.299254
,Mon 14 Nov 2016 12:10:17 PM UTC: iteration 22, lowerbound -2.299254
,Mon 14 Nov 2016 12:10:17 PM UTC: iteration 23, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:17 PM UTC: iteration 24, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:17 PM UTC: iteration 25, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:17 PM UTC: iteration 26, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:17 PM UTC: iteration 27, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:17 PM UTC: iteration 28, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:17 PM UTC: iteration 29, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:17 PM UTC: iteration 30, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:17 PM UTC: iteration 31, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:17 PM UTC: iteration 32, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:17 PM UTC: iteration 33, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:18 PM UTC: iteration 34, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:18 PM UTC: iteration 35, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:18 PM UTC: iteration 36, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:18 PM UTC: iteration 37, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:18 PM UTC: iteration 38, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:18 PM UTC: iteration 39, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:18 PM UTC: iteration 40, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:18 PM UTC: iteration 41, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:18 PM UTC: iteration 42, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:18 PM UTC: iteration 43, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:18 PM UTC: iteration 44, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:18 PM UTC: iteration 45, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:18 PM UTC: iteration 46, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:18 PM UTC: iteration 47, lowerbound -2.299253
,Mon 14 Nov 2016 12:10:18 PM UTC: 48 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
Î± = [178.045,95.9549]
Î² = [178.045,95.9549]
m = [4.2503 79.2869; 2.00023 53.852]
Î½ = [180.045,97.9549]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.184042 -0.00764405; 0.0 0.00858171],

[0.375876 -0.00895312; 0.0 0.0127487]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 99999.99999999999
avll from stats: -0.9934798589441773
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.993479858944176
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9934798589441761
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 99999.99999999999
avll from stats: -0.9967615850784328
avll from llpg:  -0.9967615850784328
avll direct:     -0.9967615850784329
sum posterior: 100000.0
32Ã—26 Array{Float64,2}:
 -0.199498     0.032299   -0.124968      0.176035    -0.0661367    0.030002     0.0246674   -0.0186944   0.031674     0.115291   -0.0815859   -0.174966    -0.0250913    0.128811    0.242425     0.143184      0.0865438   -0.023823    0.140959     -0.0689438   -0.0130701     0.0836318    0.0289133    -0.067221    -0.0300787   -0.2042    
 -0.192255     0.0362828   0.224398      0.0367932   -0.0911865    0.153456     0.0243698   -0.22638    -0.167837     0.0468821   0.150285    -0.0291735    0.0382309    0.1286      0.0569979    0.0943983    -0.0204147    0.0657945   0.0260528    -0.0963114   -0.160655     -0.066386    -0.01012       0.0746594    0.136602     0.0529491 
  0.136961     0.0694793   0.0760693    -0.0240254   -0.069592    -0.00311869  -0.0998021   -0.26154    -0.166932    -0.103816    0.0777652   -0.141274    -0.103723     0.119807   -0.0436087   -0.188535     -0.0992335    0.0020593   0.100645      0.0193057   -0.0469737     0.228015     0.0261856     0.106405     0.0519963    0.0215398 
 -0.0302291   -0.0540071   0.0271724     0.0538524   -0.0439805    0.0519071    0.00148135  -0.0768374   0.0596058    0.0942098  -0.00102142  -0.0134154   -0.00745375  -0.133902   -0.126588     0.0891707    -0.0272081    0.1796      0.15154       0.0150281   -0.172832     -0.229568    -0.0656414    -0.202877    -0.128007     0.0511828 
 -0.0180109   -0.0205624   0.238684     -0.0539211   -0.115359     0.0763601    0.0314638    0.0445391  -0.00732363   0.0192386   0.109223    -0.0478806    0.0330249    0.0450901   0.0462299    0.0818686    -0.0710577    0.0559962  -0.0425435     0.0182212   -0.0912587     0.0211999    0.0162649     0.109427     0.0159418   -0.0892657 
 -0.0692161   -0.0832554   0.254442     -0.0457468   -0.098214     0.0280492    0.194873    -0.0859817   0.0646389   -0.0178946  -0.0585677   -0.00847281  -0.0756125   -0.077324   -0.038931    -0.0447502     0.0305053   -0.0780414  -0.095261      0.235383    -0.000392453  -0.0358828   -0.150869      0.0579784   -0.0135283   -0.0412971 
  0.187251     0.0824006  -0.0210528     0.0112896    0.0597577    0.107333    -0.0848907   -0.0497999   0.0779769    0.0467937  -0.09        -0.0198137   -0.169525    -0.0766206  -0.100592     0.21912      -0.0146565    0.128466    0.00999658    0.0903042   -0.0789856     0.0546869    0.133106     -0.0469788   -0.0612313   -0.137258  
 -0.182449    -0.0502803   0.0447357    -0.0994949   -0.118502    -0.172171     0.19999     -0.155071   -0.0943514    0.0998388  -0.0523507    0.0297604   -0.139153    -0.0472678  -0.060012    -0.0291565    -0.0226002    0.0935405   0.192288     -0.0136718   -0.0349578    -0.129956     0.0151573    -0.194698     0.0535363   -0.00092101
  0.0793774   -0.0637687  -0.0275869    -0.133847    -0.0499804    0.233403     0.0684797   -0.071864   -0.0656262   -0.0191985   0.0330139    0.0489891   -0.018549    -0.0575393   0.128544    -0.0279067    -0.0919537    0.20941     0.110287      0.091086    -0.159526      0.0279151    0.0973473    -0.0109277   -0.0117367   -0.0473334 
 -0.186188     0.0491295  -0.0396687     0.00473975  -0.0296736   -0.0295605   -0.0138354   -0.0673286  -0.100751    -0.110553   -0.205259     0.0826003    0.20924      0.0127863  -0.189677    -0.111808     -0.158556    -0.104752    0.0616494     0.044557    -0.00633386    0.0306423    0.0800603     0.149774     0.173361     0.0132932 
  0.269458    -0.126017    0.0647916     0.174149     0.0185176    0.0516496    0.00489672  -0.0857829   0.0357245    0.0982522   0.123849    -0.0115719   -0.12326      0.135272    0.065618    -0.0526601    -0.0730084    0.0337966   0.11719       0.162176     0.161752      0.0323286   -0.0395505     0.0653646    0.0868841    0.0787361 
 -0.0764526    0.114848    0.017559      0.0262838   -0.0150172   -0.151941    -0.123709     0.0599852   0.222447    -0.0447523   0.0638767   -0.0761398    0.0983253    0.035542    0.0544767   -0.219974     -0.160332    -0.110809   -0.0564462    -0.156741     0.0138852    -0.0515263   -0.104565      0.0591055    0.0741961    0.0332813 
 -0.0732052    0.104013   -0.0133029     0.0525172   -0.0177575    0.058168    -0.0220289    0.292802   -0.107437    -0.0168272   0.123329    -0.0632831    0.0998581    0.0906779   0.195797     0.0812757     0.0136168    0.113464    0.134143      0.130479    -0.019854     -0.118726     0.0893299    -0.0121031    0.0723086   -0.0529091 
 -0.0265601   -0.0591315   0.00084374    0.0842101    0.0088476    0.00913321   0.0878569    0.02804     0.259132    -0.165817   -0.0623282    0.0528772   -0.163595    -0.0460264   0.0171541    0.0653054    -0.0266382    0.0848865  -0.00202623    0.0371219   -0.204236     -0.0281422    0.0701715    -0.0750141   -0.0970464    0.0439065 
 -0.058581    -0.136785   -0.0858184    -0.135712     0.043519    -0.0545041   -0.0429927    0.0900308   0.050842     0.0985392  -0.110058     0.137972    -0.0359872    0.0705386  -0.0324346   -0.167047      0.084872    -0.0294451  -0.131099     -0.082527    -0.163823      0.0659074    0.045521     -0.0688794   -0.105589    -0.00335981
  0.0067579   -0.0662942   0.202991      0.0725881    0.070328     0.0735937    0.148853     0.0235258  -0.156018    -0.0287437   0.00426168  -0.0414451   -0.173574    -0.136247    0.190901    -0.00899885    0.0290589    0.145258    0.0396243     0.0816745   -0.0507699     0.109352    -0.0799941     0.186745     0.0388003    0.203795  
 -0.00440228  -0.0933998  -0.0909913    -0.0857012    0.192492     0.00061521   0.0506798    0.0824679   0.114331    -0.0839927   0.0568804    0.037803     0.0519078   -0.0948386   0.10343      0.0284597    -0.0591253    0.0787455  -0.15173      -0.0724365    0.00126343    0.0429008    0.0562309    -0.0379302   -0.0327064    0.121517  
 -0.0465227   -0.258146    0.0751935     0.0446076   -0.0474262    0.0658162   -0.070321    -0.0651307  -0.134035     0.150121    0.00468941   0.207272     0.0612782   -0.155395   -0.036995    -0.119417      0.0649778    0.018692    0.116718     -0.0914485    0.0138629    -0.0420623    0.13128       0.0611296    0.0609521    0.0243415 
 -0.12282      0.0271379   0.127973     -0.0719734   -0.0284795   -0.0386728   -0.111963     0.127032    0.120365    -0.0564927  -0.0517762   -0.0695366    0.0122295    0.179904    0.0706303   -0.0423957     0.0760457   -0.223936    0.120305     -0.00611974   0.237978     -0.0202878    0.00833398    0.0330499    0.0281358   -0.0169767 
  0.0271591   -0.127762    0.0340749     0.0794922    0.0654847    0.15398      0.00972766   0.0622847   0.0247515   -0.139198    0.00767734  -0.108918     0.0505576   -0.0297476   0.0325479   -0.0597755     0.107181     0.0136773   0.108182      0.181546     0.0756777     0.135994     0.130889      0.0386566   -0.0258905    0.163688  
  0.129322    -0.039927    0.0318947     0.0451293   -0.03866      0.117664     0.0347752    0.0730378   0.0602619    0.11688     0.219617     0.0389628    0.152429     0.131071    0.00689065   0.00677871   -0.118717    -0.0440569  -0.0707721    -0.049304    -0.08432       0.014149     0.0453214    -0.128097    -0.1701       0.111573  
  0.01711      0.0875027   0.0275193     0.134816     0.0686447   -0.0145164    0.0150299   -0.064438   -0.0179018   -0.110367   -0.00247733   0.146494     0.0342865    0.200583    0.0627762    0.0435656    -0.199595     0.182317   -0.0820333    -0.0129641   -0.180494      0.0289632   -0.0380791    -0.0428205    0.0355318   -0.0700222 
  0.0788892    0.0700448   0.0965582     0.114556     0.0360217    0.0349335    0.0837732    0.027629    0.145904     0.0705011   0.00237647  -0.132342     0.0123462   -0.042676    0.114427     0.0267927     0.00742371  -0.0763596  -0.0367494    -0.0195524   -0.194152      0.0633651   -0.0462794    -0.0150465   -0.104848     0.0738842 
 -0.0279678   -0.0157709   0.0179925     0.232178     0.00760213  -0.14722     -0.111806    -0.0466667  -0.0321105   -0.0785674   0.00469581   0.0736391    0.0400754   -0.0747354   0.0483887   -0.0343847     0.0375835   -0.0434271   0.0892942    -0.00581415  -0.030075     -0.17065     -0.0429881    -0.194149     0.0268118   -0.014515  
 -0.118264    -0.139503    0.189388     -0.0172728    0.0819805    0.05754      0.181349    -0.130615    0.00925709  -0.108073    0.0853107    0.114057    -0.187149    -0.0796812   0.146524    -0.0762258     0.00433513   0.0113177   0.0219334     0.0758288   -0.00937632   -0.104578    -0.0292435    -0.0195242    0.0644796    0.0563732 
 -0.0527602   -0.0742018   0.0143495     0.0854221   -0.176907     0.00797008   0.120828     0.102679    0.0772894   -0.0792259  -0.0572835   -0.096453    -0.0119961    0.132905    0.111858     0.0252356     0.113359     0.0742451  -0.0198252     0.074362    -0.0647167     0.128208     0.319541      0.0736539    0.00999098   0.0457147 
 -0.0184628   -0.0684182  -0.000232829  -0.142383    -0.0186282    0.00630505  -0.0498263   -0.0663614  -0.12344      0.0679229  -0.173785    -0.118439     0.0980767    0.0651516   0.0290438    0.000394677  -0.048882    -0.0295769  -0.0644413     0.0847419    0.228836      0.0926721    0.124494      0.0622357   -0.236441    -0.0189697 
 -0.0875225    0.0770584   0.00957191    0.108948     0.0552166    0.0554682   -0.0503295    0.085379   -0.0248033    0.0812496  -0.0774897   -0.00800976  -0.0540003   -0.0143226   0.138938     0.0303505     0.0439848    0.11484     0.0820792     0.0574279   -0.115505      0.00840793   0.184045     -0.135269     0.0811382   -0.031072  
 -0.0120388   -0.0448113   0.00185952    0.111256     0.0252664    0.119499    -0.107607    -0.139054    0.0250486   -0.0319749  -0.00840597  -0.176944     0.0391326   -0.0650301  -0.038151    -0.1421        0.122815     0.0190057   0.0904886    -0.0532798    0.103312      0.121134     0.157847      0.0909546    0.144218    -0.0686926 
 -0.0395422    0.0603061   0.320008      0.112018    -0.0310402    0.0143212   -0.05482     -0.0584511   0.0221104   -0.128796    0.0720523    0.0184125    0.0951492   -0.0886378  -0.0474616   -0.0788311     0.196875    -0.0369917  -0.000905365   0.0906886   -0.000347125  -0.0606833   -0.048219      0.0193414   -0.0817042    0.108697  
 -0.180988    -0.0837335  -0.0781501    -0.0419762    0.00334739   0.043897     0.199207     0.0784834  -0.0478454    0.058889    0.113114     0.0028152    0.0589874   -0.0192897   0.204569    -0.00449333    0.0484284   -0.0834178  -0.0885482     0.0894597   -0.103003     -0.0609836   -0.000386198  -0.0532077    0.0642114   -0.14765   
 -0.0601784   -0.04231     0.068357      0.111063    -0.00449293  -0.0104607   -7.26056e-5  -0.0737793  -0.0131734    0.215652    0.123007     0.10657     -0.231703    -0.178049   -0.0539955   -0.0800489     0.200061    -0.141185    0.0350185    -0.043679     0.0858529     0.0167811   -0.013371     -3.24555e-5   0.0462704    0.03807   kind diag, method split
0: avll = -1.4268277404770204
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.426913
INFO: iteration 2, average log likelihood -1.426796
INFO: iteration 3, average log likelihood -1.425261
INFO: iteration 4, average log likelihood -1.410952
INFO: iteration 5, average log likelihood -1.392619
INFO: iteration 6, average log likelihood -1.389540
INFO: iteration 7, average log likelihood -1.388868
INFO: iteration 8, average log likelihood -1.388518
INFO: iteration 9, average log likelihood -1.388272
INFO: iteration 10, average log likelihood -1.388051
INFO: iteration 11, average log likelihood -1.387839
INFO: iteration 12, average log likelihood -1.387665
INFO: iteration 13, average log likelihood -1.387543
INFO: iteration 14, average log likelihood -1.387463
INFO: iteration 15, average log likelihood -1.387411
INFO: iteration 16, average log likelihood -1.387375
INFO: iteration 17, average log likelihood -1.387350
INFO: iteration 18, average log likelihood -1.387331
INFO: iteration 19, average log likelihood -1.387315
INFO: iteration 20, average log likelihood -1.387300
INFO: iteration 21, average log likelihood -1.387287
INFO: iteration 22, average log likelihood -1.387274
INFO: iteration 23, average log likelihood -1.387262
INFO: iteration 24, average log likelihood -1.387250
INFO: iteration 25, average log likelihood -1.387239
INFO: iteration 26, average log likelihood -1.387229
INFO: iteration 27, average log likelihood -1.387219
INFO: iteration 28, average log likelihood -1.387210
INFO: iteration 29, average log likelihood -1.387202
INFO: iteration 30, average log likelihood -1.387196
INFO: iteration 31, average log likelihood -1.387190
INFO: iteration 32, average log likelihood -1.387185
INFO: iteration 33, average log likelihood -1.387181
INFO: iteration 34, average log likelihood -1.387178
INFO: iteration 35, average log likelihood -1.387175
INFO: iteration 36, average log likelihood -1.387173
INFO: iteration 37, average log likelihood -1.387171
INFO: iteration 38, average log likelihood -1.387170
INFO: iteration 39, average log likelihood -1.387169
INFO: iteration 40, average log likelihood -1.387168
INFO: iteration 41, average log likelihood -1.387168
INFO: iteration 42, average log likelihood -1.387167
INFO: iteration 43, average log likelihood -1.387167
INFO: iteration 44, average log likelihood -1.387167
INFO: iteration 45, average log likelihood -1.387166
INFO: iteration 46, average log likelihood -1.387166
INFO: iteration 47, average log likelihood -1.387166
INFO: iteration 48, average log likelihood -1.387166
INFO: iteration 49, average log likelihood -1.387166
INFO: iteration 50, average log likelihood -1.387166
INFO: EM with 100000 data points 50 iterations avll -1.387166
952.4 data points per parameter
1: avll = [-1.42691,-1.4268,-1.42526,-1.41095,-1.39262,-1.38954,-1.38887,-1.38852,-1.38827,-1.38805,-1.38784,-1.38766,-1.38754,-1.38746,-1.38741,-1.38738,-1.38735,-1.38733,-1.38731,-1.3873,-1.38729,-1.38727,-1.38726,-1.38725,-1.38724,-1.38723,-1.38722,-1.38721,-1.3872,-1.3872,-1.38719,-1.38719,-1.38718,-1.38718,-1.38718,-1.38717,-1.38717,-1.38717,-1.38717,-1.38717,-1.38717,-1.38717,-1.38717,-1.38717,-1.38717,-1.38717,-1.38717,-1.38717,-1.38717,-1.38717]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.387369
INFO: iteration 2, average log likelihood -1.387208
INFO: iteration 3, average log likelihood -1.386803
INFO: iteration 4, average log likelihood -1.382651
INFO: iteration 5, average log likelihood -1.366392
INFO: iteration 6, average log likelihood -1.352992
INFO: iteration 7, average log likelihood -1.348564
INFO: iteration 8, average log likelihood -1.346456
INFO: iteration 9, average log likelihood -1.345203
INFO: iteration 10, average log likelihood -1.344359
INFO: iteration 11, average log likelihood -1.343743
INFO: iteration 12, average log likelihood -1.343299
INFO: iteration 13, average log likelihood -1.342982
INFO: iteration 14, average log likelihood -1.342745
INFO: iteration 15, average log likelihood -1.342554
INFO: iteration 16, average log likelihood -1.342390
INFO: iteration 17, average log likelihood -1.342237
INFO: iteration 18, average log likelihood -1.342084
INFO: iteration 19, average log likelihood -1.341916
INFO: iteration 20, average log likelihood -1.341719
INFO: iteration 21, average log likelihood -1.341482
INFO: iteration 22, average log likelihood -1.341187
INFO: iteration 23, average log likelihood -1.340835
INFO: iteration 24, average log likelihood -1.340383
INFO: iteration 25, average log likelihood -1.339844
INFO: iteration 26, average log likelihood -1.339485
INFO: iteration 27, average log likelihood -1.339264
INFO: iteration 28, average log likelihood -1.339108
INFO: iteration 29, average log likelihood -1.338994
INFO: iteration 30, average log likelihood -1.338911
INFO: iteration 31, average log likelihood -1.338847
INFO: iteration 32, average log likelihood -1.338795
INFO: iteration 33, average log likelihood -1.338751
INFO: iteration 34, average log likelihood -1.338711
INFO: iteration 35, average log likelihood -1.338675
INFO: iteration 36, average log likelihood -1.338638
INFO: iteration 37, average log likelihood -1.338598
INFO: iteration 38, average log likelihood -1.338554
INFO: iteration 39, average log likelihood -1.338503
INFO: iteration 40, average log likelihood -1.338444
INFO: iteration 41, average log likelihood -1.338376
INFO: iteration 42, average log likelihood -1.338301
INFO: iteration 43, average log likelihood -1.338227
INFO: iteration 44, average log likelihood -1.338154
INFO: iteration 45, average log likelihood -1.338084
INFO: iteration 46, average log likelihood -1.338017
INFO: iteration 47, average log likelihood -1.337955
INFO: iteration 48, average log likelihood -1.337896
INFO: iteration 49, average log likelihood -1.337842
INFO: iteration 50, average log likelihood -1.337792
INFO: EM with 100000 data points 50 iterations avll -1.337792
473.9 data points per parameter
2: avll = [-1.38737,-1.38721,-1.3868,-1.38265,-1.36639,-1.35299,-1.34856,-1.34646,-1.3452,-1.34436,-1.34374,-1.3433,-1.34298,-1.34275,-1.34255,-1.34239,-1.34224,-1.34208,-1.34192,-1.34172,-1.34148,-1.34119,-1.34084,-1.34038,-1.33984,-1.33948,-1.33926,-1.33911,-1.33899,-1.33891,-1.33885,-1.33879,-1.33875,-1.33871,-1.33867,-1.33864,-1.3386,-1.33855,-1.3385,-1.33844,-1.33838,-1.3383,-1.33823,-1.33815,-1.33808,-1.33802,-1.33796,-1.3379,-1.33784,-1.33779]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.337982
INFO: iteration 2, average log likelihood -1.337729
INFO: iteration 3, average log likelihood -1.336944
INFO: iteration 4, average log likelihood -1.329480
INFO: iteration 5, average log likelihood -1.308723
INFO: iteration 6, average log likelihood -1.294892
INFO: iteration 7, average log likelihood -1.289176
INFO: iteration 8, average log likelihood -1.285487
INFO: iteration 9, average log likelihood -1.282400
INFO: iteration 10, average log likelihood -1.279989
INFO: iteration 11, average log likelihood -1.278230
INFO: iteration 12, average log likelihood -1.277173
INFO: iteration 13, average log likelihood -1.276646
INFO: iteration 14, average log likelihood -1.276338
INFO: iteration 15, average log likelihood -1.276054
INFO: iteration 16, average log likelihood -1.275679
INFO: iteration 17, average log likelihood -1.275134
INFO: iteration 18, average log likelihood -1.274454
INFO: iteration 19, average log likelihood -1.273811
INFO: iteration 20, average log likelihood -1.273333
INFO: iteration 21, average log likelihood -1.272916
INFO: iteration 22, average log likelihood -1.272278
INFO: iteration 23, average log likelihood -1.271154
INFO: iteration 24, average log likelihood -1.269719
INFO: iteration 25, average log likelihood -1.269054
INFO: iteration 26, average log likelihood -1.268829
INFO: iteration 27, average log likelihood -1.268703
INFO: iteration 28, average log likelihood -1.268622
INFO: iteration 29, average log likelihood -1.268568
INFO: iteration 30, average log likelihood -1.268530
INFO: iteration 31, average log likelihood -1.268503
INFO: iteration 32, average log likelihood -1.268483
INFO: iteration 33, average log likelihood -1.268470
INFO: iteration 34, average log likelihood -1.268460
INFO: iteration 35, average log likelihood -1.268452
INFO: iteration 36, average log likelihood -1.268447
INFO: iteration 37, average log likelihood -1.268443
INFO: iteration 38, average log likelihood -1.268441
INFO: iteration 39, average log likelihood -1.268439
INFO: iteration 40, average log likelihood -1.268437
INFO: iteration 41, average log likelihood -1.268436
INFO: iteration 42, average log likelihood -1.268435
INFO: iteration 43, average log likelihood -1.268434
INFO: iteration 44, average log likelihood -1.268433
INFO: iteration 45, average log likelihood -1.268433
INFO: iteration 46, average log likelihood -1.268433
INFO: iteration 47, average log likelihood -1.268432
INFO: iteration 48, average log likelihood -1.268432
INFO: iteration 49, average log likelihood -1.268432
INFO: iteration 50, average log likelihood -1.268432
INFO: EM with 100000 data points 50 iterations avll -1.268432
236.4 data points per parameter
3: avll = [-1.33798,-1.33773,-1.33694,-1.32948,-1.30872,-1.29489,-1.28918,-1.28549,-1.2824,-1.27999,-1.27823,-1.27717,-1.27665,-1.27634,-1.27605,-1.27568,-1.27513,-1.27445,-1.27381,-1.27333,-1.27292,-1.27228,-1.27115,-1.26972,-1.26905,-1.26883,-1.2687,-1.26862,-1.26857,-1.26853,-1.2685,-1.26848,-1.26847,-1.26846,-1.26845,-1.26845,-1.26844,-1.26844,-1.26844,-1.26844,-1.26844,-1.26843,-1.26843,-1.26843,-1.26843,-1.26843,-1.26843,-1.26843,-1.26843,-1.26843]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.268744
INFO: iteration 2, average log likelihood -1.268391
INFO: iteration 3, average log likelihood -1.266480
INFO: iteration 4, average log likelihood -1.249816
WARNING: Variances had to be floored 11
INFO: iteration 5, average log likelihood -1.216589
INFO: iteration 6, average log likelihood -1.207095
WARNING: Variances had to be floored 5 6 11
INFO: iteration 7, average log likelihood -1.182033
INFO: iteration 8, average log likelihood -1.206775
WARNING: Variances had to be floored 11
INFO: iteration 9, average log likelihood -1.184954
WARNING: Variances had to be floored 6
INFO: iteration 10, average log likelihood -1.186182
WARNING: Variances had to be floored 1 5 11
INFO: iteration 11, average log likelihood -1.180819
INFO: iteration 12, average log likelihood -1.199756
WARNING: Variances had to be floored 6 11
INFO: iteration 13, average log likelihood -1.172646
INFO: iteration 14, average log likelihood -1.190412
WARNING: Variances had to be floored 5 11
INFO: iteration 15, average log likelihood -1.167265
WARNING: Variances had to be floored 6
INFO: iteration 16, average log likelihood -1.180939
WARNING: Variances had to be floored 11
INFO: iteration 17, average log likelihood -1.175852
WARNING: Variances had to be floored 1
INFO: iteration 18, average log likelihood -1.177948
WARNING: Variances had to be floored 5 6 11
INFO: iteration 19, average log likelihood -1.172470
INFO: iteration 20, average log likelihood -1.195741
WARNING: Variances had to be floored 11
INFO: iteration 21, average log likelihood -1.173946
WARNING: Variances had to be floored 6
INFO: iteration 22, average log likelihood -1.176367
WARNING: Variances had to be floored 5 11
INFO: iteration 23, average log likelihood -1.172305
INFO: iteration 24, average log likelihood -1.182651
WARNING: Variances had to be floored 6 11
INFO: iteration 25, average log likelihood -1.163116
INFO: iteration 26, average log likelihood -1.184287
WARNING: Variances had to be floored 5 11
INFO: iteration 27, average log likelihood -1.163659
WARNING: Variances had to be floored 6
INFO: iteration 28, average log likelihood -1.178091
WARNING: Variances had to be floored 11 16
INFO: iteration 29, average log likelihood -1.172853
WARNING: Variances had to be floored 1
INFO: iteration 30, average log likelihood -1.187154
WARNING: Variances had to be floored 5 6 11 13
INFO: iteration 31, average log likelihood -1.176714
INFO: iteration 32, average log likelihood -1.208593
WARNING: Variances had to be floored 11
INFO: iteration 33, average log likelihood -1.181736
INFO: iteration 34, average log likelihood -1.181113
WARNING: Variances had to be floored 5 6 11
INFO: iteration 35, average log likelihood -1.163081
INFO: iteration 36, average log likelihood -1.192284
WARNING: Variances had to be floored 11
INFO: iteration 37, average log likelihood -1.172468
WARNING: Variances had to be floored 6
INFO: iteration 38, average log likelihood -1.175507
WARNING: Variances had to be floored 5 11
INFO: iteration 39, average log likelihood -1.171630
INFO: iteration 40, average log likelihood -1.180870
WARNING: Variances had to be floored 6 11 13
INFO: iteration 41, average log likelihood -1.161264
INFO: iteration 42, average log likelihood -1.193638
WARNING: Variances had to be floored 5 11
INFO: iteration 43, average log likelihood -1.168809
WARNING: Variances had to be floored 1 6
INFO: iteration 44, average log likelihood -1.180490
WARNING: Variances had to be floored 11
INFO: iteration 45, average log likelihood -1.185618
INFO: iteration 46, average log likelihood -1.180917
WARNING: Variances had to be floored 5 6 11
INFO: iteration 47, average log likelihood -1.162448
INFO: iteration 48, average log likelihood -1.189158
WARNING: Variances had to be floored 11
INFO: iteration 49, average log likelihood -1.169612
INFO: iteration 50, average log likelihood -1.172811
INFO: EM with 100000 data points 50 iterations avll -1.172811
118.1 data points per parameter
4: avll = [-1.26874,-1.26839,-1.26648,-1.24982,-1.21659,-1.2071,-1.18203,-1.20678,-1.18495,-1.18618,-1.18082,-1.19976,-1.17265,-1.19041,-1.16726,-1.18094,-1.17585,-1.17795,-1.17247,-1.19574,-1.17395,-1.17637,-1.1723,-1.18265,-1.16312,-1.18429,-1.16366,-1.17809,-1.17285,-1.18715,-1.17671,-1.20859,-1.18174,-1.18111,-1.16308,-1.19228,-1.17247,-1.17551,-1.17163,-1.18087,-1.16126,-1.19364,-1.16881,-1.18049,-1.18562,-1.18092,-1.16245,-1.18916,-1.16961,-1.17281]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 9 10 11 12 21 22 31 32
INFO: iteration 1, average log likelihood -1.157791
WARNING: Variances had to be floored 1 9 10 11 12 21 22 31 32
INFO: iteration 2, average log likelihood -1.157367
WARNING: Variances had to be floored 1 9 10 11 12 21 22 31 32
INFO: iteration 3, average log likelihood -1.155996
WARNING: Variances had to be floored 1 9 10 11 12 21 22 25 31 32
INFO: iteration 4, average log likelihood -1.142339
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 31 32
INFO: iteration 5, average log likelihood -1.113682
WARNING: Variances had to be floored 1 2 9 10 11 12 21 22 25 31 32
INFO: iteration 6, average log likelihood -1.112434
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 31 32
INFO: iteration 7, average log likelihood -1.103783
WARNING: Variances had to be floored 1 2 9 10 11 12 21 22 25 31 32
INFO: iteration 8, average log likelihood -1.103085
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 30 31 32
INFO: iteration 9, average log likelihood -1.097317
WARNING: Variances had to be floored 1 2 9 10 11 12 21 22 25 31 32
INFO: iteration 10, average log likelihood -1.108358
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 31 32
INFO: iteration 11, average log likelihood -1.100023
WARNING: Variances had to be floored 1 2 9 10 11 12 21 22 25 31 32
INFO: iteration 12, average log likelihood -1.101393
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 30 31 32
INFO: iteration 13, average log likelihood -1.096185
WARNING: Variances had to be floored 1 2 9 10 11 12 21 22 25 31 32
INFO: iteration 14, average log likelihood -1.106982
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 31 32
INFO: iteration 15, average log likelihood -1.098155
WARNING: Variances had to be floored 1 2 9 10 11 12 21 22 25 31 32
INFO: iteration 16, average log likelihood -1.099379
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 30 31 32
INFO: iteration 17, average log likelihood -1.094420
WARNING: Variances had to be floored 1 2 9 10 11 12 21 22 25 31 32
INFO: iteration 18, average log likelihood -1.105842
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 31 32
INFO: iteration 19, average log likelihood -1.097669
WARNING: Variances had to be floored 1 2 9 10 11 12 21 22 25 31 32
INFO: iteration 20, average log likelihood -1.099281
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 30 31 32
INFO: iteration 21, average log likelihood -1.094349
WARNING: Variances had to be floored 1 2 9 10 11 12 21 22 25 31 32
INFO: iteration 22, average log likelihood -1.105823
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 31 32
INFO: iteration 23, average log likelihood -1.097629
WARNING: Variances had to be floored 1 2 9 10 11 12 21 22 25 31 32
INFO: iteration 24, average log likelihood -1.099243
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 30 31 32
INFO: iteration 25, average log likelihood -1.094302
WARNING: Variances had to be floored 1 2 9 10 11 12 21 22 25 31 32
INFO: iteration 26, average log likelihood -1.105790
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 31 32
INFO: iteration 27, average log likelihood -1.097581
WARNING: Variances had to be floored 1 2 9 10 11 12 21 22 25 31 32
INFO: iteration 28, average log likelihood -1.099174
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 30 31 32
INFO: iteration 29, average log likelihood -1.094217
WARNING: Variances had to be floored 1 2 9 10 11 12 21 22 25 31 32
INFO: iteration 30, average log likelihood -1.105698
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 31 32
INFO: iteration 31, average log likelihood -1.097445
WARNING: Variances had to be floored 1 2 9 10 11 12 21 22 25 31 32
INFO: iteration 32, average log likelihood -1.098968
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 30 31 32
INFO: iteration 33, average log likelihood -1.093875
WARNING: Variances had to be floored 1 2 9 10 11 12 21 22 25 31 32
INFO: iteration 34, average log likelihood -1.105126
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 31 32
INFO: iteration 35, average log likelihood -1.096354
WARNING: Variances had to be floored 1 2 9 10 11 12 21 22 25 31 32
INFO: iteration 36, average log likelihood -1.096947
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 30 31 32
INFO: iteration 37, average log likelihood -1.091234
WARNING: Variances had to be floored 1 2 9 10 11 12 21 22 25 31 32
INFO: iteration 38, average log likelihood -1.102429
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 31 32
INFO: iteration 39, average log likelihood -1.094206
WARNING: Variances had to be floored 1 2 9 10 11 12 21 22 25 31 32
INFO: iteration 40, average log likelihood -1.095701
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 30 31 32
INFO: iteration 41, average log likelihood -1.090880
WARNING: Variances had to be floored 1 2 9 10 11 12 21 22 25 31 32
INFO: iteration 42, average log likelihood -1.102346
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 31 32
INFO: iteration 43, average log likelihood -1.094178
WARNING: Variances had to be floored 1 2 9 10 11 12 21 22 25 31 32
INFO: iteration 44, average log likelihood -1.095667
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 30 31 32
INFO: iteration 45, average log likelihood -1.090853
WARNING: Variances had to be floored 1 2 9 10 11 12 21 22 25 31 32
INFO: iteration 46, average log likelihood -1.102342
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 31 32
INFO: iteration 47, average log likelihood -1.094161
WARNING: Variances had to be floored 1 2 9 10 11 12 21 22 25 31 32
INFO: iteration 48, average log likelihood -1.095639
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 30 31 32
INFO: iteration 49, average log likelihood -1.090828
WARNING: Variances had to be floored 1 2 9 10 11 12 21 22 25 31 32
INFO: iteration 50, average log likelihood -1.102339
INFO: EM with 100000 data points 50 iterations avll -1.102339
59.0 data points per parameter
5: avll = [-1.15779,-1.15737,-1.156,-1.14234,-1.11368,-1.11243,-1.10378,-1.10309,-1.09732,-1.10836,-1.10002,-1.10139,-1.09619,-1.10698,-1.09815,-1.09938,-1.09442,-1.10584,-1.09767,-1.09928,-1.09435,-1.10582,-1.09763,-1.09924,-1.0943,-1.10579,-1.09758,-1.09917,-1.09422,-1.1057,-1.09744,-1.09897,-1.09387,-1.10513,-1.09635,-1.09695,-1.09123,-1.10243,-1.09421,-1.0957,-1.09088,-1.10235,-1.09418,-1.09567,-1.09085,-1.10234,-1.09416,-1.09564,-1.09083,-1.10234]
[-1.42683,-1.42691,-1.4268,-1.42526,-1.41095,-1.39262,-1.38954,-1.38887,-1.38852,-1.38827,-1.38805,-1.38784,-1.38766,-1.38754,-1.38746,-1.38741,-1.38738,-1.38735,-1.38733,-1.38731,-1.3873,-1.38729,-1.38727,-1.38726,-1.38725,-1.38724,-1.38723,-1.38722,-1.38721,-1.3872,-1.3872,-1.38719,-1.38719,-1.38718,-1.38718,-1.38718,-1.38717,-1.38717,-1.38717,-1.38717,-1.38717,-1.38717,-1.38717,-1.38717,-1.38717,-1.38717,-1.38717,-1.38717,-1.38717,-1.38717,-1.38717,-1.38737,-1.38721,-1.3868,-1.38265,-1.36639,-1.35299,-1.34856,-1.34646,-1.3452,-1.34436,-1.34374,-1.3433,-1.34298,-1.34275,-1.34255,-1.34239,-1.34224,-1.34208,-1.34192,-1.34172,-1.34148,-1.34119,-1.34084,-1.34038,-1.33984,-1.33948,-1.33926,-1.33911,-1.33899,-1.33891,-1.33885,-1.33879,-1.33875,-1.33871,-1.33867,-1.33864,-1.3386,-1.33855,-1.3385,-1.33844,-1.33838,-1.3383,-1.33823,-1.33815,-1.33808,-1.33802,-1.33796,-1.3379,-1.33784,-1.33779,-1.33798,-1.33773,-1.33694,-1.32948,-1.30872,-1.29489,-1.28918,-1.28549,-1.2824,-1.27999,-1.27823,-1.27717,-1.27665,-1.27634,-1.27605,-1.27568,-1.27513,-1.27445,-1.27381,-1.27333,-1.27292,-1.27228,-1.27115,-1.26972,-1.26905,-1.26883,-1.2687,-1.26862,-1.26857,-1.26853,-1.2685,-1.26848,-1.26847,-1.26846,-1.26845,-1.26845,-1.26844,-1.26844,-1.26844,-1.26844,-1.26844,-1.26843,-1.26843,-1.26843,-1.26843,-1.26843,-1.26843,-1.26843,-1.26843,-1.26843,-1.26874,-1.26839,-1.26648,-1.24982,-1.21659,-1.2071,-1.18203,-1.20678,-1.18495,-1.18618,-1.18082,-1.19976,-1.17265,-1.19041,-1.16726,-1.18094,-1.17585,-1.17795,-1.17247,-1.19574,-1.17395,-1.17637,-1.1723,-1.18265,-1.16312,-1.18429,-1.16366,-1.17809,-1.17285,-1.18715,-1.17671,-1.20859,-1.18174,-1.18111,-1.16308,-1.19228,-1.17247,-1.17551,-1.17163,-1.18087,-1.16126,-1.19364,-1.16881,-1.18049,-1.18562,-1.18092,-1.16245,-1.18916,-1.16961,-1.17281,-1.15779,-1.15737,-1.156,-1.14234,-1.11368,-1.11243,-1.10378,-1.10309,-1.09732,-1.10836,-1.10002,-1.10139,-1.09619,-1.10698,-1.09815,-1.09938,-1.09442,-1.10584,-1.09767,-1.09928,-1.09435,-1.10582,-1.09763,-1.09924,-1.0943,-1.10579,-1.09758,-1.09917,-1.09422,-1.1057,-1.09744,-1.09897,-1.09387,-1.10513,-1.09635,-1.09695,-1.09123,-1.10243,-1.09421,-1.0957,-1.09088,-1.10235,-1.09418,-1.09567,-1.09085,-1.10234,-1.09416,-1.09564,-1.09083,-1.10234]
32Ã—26 Array{Float64,2}:
  0.0932927   -0.646316    0.141621      0.00416358  -0.118045    -0.213771     0.209566    -0.0783672   -0.0856952    0.109351     0.0531777    -0.000881837  -0.175288    -0.050913      0.166979    -0.0976359    0.0789974    0.151637     0.267383     -0.0277255   -0.136302    -0.113635      0.0196946   -0.137403      0.0945541   -0.0415732 
 -0.429393     0.431723   -0.108781     -0.194113    -0.119417    -0.0962337    0.184694    -0.140287    -0.105937     0.09072     -0.136953      0.0525625    -0.0250974   -0.0413103    -0.412602     0.0346946   -0.111526     0.00903623   0.213354     -0.0102309    0.0337397   -0.146961      0.0135596   -0.232148      0.0779689    0.0350151 
  0.258851    -0.136851    0.0652604     0.191533     0.0261018    0.0522203   -0.0125022   -0.122618     0.0332448    0.0978426    0.123615     -0.0165053    -0.128378     0.134291      0.0624814   -0.0422505    0.0132034    0.0257575    0.103804      0.147859     0.153347     0.0401923    -0.0227142    0.0653503     0.116828     0.103893  
 -0.0907648    0.062112   -0.0248316     0.0282267   -0.0170224    0.0610612   -0.0313059    0.287467    -0.107864    -0.0255342    0.111217     -0.0164441     0.105668     0.0835875     0.183657     0.069595     0.0281416    0.117656     0.157311      0.105435    -0.0451025   -0.133142      0.0774382   -0.0171633     0.0801616   -0.0535693 
 -0.0574888   -0.075073   -0.0302446     0.0257678    0.0301154    0.0272315   -0.0502613   -0.00500739   0.102194    -0.039165    -0.0221111     0.00102056   -0.0521485   -0.015676     -0.023207    -0.0923253    0.0585495    0.015371    -0.0139258    -0.0247452   -0.105077     0.0531776     0.0897673   -0.0203248    -0.0282344   -0.00570203
 -0.0757356   -0.0287499   0.0189333     0.0584709   -0.0367323    0.0523721    0.0880555    0.0275732    0.0557996    0.0447446   -0.0129557    -0.0546498     0.00421061  -0.0251886     0.108349     0.0423343    0.0397294    0.0106712    0.000408601   0.0293337   -0.127222    -0.00775529    0.0459071   -0.0542207    -0.0406868    0.00125408
  0.0580458   -0.109907    0.0755376     0.0917476    0.00571895   0.159877     0.111337     0.07372     -0.0125429   -0.140506    -0.0179785    -0.100226     -0.387156    -0.0416055     0.0826958   -0.019877     0.210289    -0.0250492    0.16793       0.151845     0.101969     0.17117       0.124277     0.0751664    -0.0144565    0.323271  
 -0.0142866   -0.144724   -0.00176804    0.040235     0.099867     0.144481    -0.0859003    0.0573917    0.075393    -0.136252     0.0303547    -0.128227      0.370493    -0.00511655   -0.00784599  -0.0816952   -0.00819287   0.0755288    0.0804056     0.176511     0.0563387    0.146037      0.136648     0.0234745    -0.0373013   -0.00369425
 -0.138809    -0.0236333   0.0817297    -0.0621827   -0.0430915   -0.00229952  -0.0260183    0.264365     0.120007    -0.0157998    0.0441721    -0.0563662    -0.0466795    0.206052      0.0663936    0.0170423    0.106961    -0.223321     0.242241     -0.0690605    0.26579     -0.479115      0.0440806    0.0331524    -0.0361983   -0.00683149
 -0.106659     0.0506848   0.159271     -0.083251    -0.00455132  -0.0390576   -0.200846    -0.0053657    0.119969    -0.0788762   -0.0478401    -0.0710649     0.00483029   0.125968      0.081687    -0.11753      0.0471457   -0.225835    -0.00850113    0.0709698    0.219387     0.338425     -0.0163436    0.0356206     0.091506    -0.0323706 
 -0.01915     -0.213894    0.460698     -0.0533077   -0.130486     0.0731985    0.0595074    0.106242     0.0320482    0.0161246    0.11192       0.0209619     0.0784301    0.0434649     0.0497451    0.0778869   -0.186486     0.0513714   -0.0865653     0.00789633  -0.0921414   -0.232743     -0.00306082   0.0859888     0.0162256   -0.0143715 
 -0.00876126   0.302992    0.0518346    -0.0538821   -0.104635     0.0782601    0.0652093   -0.0813678   -0.0659719    0.0191483    0.123604     -0.0366207     0.0109554    0.0536893     0.0369637    0.0844396    0.0703865    0.0589365   -0.0498695    -0.00476326  -0.0903739    0.244726      0.0499592    0.143545      0.016036    -0.165708  
 -0.0575618   -0.043558    0.0719152     0.109118    -0.00676872   0.00128111  -0.00662029  -0.0816421   -0.0132899    0.225126     0.11806       0.109568     -0.239199    -0.180379     -0.00893471  -0.0778186    0.20111     -0.136066     0.0354716    -0.0470441    0.071457     0.0168238    -0.0228093   -0.000745754   0.0522581    0.0518489 
 -0.0538192   -0.0462461   0.114055      0.0560517    0.0702049    0.0180583    0.0904858   -0.095308    -0.00578954  -0.102771     0.0425054     0.140666     -0.0737794    0.0461651     0.0763187   -0.0186834   -0.091807     0.0927129   -0.0295181     0.0295711   -0.096971    -0.0366654    -0.0341842   -0.0177323     0.0143152   -0.00937561
 -0.191828     0.0294153   0.226158      0.0362775   -0.0691434    0.153437     0.0279665   -0.22475     -0.158528     0.0492488    0.14663      -0.0418267     0.0359785    0.124445      0.0565325    0.131051    -0.0199538    0.0734072    0.0217165    -0.119669    -0.135816    -0.0530559     0.0108378    0.0738646     0.0612326    0.0441391 
  0.126855    -0.0371831   0.0162655     0.0355915   -0.0329412    0.112745     0.0385396    0.0846576    0.0556432    0.118462     0.218586      0.0788175     0.162475     0.137946      0.0034072    0.00193028  -0.116026    -0.0667335   -0.0801435    -0.0506061   -0.0855854    0.00811896    0.0542851   -0.130533     -0.147587     0.108691  
  0.251428     0.0980871   0.000901568  -0.0142155    0.0688373    0.123972    -0.0543496   -0.0536702    0.00314685   0.0530274   -0.0985291    -0.00157184   -0.179899    -0.0622178    -0.0809614    0.21043     -0.0394765    0.170961    -0.000936628   0.0863689   -0.0646647    0.10592       0.170225    -3.13784      -0.111135    -0.187313  
  0.19497      0.0842527   0.0262845     0.0961471    0.0628481    0.0914762   -0.165471    -0.0482131    0.0969169   -0.142242    -0.0967249    -0.11826      -0.148494    -0.0812021    -0.0906828    0.234545    -0.268229     0.185328     0.0416879     0.210378    -0.0801891    0.0729986     0.138446     0.14941      -0.0175812    0.0282946 
  0.0167053    0.0692863  -0.00957435   -0.0937184    0.0561258    0.076116    -0.0785496   -0.0509124    0.0314152    0.205237    -0.0967219    -0.288843     -0.186037    -0.086441     -0.105316     0.208533    -0.881181     0.0751744   -0.0156787     0.026709    -0.0640814    0.000972447   0.22915      1.03043      -0.0886875   -0.407683  
  0.125277     0.0899979  -0.0668161     0.0385309    0.0489877    0.0937128   -0.022405    -0.0507459    0.0944346   -0.00927739  -0.109953      0.338733     -0.20724     -0.0899216    -0.0922425    0.280761     1.32946      0.12911      0.00361204    0.0346392   -0.0913124    0.0519039     0.0318696    1.0367       -0.0952791   -0.0848346 
 -0.0770475    0.134662    0.0170457     0.0276396   -0.0229975   -0.0917633   -0.293876     0.0580365    0.033895    -0.0213597    0.0668222    -0.0609477     0.0960785   -0.0865762     0.137192    -0.33074     -0.158137    -0.154966    -0.112209     -0.546786     0.0613724   -0.0511999    -0.104689     0.428537      0.074234     0.188691  
 -0.0766173    0.103218    0.0131878     0.0277569   -0.0153674   -0.18311      0.115689     0.0589301    0.345547    -0.0649332    0.0613171    -0.143428      0.0986946    0.0462538     0.0336177   -0.194846    -0.0513126   -0.00576055   0.0195687     0.148573    -0.0567894   -0.0502527    -0.104882    -0.275369      0.0743013   -0.0893796 
  0.0205815    0.0715957  -0.0618712    -0.0076388   -0.108129     0.0732118   -0.142961    -0.263913    -0.139475    -0.11198      0.0720465    -0.0504544    -0.118579     0.116833     -0.0313107   -0.199517    -0.809248     0.0125636    0.144187      0.0236383   -0.047395     0.237521      0.0228049    0.122858      0.0483753    0.0295114 
  0.30313      0.0687981   0.168825     -0.0352377   -0.05923     -0.0946809   -0.0971702   -0.265259    -0.1294      -0.0888735    0.0756906    -0.197788     -0.110148     0.122883     -0.032782    -0.17687      0.628245     0.00623998  -0.0448612     0.0746875   -0.0464642    0.22206       0.0165249    0.100892      0.0336199    0.0131857 
 -0.0351062   -0.0139674   0.206493      0.0891029   -0.0608267   -0.00937994   0.0189771   -0.0593327    0.00984863  -0.0719337    0.00225283    0.027227      0.00298013  -0.0770625    -0.0196846   -0.0551498    0.0942493   -0.0520894   -0.00422076    0.113183    -0.0179314   -0.0926358    -0.0896031   -0.0388229    -0.0185315    0.0169697 
 -0.02309     -0.0698805   0.0401153    -0.146322    -0.0360758    0.0145428   -0.0493359   -0.0533686   -0.141804     0.0782905   -0.174152     -0.116048      0.103946     0.0756381     0.0338245    0.00480044  -0.0495069   -0.0268647   -0.0717231     0.0893527    0.217202     0.0809438     0.127727     0.0636589    -0.225801    -0.0196162 
 -0.147453     0.0524199  -0.0280504    -0.019826    -0.0273743   -0.0306503   -0.0115108   -0.062048    -0.119146    -0.106064    -0.227261      0.0974284     0.196349     0.0344897    -0.148022    -0.122287    -0.15532     -0.11123      0.0530672     0.0436756   -0.0143066    0.0374967     0.101067     0.151327      0.169591     0.00976387
 -0.0214293   -0.0636985  -0.0594886    -0.0203924    0.159981    -0.0288765    0.0287726    0.087756     0.105511    -0.0324915    0.0208489     0.0239913     0.0364744   -0.0472707     0.0682621    0.0346423   -0.0365092    0.102781    -0.072436     -0.0386025   -0.00264036   0.0175357     0.0918984   -0.05451       0.0147189    0.0454046 
 -0.154965    -0.0914336  -0.0479887     0.112014    -0.0594469    0.0442991   -0.026045    -0.0622504   -0.0527826    0.139761    -0.0399789     0.00951322    0.027596    -0.000561822   0.0934615    0.0404496    0.0637293   -0.00783205   0.133019     -0.0876768    0.00922355   0.0202872     0.11173     -0.0200391     0.0474306   -0.121985  
  0.00540956  -0.0898842   0.177404      0.0702027    0.0611862    0.104026     0.111998     0.0221882   -0.138619    -0.0166893    0.000139143  -0.0334571    -0.163259    -0.114927      0.167779    -0.00496007   0.0820833    0.135459     0.0428364     0.0852787   -0.0528875    0.0924212    -0.0300862    0.136142      0.00563024   0.167795  
 -0.104045    -0.0656809  -0.0866661    -0.119623     0.120373     0.231703     0.0539635   -0.0892215   -0.0627089   -0.0031769    0.268398      0.0459273    -0.0175684   -0.0580038     0.113445    -0.0212541   -0.632511     0.210133     0.0601518     0.0885421   -0.288693    -0.00802799    0.111997     0.0113383    -0.00455926  -0.0470707 
  0.162726    -0.0324782   0.0180837    -0.0896455   -0.17016      0.220889     0.0293127   -0.0270902   -0.0796458   -0.00196878  -0.187977      0.04224      -0.0295739   -0.0490497     0.128093    -0.0251371    0.308512     0.201961     0.145909      0.0869133   -0.0569223    0.0394579     0.142444    -0.0749899     0.00265432  -0.0501694 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 31 32
INFO: iteration 1, average log likelihood -1.094148
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 25 31 32
INFO: iteration 2, average log likelihood -1.089460
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 30 31 32
INFO: iteration 3, average log likelihood -1.090775
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 25 31 32
INFO: iteration 4, average log likelihood -1.091402
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 31 32
INFO: iteration 5, average log likelihood -1.091931
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 25 30 31 32
INFO: iteration 6, average log likelihood -1.087895
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 31 32
INFO: iteration 7, average log likelihood -1.094107
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 25 31 32
INFO: iteration 8, average log likelihood -1.089220
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 30 31 32
INFO: iteration 9, average log likelihood -1.090574
WARNING: Variances had to be floored 1 2 9 10 11 12 15 16 21 22 25 31 32
INFO: iteration 10, average log likelihood -1.091399
INFO: EM with 100000 data points 10 iterations avll -1.091399
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.036652e+05
      1       7.073173e+05      -1.963479e+05 |       32
      2       6.783454e+05      -2.897185e+04 |       32
      3       6.613384e+05      -1.700704e+04 |       32
      4       6.518067e+05      -9.531678e+03 |       32
      5       6.457877e+05      -6.018972e+03 |       32
      6       6.410581e+05      -4.729645e+03 |       32
      7       6.376125e+05      -3.445635e+03 |       32
      8       6.357169e+05      -1.895586e+03 |       32
      9       6.345558e+05      -1.161027e+03 |       32
     10       6.337245e+05      -8.313626e+02 |       32
     11       6.329605e+05      -7.640138e+02 |       32
     12       6.320761e+05      -8.844063e+02 |       32
     13       6.311378e+05      -9.382604e+02 |       32
     14       6.305414e+05      -5.963516e+02 |       32
     15       6.302350e+05      -3.063991e+02 |       32
     16       6.300418e+05      -1.931964e+02 |       32
     17       6.299059e+05      -1.359899e+02 |       32
     18       6.298060e+05      -9.990523e+01 |       32
     19       6.296999e+05      -1.060725e+02 |       32
     20       6.295853e+05      -1.146259e+02 |       32
     21       6.294483e+05      -1.369469e+02 |       32
     22       6.293386e+05      -1.097572e+02 |       32
     23       6.292387e+05      -9.990045e+01 |       32
     24       6.291459e+05      -9.272781e+01 |       32
     25       6.290622e+05      -8.371151e+01 |       32
     26       6.289855e+05      -7.669260e+01 |       31
     27       6.289370e+05      -4.855125e+01 |       31
     28       6.289050e+05      -3.197553e+01 |       30
     29       6.288820e+05      -2.303598e+01 |       31
     30       6.288616e+05      -2.034403e+01 |       31
     31       6.288420e+05      -1.961050e+01 |       29
     32       6.288216e+05      -2.035649e+01 |       31
     33       6.288031e+05      -1.853204e+01 |       31
     34       6.287885e+05      -1.466030e+01 |       29
     35       6.287784e+05      -1.002713e+01 |       28
     36       6.287694e+05      -9.013708e+00 |       28
     37       6.287616e+05      -7.782452e+00 |       31
     38       6.287552e+05      -6.421308e+00 |       29
     39       6.287493e+05      -5.907850e+00 |       26
     40       6.287436e+05      -5.664503e+00 |       27
     41       6.287389e+05      -4.748277e+00 |       26
     42       6.287355e+05      -3.376952e+00 |       24
     43       6.287314e+05      -4.127229e+00 |       27
     44       6.287272e+05      -4.214558e+00 |       24
     45       6.287240e+05      -3.177519e+00 |       22
     46       6.287207e+05      -3.256830e+00 |       26
     47       6.287177e+05      -3.022410e+00 |       24
     48       6.287157e+05      -1.994073e+00 |       21
     49       6.287137e+05      -1.985692e+00 |       23
     50       6.287110e+05      -2.724872e+00 |       22
K-means terminated without convergence after 50 iterations (objv = 628711.0074258221)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.333059
INFO: iteration 2, average log likelihood -1.297790
INFO: iteration 3, average log likelihood -1.267145
INFO: iteration 4, average log likelihood -1.234675
WARNING: Variances had to be floored 26
INFO: iteration 5, average log likelihood -1.189363
WARNING: Variances had to be floored 24 31
INFO: iteration 6, average log likelihood -1.145198
WARNING: Variances had to be floored 4 11 30
INFO: iteration 7, average log likelihood -1.106066
WARNING: Variances had to be floored 1 3 8 9
INFO: iteration 8, average log likelihood -1.091524
WARNING: Variances had to be floored 2 10 12 24 26
INFO: iteration 9, average log likelihood -1.108557
WARNING: Variances had to be floored 6 13 18 31 32
INFO: iteration 10, average log likelihood -1.118711
WARNING: Variances had to be floored 4 11 21 30
INFO: iteration 11, average log likelihood -1.095690
WARNING: Variances had to be floored 1 3 24
INFO: iteration 12, average log likelihood -1.102402
WARNING: Variances had to be floored 2 8 9 26
INFO: iteration 13, average log likelihood -1.111954
WARNING: Variances had to be floored 10 13 17 31
INFO: iteration 14, average log likelihood -1.095011
WARNING: Variances had to be floored 3 4 6 11 18 24 30
INFO: iteration 15, average log likelihood -1.073154
WARNING: Variances had to be floored 12 21
INFO: iteration 16, average log likelihood -1.144489
WARNING: Variances had to be floored 2 8 26
INFO: iteration 17, average log likelihood -1.093084
WARNING: Variances had to be floored 3 9 13 20 31 32
INFO: iteration 18, average log likelihood -1.064731
WARNING: Variances had to be floored 6 11 18 24 30
INFO: iteration 19, average log likelihood -1.080247
WARNING: Variances had to be floored 1 4 10 17 21
INFO: iteration 20, average log likelihood -1.116216
WARNING: Variances had to be floored 2 8 26
INFO: iteration 21, average log likelihood -1.124022
WARNING: Variances had to be floored 9 13 24 32
INFO: iteration 22, average log likelihood -1.091238
WARNING: Variances had to be floored 4 6 11 30 31
INFO: iteration 23, average log likelihood -1.082600
WARNING: Variances had to be floored 1 2 3 18
INFO: iteration 24, average log likelihood -1.106346
WARNING: Variances had to be floored 8 17 21
INFO: iteration 25, average log likelihood -1.096919
WARNING: Variances had to be floored 9 12 20 24 26 32
INFO: iteration 26, average log likelihood -1.058436
WARNING: Variances had to be floored 4 6 11 13 30
INFO: iteration 27, average log likelihood -1.099097
WARNING: Variances had to be floored 1 31
INFO: iteration 28, average log likelihood -1.117485
WARNING: Variances had to be floored 2 3 10 18 21 26
INFO: iteration 29, average log likelihood -1.066662
WARNING: Variances had to be floored 4 8 9 11 12 17 24 30 32
INFO: iteration 30, average log likelihood -1.063034
WARNING: Variances had to be floored 1 20
INFO: iteration 31, average log likelihood -1.145343
WARNING: Variances had to be floored 6 31
INFO: iteration 32, average log likelihood -1.114737
WARNING: Variances had to be floored 2 4 24 26
INFO: iteration 33, average log likelihood -1.077977
WARNING: Variances had to be floored 1 3 8 9 11 12 13 18 30
INFO: iteration 34, average log likelihood -1.064201
WARNING: Variances had to be floored 17 21 31
INFO: iteration 35, average log likelihood -1.130385
WARNING: Variances had to be floored 6 20 24
INFO: iteration 36, average log likelihood -1.100854
WARNING: Variances had to be floored 2 4 26 32
INFO: iteration 37, average log likelihood -1.088089
WARNING: Variances had to be floored 1 8 9 10 11 30
INFO: iteration 38, average log likelihood -1.064356
WARNING: Variances had to be floored 12 18 24 31
INFO: iteration 39, average log likelihood -1.084246
WARNING: Variances had to be floored 2 3 4 6 17 21
INFO: iteration 40, average log likelihood -1.105734
WARNING: Variances had to be floored 13 26
INFO: iteration 41, average log likelihood -1.122079
WARNING: Variances had to be floored 1 8 10 11 24 30
INFO: iteration 42, average log likelihood -1.073187
WARNING: Variances had to be floored 9
INFO: iteration 43, average log likelihood -1.119827
WARNING: Variances had to be floored 2 12 18 26 31 32
INFO: iteration 44, average log likelihood -1.069245
WARNING: Variances had to be floored 4 24
INFO: iteration 45, average log likelihood -1.081260
WARNING: Variances had to be floored 1 6 8 10 11 13 17 30
INFO: iteration 46, average log likelihood -1.040453
WARNING: Variances had to be floored 3 21
INFO: iteration 47, average log likelihood -1.111994
WARNING: Variances had to be floored 2 9 12 24 26 31 32
INFO: iteration 48, average log likelihood -1.063304
WARNING: Variances had to be floored 4 18
INFO: iteration 49, average log likelihood -1.116970
WARNING: Variances had to be floored 1 8 11
INFO: iteration 50, average log likelihood -1.064333
INFO: EM with 100000 data points 50 iterations avll -1.064333
59.0 data points per parameter
32Ã—26 Array{Float64,2}:
 -0.0476385    -0.0833794   0.249573    -0.0458959   -0.098643     0.120785      0.189741    -0.0797401    0.0279      -0.0172517  -0.0546232   -0.00364591  -0.108843    -0.070464    -0.0546625  -0.0437023   0.0314302   -0.0789971   -0.0979183    0.235015     -0.0183189   -0.0626967   -0.150844     0.0604013    -0.0143275   -0.0423728 
 -0.210148      0.0213968  -0.263191     0.169717    -0.058268    -0.0130749     0.0306668    0.0319291    0.0634651    0.105227   -0.0809094   -0.225936    -0.0381144    0.184619     0.244857    0.172891    0.0935798   -0.0285162    0.137895    -0.0591825    -0.00906526   0.0814584    0.0312427   -0.0934182    -0.0765284   -0.359386  
 -0.223507     -0.102178   -0.0934253    0.00323316   0.0145793    0.0584678     0.179763     0.0746548   -0.0469121    0.0589743   0.115565     0.0117108    0.0553902   -0.0261508    0.195528    0.0305505   0.0284561   -0.075479    -0.0730284    0.0685527    -0.0796425   -0.0341645   -0.00693582  -0.0621202     0.0479349   -0.130355  
  0.0655108    -0.0798929   0.212555     0.0720407    0.0793114    0.150669      0.156803     0.0239322   -0.210112    -0.0195972   0.010935    -0.0586593   -0.177423    -0.232109     0.178821   -0.0128561   0.081602     0.139134     0.0431692    0.0970168    -0.0493259    0.0969923   -0.122549     0.150941     -0.00740789   0.22672   
 -0.0133177    -0.0714701   0.0305549   -0.152014    -0.0306988    0.0122081    -0.0480409   -0.0512125   -0.144968     0.0786233  -0.172432    -0.111424     0.103531     0.0796728    0.0387931   0.0109203  -0.0581496   -0.0244362   -0.0730636    0.087612      0.219991     0.0831279    0.135863     0.062814     -0.24306     -0.011917  
 -0.120525      0.0174606   0.122005    -0.0717439   -0.0268665   -0.0164621    -0.111471     0.12576      0.118538    -0.0417541  -0.0025129   -0.0582199   -0.0188452    0.168236     0.0731001  -0.0479618   0.073602    -0.222405     0.121782    -0.00264404    0.240672    -0.0708937    0.0161715    0.0338927     0.02766     -0.0245843 
 -0.139854      0.0548936  -0.0294388   -0.0187435   -0.0339266   -0.0243715    -0.0115382   -0.0625358   -0.11701     -0.103604   -0.228188     0.0957504    0.190932     0.0353175   -0.145557   -0.123185   -0.15697     -0.109115     0.053574     0.0441182    -0.0122096    0.0368768    0.102815     0.145619      0.172546     0.0110882 
 -0.077068     -0.132909   -0.0855412   -0.115328     0.0496601   -0.0582809    -0.0370888    0.0911211    0.0297483    0.0717395  -0.0661202    0.124181    -0.0340337    0.0662112   -0.0321543  -0.175289    0.0794701   -0.019867    -0.125514    -0.061867     -0.16638      0.0680815    0.0285555   -0.0403286    -0.106029    -0.00362882
 -0.191943      0.0256805   0.23105      0.077163    -0.0700846    0.135124      0.028993    -0.217542    -0.108725     0.0666577   0.0862063   -0.0603758    0.0470682    0.12042      0.0905486   0.143515   -0.0326191    0.0437646    0.0568063   -0.125793     -0.174534    -0.0217945   -0.0122137    0.0493411     0.0548261    0.0864307 
 -0.0287286    -0.015506    0.0455512    0.210007     0.00344484  -0.135995     -0.115776    -0.0492377   -0.0318564   -0.0604816   0.0007609    0.0837262    0.0319959   -0.0725613    0.0433259  -0.0391582   0.0381588   -0.0446174    0.0795451   -0.0102311    -0.0388632   -0.147088    -0.0755311   -0.190588      0.0315426   -0.0153294 
 -0.0502727    -0.0279396   0.0157459    0.111375     0.0179075    0.119926     -0.111844    -0.136341    -0.0202414   -0.0207056   0.00670446  -0.174348     0.0440392   -0.0639035   -0.0422974  -0.137216    0.122534     0.0172955    0.107853    -0.0365062     0.0988667    0.137831     0.174341     0.0898303     0.145722    -0.0729389 
  0.0142682     0.0418212   0.03732      0.11052      0.0435893    0.0181021    -0.0153594   -0.0607197   -0.0150139   -0.061408    0.0528821    0.135004     0.0600763    0.193847     0.0646574   0.0312836  -0.181265     0.180121    -0.0686755   -0.0118865    -0.173244     0.0365176   -0.0248668   -0.0295488    -0.0633153   -0.0303635 
 -0.0214762    -0.0748443  -0.0199381    0.0240346    0.00394367   0.0585868     0.0278308    0.00550303   0.194715    -0.130529    0.0159609    0.0526357   -0.125333    -0.0479694    0.0323228   0.0186678  -0.0386665    0.102266     0.0290877    0.0529452    -0.216159    -0.0290307    0.0779998   -0.0729839    -0.0787944    0.0190894 
 -0.0820992     0.0754011  -0.0305533    0.0104753   -0.0175771    0.0559336    -0.0376428    0.322202    -0.107998    -0.0425149   0.130904    -0.00805213   0.111507     0.0950328    0.201326    0.0780211   0.0227651    0.115837     0.150595     0.106484     -0.0396621   -0.164067     0.0526307   -0.0185999     0.075278    -0.0435295 
  0.162324      0.0698359   0.0577701   -0.0218785   -0.0838834   -0.0109536    -0.119762    -0.264178    -0.1358      -0.0980478   0.0762954   -0.124213    -0.113619     0.11966     -0.0281762  -0.180397   -0.0697687    0.00905188   0.0451906    0.0442504    -0.0468073    0.229198     0.0209058    0.112018      0.0472583    0.0210832 
 -0.0763774     0.0695656   0.00832888   0.0729338    0.0611631    0.0858145    -0.0641554    0.0566146   -0.0402665    0.0704643  -0.07787      0.00108611  -0.0558046   -0.0145099    0.135494    0.0235245   0.06061      0.134847     0.0820531    0.0585163    -0.107992     0.0321565    0.185093    -0.140121      0.0726451   -0.0329637 
 -0.0767461    -0.20182     0.0857822    0.0408556   -0.0414785    0.036375     -0.0601991   -0.0598677   -0.127051     0.140409    0.00631096   0.192841     0.0379002   -0.180544    -0.0349603  -0.0931022   0.0618406    0.0320101    0.106198    -0.0847922     0.0465023   -0.00426853   0.182812     0.0588276     0.12158      0.0305556 
  0.14953      -0.0332678   0.0018294    0.0179243   -0.033002     0.116329      0.040083     0.10039      0.0485261    0.103488    0.252278     0.0687534    0.163583     0.142849    -0.0232485   0.0277402  -0.118542    -0.0896853   -0.11433     -0.0877969    -0.0790048   -0.00127691   0.131424    -0.133436     -0.22625      0.108189  
 -0.0571741    -0.0434492   0.0717484    0.109074    -0.00668282   0.000754991  -0.00742989  -0.0816951   -0.0134939    0.224925    0.11882      0.110459    -0.23866     -0.180087    -0.0116225  -0.0775307   0.200663    -0.136253     0.0361125   -0.0477168     0.0708641    0.0166276   -0.0223356   -0.000640695   0.0525119    0.0527489 
 -0.115464     -0.133535    0.190225    -0.0139221    0.0738346    0.056812      0.20047     -0.131046     0.00628252  -0.100468    0.0679155    0.129317    -0.180997    -0.0865165    0.101353   -0.0761244   0.00489583  -0.00440356   0.0180601    0.0683884    -0.0107125   -0.101092    -0.0338902   -0.0170273     0.0518563    0.0505679 
 -0.0308625    -0.0492799   0.0280661    0.00622207  -0.0273619    0.0543067     0.00512317  -0.0992582    0.0507001    0.0965571  -0.0566324    0.0208465   -0.0101356   -0.163771    -0.118627    0.0917769  -0.0307559    0.156996     0.138529     0.000546092  -0.183628    -0.20163     -0.079364    -0.195098     -0.120747     0.0818961 
  0.0842094     0.0660869   0.114574     0.150685     0.0355929    0.083671      0.124779     0.0462189    0.146662     0.0697736  -0.0336042   -0.136141    -0.00370435  -0.0210427    0.131463    0.0302102   0.0520924   -0.0779839   -0.0681111   -0.00465818   -0.172847     0.073042    -0.0664649   -0.0124189    -0.107951     0.0838384 
  0.0264693    -0.131931    0.0338963    0.0658011    0.0589687    0.152372      0.00942306   0.0656824    0.0324886   -0.138104    0.00897251  -0.117525     0.02147     -0.0208423    0.0362053  -0.050621    0.0995221    0.0273829    0.122992     0.159939      0.0844365    0.154873     0.130543     0.0456084    -0.0275586    0.157275  
 -0.0744445     0.119665    0.0150162    0.0285789   -0.0167078   -0.131683     -0.0927211    0.0581849    0.182603    -0.0422722   0.0661008   -0.0957613    0.0941903   -0.00842618   0.0877658  -0.258272   -0.110611    -0.0786198   -0.0496847   -0.207666      0.00231592  -0.0490331   -0.105466     0.0885352     0.075989     0.0536124 
  0.000499505  -0.153434   -0.0837219   -0.0877219    0.206314    -0.0564155     0.0512561    0.0849409    0.120475    -0.0780817   0.0687268    0.039479     0.0567307   -0.0841301    0.032844    0.0270823  -0.0549921    0.0923786   -0.13448     -0.0709944     0.0443624    0.00112547   0.0668816   -0.0145041    -0.0117718    0.0764801 
 -0.171955     -0.0926157   0.00698669  -0.0967628   -0.11793     -0.141223      0.193685    -0.112282    -0.0943665    0.0990196  -0.0426538    0.0252956   -0.0927642   -0.0457398   -0.130104   -0.0303073  -0.0202399    0.0752067    0.235307    -0.0182412    -0.0473661   -0.129692     0.0162739   -0.183289      0.0871821   -5.85327e-5
  0.146115      0.0844785  -0.010792     0.00968443   0.0592708    0.09696      -0.0845643   -0.050785     0.0594075    0.0210775  -0.09845     -0.0274798   -0.179323    -0.0805893   -0.0927473   0.232424    0.00586365   0.141068     0.00873269   0.0945748    -0.0757139    0.055421     0.145034    -0.114049     -0.0753794   -0.157668  
 -0.10397      -0.0855328   0.0333201    0.0746709   -0.176715     0.0910356     0.0842276    0.0943077    0.0439128   -0.0706788  -0.0478399   -0.102638    -0.00941249   0.132458     0.192931    0.0258157   0.103778     0.0839528   -0.00952724   0.0882651    -0.0674572    0.144442     0.31009      0.122915      0.0151812    0.0323867 
  0.226742     -0.120217    0.080286     0.178543     0.0195234    0.0551464    -0.005999    -0.136493     0.0181945    0.0930116   0.124157    -0.00828476  -0.109399     0.136265     0.0622201  -0.0351626   0.0100394    0.0290023    0.0988294    0.141989      0.162749     0.0313334   -0.0235218    0.0658609     0.121181     0.100478  
 -0.0150209     0.0321229   0.265343    -0.0532131   -0.117654     0.0750023     0.0617518    0.0179576   -0.0167971    0.0169228   0.11773     -0.00346469   0.0466669    0.0481904    0.0445173   0.0803395  -0.0683434    0.0531181   -0.0678503    0.000278144  -0.0913699   -0.00691144   0.0205299    0.112524      0.0167467   -0.0851955 
 -0.0255368     0.0588807   0.373468     0.0861791   -0.0754946    0.00956428   -0.019159    -0.0657865    0.0219073   -0.134566    0.0715572    0.00820886   0.102949    -0.0921219   -0.0530082  -0.0806125   0.196591    -0.0349036    0.0111572    0.103071      0.0117666   -0.0778636   -0.0649156    0.010105     -0.0780113    0.0966135 
  0.145174     -0.0366367   0.0164658   -0.0965019   -0.139227     0.222646      0.0744969   -0.0805216   -0.0459616   -0.012705    0.0313999    0.0327289   -0.0520365   -0.0174696    0.141303   -0.0274073  -0.198045     0.199487     0.110417     0.0949523    -0.17224      0.0282032    0.137403     0.0200307    -0.00583412  -0.00713214INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 2 3 6 10 13 17 21 24 26 30 31
INFO: iteration 1, average log likelihood -1.035947
WARNING: Variances had to be floored 2 3 4 6 8 9 10 12 13 17 18 21 24 26 30 31 32
INFO: iteration 2, average log likelihood -0.980218
WARNING: Variances had to be floored 1 2 3 4 6 10 11 13 17 21 24 26 30 31
INFO: iteration 3, average log likelihood -0.997531
WARNING: Variances had to be floored 2 3 6 9 10 12 13 17 18 21 24 26 30 31 32
INFO: iteration 4, average log likelihood -0.994464
WARNING: Variances had to be floored 2 3 4 6 8 10 13 17 21 24 26 30 31
INFO: iteration 5, average log likelihood -1.001656
WARNING: Variances had to be floored 1 2 3 4 6 9 10 11 12 13 17 18 21 24 26 30 31 32
INFO: iteration 6, average log likelihood -0.963278
WARNING: Variances had to be floored 2 3 6 10 13 17 21 24 26 30 31
INFO: iteration 7, average log likelihood -1.025524
WARNING: Variances had to be floored 2 3 4 6 8 9 10 12 13 17 18 21 24 26 30 31 32
INFO: iteration 8, average log likelihood -0.974423
WARNING: Variances had to be floored 1 2 3 4 6 10 11 13 17 21 24 26 30 31
INFO: iteration 9, average log likelihood -0.996262
WARNING: Variances had to be floored 2 3 6 9 10 12 13 17 18 21 24 26 30 31 32
INFO: iteration 10, average log likelihood -0.994471
INFO: EM with 100000 data points 10 iterations avll -0.994471
59.0 data points per parameter
32Ã—26 Array{Float64,2}:
 -0.192224    -0.102763    -0.02299     -0.00429885  -0.0890983    0.0981987   -0.0345123    0.104195     -0.0491389   -0.128864     -0.0111129   -0.0181376    0.0752087   -0.0998973    -0.0534366    0.00857284  -0.0105892    0.114148    -0.174139    -0.19058      -0.139905     0.0453842    -0.0960637   0.0313527    0.130607    0.078707 
  0.143472     0.225277     0.118864    -0.0614293    0.0104219    0.133758     0.12582     -0.0863265    -0.083671    -0.051461      0.0249482   -0.0494638    0.0845127    0.0282403     0.00321171   0.177991    -0.0308922   -0.027418     0.0915913   -0.016914     -0.00306505   0.178948     -0.0259054   0.0312909   -0.108834    0.134069 
 -0.0110655    0.052373     0.0170663   -0.0812752   -0.0066359   -0.0754907   -0.111226     0.0299374     0.0397285   -0.15581       0.0681472   -0.0766865    0.083889     0.0727416     0.00462098   0.0162386   -0.203904     0.0902542    0.120341    -0.0520177     0.0118121   -0.116137     -0.251618    0.148066    -0.095149    0.0454024
  0.11199     -0.0656871   -0.170431     0.0335315    0.00137605   0.209363    -0.150376     0.124227     -0.0622361    0.0356958    -0.102831     0.19878      0.0343955    0.151498      0.0585903   -0.168318    -0.0258796   -0.0185799   -0.0902504   -0.0116876    -0.019973     0.0772316     0.0236397  -0.0260017    0.0683708  -0.0811458
 -0.0789505    0.0844445    0.0209052    0.0337826   -0.0553388   -0.0379008    0.0329041    0.264555      0.184992    -0.0948575     0.141964     0.00458943   0.132878     0.170168     -0.0848852    0.118249     0.0267834   -0.143226     0.30497     -0.142296      0.192263     0.162334      0.113363    0.0901768   -0.0510832  -0.0731374
 -0.112047    -0.0306321   -0.0649321    0.0660154   -0.14013      0.0469534   -0.0854076    0.00720771    0.0477032    0.0447471    -0.013919     0.227397    -0.152138    -0.0910953     0.117931    -0.0013963    0.0104972    0.115547     0.0654829    0.0820221    -0.0231182    0.0276665     0.114199    0.0344541    0.166271    0.123231 
 -0.0321023   -0.112729     0.056779    -0.0294129   -0.0849033   -0.0104971   -0.022899     0.0511537     0.120307     0.181366      0.0389919    0.0598448    0.100755     0.0427999    -0.0677178   -0.257232     0.0787101    0.104576     0.140788    -0.027189     -0.0463272   -0.0368277    -0.0975177  -0.00781213  -0.0412648  -0.0403105
 -0.071927     0.0782568    0.0715063    0.183927    -0.00126099   0.0545619   -0.0261718    0.0978757     0.00993681  -0.0456747    -0.063606     0.0810778    0.0312734    0.218983     -0.0864201   -0.00990931  -0.0387583    0.0231872    0.147717     0.0751109    -0.10023      0.160874     -0.0885101  -0.0734229   -0.136427    0.0187708
  0.0578014    0.0289416    0.0349009   -0.00354594  -0.151174     0.21745     -0.0964243    0.113242      0.0246677   -0.207094      0.125268     0.141935     0.00905966  -0.00302509    0.109894    -0.0253704    0.0821377   -0.03666      0.0127948    0.128191     -0.064229     0.0410341     0.0329946  -0.110244    -0.126197    0.0936086
 -0.00205702  -0.0247255    0.00204335  -0.153054     0.031334     0.0837605    0.0844344    0.129398     -0.0273687   -0.063135     -0.0160029    0.186849    -0.066774    -0.00947956   -0.168378     0.0271866   -0.162004     0.0709662   -0.0381153   -0.0335143    -0.075623    -0.197738      0.140912   -0.231692    -0.0550722   0.0619304
 -0.00437828  -0.110509     0.205791    -0.0697776   -0.0846458   -0.132684    -0.0718675   -0.0245039    -0.0794293    0.000863031  -0.0352797    0.0231986    0.138123     0.0998268     0.0980798   -0.118869     0.0989841    0.0198336   -0.0607397   -0.255625     -0.0291364   -0.0700708    -0.140447   -0.123       -0.121457    0.0968163
  0.048235    -0.0433115   -0.00886656   0.0793017   -0.0651273    0.0142055   -0.0760738   -0.00575824    0.0976678   -0.0139541    -0.0455707    0.0847988    0.201682    -0.0767912     0.0706026   -0.023452    -0.141299    -0.0493584   -0.0214023    0.0820567     0.0201948    0.327809      0.0249405   0.0706621   -0.0817681  -0.177852 
 -0.103643    -0.115864    -0.0480463    0.0331365   -0.0130472    0.0201439    0.0976886   -0.103834     -0.00643878  -0.0863943    -0.0914653    0.0164994   -0.0195477    0.112901      0.00117789   0.0883162    0.0306938    0.0577251   -0.195342    -0.0150643    -0.00365934   0.0809086     0.0313533   0.240386     0.0739459   0.0867958
 -0.0800519    0.0691311   -0.0170204   -0.0829524    0.145431    -0.0654522   -0.0237369    0.0184104    -0.0636391    0.0104187    -0.0492803    0.0800977   -0.0691485   -0.0925858    -0.0932015    0.0321122    0.0428625    0.0122636   -0.0231278    0.10721      -0.042911    -0.0600226     0.016074    0.0287674    0.0111899  -0.0595459
 -0.112066    -0.122322    -0.107231    -0.133943     0.0484081   -0.0307601    0.120725     0.0179045    -0.0569993    0.059945     -0.0582974   -0.043986    -0.0127464    0.0367992     0.0266      -0.120721    -0.109513    -0.0722229    0.00786016   0.127622     -0.04511      0.212359      0.198687   -0.0375874   -0.0465337   0.0982158
 -0.146544     0.00688352   0.0774542    0.135684     0.0871212    0.0859585   -0.129516     0.0359262     0.216096    -0.163284      0.0644469    0.0262839    0.19509      0.0414116    -0.0329935    0.0231104    0.0973324    0.0582419    0.0626746    0.000606188   0.11061      0.000734273   0.11843     0.0120496    0.0764666   0.0917835
  0.0125827    0.136167    -0.141502     0.0342947   -0.0969308   -0.296346     0.0748236   -0.0314113    -0.024907     0.075591      0.0809952    0.0857703   -0.0416921    0.113417     -0.122619    -0.0179158    0.0888396   -0.0592581    0.0453286   -0.0722614     0.0761056   -0.102438      0.0531829  -0.251988     0.0506937   0.102509 
  0.00532079   0.00976355  -0.0126585    0.178932     0.0151509    0.0386104    0.00103598  -0.0529661     0.0557641   -0.100427     -0.0203701   -0.0291979   -0.0830392    0.124813      0.0757117    0.0280174    0.0607044    0.0943467    0.12671      0.171459     -0.0455088   -0.0783569     0.0980077   0.194958    -0.073332   -0.125753 
  0.07195     -0.0497628   -0.0291964   -0.0267293   -0.0301732    0.035773     0.0666969   -0.00346264    0.0657277    0.217626      0.133261    -0.0125266    0.0548458   -0.135859      0.0643378    0.0421619    0.069587    -0.0458096   -0.0979891   -0.0355638    -0.0990612   -0.080626      0.0487395  -0.120751     0.0901037  -0.034684 
  0.0179675   -0.0618916    0.0428776    0.115648     0.0104491   -0.174473     0.14691     -0.0152264    -0.0248941    0.106435      0.0649354   -0.0211923   -0.100792     0.116638      0.0192957    0.00793821  -0.0132335   -0.0359874   -0.0677237    0.145074      0.135015    -0.0617681    -0.0576569   0.209722     0.0229474  -0.051681 
 -0.0146452    0.00125942   0.0611877   -0.0667762   -0.0445308   -0.0707598    0.0962849   -0.133872     -0.170276    -0.18001       0.0435921   -0.0702926   -0.0523129   -0.0179858    -0.0286976    0.0292975   -0.00197046   0.0466318    0.188973    -0.00257598   -0.172683     0.0380242     0.160323    0.0759415    0.0887841  -0.0224105
  0.0314328   -0.00641743   0.150401    -0.0890844    0.00401108   0.14645     -0.0254554   -0.138272      0.0102093   -0.0473324     0.0934429    0.0735407   -0.0543377   -0.0298238    -0.0556513   -0.165695    -0.214571    -0.11719      0.0928404    0.132822     -0.0237749    0.0264909    -0.101463   -0.0157408    0.0202524  -0.114579 
 -0.0275263   -0.185158     0.183541    -0.086722     0.0215726   -0.0723528    0.058743    -0.0187825    -0.254274     0.0417592     0.143402     0.0342277   -0.043473    -0.0214985     0.0376664    0.041163     0.201713     0.0564073    0.0595132    0.11896      -0.210936     0.149109     -0.0218179   0.131844    -0.0786498   0.108874 
 -0.149016     0.0603926   -0.0875473    0.0163668   -0.076633     0.213517     0.0451425    0.0427318     0.0934842   -0.0583353     0.105003    -0.203701     0.0751518   -0.00486922   -0.0743257   -0.0479265    0.0669292    0.150922    -0.066296    -0.11711      -0.00216103  -0.0653225     0.149675   -0.0541302    0.0746017   0.0790033
 -0.0479233    0.187901     0.0600811   -0.153901    -0.0375378    0.151467    -0.0472562    0.21878       0.191161     0.0705424    -0.1103      -0.0408367    0.0160245    0.134278     -0.241333     0.0495798   -0.166995     0.0484114   -0.114865    -0.0317994     0.0178537    0.0227606    -0.0889891  -0.0785315   -0.0109936  -0.231231 
  0.0989393   -0.00124214  -0.114886     0.147732    -0.0452994    0.0734563    0.0594449    0.153532     -0.0164641   -0.0302499    -0.0154231    0.0453041   -0.137549     0.0526693     0.065481    -0.0338582   -0.102123    -0.0014201    0.0821588    0.053844     -0.111268    -0.0691012    -0.0555143   0.054805    -0.0814672  -0.0374322
  0.243091    -0.0140034    0.0430329    0.0687629    0.27525     -0.00866493   0.0544619    0.022999     -0.00804719  -0.054627      0.0822344    0.0697674    0.0369149   -0.00469757   -0.00980646   0.0251162    0.00985086  -0.0101849   -0.122896    -0.148222      0.141949     0.108042     -0.165748   -0.0501158   -0.015492    0.0129934
  0.0140798   -0.0779508   -0.0495797    0.135228    -0.0493508   -0.0323369    0.128863     0.000769022   0.129018    -0.0567796    -0.0359467   -0.106438     0.00281576   0.000555009   0.0775043    0.0269126   -0.00602491  -0.104712    -0.0389665    0.161764      0.166686     0.141103      0.103201   -0.0271592   -0.0471091  -0.0806683
  0.0620582   -0.232003    -0.0279477   -0.0814155    0.0584265    0.177297    -0.0840271   -0.122996     -0.0057623    0.0785021    -0.00773788  -0.054432     0.0362329   -0.0537811     0.118386    -0.086353    -0.0382623   -0.00713593   0.123517    -0.0101604     0.03005     -0.152663      0.125737    0.260939    -0.0100177  -0.0128259
  0.0366372   -0.0662503   -0.132846    -0.0327556    0.00152601   0.122752    -0.00382948  -0.0342166    -0.0514684    0.0178593    -0.0402922    0.0506333   -0.0852794    0.051564     -0.145241     0.0404002   -0.0657582   -0.0513836   -0.0700186   -0.14582       0.0171351    0.103303      0.0659981   0.117677     0.0476711   0.0520212
  0.0483689    0.0343656   -0.169027    -0.0603008   -0.0988178   -0.14011      0.187927     0.05078      -0.104463    -0.0556629     0.0564819    0.260089     0.0926424    0.0577638     0.0822664   -0.0345536   -0.019371     0.0995176    0.0148407    0.0201141     0.0886198   -0.064393     -0.143569   -0.183093    -0.057028    0.138272 
  0.0386339    0.202056     0.0101384   -0.128492    -0.0304641   -0.00110136   0.1022      -0.102646     -0.142472    -0.0207132     0.200916    -0.0707666   -0.0563293    0.110754      0.0131559    0.127231     0.0109459    0.117558     0.0747737    0.0433909     0.229748    -0.179967      0.130635   -0.0233417    0.112664    0.0399222kind full, method split
0: avll = -1.4182765728149103
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.418295
INFO: iteration 2, average log likelihood -1.418211
INFO: iteration 3, average log likelihood -1.418141
INFO: iteration 4, average log likelihood -1.418058
INFO: iteration 5, average log likelihood -1.417962
INFO: iteration 6, average log likelihood -1.417855
INFO: iteration 7, average log likelihood -1.417745
INFO: iteration 8, average log likelihood -1.417633
INFO: iteration 9, average log likelihood -1.417503
INFO: iteration 10, average log likelihood -1.417312
INFO: iteration 11, average log likelihood -1.416991
INFO: iteration 12, average log likelihood -1.416449
INFO: iteration 13, average log likelihood -1.415648
INFO: iteration 14, average log likelihood -1.414713
INFO: iteration 15, average log likelihood -1.413909
INFO: iteration 16, average log likelihood -1.413405
INFO: iteration 17, average log likelihood -1.413156
INFO: iteration 18, average log likelihood -1.413048
INFO: iteration 19, average log likelihood -1.413003
INFO: iteration 20, average log likelihood -1.412985
INFO: iteration 21, average log likelihood -1.412977
INFO: iteration 22, average log likelihood -1.412974
INFO: iteration 23, average log likelihood -1.412972
INFO: iteration 24, average log likelihood -1.412971
INFO: iteration 25, average log likelihood -1.412971
INFO: iteration 26, average log likelihood -1.412971
INFO: iteration 27, average log likelihood -1.412971
INFO: iteration 28, average log likelihood -1.412970
INFO: iteration 29, average log likelihood -1.412970
INFO: iteration 30, average log likelihood -1.412970
INFO: iteration 31, average log likelihood -1.412970
INFO: iteration 32, average log likelihood -1.412970
INFO: iteration 33, average log likelihood -1.412970
INFO: iteration 34, average log likelihood -1.412970
INFO: iteration 35, average log likelihood -1.412970
INFO: iteration 36, average log likelihood -1.412970
INFO: iteration 37, average log likelihood -1.412970
INFO: iteration 38, average log likelihood -1.412970
INFO: iteration 39, average log likelihood -1.412970
INFO: iteration 40, average log likelihood -1.412969
INFO: iteration 41, average log likelihood -1.412969
INFO: iteration 42, average log likelihood -1.412969
INFO: iteration 43, average log likelihood -1.412969
INFO: iteration 44, average log likelihood -1.412969
INFO: iteration 45, average log likelihood -1.412969
INFO: iteration 46, average log likelihood -1.412969
INFO: iteration 47, average log likelihood -1.412969
INFO: iteration 48, average log likelihood -1.412969
INFO: iteration 49, average log likelihood -1.412969
INFO: iteration 50, average log likelihood -1.412969
INFO: EM with 100000 data points 50 iterations avll -1.412969
952.4 data points per parameter
1: avll = [-1.4183,-1.41821,-1.41814,-1.41806,-1.41796,-1.41786,-1.41775,-1.41763,-1.4175,-1.41731,-1.41699,-1.41645,-1.41565,-1.41471,-1.41391,-1.41341,-1.41316,-1.41305,-1.413,-1.41298,-1.41298,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.412984
INFO: iteration 2, average log likelihood -1.412910
INFO: iteration 3, average log likelihood -1.412846
INFO: iteration 4, average log likelihood -1.412770
INFO: iteration 5, average log likelihood -1.412681
INFO: iteration 6, average log likelihood -1.412584
INFO: iteration 7, average log likelihood -1.412491
INFO: iteration 8, average log likelihood -1.412410
INFO: iteration 9, average log likelihood -1.412345
INFO: iteration 10, average log likelihood -1.412294
INFO: iteration 11, average log likelihood -1.412252
INFO: iteration 12, average log likelihood -1.412217
INFO: iteration 13, average log likelihood -1.412187
INFO: iteration 14, average log likelihood -1.412162
INFO: iteration 15, average log likelihood -1.412140
INFO: iteration 16, average log likelihood -1.412123
INFO: iteration 17, average log likelihood -1.412108
INFO: iteration 18, average log likelihood -1.412097
INFO: iteration 19, average log likelihood -1.412087
INFO: iteration 20, average log likelihood -1.412079
INFO: iteration 21, average log likelihood -1.412073
INFO: iteration 22, average log likelihood -1.412067
INFO: iteration 23, average log likelihood -1.412061
INFO: iteration 24, average log likelihood -1.412057
INFO: iteration 25, average log likelihood -1.412052
INFO: iteration 26, average log likelihood -1.412048
INFO: iteration 27, average log likelihood -1.412044
INFO: iteration 28, average log likelihood -1.412040
INFO: iteration 29, average log likelihood -1.412036
INFO: iteration 30, average log likelihood -1.412033
INFO: iteration 31, average log likelihood -1.412030
INFO: iteration 32, average log likelihood -1.412027
INFO: iteration 33, average log likelihood -1.412024
INFO: iteration 34, average log likelihood -1.412021
INFO: iteration 35, average log likelihood -1.412019
INFO: iteration 36, average log likelihood -1.412016
INFO: iteration 37, average log likelihood -1.412014
INFO: iteration 38, average log likelihood -1.412012
INFO: iteration 39, average log likelihood -1.412010
INFO: iteration 40, average log likelihood -1.412008
INFO: iteration 41, average log likelihood -1.412006
INFO: iteration 42, average log likelihood -1.412004
INFO: iteration 43, average log likelihood -1.412002
INFO: iteration 44, average log likelihood -1.412001
INFO: iteration 45, average log likelihood -1.411999
INFO: iteration 46, average log likelihood -1.411998
INFO: iteration 47, average log likelihood -1.411996
INFO: iteration 48, average log likelihood -1.411994
INFO: iteration 49, average log likelihood -1.411993
INFO: iteration 50, average log likelihood -1.411991
INFO: EM with 100000 data points 50 iterations avll -1.411991
473.9 data points per parameter
2: avll = [-1.41298,-1.41291,-1.41285,-1.41277,-1.41268,-1.41258,-1.41249,-1.41241,-1.41235,-1.41229,-1.41225,-1.41222,-1.41219,-1.41216,-1.41214,-1.41212,-1.41211,-1.4121,-1.41209,-1.41208,-1.41207,-1.41207,-1.41206,-1.41206,-1.41205,-1.41205,-1.41204,-1.41204,-1.41204,-1.41203,-1.41203,-1.41203,-1.41202,-1.41202,-1.41202,-1.41202,-1.41201,-1.41201,-1.41201,-1.41201,-1.41201,-1.412,-1.412,-1.412,-1.412,-1.412,-1.412,-1.41199,-1.41199,-1.41199]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.412000
INFO: iteration 2, average log likelihood -1.411946
INFO: iteration 3, average log likelihood -1.411899
INFO: iteration 4, average log likelihood -1.411847
INFO: iteration 5, average log likelihood -1.411784
INFO: iteration 6, average log likelihood -1.411708
INFO: iteration 7, average log likelihood -1.411619
INFO: iteration 8, average log likelihood -1.411518
INFO: iteration 9, average log likelihood -1.411412
INFO: iteration 10, average log likelihood -1.411305
INFO: iteration 11, average log likelihood -1.411203
INFO: iteration 12, average log likelihood -1.411112
INFO: iteration 13, average log likelihood -1.411034
INFO: iteration 14, average log likelihood -1.410971
INFO: iteration 15, average log likelihood -1.410920
INFO: iteration 16, average log likelihood -1.410881
INFO: iteration 17, average log likelihood -1.410849
INFO: iteration 18, average log likelihood -1.410824
INFO: iteration 19, average log likelihood -1.410803
INFO: iteration 20, average log likelihood -1.410785
INFO: iteration 21, average log likelihood -1.410769
INFO: iteration 22, average log likelihood -1.410754
INFO: iteration 23, average log likelihood -1.410741
INFO: iteration 24, average log likelihood -1.410729
INFO: iteration 25, average log likelihood -1.410717
INFO: iteration 26, average log likelihood -1.410706
INFO: iteration 27, average log likelihood -1.410696
INFO: iteration 28, average log likelihood -1.410685
INFO: iteration 29, average log likelihood -1.410676
INFO: iteration 30, average log likelihood -1.410666
INFO: iteration 31, average log likelihood -1.410657
INFO: iteration 32, average log likelihood -1.410648
INFO: iteration 33, average log likelihood -1.410639
INFO: iteration 34, average log likelihood -1.410630
INFO: iteration 35, average log likelihood -1.410622
INFO: iteration 36, average log likelihood -1.410614
INFO: iteration 37, average log likelihood -1.410606
INFO: iteration 38, average log likelihood -1.410598
INFO: iteration 39, average log likelihood -1.410590
INFO: iteration 40, average log likelihood -1.410582
INFO: iteration 41, average log likelihood -1.410575
INFO: iteration 42, average log likelihood -1.410568
INFO: iteration 43, average log likelihood -1.410561
INFO: iteration 44, average log likelihood -1.410554
INFO: iteration 45, average log likelihood -1.410548
INFO: iteration 46, average log likelihood -1.410541
INFO: iteration 47, average log likelihood -1.410535
INFO: iteration 48, average log likelihood -1.410529
INFO: iteration 49, average log likelihood -1.410523
INFO: iteration 50, average log likelihood -1.410517
INFO: EM with 100000 data points 50 iterations avll -1.410517
236.4 data points per parameter
3: avll = [-1.412,-1.41195,-1.4119,-1.41185,-1.41178,-1.41171,-1.41162,-1.41152,-1.41141,-1.4113,-1.4112,-1.41111,-1.41103,-1.41097,-1.41092,-1.41088,-1.41085,-1.41082,-1.4108,-1.41078,-1.41077,-1.41075,-1.41074,-1.41073,-1.41072,-1.41071,-1.4107,-1.41069,-1.41068,-1.41067,-1.41066,-1.41065,-1.41064,-1.41063,-1.41062,-1.41061,-1.41061,-1.4106,-1.41059,-1.41058,-1.41058,-1.41057,-1.41056,-1.41055,-1.41055,-1.41054,-1.41053,-1.41053,-1.41052,-1.41052]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.410520
INFO: iteration 2, average log likelihood -1.410463
INFO: iteration 3, average log likelihood -1.410411
INFO: iteration 4, average log likelihood -1.410353
INFO: iteration 5, average log likelihood -1.410282
INFO: iteration 6, average log likelihood -1.410197
INFO: iteration 7, average log likelihood -1.410097
INFO: iteration 8, average log likelihood -1.409986
INFO: iteration 9, average log likelihood -1.409870
INFO: iteration 10, average log likelihood -1.409755
INFO: iteration 11, average log likelihood -1.409646
INFO: iteration 12, average log likelihood -1.409546
INFO: iteration 13, average log likelihood -1.409455
INFO: iteration 14, average log likelihood -1.409375
INFO: iteration 15, average log likelihood -1.409305
INFO: iteration 16, average log likelihood -1.409245
INFO: iteration 17, average log likelihood -1.409192
INFO: iteration 18, average log likelihood -1.409147
INFO: iteration 19, average log likelihood -1.409108
INFO: iteration 20, average log likelihood -1.409073
INFO: iteration 21, average log likelihood -1.409043
INFO: iteration 22, average log likelihood -1.409015
INFO: iteration 23, average log likelihood -1.408989
INFO: iteration 24, average log likelihood -1.408966
INFO: iteration 25, average log likelihood -1.408943
INFO: iteration 26, average log likelihood -1.408922
INFO: iteration 27, average log likelihood -1.408901
INFO: iteration 28, average log likelihood -1.408882
INFO: iteration 29, average log likelihood -1.408862
INFO: iteration 30, average log likelihood -1.408843
INFO: iteration 31, average log likelihood -1.408825
INFO: iteration 32, average log likelihood -1.408807
INFO: iteration 33, average log likelihood -1.408789
INFO: iteration 34, average log likelihood -1.408771
INFO: iteration 35, average log likelihood -1.408754
INFO: iteration 36, average log likelihood -1.408737
INFO: iteration 37, average log likelihood -1.408720
INFO: iteration 38, average log likelihood -1.408704
INFO: iteration 39, average log likelihood -1.408688
INFO: iteration 40, average log likelihood -1.408672
INFO: iteration 41, average log likelihood -1.408656
INFO: iteration 42, average log likelihood -1.408641
INFO: iteration 43, average log likelihood -1.408627
INFO: iteration 44, average log likelihood -1.408613
INFO: iteration 45, average log likelihood -1.408599
INFO: iteration 46, average log likelihood -1.408585
INFO: iteration 47, average log likelihood -1.408572
INFO: iteration 48, average log likelihood -1.408560
INFO: iteration 49, average log likelihood -1.408547
INFO: iteration 50, average log likelihood -1.408535
INFO: EM with 100000 data points 50 iterations avll -1.408535
118.1 data points per parameter
4: avll = [-1.41052,-1.41046,-1.41041,-1.41035,-1.41028,-1.4102,-1.4101,-1.40999,-1.40987,-1.40976,-1.40965,-1.40955,-1.40946,-1.40938,-1.40931,-1.40924,-1.40919,-1.40915,-1.40911,-1.40907,-1.40904,-1.40901,-1.40899,-1.40897,-1.40894,-1.40892,-1.4089,-1.40888,-1.40886,-1.40884,-1.40882,-1.40881,-1.40879,-1.40877,-1.40875,-1.40874,-1.40872,-1.4087,-1.40869,-1.40867,-1.40866,-1.40864,-1.40863,-1.40861,-1.4086,-1.40859,-1.40857,-1.40856,-1.40855,-1.40854]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.408532
INFO: iteration 2, average log likelihood -1.408464
INFO: iteration 3, average log likelihood -1.408398
INFO: iteration 4, average log likelihood -1.408321
INFO: iteration 5, average log likelihood -1.408223
INFO: iteration 6, average log likelihood -1.408098
INFO: iteration 7, average log likelihood -1.407944
INFO: iteration 8, average log likelihood -1.407765
INFO: iteration 9, average log likelihood -1.407571
INFO: iteration 10, average log likelihood -1.407373
INFO: iteration 11, average log likelihood -1.407185
INFO: iteration 12, average log likelihood -1.407014
INFO: iteration 13, average log likelihood -1.406864
INFO: iteration 14, average log likelihood -1.406734
INFO: iteration 15, average log likelihood -1.406621
INFO: iteration 16, average log likelihood -1.406522
INFO: iteration 17, average log likelihood -1.406435
INFO: iteration 18, average log likelihood -1.406357
INFO: iteration 19, average log likelihood -1.406287
INFO: iteration 20, average log likelihood -1.406223
INFO: iteration 21, average log likelihood -1.406164
INFO: iteration 22, average log likelihood -1.406111
INFO: iteration 23, average log likelihood -1.406061
INFO: iteration 24, average log likelihood -1.406016
INFO: iteration 25, average log likelihood -1.405974
INFO: iteration 26, average log likelihood -1.405935
INFO: iteration 27, average log likelihood -1.405898
INFO: iteration 28, average log likelihood -1.405865
INFO: iteration 29, average log likelihood -1.405833
INFO: iteration 30, average log likelihood -1.405804
INFO: iteration 31, average log likelihood -1.405776
INFO: iteration 32, average log likelihood -1.405750
INFO: iteration 33, average log likelihood -1.405726
INFO: iteration 34, average log likelihood -1.405703
INFO: iteration 35, average log likelihood -1.405681
INFO: iteration 36, average log likelihood -1.405661
INFO: iteration 37, average log likelihood -1.405642
INFO: iteration 38, average log likelihood -1.405624
INFO: iteration 39, average log likelihood -1.405607
INFO: iteration 40, average log likelihood -1.405591
INFO: iteration 41, average log likelihood -1.405576
INFO: iteration 42, average log likelihood -1.405562
INFO: iteration 43, average log likelihood -1.405549
INFO: iteration 44, average log likelihood -1.405536
INFO: iteration 45, average log likelihood -1.405524
INFO: iteration 46, average log likelihood -1.405513
INFO: iteration 47, average log likelihood -1.405503
INFO: iteration 48, average log likelihood -1.405493
INFO: iteration 49, average log likelihood -1.405484
INFO: iteration 50, average log likelihood -1.405475
INFO: EM with 100000 data points 50 iterations avll -1.405475
59.0 data points per parameter
5: avll = [-1.40853,-1.40846,-1.4084,-1.40832,-1.40822,-1.4081,-1.40794,-1.40777,-1.40757,-1.40737,-1.40719,-1.40701,-1.40686,-1.40673,-1.40662,-1.40652,-1.40643,-1.40636,-1.40629,-1.40622,-1.40616,-1.40611,-1.40606,-1.40602,-1.40597,-1.40593,-1.4059,-1.40586,-1.40583,-1.4058,-1.40578,-1.40575,-1.40573,-1.4057,-1.40568,-1.40566,-1.40564,-1.40562,-1.40561,-1.40559,-1.40558,-1.40556,-1.40555,-1.40554,-1.40552,-1.40551,-1.4055,-1.40549,-1.40548,-1.40548]
[-1.41828,-1.4183,-1.41821,-1.41814,-1.41806,-1.41796,-1.41786,-1.41775,-1.41763,-1.4175,-1.41731,-1.41699,-1.41645,-1.41565,-1.41471,-1.41391,-1.41341,-1.41316,-1.41305,-1.413,-1.41298,-1.41298,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41297,-1.41298,-1.41291,-1.41285,-1.41277,-1.41268,-1.41258,-1.41249,-1.41241,-1.41235,-1.41229,-1.41225,-1.41222,-1.41219,-1.41216,-1.41214,-1.41212,-1.41211,-1.4121,-1.41209,-1.41208,-1.41207,-1.41207,-1.41206,-1.41206,-1.41205,-1.41205,-1.41204,-1.41204,-1.41204,-1.41203,-1.41203,-1.41203,-1.41202,-1.41202,-1.41202,-1.41202,-1.41201,-1.41201,-1.41201,-1.41201,-1.41201,-1.412,-1.412,-1.412,-1.412,-1.412,-1.412,-1.41199,-1.41199,-1.41199,-1.412,-1.41195,-1.4119,-1.41185,-1.41178,-1.41171,-1.41162,-1.41152,-1.41141,-1.4113,-1.4112,-1.41111,-1.41103,-1.41097,-1.41092,-1.41088,-1.41085,-1.41082,-1.4108,-1.41078,-1.41077,-1.41075,-1.41074,-1.41073,-1.41072,-1.41071,-1.4107,-1.41069,-1.41068,-1.41067,-1.41066,-1.41065,-1.41064,-1.41063,-1.41062,-1.41061,-1.41061,-1.4106,-1.41059,-1.41058,-1.41058,-1.41057,-1.41056,-1.41055,-1.41055,-1.41054,-1.41053,-1.41053,-1.41052,-1.41052,-1.41052,-1.41046,-1.41041,-1.41035,-1.41028,-1.4102,-1.4101,-1.40999,-1.40987,-1.40976,-1.40965,-1.40955,-1.40946,-1.40938,-1.40931,-1.40924,-1.40919,-1.40915,-1.40911,-1.40907,-1.40904,-1.40901,-1.40899,-1.40897,-1.40894,-1.40892,-1.4089,-1.40888,-1.40886,-1.40884,-1.40882,-1.40881,-1.40879,-1.40877,-1.40875,-1.40874,-1.40872,-1.4087,-1.40869,-1.40867,-1.40866,-1.40864,-1.40863,-1.40861,-1.4086,-1.40859,-1.40857,-1.40856,-1.40855,-1.40854,-1.40853,-1.40846,-1.4084,-1.40832,-1.40822,-1.4081,-1.40794,-1.40777,-1.40757,-1.40737,-1.40719,-1.40701,-1.40686,-1.40673,-1.40662,-1.40652,-1.40643,-1.40636,-1.40629,-1.40622,-1.40616,-1.40611,-1.40606,-1.40602,-1.40597,-1.40593,-1.4059,-1.40586,-1.40583,-1.4058,-1.40578,-1.40575,-1.40573,-1.4057,-1.40568,-1.40566,-1.40564,-1.40562,-1.40561,-1.40559,-1.40558,-1.40556,-1.40555,-1.40554,-1.40552,-1.40551,-1.4055,-1.40549,-1.40548,-1.40548]
32Ã—26 Array{Float64,2}:
 -0.186829     -0.264461   -0.299607     0.580213      0.367606    -0.552589     0.29144    -0.376827   -0.127957   -0.336635   -0.0784304     0.371086    -0.385205   -0.151561    -0.345836    0.310878   -0.0707956    0.632842   -0.231372    -0.256992    -0.377451    -0.405292     0.166515   -0.308552   -0.421764     0.213017 
 -0.0060756    -0.501231    0.149006    -0.258739      0.556918    -0.264921    -0.261805   -0.242799   -0.181974   -0.672481    0.669899      0.0413297   -0.112809    0.174307     0.234694   -0.239368    0.00137763   0.272221   -0.10797     -0.492643    -0.709935     0.0673121    0.39617     0.279412    0.721222     0.0848964
  0.572932     -0.291984    0.15855     -0.355149      0.519254    -0.283762     0.336404    0.0946407   0.297996    0.456526    0.0512044     0.384464    -0.576489   -0.295571     0.220763   -0.868491    0.431488     0.538804    0.0354643   -0.129375    -0.903757     0.301045    -0.469458   -0.052696   -0.0190151   -0.223276 
 -0.311853     -0.245943   -0.481199    -0.0894704     0.673754    -0.0534815   -0.284421    0.304049   -0.222532   -0.498068    0.524421      0.260857    -0.237507   -0.218387     0.275779   -1.01252    -0.0744962    0.659121    0.30899      0.560498     0.615333    -0.124091     0.0798634   0.172232   -0.0807367   -0.342704 
 -0.517081     -0.144527   -0.116954    -0.0760248    -0.11637      0.676452    -0.0694417   0.204822   -0.0189042   0.304765    0.327651     -1.21416     -0.450665    0.231768    -0.0448707  -0.284241    0.156847    -0.0215163   0.632648     0.166044    -0.607641    -0.151117     0.0432154   0.374965    0.475857    -0.0495643
  0.000732802   0.0126015   0.0531228    0.0397963    -0.0874388    0.117981     0.102568    0.0629056  -0.0366555   0.152157   -0.16842       0.0685696    0.0763362   0.218866    -0.0454527   0.128371   -0.0367736   -0.0753341  -0.105604     0.0250629    0.0997242    0.00483124  -0.107747   -0.144472   -0.0784799    0.161921 
  1.03806      -0.0337018   0.182913    -0.430961     -0.52773      0.600632    -0.321301    0.14135    -0.247552    0.455047    0.23233      -0.475865     0.0582016   0.0908499    0.514092   -0.0161996  -0.184925    -0.778243    0.401814     0.00867802  -0.00812628  -0.109046     0.0651627  -0.254363   -0.0134199   -0.069749 
  0.692635      0.239005   -0.0876835   -0.158784     -0.25207      0.737144     0.444067   -0.0943372   1.17352     0.068349    0.233124      0.0577479   -0.014885   -0.750989     0.0675438  -0.0928588   0.590782    -0.237783   -0.262496     0.287173    -0.402009     0.0233646    0.249588    0.0367029  -0.0547534   -0.15326  
 -0.0629913     0.377794   -0.162627    -0.332395     -0.370707    -0.207365     0.0646337  -0.247524    0.313146   -0.518397   -0.0766335    -0.448576     0.679349    0.0992178   -0.446814    0.303845   -0.0780114   -0.425067   -0.183753    -1.06837     -0.633997     0.302041     0.54162     0.647022   -0.178118     0.168164 
 -0.0713283     0.316063    0.606932     0.193839     -0.372836    -0.262193     0.409062   -0.549252    0.233601   -0.0949098   0.526896     -0.428854     0.0127747  -0.272305     0.101528    0.316962   -0.268488     0.0112763   0.187954     0.335289    -0.410664     0.0671523   -0.0366832   0.916618   -0.235581    -0.0472432
  0.387205     -0.104538    0.121322     0.339663      0.0103809   -0.123907    -0.18678     0.356991    0.149847    0.26198    -0.244823      0.276414     0.387873    0.0104497    0.0261439  -0.0325269  -0.596408     0.122014   -0.696627     0.12606      0.729017     0.0969637   -0.215827    0.117283   -0.798714    -0.132255 
 -0.112612      0.373426   -0.127713     0.0653185    -0.248525    -0.298209     0.446008    0.346679    0.305307    0.0959101   0.071032      0.661802     1.07547    -0.314884    -0.0978958  -0.184772    0.0122868   -0.210621    0.473006    -0.569173     0.598228     0.117011     0.214968    0.432412   -0.546991    -0.351723 
 -0.251687      0.772789    0.00955989   0.0510455    -0.023551    -0.369003    -0.7585     -0.0333159  -0.0124312  -0.352467    0.151654      0.448859     0.134126   -0.360311     0.223579    0.435053    0.528829    -0.398816    0.0573783    0.0928356    0.121646    -0.104346     0.147873   -0.10917     0.00981952  -0.347776 
 -0.585618      0.293937   -0.147141     0.209771     -0.515738    -0.0103846   -0.416218   -0.0568018  -0.188746   -0.303399   -0.0538925    -0.429533     0.397997    0.293731     0.11839     0.82049    -0.766152    -0.508678    0.248371     0.290485     0.706138    -0.481591     0.322925    0.119924   -0.01988      0.172765 
 -0.227169     -0.24361     0.0656829   -0.470325     -0.47575      0.344459    -0.278613   -0.0219946  -0.361876   -0.212307   -0.131634     -0.304178     0.621439    0.197919    -0.538756   -0.277171   -0.357569    -0.0824177   0.0227366   -0.104824     0.551278     0.766651     0.355034    0.326011    0.673344    -0.508884 
 -0.535455     -0.0124631  -0.266291     0.370494      0.311997     0.271256     0.247285   -0.488788   -0.104194   -0.657065    0.215464      0.58066      0.446576    0.0477819   -0.116699    0.524958    0.0648339   -0.0344222  -0.188823     0.791097     0.351558     0.474357     0.338283    0.352298    0.246934    -0.0775077
 -0.193706      0.477271   -0.0892392   -0.117196     -0.329965    -0.449648    -0.591955    0.118246    0.0749963   0.305477   -0.352439      0.182642    -0.918047    0.543314     0.290405    0.236697    0.883495     0.0972701   0.051413     0.110987     0.350285    -0.0577822    0.179834   -0.519644    0.376548     0.348948 
  0.285755      0.202734   -0.13596     -0.0218355    -0.289043     0.496502     0.0116412   0.115862   -0.1561      0.179629   -0.787972      0.525534     0.175426    0.0104602   -0.400157    0.217001    0.149624    -0.182371   -0.286051    -0.043687     0.357282     0.21252     -0.178902   -0.570281    0.137176    -0.261617 
  0.150392      0.119032    0.712406     0.178837     -0.225424    -0.325504    -0.337232    0.439148   -0.0594988   0.296195   -0.278884     -0.359113    -0.125572    0.263704     0.185149   -0.402665   -0.230499     0.144091    0.321746    -0.570075     0.0806575   -0.598752    -0.467052   -0.412822   -0.2452       0.464387 
  0.169728     -0.150842    0.39771      0.341865      0.415586     0.31312      0.0742925   0.107241   -0.0518408   0.0469141  -0.189503     -0.3105      -0.474674    0.291382    -0.250326    0.461056   -0.0746153    0.273592   -0.42395      0.383656    -0.409694    -0.312215    -0.608652   -0.717479    0.0505622    0.692837 
 -0.0805313    -0.308437   -0.148046    -0.464437      0.115409     0.328014     1.14553    -0.0381496  -0.151658    0.362574    0.19392       0.106502     0.078465    0.0844519   -0.41683     0.052793   -0.849228    -0.15022    -0.548954    -0.113114    -0.4122      -0.798132     0.0530323  -0.203054   -0.141715    -0.243909 
  0.0391714    -0.244996   -0.389488    -0.546922     -0.28239      0.298036     0.587462    0.0460813  -0.438938    0.641804   -0.306835     -0.00838749  -0.0543977   0.295631    -0.356209   -0.263186    0.237015     0.130285    0.00906798   0.0241617   -0.0607884    0.0419809   -0.463141    0.343197   -0.368851     0.15868  
 -0.136688     -0.365432    0.1171      -0.429973     -0.0588196   -0.436016     0.303836   -0.833941   -0.0678888   0.354529   -0.197398      0.0409667   -0.234823    0.23785      0.0524543   0.662933   -0.198091    -0.47227    -0.178543    -0.714978    -0.116907     0.61311      0.0673774  -0.0980765  -0.23724      0.495516 
 -0.112777     -0.416139    0.163092     0.143829     -0.0081248   -0.00683508   0.809865   -0.0233166  -0.196483   -0.235679   -0.174732      0.418488    -0.0542577   0.0980717    0.319781   -0.627947   -0.218607     0.37078    -0.168913     0.301941    -0.0637795    0.630639     0.0864264   0.0736053   0.0645704    0.561437 
 -0.20087       0.107913   -0.249391     0.000672783   0.0368407    0.358595    -0.152266   -0.0359516   0.427918   -0.324533   -0.000117932  -0.345357    -0.501219   -0.168846    -0.12757     0.130263    0.468902     0.0223298   0.50904     -0.470273    -0.171671    -0.350785     0.35184    -0.284056    0.455842     0.266789 
 -0.141924     -0.215971    0.264578    -0.126223      0.0516403    0.448515     0.17891    -0.0511288  -0.110319    0.0586454   0.246764     -0.555332     0.0878329  -0.00530764  -0.0394196   0.0774121  -0.0080083   -0.106534    0.195281     0.135151    -0.301453     0.327441    -0.194248   -0.0150778   0.471338     0.0461091
 -0.063361     -0.175847    0.0982347   -0.270954     -0.165242    -0.362476    -0.1722     -0.315341   -0.228412   -0.322384    0.229045      0.225366    -0.14753    -0.116495    -0.0710331   0.0800582  -0.0721314    0.235991   -0.320994     0.10135     -0.51581     -0.320907     0.6005      0.128001    0.414706    -0.33125  
 -0.25266      -0.086226    0.0756263   -0.0393981     0.149211    -0.331165    -0.345608    0.246922    0.156589   -0.170527    0.458557     -0.0173529   -0.152556    0.211486     0.624831   -0.367749   -0.0580047    0.0390728   0.0259234    0.169938     0.0578505   -0.0959028    0.0895632   0.211855   -0.0187828    0.0376675
 -0.390714      0.0674826  -0.034005     0.131513     -0.00686458   0.0505734    0.164982    0.0283933   0.0224542  -0.0911102  -0.208605      0.215656     0.020144    0.145845    -0.0137445  -0.046339   -0.0568043    0.129415   -0.122685     0.00542463   0.212282     0.0719771   -0.0284768   0.0726692   0.0108904    0.131229 
  0.465657      0.054911   -0.0475215   -0.0272232    -0.0224203   -0.0424005   -0.0457363   0.0526823   0.0344708   0.129402    0.104392      0.104435     0.123874   -0.0443985   -0.0259529   0.0766414  -0.00979087  -0.113457    0.0633754   -0.0217929    0.0239016   -0.0204355    0.0138987  -0.0357237  -0.258501    -0.0610647
  0.17989       0.444359   -0.216152     0.465792      0.161108     0.421479    -0.305354    0.442923    0.052053   -0.0729525   0.145276     -0.109705     0.0253399  -0.0474285   -0.266263   -0.261599    0.115249     0.421432    0.615474     0.53982      0.183994    -0.385724    -0.217146    0.37446     0.00245525  -0.60575  
  0.508903     -0.220951   -0.311421     0.404021      0.440558     0.00917187  -0.282218    0.512587    0.339394   -0.118452    0.21265       0.00187127   0.450523    0.303837    -0.175297   -0.323677    0.204005    -0.0837981   0.50948     -0.456146     0.402266     0.739516    -0.156378   -0.447913   -0.408423    -0.135487 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.405467
INFO: iteration 2, average log likelihood -1.405459
INFO: iteration 3, average log likelihood -1.405451
INFO: iteration 4, average log likelihood -1.405444
INFO: iteration 5, average log likelihood -1.405437
INFO: iteration 6, average log likelihood -1.405430
INFO: iteration 7, average log likelihood -1.405424
INFO: iteration 8, average log likelihood -1.405417
INFO: iteration 9, average log likelihood -1.405411
INFO: iteration 10, average log likelihood -1.405405
INFO: EM with 100000 data points 10 iterations avll -1.405405
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.809884e+05
      1       6.969655e+05      -1.840229e+05 |       32
      2       6.835596e+05      -1.340587e+04 |       32
      3       6.783598e+05      -5.199838e+03 |       32
      4       6.756905e+05      -2.669236e+03 |       32
      5       6.739506e+05      -1.739887e+03 |       32
      6       6.727911e+05      -1.159540e+03 |       32
      7       6.718834e+05      -9.076890e+02 |       32
      8       6.711180e+05      -7.654222e+02 |       32
      9       6.704999e+05      -6.181154e+02 |       32
     10       6.699794e+05      -5.204636e+02 |       32
     11       6.695107e+05      -4.686764e+02 |       32
     12       6.690982e+05      -4.125275e+02 |       32
     13       6.687436e+05      -3.545605e+02 |       32
     14       6.684392e+05      -3.044818e+02 |       32
     15       6.681540e+05      -2.851636e+02 |       32
     16       6.679094e+05      -2.445918e+02 |       32
     17       6.676775e+05      -2.319351e+02 |       32
     18       6.674687e+05      -2.087923e+02 |       32
     19       6.672764e+05      -1.922325e+02 |       32
     20       6.671172e+05      -1.592554e+02 |       32
     21       6.669567e+05      -1.605196e+02 |       32
     22       6.668117e+05      -1.449841e+02 |       32
     23       6.666846e+05      -1.270755e+02 |       32
     24       6.665615e+05      -1.230837e+02 |       32
     25       6.664490e+05      -1.125572e+02 |       32
     26       6.663384e+05      -1.105620e+02 |       32
     27       6.662398e+05      -9.862679e+01 |       32
     28       6.661497e+05      -9.007401e+01 |       32
     29       6.660588e+05      -9.094433e+01 |       32
     30       6.659808e+05      -7.799413e+01 |       32
     31       6.659002e+05      -8.060679e+01 |       32
     32       6.658300e+05      -7.011541e+01 |       32
     33       6.657684e+05      -6.161957e+01 |       32
     34       6.657051e+05      -6.329939e+01 |       32
     35       6.656456e+05      -5.957927e+01 |       32
     36       6.655891e+05      -5.646291e+01 |       32
     37       6.655360e+05      -5.312064e+01 |       32
     38       6.654849e+05      -5.106511e+01 |       32
     39       6.654339e+05      -5.099415e+01 |       32
     40       6.653817e+05      -5.221870e+01 |       32
     41       6.653384e+05      -4.329032e+01 |       32
     42       6.653009e+05      -3.748058e+01 |       32
     43       6.652627e+05      -3.822720e+01 |       32
     44       6.652218e+05      -4.092072e+01 |       32
     45       6.651810e+05      -4.075854e+01 |       32
     46       6.651491e+05      -3.187676e+01 |       32
     47       6.651184e+05      -3.072828e+01 |       32
     48       6.650954e+05      -2.297676e+01 |       32
     49       6.650769e+05      -1.850305e+01 |       32
     50       6.650591e+05      -1.784367e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 665059.0838162052)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.417242
INFO: iteration 2, average log likelihood -1.412262
INFO: iteration 3, average log likelihood -1.410978
INFO: iteration 4, average log likelihood -1.410076
INFO: iteration 5, average log likelihood -1.409116
INFO: iteration 6, average log likelihood -1.408157
INFO: iteration 7, average log likelihood -1.407433
INFO: iteration 8, average log likelihood -1.407011
INFO: iteration 9, average log likelihood -1.406782
INFO: iteration 10, average log likelihood -1.406646
INFO: iteration 11, average log likelihood -1.406550
INFO: iteration 12, average log likelihood -1.406474
INFO: iteration 13, average log likelihood -1.406411
INFO: iteration 14, average log likelihood -1.406354
INFO: iteration 15, average log likelihood -1.406302
INFO: iteration 16, average log likelihood -1.406254
INFO: iteration 17, average log likelihood -1.406209
INFO: iteration 18, average log likelihood -1.406166
INFO: iteration 19, average log likelihood -1.406125
INFO: iteration 20, average log likelihood -1.406086
INFO: iteration 21, average log likelihood -1.406048
INFO: iteration 22, average log likelihood -1.406013
INFO: iteration 23, average log likelihood -1.405979
INFO: iteration 24, average log likelihood -1.405948
INFO: iteration 25, average log likelihood -1.405918
INFO: iteration 26, average log likelihood -1.405890
INFO: iteration 27, average log likelihood -1.405865
INFO: iteration 28, average log likelihood -1.405840
INFO: iteration 29, average log likelihood -1.405818
INFO: iteration 30, average log likelihood -1.405796
INFO: iteration 31, average log likelihood -1.405776
INFO: iteration 32, average log likelihood -1.405757
INFO: iteration 33, average log likelihood -1.405738
INFO: iteration 34, average log likelihood -1.405721
INFO: iteration 35, average log likelihood -1.405704
INFO: iteration 36, average log likelihood -1.405687
INFO: iteration 37, average log likelihood -1.405671
INFO: iteration 38, average log likelihood -1.405656
INFO: iteration 39, average log likelihood -1.405641
INFO: iteration 40, average log likelihood -1.405626
INFO: iteration 41, average log likelihood -1.405612
INFO: iteration 42, average log likelihood -1.405598
INFO: iteration 43, average log likelihood -1.405585
INFO: iteration 44, average log likelihood -1.405571
INFO: iteration 45, average log likelihood -1.405558
INFO: iteration 46, average log likelihood -1.405546
INFO: iteration 47, average log likelihood -1.405533
INFO: iteration 48, average log likelihood -1.405521
INFO: iteration 49, average log likelihood -1.405509
INFO: iteration 50, average log likelihood -1.405497
INFO: EM with 100000 data points 50 iterations avll -1.405497
59.0 data points per parameter
32Ã—26 Array{Float64,2}:
  0.334043    -0.583808     0.311199    -0.336256     0.310125   -0.00682092   0.483404     0.00809433   0.142138   -0.0237193   0.00206489   0.396133   -0.333721    -0.19886       0.118723    -0.823305     0.248421     0.825824   -0.165436     0.0409364   -0.658067     0.678409     0.261119    0.274059     0.346818    0.1507    
  0.278555    -0.240547     0.687917     0.0502403    0.0109313  -0.366319    -0.855385     0.55231     -0.281571   -0.094199    0.553961    -0.0947731  -0.0625655    0.33772       0.610776    -0.397348    -0.259889     0.302881    0.0584885    0.0625242    0.114639    -0.37759     -0.266559   -0.0714829   -0.179879   -0.100773  
  0.0806983   -0.0765703   -0.479984    -0.556642    -0.411186   -0.00725111  -0.0882287   -0.132171    -0.262318   -0.0704693   0.0176262    0.267935    0.253584    -0.0201413    -0.0821022   -0.0664561   -0.0214213   -0.0696026  -0.0165382    0.0929742    0.189221     0.254405     0.537689    0.296753     0.0483608  -0.471445  
  0.0307783    0.0784787    0.662123     0.625402     0.282404    0.0935651    0.0571088    0.122123     0.0322067  -0.106215   -0.2791      -0.464426   -0.492295     0.311952     -0.385573     0.540902     0.0490232    0.366424   -0.207495     0.0623716   -0.640152    -0.433101    -0.508675   -0.580815    -0.140476    0.895713  
 -0.125821     0.925954     0.233749     0.129394    -0.0626791  -0.163848    -1.24272     -0.103524     0.321138   -0.020115   -0.294219     0.0967306  -0.614273     0.485505      0.229641     0.230713     0.809344     0.139596    0.746777     0.526627     0.905177     0.467566    -0.0607558  -0.182819     0.440579    0.277608  
  0.311407     0.130473     0.113259     0.334986    -0.147668   -0.128753    -0.233102     0.359709     0.36936     0.161464   -0.324323     0.282173    0.451375    -0.0541812     0.0475326    0.0605757   -0.51692     -0.0736769  -0.515998    -0.0493509    0.860442    -0.0216154   -0.0573294   0.0982235   -0.805074   -0.0121938 
  0.121753    -0.0632948   -0.0585635   -0.1494       0.0783185  -0.598441    -0.044885    -1.11304     -0.389849    0.0900705   0.0588173    0.225486   -0.535879     0.251393     -0.101063     0.678021    -0.0549035    0.0285081  -0.142735    -0.522975    -0.273099     0.105528     0.410132   -0.235991    -0.41732     0.598084  
 -0.546177     0.073367    -0.485462     0.514589     0.206907    0.206481     0.252367    -0.511451     0.0153991  -0.50154     0.0747822    0.504327    0.326529     0.000288127  -0.20714      0.671352     0.0835152    0.105595   -0.287127     0.82657      0.451791     0.201915     0.186051    0.393636     0.0610289  -0.115818  
 -0.366177     0.286027    -0.125057    -0.260919    -0.499134   -0.0898883    0.473101    -0.23759      0.309794   -0.256492   -0.385309    -0.223508    0.448113     0.231045     -0.477284     0.32888      0.00284459  -0.533988   -0.00809338  -0.925303    -0.305475     0.42656      0.468887    0.371565    -0.0632112   0.336633  
 -0.178861    -0.176395     0.371544    -0.0682262   -0.281452    0.0802794   -0.113033    -0.559731    -0.219773   -0.289762    0.290724    -0.473242   -0.0444803   -0.0694147     0.069818     0.482993    -0.426839    -0.048495   -0.105475     0.296153    -0.370669     0.0428936    0.33112     0.181159     0.648822   -0.0681451 
  0.0325038    0.314743     0.00396525   0.265569    -0.792308    0.409743    -0.00643276  -0.0729746   -0.286224    0.28077     0.0237622   -0.704613    0.225703     0.261632      0.246199     0.72608     -0.834778    -0.70835     0.627103     0.115065     0.469155    -0.658038     0.0690464  -0.124608    -0.42505     0.159941  
  0.228407     0.0208984   -0.051252    -0.118284    -0.133803    0.0746917    0.0865905    0.0881594    0.0981833   0.324711   -0.166704     0.122807    0.00583651   0.0614301    -0.0675515   -0.0485959    0.0883275   -0.0175197  -0.0716734   -0.126854     0.108669     0.011302    -0.163579   -0.14483     -0.187046    0.0473046 
 -0.771383     0.200878     0.233354     0.125466     0.110856   -0.343729    -0.384067    -0.0203383   -0.336902   -0.748709    0.167853     0.0401972   0.149641     0.163341      0.132966     0.303742    -0.310096    -0.0827291  -0.0190596    0.25276      0.223113     0.0138016    0.352404    0.303001     0.273089   -0.00597299
  0.0613545    0.191592     0.0892174    0.270281    -0.0955928  -0.227175    -0.0568702   -0.136162     0.231021   -0.491096    0.614024     0.0204389   0.972961    -0.185493     -0.204323    -0.245749    -0.295859     0.0403307   0.582555    -0.395246     0.142678     0.44894      0.274314    0.606392    -0.0145631  -0.389803  
  0.176699    -0.269628     0.361771     0.706389    -0.0630533   0.445462    -0.0171388    0.090775    -0.70395     0.311824   -0.386915     0.661212    0.0962027   -0.0111145    -0.29989     -0.103543     0.0270465    0.224486   -0.0544711    0.385272     0.450574     0.102622    -0.506344   -0.53863      0.248288   -0.289771  
  0.50159     -0.413285     0.223883    -0.546076     0.338357    0.196622    -0.348148     0.0294547    0.224923    0.122097    0.176593    -0.30908    -0.0799363   -0.125206      0.205208    -0.0427016    0.120445    -0.56805    -0.105515    -0.312472    -0.360042     0.150038    -0.390195   -0.448532     0.498393    0.113286  
 -0.307099    -0.344241    -0.808384    -0.795494     0.279026    0.400977    -0.219312     0.221409    -0.745673    0.220882    0.238682    -0.295206   -0.301747     1.15564       0.0435577   -0.62992      0.229133    -0.125231    0.0775818    0.216505    -0.451292     0.350701    -0.178268    0.050085     0.226438    0.0552617 
 -0.215148    -0.631376    -0.085607     0.0201865    0.0314618   0.00119444   0.798864    -0.152104    -0.167098    0.459856   -0.271927    -0.110562    0.0472089    0.230866      0.0948986   -0.205786    -0.334015     0.12482    -0.00455212  -0.0705715    0.0539197    0.618981    -0.466908    0.2873      -0.372507    0.498063  
  0.541451     0.150775    -0.362306     0.126232     0.359075    0.0293883   -0.23145      0.762385     0.269287    0.228922    0.0307353    0.185741    0.499265     0.126571     -0.234185    -0.16726      0.315946    -0.250884    0.291138    -0.12364      0.422564     0.490854    -0.341307   -0.075409    -0.493142   -0.558319  
 -0.0991127   -0.6766      -0.216507    -0.182975     0.399535   -0.100177     0.863181    -0.0939597   -0.174653   -0.127392    0.278586     0.0868351   0.0950028   -0.164223     -0.359833    -0.0760167   -0.612437     0.17808    -0.721996    -0.183871    -0.595913    -0.622175     0.167269   -0.0782368   -0.164377   -0.279129  
 -0.62049     -0.344575     0.480953    -0.39082     -0.438402    0.745304    -0.0478662    0.174879    -0.17252     0.319869    0.0881017   -1.28233     0.154737     0.0418832    -0.216458    -0.131428     0.0523306   -0.355154    0.485904     0.0846582   -0.121833     0.498735    -0.32632     0.263921     0.521621   -0.392729  
 -0.354394     0.554048    -0.373461     0.00396413  -0.188552   -0.321059    -0.701004     0.128339     0.120784   -0.232489   -0.108633     0.281942   -0.0606366   -0.234795     -0.00655889   0.45379      0.578067    -0.283556    0.0926406   -0.420542     0.0121945   -0.671885     0.375244   -0.341416     0.0827598  -0.179685  
  0.00854265   0.0163125    0.0126045   -0.0327842   -0.0120986   0.100111     0.0252943    0.0297685   -0.0166547   0.0401411   0.0436206   -0.110296    0.0495818    0.109074     -0.00319736   0.0363094   -0.0559706   -0.0547944   0.0724866   -0.00122739   0.00769754   0.00677788  -0.0170835   0.0581692   -0.0227607   0.0735885 
  0.157211     0.269189     0.0107638   -0.662117    -0.346709    0.400428     0.384669     0.0135614   -0.319928    0.562939   -0.453181     0.192984    0.159948     0.3714       -0.484566     0.156886    -0.116422     0.0822723  -0.379786    -0.119421     0.0730907   -0.229093    -0.295835   -0.198285    -0.0652503   0.0155484 
  0.0741646   -0.539604    -0.553448     0.72154      0.38982     0.627759    -0.0473587    0.410271     0.3202     -0.100885   -0.0768408   -0.289795   -0.0757178    0.667644      0.11558      0.00343704   0.35926      0.0739412   0.215093    -0.173741     0.218468     0.156206     0.467023   -0.755639     0.618377    0.781073  
  0.0355533    0.00485177   0.122078    -0.178403    -0.214112   -0.68868      0.238454     0.276356     0.030737    0.811569   -0.0905235   -0.142829   -1.11861      0.259404      0.308083    -0.374727     0.257383     0.156488    0.153845     0.0282516   -0.364987    -0.528321    -0.327038   -0.144804     0.0435736   0.167594  
  0.0547259    0.232895    -0.420737     0.351215     0.0292701   1.00303      0.121353     0.836971    -0.0419368  -0.327601    0.0780998   -0.288855    0.0468886    0.099115     -0.237291    -0.503588    -0.0667466    0.485028    0.425199     0.681387     0.0344247   -0.852864    -0.0531532   0.416045     0.459195   -0.417279  
  0.217296     0.180898    -0.133076     0.466689     0.340842   -0.0469441    0.116985     0.0793213    0.292875   -0.0502433  -0.048649     0.177607   -0.219805    -0.194068     -0.0436738   -0.238955    -0.0875799    0.479768    0.266445    -0.0116242   -0.0945799   -0.168142    -0.105408    0.00724498  -0.64555     0.100322  
 -0.294446     0.0301415    0.279311     0.0254267    0.110316   -0.228745     0.456844    -0.208458    -0.317391   -0.514559   -0.113281     0.837181    0.155506     0.144638      0.491054    -0.26827     -0.197422    -0.138246   -0.442887     0.494246     0.226221     0.621968     0.026385   -0.172587    -0.161372    0.361818  
 -0.336818     0.0215268   -0.592711    -0.0960243    0.610163    0.185739    -0.165543    -0.0400588    0.0934229  -0.425391    0.741085    -0.216129   -0.481472    -0.415177      0.180411    -0.826875     0.352699     0.547102    0.759799     0.240908    -0.136229    -0.305476     0.332298    0.213367     0.105484   -0.278035  
 -0.345496     0.00292623   0.104341     0.0955004    0.0178377  -0.192886     0.0147171    0.010774     0.247827   -0.393531    0.143513     0.252669   -0.224372    -0.11882       0.139195    -0.0650634    0.248562     0.148636   -0.120806     0.00430964  -0.108642    -0.0112597    0.21027     0.020134     0.493322    0.0209696 
  0.767301     0.36597      0.232269    -0.204909    -0.449629    0.568174     0.278245    -0.203945     0.615943    0.157794    0.28028     -0.104703   -0.0338773   -0.71197       0.204657     0.102114     0.548463    -0.44853     0.0677942    0.217716    -0.501448    -0.0213485    0.13445     0.0735837   -0.0831288  -0.160486  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.405486
INFO: iteration 2, average log likelihood -1.405474
INFO: iteration 3, average log likelihood -1.405463
INFO: iteration 4, average log likelihood -1.405452
INFO: iteration 5, average log likelihood -1.405442
INFO: iteration 6, average log likelihood -1.405431
INFO: iteration 7, average log likelihood -1.405421
INFO: iteration 8, average log likelihood -1.405411
INFO: iteration 9, average log likelihood -1.405401
INFO: iteration 10, average log likelihood -1.405392
INFO: EM with 100000 data points 10 iterations avll -1.405392
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
