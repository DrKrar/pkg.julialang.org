>>> 'Pkg.add("GaussianMixtures")' log
INFO: Installing ArrayViews v0.6.4
INFO: Installing Blosc v0.1.5
INFO: Installing Clustering v0.5.0
INFO: Installing Distances v0.3.1
INFO: Installing Distributions v0.9.0
INFO: Installing Docile v0.5.23
INFO: Installing FileIO v0.0.6
INFO: Installing GaussianMixtures v0.0.12
INFO: Installing HDF5 v0.6.1
INFO: Installing JLD v0.6.0
INFO: Installing PDMats v0.4.1
INFO: Installing ScikitLearnBase v0.0.5
INFO: Installing StatsBase v0.8.1
INFO: Installing StatsFuns v0.2.2
INFO: Building Blosc
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.4.5
Commit 2ac304d (2016-03-18 00:58 UTC)
Platform Info:
  System: Linux (x86_64-unknown-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.3
INFO: Testing GaussianMixtures
INFO: Recompiling stale cache file /home/vagrant/.julia/lib/v0.4/HDF5.ji for module HDF5.
INFO: Recompiling stale cache file /home/vagrant/.julia/lib/v0.4/JLD.ji for module JLD.
INFO: Testing Data
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.849979e+03
      1       1.376107e+03      -4.738725e+02 |        6
      2       1.276815e+03      -9.929135e+01 |        2
      3       1.238907e+03      -3.790824e+01 |        2
      4       1.233667e+03      -5.240520e+00 |        4
      5       1.201150e+03      -3.251672e+01 |        4
      6       1.158402e+03      -4.274847e+01 |        2
      7       1.130320e+03      -2.808126e+01 |        2
      8       1.117574e+03      -1.274635e+01 |        3
      9       1.067277e+03      -5.029711e+01 |        4
     10       9.550770e+02      -1.121998e+02 |        6
     11       9.026662e+02      -5.241088e+01 |        2
     12       8.961275e+02      -6.538659e+00 |        3
     13       8.787077e+02      -1.741983e+01 |        2
     14       8.403234e+02      -3.838427e+01 |        2
     15       8.136883e+02      -2.663511e+01 |        2
     16       7.972853e+02      -1.640299e+01 |        0
     17       7.972853e+02       0.000000e+00 |        0
K-means converged with 17 iterations (objv = 797.2853036420329)
INFO: K-means with 272 data points using 17 iterations
11.3 data points per parameter
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
INFO: EM with 272 data points 0 iterations avll -2.054830
5.8 data points per parameter
INFO: iteration 1, lowerbound -3.808658
INFO: iteration 2, lowerbound -3.689993
INFO: iteration 3, lowerbound -3.557616
INFO: iteration 4, lowerbound -3.400950
INFO: iteration 5, lowerbound -3.236448
INFO: iteration 6, lowerbound -3.084259
INFO: dropping number of Gaussions to 7
INFO: iteration 7, lowerbound -2.952876
INFO: dropping number of Gaussions to 6
INFO: iteration 8, lowerbound -2.843437
INFO: dropping number of Gaussions to 5
INFO: iteration 9, lowerbound -2.749415
INFO: dropping number of Gaussions to 4
INFO: iteration 10, lowerbound -2.658172
INFO: iteration 11, lowerbound -2.577859
INFO: dropping number of Gaussions to 3
INFO: iteration 12, lowerbound -2.504898
INFO: iteration 13, lowerbound -2.438422
INFO: iteration 14, lowerbound -2.388401
INFO: iteration 15, lowerbound -2.350943
INFO: iteration 16, lowerbound -2.324193
INFO: iteration 17, lowerbound -2.309489
INFO: iteration 18, lowerbound -2.308596
INFO: dropping number of Gaussions to 2
INFO: iteration 19, lowerbound -2.302915
INFO: iteration 20, lowerbound -2.299259
INFO: iteration 21, lowerbound -2.299256
INFO: iteration 22, lowerbound -2.299254
INFO: iteration 23, lowerbound -2.299254
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
[Sun 29 May 2016 08:55:35 AM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Sun 29 May 2016 08:55:36 AM UTC: K-means with 272 data points using 17 iterations
11.3 data points per parameter
,Sun 29 May 2016 08:55:37 AM UTC: EM with 272 data points 0 iterations avll -2.054830
5.8 data points per parameter
,Sun 29 May 2016 08:55:37 AM UTC: GMM converted to Variational GMM
,Sun 29 May 2016 08:55:38 AM UTC: iteration 1, lowerbound -3.808658
,Sun 29 May 2016 08:55:38 AM UTC: iteration 2, lowerbound -3.689993
,Sun 29 May 2016 08:55:38 AM UTC: iteration 3, lowerbound -3.557616
,Sun 29 May 2016 08:55:38 AM UTC: iteration 4, lowerbound -3.400950
,Sun 29 May 2016 08:55:38 AM UTC: iteration 5, lowerbound -3.236448
,Sun 29 May 2016 08:55:38 AM UTC: iteration 6, lowerbound -3.084259
,Sun 29 May 2016 08:55:39 AM UTC: dropping number of Gaussions to 7
,Sun 29 May 2016 08:55:39 AM UTC: iteration 7, lowerbound -2.952876
,Sun 29 May 2016 08:55:39 AM UTC: dropping number of Gaussions to 6
,Sun 29 May 2016 08:55:39 AM UTC: iteration 8, lowerbound -2.843437
,Sun 29 May 2016 08:55:39 AM UTC: dropping number of Gaussions to 5
,Sun 29 May 2016 08:55:39 AM UTC: iteration 9, lowerbound -2.749415
,Sun 29 May 2016 08:55:39 AM UTC: dropping number of Gaussions to 4
,Sun 29 May 2016 08:55:39 AM UTC: iteration 10, lowerbound -2.658172
,Sun 29 May 2016 08:55:39 AM UTC: iteration 11, lowerbound -2.577859
,Sun 29 May 2016 08:55:39 AM UTC: dropping number of Gaussions to 3
,Sun 29 May 2016 08:55:39 AM UTC: iteration 12, lowerbound -2.504898
,Sun 29 May 2016 08:55:39 AM UTC: iteration 13, lowerbound -2.438422
,Sun 29 May 2016 08:55:39 AM UTC: iteration 14, lowerbound -2.388401
,Sun 29 May 2016 08:55:39 AM UTC: iteration 15, lowerbound -2.350943
,Sun 29 May 2016 08:55:39 AM UTC: iteration 16, lowerbound -2.324193
,Sun 29 May 2016 08:55:39 AM UTC: iteration 17, lowerbound -2.309489
,Sun 29 May 2016 08:55:39 AM UTC: iteration 18, lowerbound -2.308596
,Sun 29 May 2016 08:55:39 AM UTC: dropping number of Gaussions to 2
,Sun 29 May 2016 08:55:39 AM UTC: iteration 19, lowerbound -2.302915
,Sun 29 May 2016 08:55:39 AM UTC: iteration 20, lowerbound -2.299259
,Sun 29 May 2016 08:55:39 AM UTC: iteration 21, lowerbound -2.299256
,Sun 29 May 2016 08:55:39 AM UTC: iteration 22, lowerbound -2.299254
,Sun 29 May 2016 08:55:39 AM UTC: iteration 23, lowerbound -2.299254
,Sun 29 May 2016 08:55:39 AM UTC: iteration 24, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 25, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 26, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 27, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 28, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 29, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 30, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 31, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 32, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 33, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 34, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 35, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 36, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 37, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 38, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 39, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 40, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 41, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 42, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 43, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 44, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 45, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 46, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 47, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 48, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 49, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: iteration 50, lowerbound -2.299253
,Sun 29 May 2016 08:55:39 AM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.04509222601388,95.95490777398612]
β = [178.04509222601388,95.95490777398612]
m = [4.25030073326991 79.28686694436182
 2.00022925777537 53.85198717246129]
ν = [180.04509222601388,97.95490777398612]
W = [
[0.18404155547484732 -0.007644049042327304
 0.0 0.008581705166333307],

[0.3758763611948433 -0.008953123827346272
 0.0 0.012748664777409381]]
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 100000.00000000001
avll from stats: -0.9807853944913821
avll from llpg:  -0.9807853944913817
ERROR: LoadError: LoadError: OutOfMemoryError()
 in avll at /home/vagrant/.julia/v0.4/GaussianMixtures/src/train.jl:336
 [inlined code] from /home/vagrant/.julia/v0.4/GaussianMixtures/test/train.jl:15
 in anonymous at no file:0
 in include at ./boot.jl:261
 in include_from_node1 at ./loading.jl:320
 in include at ./boot.jl:261
 in include_from_node1 at ./loading.jl:320
 in process_options at ./client.jl:280
 in _start at ./client.jl:378
while loading /home/vagrant/.julia/v0.4/GaussianMixtures/test/train.jl, in expression starting on line 2
while loading /home/vagrant/.julia/v0.4/GaussianMixtures/test/runtests.jl, in expression starting on line 7
==========================[ ERROR: GaussianMixtures ]===========================

failed process: Process(`/home/vagrant/julia/bin/julia --check-bounds=yes --code-coverage=none --color=no /home/vagrant/.julia/v0.4/GaussianMixtures/test/runtests.jl`, ProcessExited(1)) [1]

================================================================================
ERROR: GaussianMixtures had test errors
 in error at ./error.jl:21
 in test at pkg/entry.jl:803
 in anonymous at pkg/dir.jl:31
 in cd at file.jl:22
 in cd at pkg/dir.jl:31
 in test at pkg.jl:71
 in process_options at ./client.jl:257
 in _start at ./client.jl:378

>>> End of log
