>>> 'Pkg.add("GradientBoost")' log
INFO: Cloning cache of GradientBoost from git://github.com/svs14/GradientBoost.jl.git
INFO: Installing ArrayViews v0.6.3
INFO: Installing DataStructures v0.3.12
INFO: Installing DecisionTree v0.3.9
INFO: Installing Distributions v0.8.5
INFO: Installing Docile v0.5.17
INFO: Installing FactCheck v0.4.0
INFO: Installing GLM v0.4.7
INFO: Installing GradientBoost v0.0.1
INFO: Installing NumericFuns v0.2.3
INFO: Installing PDMats v0.3.5
INFO: Installing Reexport v0.0.3
INFO: Installing StatsBase v0.7.2
INFO: Installing StatsFuns v0.1.3
INFO: Package database updated

>>> 'Pkg.test("GradientBoost")' log
Julia Version 0.4.0-pre+7304
Commit b3a1be5 (2015-09-05 13:59 UTC)
Platform Info:
  System: Linux (x86_64-unknown-linux-gnu)
  CPU: Intel(R) Core(TM) i5-2500K CPU @ 3.30GHz
  WORD_SIZE: 64
  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Sandybridge)
  LAPACK: libopenblas
  LIBM: libopenlibm
  LLVM: libLLVM-3.3
INFO: Testing GradientBoost

WARNING: deprecated syntax "{a=>b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/src/gb_dt.jl:28.
Use "Dict{Any,Any}(a=>b, ...)" instead.
WARNING: The `=>` syntax is deprecated, use `-->` instead
Util functions
  > err_must_be_overriden throws an error
  > weighted_median works
  > holdout returns proportional partitions
WARNING: int(x::AbstractFloat) is deprecated, use round(Int,x) instead.
 in depwarn at ./deprecated.jl:73
 in int at deprecated.jl:50
 in holdout at /home/vagrant/.julia/v0.4/GradientBoost/src/util.jl:41
 in anonymous at /home/vagrant/.julia/v0.4/GradientBoost/test/test_util.jl:44
 in context at /home/vagrant/.julia/v0.4/FactCheck/src/FactCheck.jl:474
 in anonymous at /home/vagrant/.julia/v0.4/GradientBoost/test/test_util.jl:41
 in facts at /home/vagrant/.julia/v0.4/FactCheck/src/FactCheck.jl:448
 in include at ./boot.jl:260
 in include_from_node1 at ./loading.jl:271
 in include at ./boot.jl:260
 in include_from_node1 at ./loading.jl:271
 in process_options at ./client.jl:308
 in _start at ./client.jl:411
while loading /home/vagrant/.julia/v0.4/GradientBoost/test/test_util.jl, in expression starting on line 6
12 facts verified.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:14.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:21.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:28.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:66.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:71.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:76.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:82.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:87.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:92.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:100.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:111.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:116.
Use "Any[a,b, ...]" instead.
Loss functions
  > not implemented functions throw an error
  > LeastSquares loss works
  > LeastSquares negative_gradient works
  > LeastSquares minimizing_scalar works
  > LeastAbsoluteDeviation loss works
  > LeastAbsoluteDeviation negative_gradient works
  > LeastAbsoluteDeviation minimizing_scalar works
  > BinomialDeviance loss works
    Failure :: (line:-1) :: BinomialDeviance loss works :: fact was false
      Expression: loss(lf,y_examples[i],y_pred_examples[i]) --> roughly(expected[i])
        Expected: 0.6265233750364456 ≅ 0.626523
    Failure :: (line:-1) :: BinomialDeviance loss works :: fact was false
      Expression: loss(lf,y_examples[i],y_pred_examples[i]) --> roughly(expected[i])
        Expected: 1.6265233750364456 ≅ 1.626523
    Failure :: (line:-1) :: BinomialDeviance loss works :: fact was false
      Expression: loss(lf,y_examples[i],y_pred_examples[i]) --> roughly(expected[i])
        Expected: 1.6265233750364456 ≅ 1.626523
    Failure :: (line:-1) :: BinomialDeviance loss works :: fact was false
      Expression: loss(lf,y_examples[i],y_pred_examples[i]) --> roughly(expected[i])
        Expected: 2.6265233750364456 ≅ 2.626523
    Failure :: (line:-1) :: BinomialDeviance loss works :: fact was false
      Expression: loss(lf,y_examples[i],y_pred_examples[i]) --> roughly(expected[i])
        Expected: 2.6265233750364456 ≅ 2.626523
  > BinomialDeviance negative_gradient works
    Failure :: (line:-1) :: BinomialDeviance negative_gradient works :: fact was false
      Expression: negative_gradient(lf,y_examples[i],y_pred_examples[i]) --> roughly(expected[i])
        Expected: [0.2689414213699951,0.2689414213699951] ≅ [0.268941,0.268941]
    Failure :: (line:-1) :: BinomialDeviance negative_gradient works :: fact was false
      Expression: negative_gradient(lf,y_examples[i],y_pred_examples[i]) --> roughly(expected[i])
        Expected: [0.2689414213699951,-0.7310585786300049] ≅ [0.268941,-0.731059]
    Failure :: (line:-1) :: BinomialDeviance negative_gradient works :: fact was false
      Expression: negative_gradient(lf,y_examples[i],y_pred_examples[i]) --> roughly(expected[i])
        Expected: [-0.7310585786300049,0.2689414213699951] ≅ [-0.731059,0.268941]
    Failure :: (line:-1) :: BinomialDeviance negative_gradient works :: fact was false
      Expression: negative_gradient(lf,y_examples[i],y_pred_examples[i]) --> roughly(expected[i])
        Expected: [-0.7310585786300049,-0.7310585786300049] ≅ [-0.731059,-0.731059]
    Failure :: (line:-1) :: BinomialDeviance negative_gradient works :: fact was false
      Expression: negative_gradient(lf,y_examples[i],y_pred_examples[i]) --> roughly(expected[i])
        Expected: [-0.7310585786300049,-0.7310585786300049] ≅ [-0.731059,-0.731059]
  > BinomialDeviance minimizing_scalar works
Out of 48 total facts:
  Verified: 38
  Failed:   10
ERROR: LoadError: LoadError: Incorrect usage of @fact: typeof(model) <: GBModel => true
 in include at ./boot.jl:260
 in include_from_node1 at ./loading.jl:271
 in include at ./boot.jl:260
 in include_from_node1 at ./loading.jl:271
 in process_options at ./client.jl:308
 in _start at ./client.jl:411
while loading /home/vagrant/.julia/v0.4/GradientBoost/test/test_gb.jl, in expression starting on line 45
while loading /home/vagrant/.julia/v0.4/GradientBoost/test/runtests.jl, in expression starting on line 8
============================[ ERROR: GradientBoost ]============================

failed process: Process(`/home/vagrant/julia/bin/julia --check-bounds=yes --code-coverage=none --color=no /home/vagrant/.julia/v0.4/GradientBoost/test/runtests.jl`, ProcessExited(1)) [1]

================================================================================
ERROR: GradientBoost had test errors
 in error at ./error.jl:21
 [inlined code] from pkg/entry.jl:753
 in __test#412__ at no file:0
 in test at no file
 in anonymous at pkg/dir.jl:31
 in cd at file.jl:22
 [inlined code] from pkg/dir.jl:31
 in __cd#396__ at no file:0
 in cd at no file
 in __test#420__ at no file
 in test at no file
 in process_options at ./client.jl:284
 in _start at ./client.jl:411

>>> End of log
