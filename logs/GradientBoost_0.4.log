>>> 'Pkg.add("GradientBoost")' log
INFO: Cloning cache of GradientBoost from git://github.com/svs14/GradientBoost.jl.git
INFO: Installing DataStructures v0.3.11
INFO: Installing DecisionTree v0.3.8
INFO: Installing Distributions v0.8.3
INFO: Installing FactCheck v0.3.1
INFO: Installing GLM v0.4.6
INFO: Installing GradientBoost v0.0.1
INFO: Installing NumericFuns v0.2.3
INFO: Installing PDMats v0.3.5
INFO: Package database updated

>>> 'Pkg.test("GradientBoost")' log
Julia Version 0.4.0-dev+6661
Commit 9c3cb36 (2015-08-12 03:21 UTC)
Platform Info:
  System: Linux (x86_64-unknown-linux-gnu)
  CPU: Intel(R) Core(TM) i7-4960HQ CPU @ 2.60GHz
  WORD_SIZE: 64
  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Haswell)
  LAPACK: libopenblas
  LIBM: libopenlibm
  LLVM: libLLVM-3.3
INFO: Testing GradientBoost

WARNING: deprecated syntax "{a=>b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/src/gb_dt.jl:28.
Use "Dict{Any,Any}(a=>b, ...)" instead.
WARNING: module DataStructures should explicitly import < from Base
WARNING: module DataStructures should explicitly import <= from Base
WARNING: the `=>` syntax is deprecated, use `-->` instead
Util functions
     - err_must_be_overriden throws an error
     - weighted_median works
     - holdout returns proportional partitions
WARNING: int(x::AbstractFloat) is deprecated, use round(Int,x) instead.
 in depwarn at ./deprecated.jl:63
 in int at deprecated.jl:49
 in holdout at /home/vagrant/.julia/v0.4/GradientBoost/src/util.jl:41
 in anonymous at /home/vagrant/.julia/v0.4/GradientBoost/test/test_util.jl:44
 in context at /home/vagrant/.julia/v0.4/FactCheck/src/FactCheck.jl:417
 in anonymous at /home/vagrant/.julia/v0.4/GradientBoost/test/test_util.jl:41
 in facts at /home/vagrant/.julia/v0.4/FactCheck/src/FactCheck.jl:391
 in include at ./boot.jl:254
 in include_from_node1 at ./loading.jl:264
 in include at ./boot.jl:254
 in include_from_node1 at ./loading.jl:264
 in process_options at ./client.jl:308
 in _start at ./client.jl:411
while loading /home/vagrant/.julia/v0.4/GradientBoost/test/test_util.jl, in expression starting on line 6
12 facts verified.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:14.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:21.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:28.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:66.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:71.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:76.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:82.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:87.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:92.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:100.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:111.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:116.
Use "Any[a,b, ...]" instead.
Loss functions
     - not implemented functions throw an error
     - LeastSquares loss works
     - LeastSquares negative_gradient works
     - LeastSquares minimizing_scalar works
     - LeastAbsoluteDeviation loss works
     - LeastAbsoluteDeviation negative_gradient works
     - LeastAbsoluteDeviation minimizing_scalar works
     - BinomialDeviance loss works
  Failure   :: (line:-1) :: BinomialDeviance loss works :: Expected: 0.6265233750364456 ≅ 0.626523
    loss(lf,y_examples[i],y_pred_examples[i]) --> roughly(expected[i])
  Failure   :: (line:-1) :: BinomialDeviance loss works :: Expected: 1.6265233750364456 ≅ 1.626523
    loss(lf,y_examples[i],y_pred_examples[i]) --> roughly(expected[i])
  Failure   :: (line:-1) :: BinomialDeviance loss works :: Expected: 1.6265233750364456 ≅ 1.626523
    loss(lf,y_examples[i],y_pred_examples[i]) --> roughly(expected[i])
  Failure   :: (line:-1) :: BinomialDeviance loss works :: Expected: 2.6265233750364456 ≅ 2.626523
    loss(lf,y_examples[i],y_pred_examples[i]) --> roughly(expected[i])
  Failure   :: (line:-1) :: BinomialDeviance loss works :: Expected: 2.6265233750364456 ≅ 2.626523
    loss(lf,y_examples[i],y_pred_examples[i]) --> roughly(expected[i])
     - BinomialDeviance negative_gradient works
  Failure   :: (line:-1) :: BinomialDeviance negative_gradient works :: Expected: [0.2689414213699951,0.2689414213699951] ≅ [0.268941,0.268941]
    negative_gradient(lf,y_examples[i],y_pred_examples[i]) --> roughly(expected[i])
  Failure   :: (line:-1) :: BinomialDeviance negative_gradient works :: Expected: [0.2689414213699951,-0.7310585786300049] ≅ [0.268941,-0.731059]
    negative_gradient(lf,y_examples[i],y_pred_examples[i]) --> roughly(expected[i])
  Failure   :: (line:-1) :: BinomialDeviance negative_gradient works :: Expected: [-0.7310585786300049,0.2689414213699951] ≅ [-0.731059,0.268941]
    negative_gradient(lf,y_examples[i],y_pred_examples[i]) --> roughly(expected[i])
  Failure   :: (line:-1) :: BinomialDeviance negative_gradient works :: Expected: [-0.7310585786300049,-0.7310585786300049] ≅ [-0.731059,-0.731059]
    negative_gradient(lf,y_examples[i],y_pred_examples[i]) --> roughly(expected[i])
  Failure   :: (line:-1) :: BinomialDeviance negative_gradient works :: Expected: [-0.7310585786300049,-0.7310585786300049] ≅ [-0.731059,-0.731059]
    negative_gradient(lf,y_examples[i],y_pred_examples[i]) --> roughly(expected[i])
     - BinomialDeviance minimizing_scalar works
Out of 48 total facts:
  Verified: 38
  Failed:   10
ERROR: LoadError: LoadError: Incorrect usage of @fact: typeof(model) <: GBModel => true
 in include at ./boot.jl:254
 in include_from_node1 at ./loading.jl:264
 in include at ./boot.jl:254
 in include_from_node1 at ./loading.jl:264
 in process_options at ./client.jl:308
 in _start at ./client.jl:411
while loading /home/vagrant/.julia/v0.4/GradientBoost/test/test_gb.jl, in expression starting on line 45
while loading /home/vagrant/.julia/v0.4/GradientBoost/test/runtests.jl, in expression starting on line 8
============================[ ERROR: GradientBoost ]============================

failed process: Process(`/home/vagrant/julia/bin/julia --check-bounds=yes --code-coverage=none --color=no /home/vagrant/.julia/v0.4/GradientBoost/test/runtests.jl`, ProcessExited(1)) [1]

================================================================================
ERROR: GradientBoost had test errors
 in error at ./error.jl:21
 in test at pkg/entry.jl:746
 in anonymous at pkg/dir.jl:31
 in cd at file.jl:22
 in cd at pkg/dir.jl:31
 in test at pkg.jl:71
 in process_options at ./client.jl:284
 in _start at ./client.jl:411

>>> End of log
