>>> 'Pkg.add("GradientBoost")' log
INFO: Cloning cache of GradientBoost from git://github.com/svs14/GradientBoost.jl.git
INFO: Installing DataStructures v0.3.11
INFO: Installing DecisionTree v0.3.8
INFO: Installing Distributions v0.8.0
INFO: Installing FactCheck v0.2.8
INFO: Installing GLM v0.4.6
INFO: Installing GradientBoost v0.0.1
INFO: Installing NumericFuns v0.2.3
INFO: Installing PDMats v0.3.4
INFO: Package database updated
INFO: METADATA is out-of-date â€” you may not have the latest version of GradientBoost
INFO: Use `Pkg.update()` to get the latest versions of your packages

>>> 'Pkg.test("GradientBoost")' log
Julia Version 0.4.0-dev+6253
Commit a97b24b (2015-07-26 15:40 UTC)
Platform Info:
  System: Linux (x86_64-unknown-linux-gnu)
  CPU: Intel(R) Core(TM) i7-4960HQ CPU @ 2.60GHz
  WORD_SIZE: 64
  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas
  LIBM: libopenlibm
  LLVM: libLLVM-3.3
INFO: Testing GradientBoost

WARNING: deprecated syntax "{a=>b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/src/gb_dt.jl:28.
Use "Dict{Any,Any}(a=>b, ...)" instead.
Util functions
     - err_must_be_overriden throws an error
     - weighted_median works
     - holdout returns proportional partitions
WARNING: int(x::FloatingPoint) is deprecated, use round(Int,x) instead.
 in depwarn at ./deprecated.jl:63
 in int at deprecated.jl:49
 in holdout at /home/vagrant/.julia/v0.4/GradientBoost/src/util.jl:41
 in anonymous at /home/vagrant/.julia/v0.4/GradientBoost/test/test_util.jl:44
 in context at /home/vagrant/.julia/v0.4/FactCheck/src/FactCheck.jl:341
 in anonymous at /home/vagrant/.julia/v0.4/GradientBoost/test/test_util.jl:41
 in facts at /home/vagrant/.julia/v0.4/FactCheck/src/FactCheck.jl:315
 in include at ./boot.jl:254
 in include_from_node1 at ./loading.jl:197
 in include at ./boot.jl:254
 in include_from_node1 at ./loading.jl:197
 in process_options at ./client.jl:308
 in _start at ./client.jl:411
while loading /home/vagrant/.julia/v0.4/GradientBoost/test/test_util.jl, in expression starting on line 6
12 facts verified.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:14.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:21.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:28.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:66.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:71.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:76.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:82.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:87.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:92.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:100.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:111.
Use "Any[a,b, ...]" instead.

WARNING: deprecated syntax "{a,b, ...}" at /home/vagrant/.julia/v0.4/GradientBoost/test/test_loss.jl:116.
Use "Any[a,b, ...]" instead.
Loss functions
     - not implemented functions throw an error
     - LeastSquares loss works
     - LeastSquares negative_gradient works
     - LeastSquares minimizing_scalar works
     - LeastAbsoluteDeviation loss works
     - LeastAbsoluteDeviation negative_gradient works
     - LeastAbsoluteDeviation minimizing_scalar works
     - BinomialDeviance loss works
     - BinomialDeviance negative_gradient works
     - BinomialDeviance minimizing_scalar works
48 facts verified.
ERROR: LoadError: LoadError: Incorrect usage of @fact: typeof(model) <: GBModel => true
 in include at ./boot.jl:254
 in include_from_node1 at ./loading.jl:197
 in include at ./boot.jl:254
 in include_from_node1 at ./loading.jl:197
 in process_options at ./client.jl:308
 in _start at ./client.jl:411
while loading /home/vagrant/.julia/v0.4/GradientBoost/test/test_gb.jl, in expression starting on line 45
while loading /home/vagrant/.julia/v0.4/GradientBoost/test/runtests.jl, in expression starting on line 8
============================[ ERROR: GradientBoost ]============================

failed process: Process(`/home/vagrant/julia/bin/julia --check-bounds=yes --code-coverage=none --color=no /home/vagrant/.julia/v0.4/GradientBoost/test/runtests.jl`, ProcessExited(1)) [1]

================================================================================
INFO: No packages to install, update or remove
ERROR: GradientBoost had test errors
 in error at ./error.jl:21
 in test at pkg/entry.jl:746
 in anonymous at pkg/dir.jl:31
 in cd at file.jl:22
 in cd at pkg/dir.jl:31
 in test at pkg.jl:71
 in process_options at ./client.jl:284
 in _start at ./client.jl:411

>>> End of log
