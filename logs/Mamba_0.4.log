>>> 'Pkg.add("Mamba")' log
INFO: Cloning cache of BinDeps from git://github.com/JuliaLang/BinDeps.jl.git
INFO: Cloning cache of Cairo from git://github.com/JuliaLang/Cairo.jl.git
INFO: Cloning cache of Graphics from git://github.com/JuliaLang/Graphics.jl.git
INFO: Cloning cache of Graphs from git://github.com/JuliaLang/Graphs.jl.git
INFO: Cloning cache of Mamba from git://github.com/brian-j-smith/Mamba.jl.git
INFO: Cloning cache of SHA from git://github.com/staticfloat/SHA.jl.git
INFO: Cloning cache of URIParser from git://github.com/JuliaWeb/URIParser.jl.git
INFO: Installing ArrayViews v0.6.4
INFO: Installing BinDeps v0.3.19
INFO: Installing Cairo v0.2.31
INFO: Installing Calculus v0.1.14
INFO: Installing Codecs v0.1.5
INFO: Installing ColorTypes v0.2.0
INFO: Installing Colors v0.6.0
INFO: Installing Compose v0.3.18
INFO: Installing Contour v0.0.8
INFO: Installing DataArrays v0.2.20
INFO: Installing DataFrames v0.6.10
INFO: Installing DataStructures v0.3.13
INFO: Installing Dates v0.4.4
INFO: Installing Distances v0.2.1
INFO: Installing Distributions v0.8.7
INFO: Installing Docile v0.5.19
INFO: Installing DualNumbers v0.1.5
INFO: Installing FixedPointNumbers v0.1.1
INFO: Installing GZip v0.2.18
INFO: Installing Gadfly v0.3.18
INFO: Installing Graphics v0.1.3
INFO: Installing Graphs v0.6.0
INFO: Installing Grid v0.4.0
INFO: Installing Hexagons v0.0.4
INFO: Installing ImmutableArrays v0.0.11
INFO: Installing Iterators v0.1.9
INFO: Installing KernelDensity v0.1.2
INFO: Installing Loess v0.0.5
INFO: Installing Mamba v0.7.2
INFO: Installing NaNMath v0.1.1
INFO: Installing Optim v0.4.4
INFO: Installing PDMats v0.3.6
INFO: Installing Reexport v0.0.3
INFO: Installing SHA v0.1.2
INFO: Installing Showoff v0.0.6
INFO: Installing SortingAlgorithms v0.0.6
INFO: Installing StatsBase v0.7.4
INFO: Installing StatsFuns v0.2.0
INFO: Installing URIParser v0.1.1
INFO: Installing WoodburyMatrices v0.1.2
INFO: Building Cairo
INFO: Package database updated

>>> 'Pkg.test("Mamba")' log
Julia Version 0.4.1
Commit cbe1bee* (2015-11-08 10:33 UTC)
Platform Info:
  System: Linux (x86_64-unknown-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.3
INFO: Testing Mamba
Running tests:

>>> Testing ../doc/tutorial/line.jl

digraph MambaModel {
	"y" [shape="ellipse", fillcolor="gray85", style="filled"];
	"mu" [shape="diamond", fillcolor="gray85", style="filled"];
		"mu" -> "y";
	"s2" [shape="ellipse"];
		"s2" -> "y";
	"beta" [shape="ellipse"];
		"beta" -> "mu";
	"xmat" [shape="box", fillcolor="gray85", style="filled"];
		"xmat" -> "mu";
}
MCMC Simulation of 10000 Iterations x 3 Chains...

Chain 1:   0% [0:32:39 of 0:32:41 remaining]
Chain 1:  10% [0:00:28 of 0:00:31 remaining]
Chain 1:  20% [0:00:15 of 0:00:19 remaining]
Chain 1:  30% [0:00:10 of 0:00:15 remaining]
Chain 1:  40% [0:00:07 of 0:00:12 remaining]
Chain 1:  50% [0:00:05 of 0:00:11 remaining]
Chain 1:  60% [0:00:04 of 0:00:10 remaining]
Chain 1:  70% [0:00:03 of 0:00:09 remaining]
Chain 1:  80% [0:00:02 of 0:00:09 remaining]
Chain 1:  90% [0:00:01 of 0:00:08 remaining]
Chain 1: 100% [0:00:00 of 0:00:08 remaining]

Chain 2:   0% [0:00:04 of 0:00:04 remaining]
Chain 2:  10% [0:00:04 of 0:00:04 remaining]
Chain 2:  20% [0:00:04 of 0:00:04 remaining]
Chain 2:  30% [0:00:03 of 0:00:04 remaining]
Chain 2:  40% [0:00:02 of 0:00:04 remaining]
Chain 2:  50% [0:00:02 of 0:00:04 remaining]
Chain 2:  60% [0:00:02 of 0:00:04 remaining]
Chain 2:  70% [0:00:01 of 0:00:04 remaining]
Chain 2:  80% [0:00:01 of 0:00:04 remaining]
Chain 2:  90% [0:00:00 of 0:00:05 remaining]
Chain 2: 100% [0:00:00 of 0:00:05 remaining]

Chain 3:   0% [0:00:03 of 0:00:03 remaining]
Chain 3:  10% [0:00:03 of 0:00:03 remaining]
Chain 3:  20% [0:00:02 of 0:00:03 remaining]
Chain 3:  30% [0:00:02 of 0:00:03 remaining]
Chain 3:  40% [0:00:02 of 0:00:03 remaining]
Chain 3:  50% [0:00:02 of 0:00:03 remaining]
Chain 3:  60% [0:00:01 of 0:00:03 remaining]
Chain 3:  70% [0:00:01 of 0:00:03 remaining]
Chain 3:  80% [0:00:01 of 0:00:03 remaining]
Chain 3:  90% [0:00:00 of 0:00:03 remaining]
Chain 3: 100% [0:00:00 of 0:00:03 remaining]

MCMC Simulation of 10000 Iterations x 3 Chains...

Chain 1:   0% [0:00:19 of 0:00:19 remaining]
Chain 1:  10% [0:00:08 of 0:00:09 remaining]
Chain 1:  20% [0:00:08 of 0:00:10 remaining]
Chain 1:  30% [0:00:07 of 0:00:10 remaining]
Chain 1:  40% [0:00:06 of 0:00:10 remaining]
Chain 1:  50% [0:00:05 of 0:00:11 remaining]
Chain 1:  60% [0:00:04 of 0:00:11 remaining]
Chain 1:  70% [0:00:03 of 0:00:10 remaining]
Chain 1:  80% [0:00:02 of 0:00:09 remaining]
Chain 1:  90% [0:00:01 of 0:00:09 remaining]
Chain 1: 100% [0:00:00 of 0:00:09 remaining]

Chain 2:   0% [0:00:05 of 0:00:05 remaining]
Chain 2:  10% [0:00:05 of 0:00:06 remaining]
Chain 2:  20% [0:00:06 of 0:00:08 remaining]
Chain 2:  30% [0:00:06 of 0:00:09 remaining]
Chain 2:  40% [0:00:06 of 0:00:10 remaining]
Chain 2:  50% [0:00:04 of 0:00:09 remaining]
Chain 2:  60% [0:00:03 of 0:00:08 remaining]
Chain 2:  70% [0:00:02 of 0:00:08 remaining]
Chain 2:  80% [0:00:02 of 0:00:08 remaining]
Chain 2:  90% [0:00:01 of 0:00:08 remaining]
Chain 2: 100% [0:00:00 of 0:00:08 remaining]

Chain 3:   0% [0:00:12 of 0:00:12 remaining]
Chain 3:  10% [0:00:06 of 0:00:07 remaining]
Chain 3:  20% [0:00:04 of 0:00:05 remaining]
Chain 3:  30% [0:00:03 of 0:00:05 remaining]
Chain 3:  40% [0:00:03 of 0:00:05 remaining]
Chain 3:  50% [0:00:02 of 0:00:05 remaining]
Chain 3:  60% [0:00:02 of 0:00:05 remaining]
Chain 3:  70% [0:00:01 of 0:00:04 remaining]
Chain 3:  80% [0:00:01 of 0:00:04 remaining]
Chain 3:  90% [0:00:00 of 0:00:04 remaining]
Chain 3: 100% [0:00:00 of 0:00:04 remaining]

MCMC Simulation of 10000 Iterations x 3 Chains...

Chain 1:   0% [0:10:47 of 0:10:48 remaining]
Chain 1:  10% [0:00:06 of 0:00:07 remaining]
Chain 1:  20% [0:00:03 of 0:00:04 remaining]
Chain 1:  30% [0:00:02 of 0:00:03 remaining]
Chain 1:  40% [0:00:01 of 0:00:02 remaining]
Chain 1:  50% [0:00:01 of 0:00:02 remaining]
Chain 1:  60% [0:00:01 of 0:00:02 remaining]
Chain 1:  70% [0:00:00 of 0:00:02 remaining]
Chain 1:  80% [0:00:00 of 0:00:02 remaining]
Chain 1:  90% [0:00:00 of 0:00:01 remaining]
Chain 1: 100% [0:00:00 of 0:00:01 remaining]

Chain 2:   0% [0:00:00 of 0:00:00 remaining]
Chain 2:  10% [0:00:01 of 0:00:01 remaining]
Chain 2:  20% [0:00:01 of 0:00:01 remaining]
Chain 2:  30% [0:00:01 of 0:00:01 remaining]
Chain 2:  40% [0:00:00 of 0:00:01 remaining]
Chain 2:  50% [0:00:00 of 0:00:01 remaining]
Chain 2:  60% [0:00:00 of 0:00:01 remaining]
Chain 2:  70% [0:00:00 of 0:00:01 remaining]
Chain 2:  80% [0:00:00 of 0:00:01 remaining]
Chain 2:  90% [0:00:00 of 0:00:01 remaining]
Chain 2: 100% [0:00:00 of 0:00:01 remaining]

Chain 3:   0% [0:00:00 of 0:00:00 remaining]
Chain 3:  10% [0:00:00 of 0:00:00 remaining]
Chain 3:  20% [0:00:00 of 0:00:00 remaining]
Chain 3:  30% [0:00:00 of 0:00:00 remaining]
Chain 3:  40% [0:00:00 of 0:00:00 remaining]
Chain 3:  50% [0:00:00 of 0:00:00 remaining]
Chain 3:  60% [0:00:00 of 0:00:00 remaining]
Chain 3:  70% [0:00:00 of 0:00:01 remaining]
Chain 3:  80% [0:00:00 of 0:00:01 remaining]
Chain 3:  90% [0:00:00 of 0:00:01 remaining]
Chain 3: 100% [0:00:00 of 0:00:01 remaining]

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Gelman, Rubin, and Brooks Diagnostic:
              PSRF 97.5%
     beta[1] 1.009 1.010
     beta[2] 1.009 1.010
          s2 1.008 1.016
Multivariate 1.006   NaN

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Geweke Diagnostic:
First Window Fraction = 0.1
Second Window Fraction = 0.5

        Z-score p-value
beta[1]   1.237  0.2162
beta[2]  -1.568  0.1168
     s2   1.710  0.0872

        Z-score p-value
beta[1]  -1.457  0.1452
beta[2]   1.752  0.0797
     s2  -1.428  0.1534

        Z-score p-value
beta[1]   0.550  0.5824
beta[2]  -0.440  0.6597
     s2   0.583  0.5596

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Heidelberger and Welch Diagnostic:
Target Halfwidth Ratio = 0.1
Alpha = 0.05

        Burn-in Stationarity p-value    Mean     Halfwidth  Test
beta[1]     251            1  0.0680 0.57366275 0.053311283    1
beta[2]     738            1  0.0677 0.81285744 0.015404173    1
     s2     738            1  0.0700 1.00825202 0.094300432    1

        Burn-in Stationarity p-value    Mean     Halfwidth  Test
beta[1]     251            1  0.1356  0.6293320 0.065092099    0
beta[2]     251            1  0.0711  0.7934633 0.019215278    1
     s2     251            1  0.4435  1.4635400 0.588158612    0

        Burn-in Stationarity p-value    Mean     Halfwidth  Test
beta[1]     251            1  0.0515  0.5883602 0.058928034    0
beta[2]    1225            1  0.1479  0.8086080 0.018478999    1
     s2     251            1  0.6664  0.9942853 0.127959523    0

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Raftery and Lewis Diagnostic:
Quantile (q) = 0.025
Accuracy (r) = 0.005
Probability (s) = 0.95

        Thinning Burn-in    Total   Nmin Dependence Factor
beta[1]        2     267      17897 3746         4.7776295
beta[2]        2     267      17897 3746         4.7776295
     s2        2     257       8689 3746         2.3195408

        Thinning Burn-in    Total   Nmin Dependence Factor
beta[1]        4     271 2.1759×10⁴ 3746         5.8085958
beta[2]        4     275 2.8795×10⁴ 3746         7.6868660
     s2        2     257 8.3450×10³ 3746         2.2277096

        Thinning Burn-in    Total   Nmin Dependence Factor
beta[1]        2     269 2.0647×10⁴ 3746         5.5117459
beta[2]        2     263 1.4523×10⁴ 3746         3.8769354
     s2        2     255 7.8770×10³ 3746         2.1027763

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Empirical Posterior Estimates:
           Mean       SD       Naive SE       MCSE       ESS   
beta[1] 0.5971183 1.14894446 0.0095006014 0.016925598 4607.9743
beta[2] 0.8017036 0.34632566 0.0028637608 0.004793345 4875.0000
     s2 1.2203777 2.00876760 0.0166104638 0.101798287  389.3843

Quantiles:
            2.5%       25.0%       50.0%     75.0%     97.5%  
beta[1] -1.74343373 0.026573102 0.59122696 1.1878720 2.8308472
beta[2]  0.12168742 0.628297573 0.80357822 0.9719441 1.5051573
     s2  0.17091385 0.383671702 0.65371989 1.2206381 6.0313970

         95% Lower  95% Upper
beta[1] -1.75436235 2.8109571
beta[2]  0.09721501 1.4733163
     s2  0.08338409 3.8706865

           beta[1]      beta[2]        s2     
beta[1]  1.000000000 -0.905245029  0.027467317
beta[2] -0.905245029  1.000000000 -0.024489462
     s2  0.027467317 -0.024489462  1.000000000

           Lag 2       Lag 10        Lag 20       Lag 100   
beta[1] 0.24521566  -0.021411797 -0.0077424153  -0.044989417
beta[2] 0.20402485  -0.019107846  0.0033980453  -0.053869216
     s2 0.85931351   0.568056917  0.3248136852   0.024157524

           Lag 2       Lag 10        Lag 20       Lag 100   
beta[1] 0.28180489  -0.031007672    0.03930888  0.0394895028
beta[2] 0.25905976  -0.017946010    0.03613043  0.0227758214
     s2 0.92905843   0.761339226    0.58455868  0.0050215824

           Lag 2       Lag 10        Lag 20       Lag 100   
beta[1] 0.38634357 -0.0029361782  -0.032310111  0.0028806786
beta[2] 0.32822879 -0.0056670786  -0.020100663 -0.0062622517
     s2 0.68812720  0.2420402859   0.080495078 -0.0290205896

             Change Rate
     beta[1]       0.844
     beta[2]       0.844
          s2       1.000
Multivariate       1.000

MCMC Processing of 4875 Iterations x 3 Chains...

Chain 1:   0% [0:01:44 of 0:01:44 remaining]
Chain 1:  10% [0:00:02 of 0:00:02 remaining]
Chain 1:  20% [0:00:01 of 0:00:01 remaining]
Chain 1:  30% [0:00:01 of 0:00:01 remaining]
Chain 1:  40% [0:00:00 of 0:00:01 remaining]
Chain 1:  50% [0:00:00 of 0:00:01 remaining]
Chain 1:  60% [0:00:00 of 0:00:00 remaining]
Chain 1:  70% [0:00:00 of 0:00:00 remaining]
Chain 1:  80% [0:00:00 of 0:00:00 remaining]
Chain 1:  90% [0:00:00 of 0:00:00 remaining]
Chain 1: 100% [0:00:00 of 0:00:00 remaining]


Chain 2:   0% [0:00:00 of 0:00:00 remaining]
Chain 2:  10% [0:00:00 of 0:00:00 remaining]
Chain 2:  20% [0:00:00 of 0:00:00 remaining]
Chain 2:  30% [0:00:00 of 0:00:00 remaining]
Chain 2:  40% [0:00:00 of 0:00:00 remaining]
Chain 2:  50% [0:00:00 of 0:00:00 remaining]
Chain 2:  60% [0:00:00 of 0:00:00 remaining]
Chain 2:  70% [0:00:00 of 0:00:00 remaining]
Chain 2:  80% [0:00:00 of 0:00:00 remaining]
Chain 2:  90% [0:00:00 of 0:00:00 remaining]
Chain 2: 100% [0:00:00 of 0:00:00 remaining]


Chain 3:   0% [0:00:00 of 0:00:00 remaining]
Chain 3:  10% [0:00:00 of 0:00:00 remaining]
Chain 3:  20% [0:00:00 of 0:00:00 remaining]
Chain 3:  30% [0:00:00 of 0:00:00 remaining]
Chain 3:  40% [0:00:00 of 0:00:00 remaining]
Chain 3:  50% [0:00:00 of 0:00:00 remaining]
Chain 3:  60% [0:00:00 of 0:00:00 remaining]
Chain 3:  70% [0:00:00 of 0:00:00 remaining]
Chain 3:  80% [0:00:00 of 0:00:00 remaining]
Chain 3:  90% [0:00:00 of 0:00:00 remaining]
Chain 3: 100% [0:00:00 of 0:00:00 remaining]


      DIC    Effective Parameters
pD 13.828540            1.1661193
pV 22.624104            5.5639015

Iterations = 1000:5000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 2001

Empirical Posterior Estimates:
           Mean        SD      Naive SE      MCSE       ESS   
beta[1] 0.62445845 1.0285709 0.013275474 0.023818436 1864.8416
beta[2] 0.79392648 0.3096614 0.003996712 0.006516677 2001.0000

Quantiles:
            2.5%       25.0%       50.0%     75.0%     97.5%  
beta[1] -1.53050898 0.076745702 0.61120944 1.2174641 2.6906753
beta[2]  0.18846617 0.618849048 0.79323126 0.9619767 1.4502109

MCMC Simulation of 5000 Iterations x 3 Chains...

Chain 1:   0% [0:00:03 of 0:00:03 remaining]
Chain 1:  10% [0:00:04 of 0:00:04 remaining]
Chain 1:  20% [0:00:04 of 0:00:05 remaining]
Chain 1:  30% [0:00:03 of 0:00:05 remaining]
Chain 1:  40% [0:00:03 of 0:00:04 remaining]
Chain 1:  50% [0:00:02 of 0:00:04 remaining]
Chain 1:  60% [0:00:02 of 0:00:04 remaining]
Chain 1:  70% [0:00:01 of 0:00:04 remaining]
Chain 1:  80% [0:00:01 of 0:00:04 remaining]
Chain 1:  90% [0:00:00 of 0:00:04 remaining]
Chain 1: 100% [0:00:00 of 0:00:04 remaining]

Chain 2:   0% [0:00:04 of 0:00:04 remaining]
Chain 2:  10% [0:00:02 of 0:00:03 remaining]
Chain 2:  20% [0:00:02 of 0:00:03 remaining]
Chain 2:  30% [0:00:02 of 0:00:03 remaining]
Chain 2:  40% [0:00:02 of 0:00:03 remaining]
Chain 2:  50% [0:00:01 of 0:00:03 remaining]
Chain 2:  60% [0:00:01 of 0:00:03 remaining]
Chain 2:  70% [0:00:01 of 0:00:03 remaining]
Chain 2:  80% [0:00:01 of 0:00:03 remaining]
Chain 2:  90% [0:00:00 of 0:00:03 remaining]
Chain 2: 100% [0:00:00 of 0:00:03 remaining]

Chain 3:   0% [0:00:04 of 0:00:04 remaining]
Chain 3:  10% [0:00:02 of 0:00:03 remaining]
Chain 3:  20% [0:00:02 of 0:00:03 remaining]
Chain 3:  30% [0:00:02 of 0:00:03 remaining]
Chain 3:  40% [0:00:02 of 0:00:03 remaining]
Chain 3:  50% [0:00:01 of 0:00:03 remaining]
Chain 3:  60% [0:00:01 of 0:00:03 remaining]
Chain 3:  70% [0:00:01 of 0:00:03 remaining]
Chain 3:  80% [0:00:01 of 0:00:03 remaining]
Chain 3:  90% [0:00:00 of 0:00:03 remaining]
Chain 3: 100% [0:00:00 of 0:00:03 remaining]

Iterations = 252:15000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 7375

Empirical Posterior Estimates:
           Mean        SD       Naive SE       MCSE        ESS   
beta[1] 0.59380969 1.16220371 0.0078134100 0.0131895631 7375.0000
beta[2] 0.80250160 0.35071524 0.0023578328 0.0038841898 7375.0000
     s2 1.25280446 2.01785048 0.0135658604 0.0842657930  573.4233

Quantiles:
            2.5%        25.0%      50.0%      75.0%     97.5%  
beta[1] -1.812163822 0.022568991 0.5904957 1.18896171 2.8970634
beta[2]  0.113567574 0.625826474 0.8016583 0.97525934 1.5174224
     s2  0.170555373 0.385965070 0.6602996 1.26162974 6.2713761

Object of type "Mamba.Model"
-------------------------------------------------------------------------------
y:
An unmonitored node of type "5-element Mamba.ArrayStochastic{1}"
[1.0,3.0,3.0,3.0,5.0]

Distribution:
IsoNormal(
dim: 5
μ: [3.2384475887755166,5.286627296564756,7.334807004353996,9.382986712143236,11.431166419932476]
Σ: [1.1830434911443408 0.0 0.0 0.0 0.0
 0.0 1.1830434911443408 0.0 0.0 0.0
 0.0 0.0 1.1830434911443408 0.0 0.0
 0.0 0.0 0.0 1.1830434911443408 0.0
 0.0 0.0 0.0 0.0 1.1830434911443408]
)

Function:
AST(:($(Expr(:lambda, Any[:(model::Model)], Any[Any[Any[:model,:Any,18],Any[:f,:Any,18]],Any[],0,Any[]], :(begin 
        model = (top(typeassert))(model,Mamba.Model)
        f = (anonymous function)
        return f((Mamba.getindex)(model,:mu),(Mamba.getindex)(model,:s2))
    end)))))

Source Nodes:
[:mu,:s2]

Target Nodes:
Symbol[]
-------------------------------------------------------------------------------
s2:
A monitored node of type "Mamba.ScalarStochastic"
1.1830434911443408

Distribution:
Distributions.InverseGamma(
invd: Distributions.Gamma(α=0.001, θ=1000.0)
θ: 0.001
)

Function:
AST(:($(Expr(:lambda, Any[:(model::Model)], Any[Any[Any[:model,:Any,18],Any[:f,:Any,18]],Any[],0,Any[]], :(begin 
        model = (top(typeassert))(model,Mamba.Model)
        f = (anonymous function)
        return f()
    end)))))

Source Nodes:
Symbol[]

Target Nodes:
[:y]
-------------------------------------------------------------------------------
xmat:
[1.0 1.0
 1.0 2.0
 1.0 3.0
 1.0 4.0
 1.0 5.0]
-------------------------------------------------------------------------------
beta:
A monitored node of type "2-element Mamba.ArrayStochastic{1}"
[1.1902678809862768,2.04817970778924]

Distribution:
ZeroMeanIsoNormal(
dim: 2
μ: [0.0,0.0]
Σ: [1000.0 0.0
 0.0 1000.0]
)

Function:
AST(:($(Expr(:lambda, Any[:(model::Model)], Any[Any[Any[:model,:Any,18],Any[:f,:Any,18]],Any[],0,Any[]], :(begin 
        model = (top(typeassert))(model,Mamba.Model)
        f = (anonymous function)
        return f()
    end)))))

Source Nodes:
Symbol[]

Target Nodes:
[:mu,:y]
-------------------------------------------------------------------------------
mu:
An unmonitored node of type "5-element Mamba.ArrayLogical{1}"
[3.2384475887755166,5.286627296564756,7.334807004353996,9.382986712143236,11.431166419932476]
Function:
AST(:($(Expr(:lambda, Any[:(model::Model)], Any[Any[Any[:model,:Any,18],Any[:f,:Any,18]],Any[],0,Any[]], :(begin 
        model = (top(typeassert))(model,Mamba.Model)
        f = (anonymous function)
        return f((Mamba.getindex)(model,:xmat),(Mamba.getindex)(model,:beta))
    end)))))

Source Nodes:
[:xmat,:beta]

Target Nodes:
[:y]

>>> Testing ../doc/samplers/amm.jl

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean        SD       Naive SE       MCSE       ESS   
b0 0.67372957 1.07776604 0.0152419136 0.064652461 277.89381
b1 0.77112512 0.32135034 0.0045445801 0.017982404 319.34640
s2 1.29658296 2.18979550 0.0309683849 0.139974432 244.74267

Quantiles:
       2.5%        25.0%       50.0%      75.0%     97.5%  
b0 -1.451022943 0.069490369 0.65978541 1.19631191 3.1685322
b1  0.050258268 0.611291896 0.79094107 0.94652458 1.4132212
s2  0.174653274 0.389575233 0.65132492 1.31853897 6.8337311


>>> Testing ../doc/samplers/amwg.jl

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean       SD      Naive SE       MCSE        ESS   
b0 0.5983881 1.5898262 0.0224835377 0.162560986  95.645964
b1 0.8006155 0.4358909 0.0061644284 0.042038394 107.513597
s2 2.2710644 9.1262030 0.1290640007 0.686946293 176.495925

Quantiles:
       2.5%       25.0%      50.0%      75.0%      97.5%  
b0 -2.17471713 0.03038741 0.67912808 1.30557650  3.3496859
b1  0.03755749 0.60096712 0.77599921 0.96011511  1.7513752
s2  0.16901125 0.39945048 0.70340929 1.47274298 13.3376638


>>> Testing ../doc/samplers/bmg.jl

Iterations = 1:10000
Thinning interval = 1
Chains = 1
Samples per chain = 10000

Empirical Posterior Estimates:
           Mean      SD       Naive SE       MCSE         ESS   
 gamma[1] 0.8135 0.38952910 0.0038952910 0.0053036580  5394.2219
 gamma[2] 1.0000 0.00000000 0.0000000000 0.0000000000 10000.0000
 gamma[3] 0.0001 0.01000000 0.0001000000 0.0001000000 10000.0000
 gamma[4] 1.0000 0.00000000 0.0000000000 0.0000000000 10000.0000
 gamma[5] 0.9999 0.01000000 0.0001000000 0.0001000000 10000.0000
 gamma[6] 0.9999 0.01000000 0.0001000000 0.0001000000 10000.0000
 gamma[7] 1.0000 0.00000000 0.0000000000 0.0000000000 10000.0000
 gamma[8] 0.8371 0.36929288 0.0036929288 0.0042194715  7659.9468
 gamma[9] 0.0001 0.01000000 0.0001000000 0.0001000000 10000.0000
gamma[10] 1.0000 0.00000000 0.0000000000 0.0000000000 10000.0000

Quantiles:
          2.5% 25.0% 50.0% 75.0% 97.5%
 gamma[1]    0     1     1     1     1
 gamma[2]    1     1     1     1     1
 gamma[3]    0     0     0     0     0
 gamma[4]    1     1     1     1     1
 gamma[5]    1     1     1     1     1
 gamma[6]    1     1     1     1     1
 gamma[7]    1     1     1     1     1
 gamma[8]    0     1     1     1     1
 gamma[9]    0     0     0     0     0
gamma[10]    1     1     1     1     1


>>> Testing ../doc/samplers/bhmc.jl

Iterations = 1:10000
Thinning interval = 1
Chains = 1
Samples per chain = 10000

Empirical Posterior Estimates:
           Mean      SD       Naive SE       MCSE         ESS   
 gamma[1] 0.5425 0.49821539 0.0049821539 0.0043794170 10000.0000
 gamma[2] 1.0000 0.00000000 0.0000000000 0.0000000000 10000.0000
 gamma[3] 0.3125 0.46353558 0.0046353558 0.0052442850  7812.5639
 gamma[4] 1.0000 0.00000000 0.0000000000 0.0000000000 10000.0000
 gamma[5] 0.7132 0.45228997 0.0045228997 0.0056726884  6357.0562
 gamma[6] 0.7342 0.44178035 0.0044178035 0.0055526497  6330.1242
 gamma[7] 1.0000 0.00000000 0.0000000000 0.0000000000 10000.0000
 gamma[8] 0.5202 0.49961677 0.0049961677 0.0030648875 10000.0000
 gamma[9] 0.4972 0.50001716 0.0050001716 0.0063230858  6253.3346
gamma[10] 0.7768 0.41641218 0.0041641218 0.0044537579  8741.6542

Quantiles:
          2.5% 25.0% 50.0% 75.0% 97.5%
 gamma[1]    0     0     1     1     1
 gamma[2]    1     1     1     1     1
 gamma[3]    0     0     0     1     1
 gamma[4]    1     1     1     1     1
 gamma[5]    0     0     1     1     1
 gamma[6]    0     0     1     1     1
 gamma[7]    1     1     1     1     1
 gamma[8]    0     0     1     1     1
 gamma[9]    0     0     0     1     1
gamma[10]    0     1     1     1     1


>>> Testing ../doc/samplers/bmmg.jl

Iterations = 1:10000
Thinning interval = 1
Chains = 1
Samples per chain = 10000

Empirical Posterior Estimates:
           Mean       SD        Naive SE       MCSE          ESS    
 gamma[1] 0.6836 0.465094261 0.00465094261 0.0158008566   866.404491
 gamma[2] 0.9978 0.046854877 0.00046854877 0.0022000000   453.590814
 gamma[3] 0.0044 0.066189713 0.00066189713 0.0042027408   248.036607
 gamma[4] 1.0000 0.000000000 0.00000000000 0.0000000000 10000.000000
 gamma[5] 0.9835 0.127394556 0.00127394556 0.0118716835   115.153495
 gamma[6] 0.9963 0.060718026 0.00060718026 0.0026804756   513.111285
 gamma[7] 0.9992 0.028274369 0.00028274369 0.0008000000  1249.124912
 gamma[8] 0.6863 0.464019225 0.00464019225 0.0169542790   749.054296
 gamma[9] 0.0147 0.120355135 0.00120355135 0.0103615987   134.919808
gamma[10] 0.9884 0.107082149 0.00107082149 0.0081621447   172.117718

Quantiles:
          2.5% 25.0% 50.0% 75.0% 97.5%
 gamma[1]    0     0     1     1     1
 gamma[2]    1     1     1     1     1
 gamma[3]    0     0     0     0     0
 gamma[4]    1     1     1     1     1
 gamma[5]    1     1     1     1     1
 gamma[6]    1     1     1     1     1
 gamma[7]    1     1     1     1     1
 gamma[8]    0     0     1     1     1
 gamma[9]    0     0     0     0     0
gamma[10]    1     1     1     1     1


>>> Testing ../doc/samplers/hmc.jl

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean        SD       Naive SE       MCSE       ESS   
b0 0.5772432  1.18402293 0.0167446128 0.027883682 1803.1006
b1 0.8022538  0.37671405 0.0053275412 0.008170591 2125.7719
s2 1.4971811 11.28460781 0.1595884542 0.195664541 3326.2025

Quantiles:
       2.5%        25.0%       50.0%      75.0%     97.5%  
b0 -1.714610896 0.015592733 0.55534270 1.17186976 3.0307402
b1  0.092367742 0.631187676 0.80982814 0.97620155 1.4874951
s2  0.178874019 0.383680766 0.66198223 1.22565452 7.0403625

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean        SD       Naive SE       MCSE       ESS   
b0 0.59496699 1.37677793 0.0194705802 0.035517823 1502.5714
b1 0.79853916 0.41130398 0.0058167167 0.009917444 1719.9916
s2 1.80970811 8.36078976 0.1182394227 0.312469964  715.9423

Quantiles:
       2.5%        25.0%      50.0%     75.0%     97.5%  
b0 -1.89540401 -0.028843232 0.6365522 1.2257351 3.0797518
b1  0.06653081  0.605516575 0.7862590 0.9763511 1.5777984
s2  0.16937961  0.390805781 0.6795564 1.3775763 8.2227653


>>> Testing ../doc/samplers/mala.jl

WARNING: imported binding for scale overwritten in module Main
Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean        SD       Naive SE      MCSE        ESS   
b0 0.63913008 0.96456516 0.013641011 0.109737267  77.260028
b1 0.79071667 0.29199225 0.004129394 0.030293853  92.903834
s2 1.16990867 2.21952343 0.031388801 0.159528112 193.573104

Quantiles:
       2.5%       25.0%       50.0%     75.0%     97.5%  
b0 -1.11636066 0.021295738 0.56942794 1.1958553 2.8185967
b1  0.13351555 0.622505238 0.81840154 0.9631912 1.3448359
s2  0.15082458 0.428174945 0.70535341 1.1853768 4.7815748

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean         SD       Naive SE      MCSE       ESS   
b0 0.61457782  1.09976885 0.0155530802 0.12576800  76.46497
b1 0.73181199  0.40025128 0.0056604079 0.04444227  81.10973
s2 2.93138741 13.82161613 0.1954671698 1.21062808 130.34554

Quantiles:
       2.5%       25.0%     50.0%     75.0%      97.5%  
b0 -1.29782609 0.00000000 0.4671962 1.0542319  3.6409243
b1 -0.09214223 0.57229743 0.7547751 0.9690808  1.4525190
s2  0.23962808 0.44855541 0.8566210 1.6689977 13.7557720


>>> Testing ../doc/samplers/nuts.jl

Iterations = 1001:5000
Thinning interval = 1
Chains = 1
Samples per chain = 4000

Empirical Posterior Estimates:
      Mean        SD       Naive SE      MCSE       ESS   
b0 0.59242566 1.12664340 0.0178137962 0.07262530 240.65621
b1 0.79955933 0.34124494 0.0053955563 0.02021753 284.88937
s2 1.23041170 2.28573689 0.0361406736 0.10397815 483.24600

Quantiles:
       2.5%        25.0%       50.0%     75.0%     97.5%  
b0 -1.60156914 -0.018689413 0.55936277 1.1737069 2.9302371
b1  0.09581153  0.620652115 0.81772866 0.9840868 1.4803526
s2  0.20677131  0.401460055 0.67740807 1.2526098 5.6494793


>>> Testing ../doc/samplers/slice.jl

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean       SD       Naive SE     MCSE      ESS  
b0 1.6197669  2.9681208 0.041975568 0.4036675 54.06492
b1 0.5020914  0.8761579 0.012390744 0.1160301 57.01951
s2 4.3165749 11.7139216 0.165659868 1.2815802 83.54357

Quantiles:
       2.5%       25.0%       50.0%     75.0%     97.5%  
b0 -1.29885090 0.046809963 0.74381649 1.7293926 10.889003
b1 -2.29103352 0.489150908 0.75234625 0.9531036  1.390704
s2  0.19925837 0.461629914 0.87267462 2.0499164 38.446010

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean        SD       Naive SE      MCSE       ESS   
b0 0.66332922 1.11621572 0.015785674 0.103257562 116.85641
b1 0.77913487 0.33395473 0.004722833 0.028866714 133.83832
s2 1.18772135 1.98696317 0.028099903 0.091313412 473.48965

Quantiles:
       2.5%        25.0%      50.0%     75.0%     97.5%  
b0 -1.498928465 0.05189262 0.62539250 1.2265902 3.2134489
b1  0.009453513 0.61113139 0.79383033 0.9607895 1.4191670
s2  0.168203415 0.38155443 0.65486544 1.2358343 5.4422316


>>> Testing ../doc/samplers/slicesimplex.jl

Iterations = 1:10000
Thinning interval = 1
Chains = 1
Samples per chain = 10000

Empirical Posterior Estimates:
          Mean         SD        Naive SE        MCSE        ESS   
rho[1] 0.27754333 0.047233921 0.00047233921 0.00141592199 1112.8313
rho[2] 0.10332212 0.031263826 0.00031263826 0.00058567835 2849.4820
rho[3] 0.15258317 0.038007617 0.00038007617 0.00081859335 2155.7820
rho[4] 0.35246555 0.051090924 0.00051090924 0.00149244465 1171.9013
rho[5] 0.11408583 0.033523072 0.00033523072 0.00071863697 2176.0482

Quantiles:
           2.5%       25.0%      50.0%       75.0%      97.5%  
rho[1] 0.192874639 0.24366290 0.275752374 0.30784577 0.37691928
rho[2] 0.050788311 0.08068638 0.100682538 0.12286559 0.17351691
rho[3] 0.086277998 0.12523438 0.150148020 0.17687571 0.23401932
rho[4] 0.255446325 0.31691025 0.350797321 0.38773881 0.45478127
rho[5] 0.058278656 0.08940860 0.111306257 0.13540498 0.18591961


>>> Testing ../doc/mcmc/readcoda.jl

Iterations = 1:200
Thinning interval = 1
Chains = 1,2
Samples per chain = 200

Empirical Posterior Estimates:
         Mean       SD       Naive SE      MCSE       ESS   
alpha 3.0025394 0.53475753 0.026737877 0.018902157 200.00000
 beta 0.8013086 0.39267477 0.019633739 0.030895834 161.53482
sigma 1.0821777 0.94869150 0.047434575 0.061191837 200.00000

Quantiles:
         2.5%      25.0%   50.0%   75.0%     97.5%  
alpha  1.8322542 2.751095 3.0257 3.2709700 3.9511365
 beta -0.0125375 0.599750 0.8065 1.0079525 1.5292802
sigma  0.4329000 0.625000 0.8360 1.2378125 2.8597185


>>> Testing ../doc/mcmc/newunivardist.jl

MCMC Simulation of 10000 Iterations x 3 Chains...

Chain 1:   0% [0:00:57 of 0:00:57 remaining]
Chain 1:  10% [0:00:13 of 0:00:14 remaining]
Chain 1:  20% [0:00:11 of 0:00:14 remaining]
Chain 1:  30% [0:00:11 of 0:00:15 remaining]
Chain 1:  40% [0:00:09 of 0:00:15 remaining]
Chain 1:  50% [0:00:08 of 0:00:15 remaining]
Chain 1:  60% [0:00:06 of 0:00:14 remaining]
Chain 1:  70% [0:00:04 of 0:00:14 remaining]
Chain 1:  80% [0:00:03 of 0:00:14 remaining]
Chain 1:  90% [0:00:01 of 0:00:14 remaining]
Chain 1: 100% [0:00:00 of 0:00:14 remaining]

Chain 2:   0% [0:00:22 of 0:00:22 remaining]
Chain 2:  10% [0:00:17 of 0:00:19 remaining]
Chain 2:  20% [0:00:13 of 0:00:17 remaining]
Chain 2:  30% [0:00:11 of 0:00:16 remaining]
Chain 2:  40% [0:00:09 of 0:00:16 remaining]
Chain 2:  50% [0:00:08 of 0:00:16 remaining]
Chain 2:  60% [0:00:07 of 0:00:16 remaining]
Chain 2:  70% [0:00:05 of 0:00:17 remaining]
Chain 2:  80% [0:00:03 of 0:00:17 remaining]
Chain 2:  90% [0:00:02 of 0:00:18 remaining]
Chain 2: 100% [0:00:00 of 0:00:18 remaining]

Chain 3:   0% [0:00:22 of 0:00:22 remaining]
Chain 3:  10% [0:00:14 of 0:00:16 remaining]
Chain 3:  20% [0:00:13 of 0:00:17 remaining]
Chain 3:  30% [0:00:11 of 0:00:16 remaining]
Chain 3:  40% [0:00:09 of 0:00:15 remaining]
Chain 3:  50% [0:00:07 of 0:00:15 remaining]
Chain 3:  60% [0:00:06 of 0:00:15 remaining]
Chain 3:  70% [0:00:05 of 0:00:15 remaining]
Chain 3:  80% [0:00:03 of 0:00:15 remaining]
Chain 3:  90% [0:00:01 of 0:00:14 remaining]
Chain 3: 100% [0:00:00 of 0:00:14 remaining]

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Empirical Posterior Estimates:
           Mean        SD       Naive SE       MCSE       ESS   
beta[1] 0.58240287 1.14230234 0.0094456779 0.018305636 3893.9689
beta[2] 0.80352012 0.34248004 0.0028319614 0.005181105 4369.4396
     s2 1.21355573 1.77421793 0.0146709766 0.071581268  614.3490

Quantiles:
            2.5%       25.0%       50.0%     75.0%     97.5%  
beta[1] -1.71534530 0.023088344 0.58077083 1.1524530 2.8274529
beta[2]  0.12368846 0.630518397 0.80241394 0.9732482 1.5135523
     s2  0.16868782 0.385631053 0.65901954 1.2756509 6.1752281


>>> Testing ../doc/mcmc/newmultivardist.jl

WARNING: replacing module Testing
MCMC Simulation of 10000 Iterations x 3 Chains...

Chain 1:   0% [0:00:24 of 0:00:24 remaining]
Chain 1:  10% [0:00:05 of 0:00:05 remaining]
Chain 1:  20% [0:00:04 of 0:00:05 remaining]
Chain 1:  30% [0:00:04 of 0:00:06 remaining]
Chain 1:  40% [0:00:03 of 0:00:06 remaining]
Chain 1:  50% [0:00:03 of 0:00:06 remaining]
Chain 1:  60% [0:00:02 of 0:00:06 remaining]
Chain 1:  70% [0:00:02 of 0:00:06 remaining]
Chain 1:  80% [0:00:01 of 0:00:07 remaining]
Chain 1:  90% [0:00:01 of 0:00:07 remaining]
Chain 1: 100% [0:00:00 of 0:00:07 remaining]

Chain 2:   0% [0:00:14 of 0:00:14 remaining]
Chain 2:  10% [0:00:09 of 0:00:10 remaining]
Chain 2:  20% [0:00:08 of 0:00:10 remaining]
Chain 2:  30% [0:00:07 of 0:00:10 remaining]
Chain 2:  40% [0:00:06 of 0:00:09 remaining]
Chain 2:  50% [0:00:04 of 0:00:09 remaining]
Chain 2:  60% [0:00:03 of 0:00:08 remaining]
Chain 2:  70% [0:00:02 of 0:00:08 remaining]
Chain 2:  80% [0:00:01 of 0:00:07 remaining]
Chain 2:  90% [0:00:01 of 0:00:07 remaining]
Chain 2: 100% [0:00:00 of 0:00:08 remaining]

Chain 3:   0% [0:00:08 of 0:00:08 remaining]
Chain 3:  10% [0:00:07 of 0:00:08 remaining]
Chain 3:  20% [0:00:05 of 0:00:07 remaining]
Chain 3:  30% [0:00:04 of 0:00:06 remaining]
Chain 3:  40% [0:00:03 of 0:00:06 remaining]
Chain 3:  50% [0:00:03 of 0:00:05 remaining]
Chain 3:  60% [0:00:02 of 0:00:05 remaining]
Chain 3:  70% [0:00:02 of 0:00:05 remaining]
Chain 3:  80% [0:00:01 of 0:00:05 remaining]
Chain 3:  90% [0:00:01 of 0:00:05 remaining]
Chain 3: 100% [0:00:00 of 0:00:05 remaining]

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Empirical Posterior Estimates:
           Mean       SD       Naive SE       MCSE         ESS   
beta[1] 0.5788931 1.11232796 0.0091978202 0.0190480893 3410.06668
beta[2] 0.8052916 0.33546852 0.0027739833 0.0055032179 3715.95238
     s2 1.1638187 1.61102871 0.0133215679 0.0632533846  648.69272

Quantiles:
            2.5%       25.0%       50.0%     75.0%     97.5%  
beta[1] -1.71161012 0.030043945 0.58723102 1.1464501 2.8420839
beta[2]  0.12334042 0.628821665 0.80501158 0.9749143 1.4853405
     s2  0.17208834 0.383853394 0.66064186 1.2465013 5.5621934

INFO: Mamba tests passed

>>> End of log
